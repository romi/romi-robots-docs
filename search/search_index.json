{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Robotics for micro-farms","text":"<p>ROMI is a four-year Europe-funded research project committed to promote a sustainable, local, and human-scale agriculture. The goal is to develop an open-source, affordable, multipurpose platform adapted to support organic and poly-culture market-garden farms.</p> <p>For more information on the ROMI project, please visit the main website at https://romi-project.eu/.</p> <p>This website is dedicated to the documentation of the three devices and accompanying software developed by the ROMI project:</p> Plant Phenotyping Farmer's Dashboard Rover"},{"location":"#list-of-all-python-modules","title":"List of all Python modules","text":"Type Source code link Hardware Plant Imager  Cable Bot  Rover  Storage PlantDB  Viewers Plant 3D Explorer  Farmers Dashboard  Algorithms Virtual Plant Imager  Plant 3D Vision  romiseg  DTW  Third-party &amp; wrapping romicgal"},{"location":"about/","title":"About the ROMI project","text":""},{"location":"about/#project-funding","title":"Project funding","text":"<p>This project has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement No 773875.</p> <p></p>"},{"location":"about/#research-teams","title":"Research teams","text":"IAAC develops an aerial robot that can be used by farmers. Iaac also performs real-world tests in the experimental gardens at the Valldaura Self-Sufficient Labs and imagine end-user scenarios. They help deliver the robotics platform to new markets, managing the communication and user communities. Sony CSL is responsible for the development of the LettuceThink robot. They also contribute to the development of the computer vision and machine learning algorithms, in particular, on the 3D plant scanning and the coupling between the formal plant models and the convolutional neural networks. The Virtual Plants team brings its strong expertise in the area of 3D plant architecture reconstruction and modelling. Notably, the team develops computer pipelines to reconstruct plant architecture from 3D data, to assess their reconstruction, and to segment the architecture in its constituent organs. The Adaptive Systems Group expertise lies in models for closed-loop learning and prediction of sensorimotor data, as well as behaviour recognition and generation. The tasks planned will focus on the learning and adaptive techniques for the interaction between robots and plants. The RDP team has a deep understanding of the development and evolution of plant reproductive systems. RDP leads the advanced sensing and analysis of crops, and brings its expertise on the developmental dynamics and modelling of plant architecture. Chatelain P\u00e9pini\u00e8res runs a commercial market farm near Paris. They perform field studies to test the efficiency of the weeding robot and the usefulness of the crop monitoring applications in real-world situations. FEI provides assistance and training for projects partly funded by the European Commission, as coordinator or as partner. FEI intervenes close to them in the administrative, financial coordination and management of their projects."},{"location":"about/#official-website","title":"Official Website","text":"<p>This is the documentation website of the ROMI project, to access the public project presentation, follow this link: https://romi-project.eu/</p>"},{"location":"about/#github-sources","title":"GitHub sources","text":"<p>For now these sources are private.</p>"},{"location":"about/#presentation-videos","title":"Presentation videos","text":"<p>Preliminary videos to learn more about the project tools!</p>"},{"location":"documentation/","title":"Robotics for micro-farms","text":"<p>ROMI is a four-year Europe-funded research project committed to promote a sustainable, local, and human-scale agriculture. The goal is to develop an open-source, affordable, multipurpose platform adapted to support organic and poly-culture market-garden farms.</p> Plant Phenotyping Crop Monitoring Rover"},{"location":"documentation/#complete-module-list","title":"Complete module list","text":"Type Source code link Hardware Plant Imager  Cable Bot  Rover  Storage PlantDB  Viewers Plant 3D Explorer  Farmers Dashboard  Algorithms Virtual Plant Imager  Plant 3D Vision  romiseg  DTW  Third-party &amp; wrapping romicgal"},{"location":"glossary/","title":"Glossary","text":"<p>We hereafter defines the semantic, names and abbreviations to use in the projects documentations and communications.</p> <p>ROMI Software : the whole set of software developed by ROMI;</p> <p>ROMI Hardware : the three types of robots developed by ROMI, namely the \"cable bot\", the \"rover\" and the \"plant imager\";</p>"},{"location":"glossary/#database-related","title":"Database related","text":"<p>database : the database itself;</p> <p>scan : a set of images, and the pipelines results;</p> <p>fileset : a set of files (e.g. a set of RGB images of a plant);</p> <p>file : a file (e.g. an RGB image of a plant);</p> <p>plant metadata : set of FAIR metadata attached to the plant (e.g. species, age, growth conditions...);</p> <p>acquisition metadata : set of metadata attached to the acquisition procedure &amp; hardware configuration (e.g. version of the CNC controller, camera settings, ...);</p> <p>Danger</p> <p>\"scans\" could be renamed \"dataset\" or !</p>"},{"location":"research/","title":"Research &amp; communications","text":""},{"location":"research/#journal-papers","title":"Journal papers","text":"<p>INRIA: Chaudhury A., Godin C. Skeletonization of Plant Point Cloud Data Using Stochastic Optimization Framework. Front Plant Sci. 2020;11:773. Published 2020 Jun 16. doi:10.3389/fpls.2020.00773</p> <p>UBER, Sony : Schillaci G.,  Pico Villalpando A., Hafner V. V., Hanappe P., Colliaux D. and Wintz, T. Intrinsic motivation and episodic memories for robot exploration of high-dimensional sensory spaces. Adaptive Behavior, June 2020, doi:10.1177/1059712320922916. (arXiv:2001.01982)</p> <p>UBER : Hafner, V. V., Loviken, P., Pico Villalpando, A., Schillaci, G. (2020). Prerequisites for an Artificial Self. Frontiers in Neurorobotics. Vol. 14, p.5. doi:10.3389/fnbot.2020.00005. ISSN 1662-5218.</p>"},{"location":"research/#book-chapters","title":"Book chapters","text":"<p>INRIA: A. Chaudhury and C. Godin, Geometry Reconstruction of Plants, in Intelligent Image Analysis for Plant Phenotyping, CRC Press/Taylor and Francis, 2020 (to appear). doi: doi:10.1201/9781315177304</p>"},{"location":"research/#participation-in-conferences","title":"Participation in conferences","text":"<p>UBER: Pico, A., Schillaci, G., Hafner, V.V. and Lara, B. (2019). Ego-Noise Predictions for Echolocation in Wheeled Robots, Alife 2019 - The 2019 Conference on Artificial Life. pp. 567-573. MIT Press. doi:10.1162/isal_a_00222</p>"},{"location":"research/#participation-in-workshops","title":"Participation in workshops","text":"<p>INRIA: Chaudhury A., Boudon, F., and Godin C. 3D Plant Phenotyping: All You Need is Labelled Point Cloud Data. Workshop on Computer Vision Problems for Plant Phenotyping, 2020.</p> <p>IAAC, Sony : Sollazzo A., Colliaux D., Garivani S., Minchin J., Garlanda L. and Hanappe, P. Automated vegetable growth analysis from outdoor images acquired with a cablebot. Workshop on Computer Vision Problems for Plant Phenotyping, 2020. (PDF)</p> <p>INRIA: Florian Ingels, contributed talk at the annual meeting of the French Statistical Society Journ\u00e9es de Statistique 2019 (Romain Aza\u00efs and Florian Ingels), 2019/06/03-07.</p> <p>Sony: Wintz T., Colliaux D., Hanappe P. Automated extraction of phyllotactic traits from Arabidopsis thaliana. Workshop on Computer Vision Problems in Plant Phenotyping (CVPPP), 2018 (PDF, last visited 30/10/2020). </p> <p>Sony: Colliaux D., Lahlou A., Wintz T., Boucher E., Garlanda L., Mac\u00e9 A., Hanappe P.  Robots for data collection in biology and agriculture. Poster presentation, International Crop Modelling Symposium (iCROPM2020), 3-5 February 2020, Montpellier, France.</p> <p>Sony: Colliaux D., Lahlou A., Wintz T., Boucher E., Garlanda L., Mac\u00e9 A., Hanappe P.  Developing low-cost robots for micro-farms: the benefits of computer vision. Poster at the Plant People Planet Symposium, London,  4\u20135 September 2019 (Abstracts book)</p>"},{"location":"Farmers%20Dashboard/","title":"Introduction","text":"<p>The Farmers Dashboard is a farming tool that provides daily automated insights about your crop. It helps with mapping of crop beds, the location and identification of individual plants, and the extraction of their growth curves from the collected data.  </p> <p>The dashboard benefits polycrop farmers and researchers. It opens up technology and practices common to industrial scale agriculture, making them accessible and useful to ecological and sustainable farmers. It relies on an automated system, data acquisition, a set of tools for image analytics, and finally, an online platform for spatial management and data visualization.</p> <p></p> <p>The data can be provided by different types of devices a Cablebot, a drone or a Rover according to the configuration of each farm. Multiple iterations were needed to achieve a powerful yet robust and low-cost solution.</p> <p>The Cablebot can be fixed above a crop bed using a tension cable, we can use the manual remote to correctly position the camera in order to capture all the crops. Once set up, the cable bot will move multiple times a day across the crop bed, taking high definition images and sending them to the Romi server. The images are assembled into a unique portrait of your crop bed. After the plants are detected, a catalog of individual plants is created. By comparing them with historical data, we can obtain plant growth curves. All the information is then combined into a weed map, which is made available on the Farmers Dashboard website. The current system is a fully automated imaging device, able to collect data on a high variety of crops.</p> <p></p> <p>Because of the legal restrictions on the use of drones and because of the rapid evolution of the drone market, the ROMI project has decided to direct its effort to a hardware solution that complements the existing tools (commercial drones, the Rover): the Cablebot. The Cablebot is adapted for use in greenhouses and polytunnels. These installations take up more than 10% of microfarms and are ill-suited for the use of drones. This reorientation increases the impact of ROMI since we can handle a wider variety of contexts than planned. The use of drones is still an option. Existing drones can still be used in combination with the Farmer\u2019s Dashboard. </p> <p>Nature-based solutions are becoming more and more relevant to increasing cities\u2019 resilience and climate change adaptation.  In particular green infrastructure is an emergent trend followed by new buildings worldwide. With it, there\u2019s an emergent market of solutions to support the growth of plants in facades, rooftops and other architectural sites. One relevant task is the monitoring of those gardens, in particular in large buildings and areas with difficult access, like facades. However, the regulations affecting urban areas limit drones and other flying equipment. For that reason, the Cable Bot solution can offer a robust and autonomous solution for permanent or temporary monitoring of rooftops or even vertical gardens. Moreover, the simplicity of the installation allows the system to be set up at lower costs and even for a temporary purpose, like monitoring the plats consolidation phase for new gardens. Finally, remote monitoring of the results via the Farmers Dashboard simplifies the overall operation by offering precise insights and remote control capabilities.</p> <p>To maximize the reuse of hardware components, the Cablebot consists of the following modules:</p> <ol> <li>A mobile carrier that can move along the cable.</li> <li>A camera module thats easily attached on the mobile carrier.</li> <li>A fixed charging station.</li> </ol> <p>Info</p> <p>For convenience, when we use the term cablebot, we sometimes refer to the mobile carrier mentioned above, and sometimes to the three modules used as a whole. In general, the meaning should be clear from the context.</p>"},{"location":"Farmers%20Dashboard/app/","title":"How to use the Farmers Dashboard","text":"<p>The Farmer's Dashboard is a powerful tool developed as part of the Robots for Microfarms (ROMI) project. It collects data from the Cablebot and provides an overview of what's growing in the field. The primary purpose is to extract detailed information about individual plants and their growth progress.</p> <p>The Dashboard is currently tailored to the specific work within ROMI, focusing on data collection from the Cablebot and crop monitoring. Watch the video to learn more about the objetives and current functionality behind that tool.</p> <p>Some graphical user interfaces are currently under development. For more detailed instructions on the operation and control commands, refer to the ongoing software repositories ROMI Pipelines and ROMI Farmers Dashboard.</p>"},{"location":"Farmers%20Dashboard/app/#main-functionalities","title":"Main functionalities","text":""},{"location":"Farmers%20Dashboard/app/#combining-data-for-farm-management","title":"Combining Data for Farm Management","text":"<p>The Farmer's Dashboard combines Cablebot data with the farm's planning process, enabling effective farm management.</p> <ul> <li>Efficient Planning: It assists in determining seeding, harvesting, and the composition of vegetable baskets for sale.</li> <li>Data-driven Adaptation: Farmers can adapt and predict harvests more efficiently using the Dashboard's statistics.</li> </ul>"},{"location":"Farmers%20Dashboard/app/#analytics-and-remote-usage","title":"Analytics and Remote Usage","text":"<p>The Dashboard serves as an analytics tool, enabling remote usage and data analysis.</p> <ul> <li>Schedule-based Scans: Daily or weekly scans of the crop are taken using the Cablebot, and the data is uploaded to a computer.</li> <li>Insightful Metrics: Analytics and metrics provide insights into crop productivity, infestations, diseases, and environmental factors affecting growth.</li> </ul>"},{"location":"Farmers%20Dashboard/app/#parcel-numbering-and-geo-referencing","title":"Parcel Numbering and Geo-Referencing","text":"<p>Precise tracking of planting locations is facilitated through parcel numbering and geo-referencing.</p> <ul> <li>Fieldwork Aid: Parcel numbers and geo-references help streamline data logging during fieldwork.</li> <li>Valuable Research Data: The data serves as valuable research information for crop growth patterns, weather correlation, and statistical analysis.</li> </ul>"},{"location":"Farmers%20Dashboard/app/#evolution-of-the-farm-management-system","title":"Evolution of the Farm Management System","text":"<p>The Farmer's Dashboard aims to become an integral part of a comprehensive farm management system.</p> <ul> <li>Data Integration: The Dashboard will integrate Cablebot data with the initial farm planning done by farmers.</li> <li>Predictive Modeling: Potential incorporation of predictive modeling for plant growth based on historical statistics.</li> </ul>"},{"location":"Farmers%20Dashboard/app/#image-processing-for-crop-monitoring","title":"Image Processing for Crop Monitoring","text":"<p>The Farmer's Dashboard employs advanced image processing for detailed crop monitoring.</p> <ul> <li>Image Pair Analysis: The Dashboard analyzes image pairs from the Cablebot's camera to create comprehensive crop bed maps.</li> <li>Daily Growth Curves: Multiple images along the Cablebot's path generate daily growth curves for individual plants.</li> </ul>"},{"location":"Farmers%20Dashboard/assembly/","title":"Assembly guide","text":"<p>The cablebot assembly requires simple tools: Pliers, screwdrivers, allen keys and a plastic or wooden tool to apply pressure without damaging the pieces.  For electronics assembly some extra tools are needed: solder station, cutting pliers, shrink tube and hot air gun. </p>"},{"location":"Farmers%20Dashboard/assembly/#carrier-module-carm","title":"Carrier module (CARM)","text":"<p>The carrier module is the most complex component and it has a lot of different parts, to make the assembly process simpler this module is divided into different groups of parts that can be constructed independently and latter joint together.</p> <ul> <li>Head</li> <li>Body</li> <li>Arms (left and right)</li> <li>Hands (Left and right)</li> </ul> <p></p>"},{"location":"Farmers%20Dashboard/assembly/#hands-left-and-right","title":"Hands (left and right)","text":"<p>There are two hands on the CARM module, left and right. The assembly process for both is the same, so we only show it once. These pieces hold the pulleys that are in direct contact with the cable on both extremes. They also hold the two end stops switches that detect obstacles along the path.</p> <p></p>"},{"location":"Farmers%20Dashboard/assembly/#3d-printed-parts","title":"3d printed parts","text":"1x Hand end stop trigger left 1x Hand base left 2x Pulley 608zz 1x Hand base right 1x Hand end stop trigger right"},{"location":"Farmers%20Dashboard/assembly/#milled-alucobond-parts","title":"Milled Alucobond parts","text":"1x Hand left out small 1x Hand left middle small 1x Hand front left 1x Hand right out small 1x Hand right middle small 1x Hand front right 1x Hand left out big 1x Hand left middle big 1x Hand right out big 1x Hand right middle big <p>Just one set is needed either small or big, depending on cable tension (big is better for high tension).</p>"},{"location":"Farmers%20Dashboard/assembly/#hardware-parts","title":"Hardware parts","text":"2 608zz bearing 2 D2F-01L-D3 End stop 1 Cable 28 AWG - Black - 450mm (left hand) 1 Cable 28 AWG - Green - 450mm (left hand) 1 Cable 28 AWG - Black - 110mm (right hand) 1 Cable 28 AWG - Green - 110mm (right hand) 10 Washer M8x1.5 2 Screw M8x25 2 Nut M8 4 Screw M3x32 4 Nut M8 2 Screw M2x10 2 Nut M2"},{"location":"Farmers%20Dashboard/assembly/#assembly","title":"Assembly","text":"<p> All the needed pieces, to build the right hand. </p> <p> First install the end-stop (with the cables already soldered to it) inserting first the M2 nuts in the hexagonal holes.</p> <p>  Pass the cables through the hole to the back of the printed piece.</p> <p> And through the front hole in the alucobond piece.</p> <p> Align the two holes on both pieces.</p> <p></p> <p></p> <p> Pass the cable back to the front through the hole and align it with the pocket on the alucobond piece.</p> <p> Align and put together the two aluminum parts making sure not to pinch the cables. After joining them check if the cable can move freely.</p> <p> Insert the M8 screw.</p> <p> Place the 5 washers on the screw.</p> <p> Before this step you need to pressure fit the 608zz bearing in to the 3d printed pulley, it can be done easily on a bench press, be sure to slide it until the end. We recommend applying a couple of cyanoacrylate glue drops between the two pieces.</p> <p>Insert the bearing and the printed pulley on the M8 screw and fix it with the nut, be sure to apply enough pressure.</p> <p> Hold the end-stop trigger printed piece in place.</p> <p> Align the aluminum cap and insert the screw.</p> <p> While you push the screw be sure to keep the trigger aligned.</p> <p> Fix the nut on the back side.</p> <p> While keeping aligned the printed piece insert the other screw.</p> <p> Tighten both nuts.</p> <p> You're done with the right hand assembly!</p> <p> Follow the same procedure with the left hand, so you can start with the arms.</p>"},{"location":"Farmers%20Dashboard/assembly/#left-arm","title":"Left Arm","text":""},{"location":"Farmers%20Dashboard/assembly/#3d-printed-parts_1","title":"3d printed parts","text":"1x Left arm cap out 1x Pogo pin clip 1x Left arm cap middle"},{"location":"Farmers%20Dashboard/assembly/#milled-alucobond","title":"Milled Alucobond","text":"1x Arm left"},{"location":"Farmers%20Dashboard/assembly/#right-arm","title":"Right Arm","text":""},{"location":"Farmers%20Dashboard/assembly/#3d-printed-parts_2","title":"3d printed parts","text":"1x Control board holder 1x Right arm cap middle 1x Right arm cap out"},{"location":"Farmers%20Dashboard/assembly/#milled-alucobond_1","title":"Milled Alucobond","text":"1x Right arm"},{"location":"Farmers%20Dashboard/assembly/#head","title":"Head","text":"<p>The head part of the module holds the brushless motor and the needed electronics to control it. This part slides depending on the cable tension.</p> <p></p>"},{"location":"Farmers%20Dashboard/assembly/#3d-printed-parts_3","title":"3d printed parts","text":"1x Encoder cap 1x Head separator front 1x Head separator up 1x Spring holder up 1x Head separator back 1x Encoder holder"},{"location":"Farmers%20Dashboard/assembly/#milled-alucobond_2","title":"Milled Alucobond","text":"1x Head front 1x Head middle front 1x Head middle back 1x Head back"},{"location":"Farmers%20Dashboard/assembly/#hardware-parts_1","title":"Hardware parts","text":""},{"location":"Farmers%20Dashboard/assembly/#assembly_1","title":"Assembly","text":""},{"location":"Farmers%20Dashboard/assembly/#body","title":"Body","text":""},{"location":"Farmers%20Dashboard/assembly/#battery-module-batm","title":"Battery module (BATM)","text":""},{"location":"Farmers%20Dashboard/assembly/#camera-module-camm","title":"Camera module (CAMM)","text":""},{"location":"Farmers%20Dashboard/assembly/#charging-station-module-cham","title":"Charging station module (CHAM)","text":""},{"location":"Farmers%20Dashboard/bot/","title":"The Carrier module (CARM)","text":"<p>Size: 388mm x 216mm x 90 Weight: 1400gr Max speed: 2m/s (software limited) Max payload: 2500gr Power consumption: 1.5Wh on rest, 45Wh normal operation.  </p> <p>The mobile carrier is an autonomous motion platform capable of travelling suspended on a single tensioned cable. It can be attached to the cable in a few seconds and controlled manually vie RF remote control. It integrates the Romi Camera Module as image capture device to allow remote operation and autonomous scanning and image upload. Includes Battery module that recharges automatically in the Charging station module while not scanning. </p> <p>The primary communication is achieved via Wi-Fi to interact with the farmer phone or laptop, the local farm server or to a remote instance through the internet. Wi-Fi ensures enough bandwidth is available to perform the image uploads as well as over-the-air software updates, and it eliminates the need for customized gateways. When a remote connection is required, and the farm does not have one, a Wi-Fi to 4G (or 5G) gateway is located in the recharging station at the cable end. </p> <p></p>"},{"location":"Farmers%20Dashboard/building/","title":"Get the pieces","text":""},{"location":"Farmers%20Dashboard/building/#3d-print","title":"3d print","text":""},{"location":"Farmers%20Dashboard/building/#milling","title":"Milling","text":""},{"location":"Farmers%20Dashboard/building/#aluminum-composite","title":"Aluminum composite","text":"<p>Some cablebot parts are milled in a composite panel consisting of two aluminium cover sheets and a polymer core, this material can also be bent if cut at specific depth with a 90 degrees V-end mill. In the cablebot repository you can find all the parts as DXF files. Gcode has to be generated with specific settings for the used CNC machine.</p> <p></p> <p>Feeds and speeds for V - Curve milling bit to bend alucobond</p> <p>In our case we are using a high revolution spindle that works in between 18k-24k RPM. After some tests, we have found that this feeds and speeds work us well for cutting 3mm alucobond composite:</p> Tool RPM Feed Feed plunge Stepdown V carve - 90\u00ba 24,000 rpm 5,000 mm/min 3,000 mm/min 0.22 mm/step 3 mm flat tool 24,000 rpm 5,000 mm/min 3,000 mm/min 1.2 mm/step 6 mm flat tool 18,000 rpm 5,000 mm/min 3,000 mm/min 1.7 mm/step <p></p>"},{"location":"Farmers%20Dashboard/building/#gcode-generation","title":"Gcode generation","text":"<p>In the Romi Cablebot github repository you can find all the drawings (in DXF format) to generate the needed gcode. In the files you will find different layers depending on the operation and the depth, in this example you can see the layer called vcut for the folding marks, pocket-0.95mm indicates a pocketing operation with a depth of 0.95 mm and profiling to cut the piece. Keep in mind that you will need to add bridges on your piece, so it doesn't move during machining, this process is different depending on the CAM software you use.</p> <p></p>"},{"location":"Farmers%20Dashboard/building/#generating-gcode-in-blender","title":"Generating Gcode in Blender","text":"<p>Blendercam is a free/libre addon that allows gcode generation inside blender, in this way we avoid the use of extra software and model exporting. Don't forget to check their  documentation.</p> <p>To use it you need to clone the addon repository to some place in your computer:</p> <pre><code>git clone https://github.com/vilemduha/blendercam\n</code></pre> <p>Some python dependencies should also be installed, you can do it with pip from the command line:</p> <pre><code>$ ./pip3 install shapely\n$ ./pip3 install vtk\n$ ./pip3 install Equation\n</code></pre> <p>OpenCamLib is an optional dependency, but based on our tests we recommend its installation.</p> <p>To activate the addon, in Blender, open the Preferences window (edit \u2192 preferences). Clik on File Paths button and enter the path where you cloned the blender CAM repository in the Scripts field.</p> <p></p> <p>Save preferences and restart blender. Now enable it in Add-ons section (preferences window).</p> <p></p>"},{"location":"Farmers%20Dashboard/building/#adding-a-post-processor","title":"Adding a post processor","text":"<p>After installing the addon you will need a postprocessor script that works with your specific CNC machine. If none of the included ones works for you, you can easily create your own:</p> <p>1. Modify <code>scripts/addons/cam/__init__.py</code> and add a new item on the machineSettings class (around line 125):</p> <p></p> <p>2. Create a new file in <code>scripts/addons/cam/nc/</code> directory with your post processor name. (ej. <code>raptor.py</code>). You can copy an existing postprocessor and modify it to fit your needs.</p> <p>3. Modify <code>scripts/addons/cam/gcodepath.py</code>, search for the <code>exportGcodePath()</code> function and add a condition for your post processor where you specify the extension of the file and the name of the module you just created on step 2.</p> <p>There is example commit on what's needed to add a postprocessor here. It is a little outdated (use <code>gcodepath.py</code> instead of <code>utils.py</code>) but can be used as a general guide.</p>"},{"location":"Farmers%20Dashboard/building/#steps-to-get-folding-traces","title":"Steps to get folding traces","text":"<p>As an example on how to get the proper traces for alucobond milling with folding parts.</p> <p>1. With the object cutting side pointing up, duplicate it and rotate 90\u00ba with the bottom corner as rotation point.</p> <p></p> <p>2. Displace the duplicated part 2mm towards the center. That's one millimeter per side of the folding axis.</p> <p></p> <p>3. Repeat both steps on the other side</p> <p> </p> <p>4. Create a line at 1mm from the part border (centered between both pieces)</p> <p></p> <p>5. Repeat the process for the other side, now you have the V cutt milling traces</p> <p></p> <p>6. Join the two parts, remove the vertices outside the bottom layer and create a bridge to join both parts. This paths should be -2.2mm from the surface of the material (leaving a thickness of 0.8mm after cutting)</p> <p></p> <p>Now you can process the part with blendercam to get the gcode.</p>"},{"location":"Farmers%20Dashboard/building/#hdpe","title":"HDPE","text":"<p>For the tension adjusting slider some HDPE pieces need to be milled, the centerpiece is the more complicated since the milling has to be on both sides.</p> <p>Feeds and speeds for HDPE:</p> Tool RPM Feed Cut Feed Plunge Stepdown 3 mm flat tool 18,000 rpm 2,000 mm/min 2,000 mm/min 1.4 mm/step 6 mm flat tool 18,000 rpm 6,000 mm/min 5,000 mm/min 1.4 mm/step <p></p>"},{"location":"Farmers%20Dashboard/camera/","title":"Camera module","text":"<p>Size: 148mm x 143mm x 63mm Weight: 560gr CMOS sensor: 12.3 mp, Sony IMX477 sensor. Optics: Support for C- and CS-mount lenses. Networking: Wi-Fi 2.4GHz 802.11 b/g/n Interfaces: UART Serial, I2C, SWD.  </p> <p>Both the Cablebot and the Scanner require a reliable camera module, although the usage in both cases is slightly different For the Scanner, the camera is positioned at a given angle. The movement from angle position to the next is relatively infrequent and slow. For the Cablebot, the camera must adjust in real-time for swinging movements of the system.</p>"},{"location":"Farmers%20Dashboard/camera/#controller-board","title":"Controller board","text":"<p>We decided to use a brushless motor, as is the custom in camera mount systems. We designed a controller board that exploits the functions offered by the TI DRV8313 chip. The DRV8313 requires as an input three Pulse Width Modulation signals (PWM) that encode the phase of each of the three voltages applied to the solenoids of the brushless motor. </p> <p>The control software is integrated into the code for the Romi Rover: https://github.com/romi/libromi/tree/ci_dev/firmware/BLDC  The design files can be found at https://github.com/romi/bldc_featherwing </p>"},{"location":"Farmers%20Dashboard/camera/#microcontroller","title":"Microcontroller","text":"<p>The PWMs signals should be of a high frequency, as to avoid any ripples in the signal, and should be closely synchronised. We therefore opted for a Cortex M0 microcontroller instead of the more common AVR microcontrollers found on the Arduino Uno, for example. Concretely, we are using the Adafruit Feather M0 Basic, but the code should run on any Arduino -compatible SAMD21 microcontroller board.</p>"},{"location":"Farmers%20Dashboard/camera/#motor-and-encoder","title":"Motor and encoder","text":"<p>To estimate the angular position of the motor we use a HAL-based encoder. We are using standard components that are sold for the drone market. In particular, we are using the iPower Motor GM4108H-120T Gimbal Motor with AS5048A Encoder and slip ring from iFlight-rc.com.</p>"},{"location":"Farmers%20Dashboard/camera/#camera","title":"Camera","text":"<p>We use the recent Raspberry Pi High Quality Camera Module. The module is connected to the Raspberry Pi Zero W single-board computer. The camera module has a CS mount that allows us to change the lens.</p>"},{"location":"Farmers%20Dashboard/camera/#wiring","title":"Wiring","text":"<p>We had recurring problems with the cabling of the cameras in our previous solutions. The micro USB connectors were not reliable enough and often lost contact. The transmission of the power and the communication over a long USB cable often failed (power drop, broken serial link). We therefore choose to bring a 12V cable to the camera, connect it over a sturdy plug, and include a DC-DC converter inside the camera. Also, we use the Wi-Fi functionality offered by the Raspberry Pi Zero W to send commands and download the images.</p>"},{"location":"Farmers%20Dashboard/camera/#housing","title":"Housing","text":"<p>A housing was designed that follows the same production principles used in other cablebot components.</p> <p></p>"},{"location":"Farmers%20Dashboard/electronics/","title":"Control electronics","text":"<p>The entire motion system is controlled by a low-cost and low-power microcontroller (Microchip SAMD21) that interfaces with the camera module. The much powerful computer in the camera module runs the main logics and communication subsystem based on the software and hardware stack used in the Rover, ensuring modularity and scalability. As both the camera module and the Rover run the Raspberry PI ARM based Linux architecture our software stack is portable across each one of the robots. Those ensuring the Rover and the carrier use the same remote management interfaces. Following that approach the carrier can be managed using the same standard RC remote controller for on-site  maintenance operations.</p> <p></p> <p>The Carrier module electronics is composed by three main PCB's: the control PCB that holds the man microcontroller on charge of the navigation, the Odrvie motor driver and the power distribution PCB.</p> <p></p>"},{"location":"Farmers%20Dashboard/electronics/#control-pcb","title":"Control PCB","text":"<p>The navigation control is managed by any Arduino SAMD21 compatible board. This board will receive direct instructions via RC control or commands through the Serial port sent by the Raspberry pi in the Camera Module.</p> <p></p>"},{"location":"Farmers%20Dashboard/electronics/#inputs","title":"Inputs","text":"<ul> <li>Two endstops (2 interrupts)</li> <li>Position encoder (SPI)</li> <li>Motor encoder (1 interrupt)</li> <li>Battery voltage (from voltage dividerconnected to batt)</li> <li>RF speed channel (1 interrupts) we have a second channel with no use for now.</li> <li>USB or TX/RX only Serial port (level converter?).</li> <li>IMU (I2C)</li> <li>Charger connected input (1 interrupt)</li> <li>User button (1 interrupt)</li> </ul>"},{"location":"Farmers%20Dashboard/electronics/#outputs","title":"Outputs","text":"<ul> <li>Motor control (PWM)</li> <li>User led (Addressable RGB)</li> </ul>"},{"location":"Farmers%20Dashboard/electronics/#feather-pinout","title":"Feather Pinout","text":"Feather M0 Pin Function Int 0 - RX (Serial1) (yellow) PA11 Odrive Serial GPIO1 \u2192 TX SERCOM0.3 1 - TX (Serial1) (green) PA10 Odrive Serial GPIO2 \u2192 RX SERCOM0.2 5 PA15 Addressable Led \u2192 DIN 6 PA20 Endstop Front or right (was Back \u00b9) \ud83d\udfe2 9 PA07 Endstop Back or left (was Front \u00b9) \ud83d\udfe2 10 - TX0 (green) PA18 Camera Module \u2192 RX SERCOM1.2 11 PA16 RC \u2192 STR (Optional Gimbal control) \ud83d\udfe2 12 - RX0 (yellow) PA19 Camera Module \u2192 TX SERCOM1.3 13 PA17 RC Throttle \u2192 THR \ud83d\udfe2 15 - A1 PB08 Odrive Reset nRST (in J2) 16 - A2 PB09 User Button \ud83d\udfe2 17 - A3 PA04 Charging station home int \ud83d\udfe2 18 - A4 PA05 ADNS \u2192 MOT 19 - A5 PB02 ADNS \u2192 SS 20 - SDA PA22 I2C \u2192 SDA SERCOM3.0 21 - SCL PA23 I2C \u2192 SCL SERCOM3.1 22 - MISO PA12 ADNS \u2192 MI SERCOM_ALT4.0 23 - MOSI PB10 ADNS \u2192 MO SERCOM_ALT4.2 24 - SCK PB11 ADNS \u2192 SC SERCOM_ALT4.3 RST Reset Button"},{"location":"Farmers%20Dashboard/electronics/#mouse-laser-motion-sensor","title":"Mouse laser motion sensor","text":"<p>As a way to get real closed loop navigation the ADNS-9800 Laser Motion Sensor is being used to track the movement over the cable. This sensor is still to be integrated in the latest prototype, tests are being made to warranty that the cable is always visible to the sensor independently of the tension level.</p> <p></p> <p>Optimal position of the laser sensor.</p>"},{"location":"Farmers%20Dashboard/electronics/#imu","title":"IMU","text":"<p>An Inertial measurement unit is used to give feedback to the microcontroller about balance and vibrations, acceleration and speed algorithms to take advantage of this data are still under development. The ISM330 Adafruit QWIIC breakout board is being used through I2C bus.</p> <p>Compatible Adafruit library Arduino library</p>"},{"location":"Farmers%20Dashboard/electronics/#endstops","title":"Endstops","text":"<p>To detect collisions OMRON D3V-013-1C23 miniature switches are used as end stops on both sides of the cablebot. A 3d printed cover protects the electronic parts and triggers the switch when an obstacle is found. Cabling is routed through the structure to avoid any damage on the lines.</p> <p></p> <p>The cabling is routed trough machined channels between the aluminum sandwich sheets keeping it protected and organized.</p>"},{"location":"Farmers%20Dashboard/electronics/#remote-control","title":"Remote Control","text":"<p>Any radio frequency remote control with at least one channel can be used with the CARM. We use one PWM channel to control the speed along the cable. A second PWM channel is already wired to be used in the future, for example to control camera orientation.</p> <p>We have used HK-GT2B model with good results.</p> <ul> <li>2.4GHz AFHDS signal operation</li> <li>3CH operation</li> <li>3.7v 800mAh Rechargeable li-ion transmitter battery</li> <li>RF Power: 20dBm (100mW) max</li> <li>Modulation: GFSK</li> <li>Sensitivity: 1024</li> <li>Transmitter Weight: 270g</li> <li>Receiver power: 4.5~6.5 VDC</li> </ul> <p>Depending on the RF hardware sometimes adjusting the signal top/down limits with the remote potentiometers is not enough, this values can be adjusted on firmware changing the <code>RC_CALIBRATION</code> constant values for Min, Middle and Max values here.</p> <p>To find out the values of your remote print to the console the value of the <code>rcSpeed</code> variable, somewhere in your <code>loop()</code> function and check the serial output while the trigger is at rest, top and bottom positions.</p> <p>To minimize vibrations of the carrier module while operated with the RF remote control, the noise on the signal is cleaned, applying exponential smoothing to it.</p> <p></p>"},{"location":"Farmers%20Dashboard/install/","title":"Raspberry Pi","text":""},{"location":"Farmers%20Dashboard/install/#overview","title":"Overview","text":"<p>The software for the Cablebot consists of three components:</p> <ol> <li>The main software running on the Raspberry Pi Zero of the camera module. This is a combination of GNU/Linux and the ROMI software, found at https://github.com/romi/romi-rover-build-and-test </li> <li>The firmware for the motor controller, found at https://github.com/romi/romi-cablebot/tree/camera-integration</li> <li>The firmware to control the panning of the camera, found at https://github.com/romi/libromi/tree/ci_dev/firmware/BLDC</li> </ol> <p>In addition, the installation requires the following actions on the Raspberry Pi Zero:</p> <ol> <li>Create the \"romi\" user account.</li> <li>Configure the Wi-Fi to connect to an existing Wi-Fi</li> <li>Enable the SSH server</li> <li>Enable the legacy camera interface.</li> <li>Configure the serial ports.</li> <li>Configure the start-up scripts.</li> <li>Install and configure the Apache HTTP server for the web interface.</li> <li>Configure the backup/transfer of the images.</li> </ol>"},{"location":"Farmers%20Dashboard/install/#installing-the-raspberry-pi-zero","title":"Installing the Raspberry Pi Zero","text":""},{"location":"Farmers%20Dashboard/install/#install-the-linux-operating-system","title":"Install the Linux operating system","text":"<p>Download Raspberry Pi OS (32-bit) Lite and burn the image to a sdcard of at least 8GB. To avid connecting the Pi to a monitor you can follow the instructions to set up WiFi and ssh (scroll down to number 3) for a headless setup.</p> <p>Once your Pi is connected to the network and you're logged via ssh as pi user there are some simple tasks to do:</p> <ul> <li> <p>For security reasons it is recommended that you change the pasword of default user pi <pre><code>pi@raspberry:~ $ passwd\n</code></pre></p> </li> <li> <p>Update packages. <pre><code>sudo apt update\nsudo apt upgrade\n</code></pre></p> </li> </ul>"},{"location":"Farmers%20Dashboard/install/#basic-configuration","title":"Basic configuration","text":"<p>Run the <code>sudo raspi-config</code> command to change the base configuration:</p> <p>Keyboard:</p> <p>If you need you can change keyboard, WLAN country and timezone settings using raspi-config. Run <code>raspi-config</code> command and select <code>5 Localization Options</code>.</p> <p>Hostname:</p> <p>Still in raspi-config,</p> <ol> <li>Select <code>System Options</code></li> <li>Select <code>S4 Hostname</code></li> <li>Question: <code>Please enter a hostname</code>: Enter <code>cablebot</code></li> <li>Select <code>OK</code></li> </ol> <p>Wifi:</p> <p>In raspi-config,</p> <ol> <li>Select <code>1 System Options</code> </li> <li>Select <code>S1 Wireless LAN</code></li> <li>Question: <code>Please enter SSID</code> Type the name of the WiFi network you want to connect to.</li> <li>Question: <code>Please enter passphrase. Leave it empty if none.</code> Type the password.</li> <li>Click <code>OK</code></li> </ol> <p>SSH:</p> <p>In raspi-config,</p> <ol> <li>Select <code>3 Interface Option</code> </li> <li>Select <code>I2 SSH</code></li> <li>Question: <code>Would you like the SSH server to be enabled?</code> Select <code>Yes</code></li> </ol> <p>Make sure to test the remote login. To know the IP address of the Pi, run the following command:</p> <pre><code>$ ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\nlink/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host valid_lft forever preferred_lft forever\n2: eth0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc mq state DOWN group default qlen 1000\nlink/ether e4:5f:01:93:b6:a3 brd ff:ff:ff:ff:ff:ff\n3: wlan0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\nlink/ether e4:5f:01:93:b6:a7 brd ff:ff:ff:ff:ff:ff\n    inet 172.20.10.12/28 brd 172.20.10.15 scope global dynamic noprefixroute wlan0\n       valid_lft 86163sec preferred_lft 75363sec\n    inet6 2a02:8440:3141:50b1:d111:5312:d516:8327/64 scope global mngtmpaddr noprefixroute valid_lft forever preferred_lft forever\n    inet6 fe80::1577:9a8:248f:7648/64 scope link valid_lft forever preferred_lft forever\n</code></pre> <p>The IP address of the WiFi interface can be found in the wlan0 section. In our case, it is 172.20.10.12. Now try to log in from a remote machine on the same network:</p> <pre><code>$ ssh romi@&lt;IP-ADDRESS&gt;\n</code></pre> <p>Replace IP-ADDRESS with the address found using <code>ip a</code>. Make sure that this work before continuing because further down we are going the deactivate the login console on the screen.</p>"},{"location":"Farmers%20Dashboard/install/#enable-the-legacy-camera-interface","title":"Enable the legacy camera interface","text":"<p>On newer version of the OS image, the legacy interface has to be activated manually. This can be done using raspi-config:</p> <pre><code>$ sudo raspi-config\n</code></pre> <ol> <li>Select 3 Interface Options</li> <li>Select I1 Legacy Camera</li> <li>Select Yes</li> <li>Reboot</li> </ol>"},{"location":"Farmers%20Dashboard/install/#configure-the-serial-ports","title":"Configure the serial ports","text":"<p>The Raspberry Pi Zero W has one \"real\" serial controller, called UART0 or PL011, and one more limited serial controller, called UART1 or mini UART.</p> <p>(The list of pins on the 40-pin GPIO header can be found here: https://pinout.xyz/) (See also the Raspberry Pi documentation: https://www.raspberrypi.com/documentation/computers/configuration.html#configuring-uarts)</p> <p>By default, the Raspberry Pi is configured as follows:</p> <ul> <li>UART0 is used to communicate with the Bluetooth controller. It is mapped to /dev/ttyAMA0.</li> <li>UART1 is used for the Linux console. By default, the mini UART is mapped to the pin 8 (GPIO 14) for TX, and pin 10 (GPIO 15) for RX. The serial device is mapped to /dev/ttyS0.</li> </ul> <p>The GPIO pins 14 and 15 are used for the \"primary\" serial. By default, UART1 is the primary serial. </p> <p>For the Cablebot, we want to change this as follows:</p> <ul> <li>UART0 connects the motor controller on GPIO pins 14 and 15 (pins 8 and 10)</li> </ul> <p>Disabling Bluetooth will make UART0 available again.  </p> <p>In /boot/config.txt, add the following line at the end of the file (after [all]):</p> <pre><code>dtoverlay=disable-bt\n</code></pre> <p>This disables the Bluetooth kernel module.</p> <p>Then, disable the console on the serial port. In /boot/cmdline.txt, remove:</p> <pre><code>console=serial0,11520\n</code></pre> <p>In a terminal, run the following command to disables the modem and Bluetooth services of the OS: <pre><code>sudo systemctl disable hciuart.service\nsudo systemctl disable bluealsa.service\nsudo systemctl disable bluetooth.service\n</code></pre></p> <p>And reboot: <pre><code>sudo reboot\n</code></pre></p> <p>After these changes, UART0 will be the primary serial and connected to GPIO pins 14 and 15. </p> <p>It does not seem possible to have both serial active and available in GPIO pins. This is because both UART0 and UART1 are (hard)wired to GPIO pins 14 and 15...</p> <p>Alternatively (to be tested):</p> <pre><code>$ sudo raspi-config\n</code></pre> <ol> <li>Select 3 Interface Options</li> <li>Select I6 Serial Port</li> <li>Question: Would you like a login shell to be accessible over serial? Select No</li> <li>Question: Would you like the serial port hardware to be enabled? Select  Yes</li> </ol> <p>The confirmation dialog should display:</p> <pre><code>The serial login shell is disabled\nThe serial interface is enabled\n</code></pre>"},{"location":"Farmers%20Dashboard/install/#create-user-romi","title":"Create user romi","text":"<p>Create user romi, set password and add it to needed groups. <pre><code>sudo useradd romi\nsudo usermod -a -G sudo,adm,dialout,video,netdev,plugdev,gpio romi\n</code></pre></p> <p>Quit the current shell and reconnect using the \"romi\" login.</p>"},{"location":"Farmers%20Dashboard/install/#clone-compile-and-install-the-software","title":"Clone, compile, and install the software","text":"<p>As user \"romi\" do:</p> <p>For the main software:</p> <pre><code>$ git clone --branch ci_dev --recurse-submodules https://github.com/romi/romi-rover-build-and-test.git\n$ cd romi-rover-build-and-test\n$ mkdir build\n$ cd build\n$ cmake ..\n$ make rcom-registry\n$ make romi-cablebot\n</code></pre> <p>For the firmware of the motor controller:</p> <pre><code>$ cd ~\n$ git clone https://github.com/romi/romi-cablebot.git\n$ cd romi-cablebot\n$ TODO\n</code></pre> <p>For the firmware of the camera pan controller:</p> <pre><code>$ TODO\n</code></pre>"},{"location":"Farmers%20Dashboard/install/#install-the-apache-web-server","title":"Install the Apache web server","text":"<pre><code>$ sudo apt install apache2\n$ sudo nano /etc/apache2/sites-enabled/000-default.conf\n</code></pre> <p>Change <code>DocumentRoot</code> in <code>/etc/apache2/sites-enabled/000-default.conf</code> to <code>/home/romi/romi-rover-build-and-test/applications/romi-monitor/</code></p> <p>Also add: <pre><code>        &lt;Directory /home/romi/romi-rover-build-and-test/applications/romi-monitor/&gt;\n                   Options Indexes\n                   AllowOverride None\n                   Require all granted\n        &lt;/Directory&gt;\n</code></pre> Then restart the <code>apache2.service</code> with <code>systemctl</code>: <pre><code>sudo systemctl restart apache2.service\n</code></pre></p>"},{"location":"Farmers%20Dashboard/install/#starting-the-software-on-boot","title":"Starting the software on boot","text":"<pre><code>$ sudo nano /etc/rc.local\n</code></pre> <pre><code>#!/bin/sh -e\n#\n# rc.local\n#\n# This script is executed at the end of each multiuser runlevel.\n# Make sure that the script will \"exit 0\" on success or any other\n# value on error.\n#\n# In order to enable or disable this script just change the execution\n# bits.\n#\n# By default this script does nothing.\n\n# Print the IP address\n_IP=$(hostname -I) || true\nif [ \"$_IP\" ]; then\n  printf \"My IP address is %s\\n\" \"$_IP\"\nfi\n\nsudo -u romi /home/romi/romi-rover-build-and-test/build/bin/rcom-registry &amp;\n\nexit 0\n</code></pre>"},{"location":"Farmers%20Dashboard/install/#extra-overlayfs","title":"Extra: OverlayFS","text":"<p>To avoid file system corruption on Raspberry Pi's that can have their power interrupted suddenly, having a read-only file system it's a good option.</p> <p>In computing, OverlayFS is a union mount filesystem implementation for Linux. It combines multiple different underlying mount points into one, resulting in single directory structure that contains underlying files and sub-directories from all sources. Common applications overlay a read/write partition over a read-only partition, such as with LiveCDs and IoT devices with limited flash memory write cycles. Wikipedia</p> <p>When using OverlayFS no filesystem change will survive reboot, that means no bash history!</p>"},{"location":"Farmers%20Dashboard/install/#raspbian","title":"Raspbian","text":"<p>In raspbian you can run <code>raspi-config</code> and under Advanced Options you will find Overlay FS option:</p> <p></p> <p>Just enable it and set boot partition to read only (raspi-config will ask you for this) and reboot. You can revert this changes with the same procedure.</p>"},{"location":"Farmers%20Dashboard/install/#archlinux","title":"Archlinux","text":""},{"location":"Farmers%20Dashboard/install/#install-raspi-overlayroot","title":"Install Raspi-Overlayroot","text":"<p><pre><code>git clone https://github.com/nils-werner/raspi-overlayroot\ncd raspi-overlayroot\nmakepkg -si\n</code></pre> Then try rebooting, it should boot as normal.</p>"},{"location":"Farmers%20Dashboard/install/#enable-overlayroot-hook","title":"Enable overlayroot hook","text":"<p>Then in <code>/etc/mkinitcpio.conf</code></p> <ol> <li>add <code>overlay</code> to your <code>MODULES</code> array</li> <li>add <code>overlayroot</code> to your <code>HOOKS</code> array</li> </ol> <p>and rebuild the initramfs by running <pre><code>mkinitcpio -P\n</code></pre> and reboot. It should boot as normal.</p>"},{"location":"Farmers%20Dashboard/install/#enable-overlayroot-in-commandline","title":"Enable overlayroot in commandline","text":"<p>With the initramfs in place, you can now enable overlayroot by adding <code>overlayroot</code> to the end of the Kernel commandline, editing <code>/boot/cmdline.txt</code></p> <p><pre><code>root=/dev/mmcblk0p2 rw rootwait console=ttyAMA0,115200 console=tty1 selinux=0 plymouth.enable=0 smsc95xx.turbo_mode=N dwc_otg.lpm_enable=0 kgdboc=ttyAMA0,115200 elevator=noop overlayroot\n</code></pre> and reboot. You should see a warning during login that any changes you make to your filesystem will be non-persistent after this point.</p>"},{"location":"Farmers%20Dashboard/install/#set-filesystems-readonly","title":"Set filesystems readonly","text":"<p>You can now also set the entire root filesystem as readonly by changing <code>rw</code> to <code>ro</code> in the Kernel commandline <pre><code>root=/dev/mmcblk0p2 ro rootwait console=ttyAMA0,115200 console=tty1 selinux=0 plymouth.enable=0 smsc95xx.turbo_mode=N dwc_otg.lpm_enable=0 kgdboc=ttyAMA0,115200 elevator=noop overlayroot\n</code></pre> and adding <code>ro</code> to <code>/etc/fstab</code> <pre><code>#\n# /etc/fstab: static file system information\n#\n# &lt;file system&gt; &lt;dir&gt;   &lt;type&gt;  &lt;options&gt;   &lt;dump&gt;  &lt;pass&gt;\n/dev/mmcblk0p1  /boot   vfat    defaults,ro     0       0\n</code></pre></p>"},{"location":"Farmers%20Dashboard/install/#editing-the-root-filesystem","title":"Editing the root filesystem","text":"<p>You can run <code>rwrootfs</code> to remount all file systems as read-write and change into an interactive shell in your SD card file system. After exiting that shell, the fileystems will remain read-write until next reboot.</p> <p>Alternatively you can undo all changes from Enable overlayroot in commandline and Set filesystems readonly and reboot. This is the recommended way of system upgrades.</p> <p>Info</p> <p>Resources * https://github.com/nils-werner/raspi-overlayroot * https://wiki.archlinux.org/title/Overlay_filesystem </p>"},{"location":"Farmers%20Dashboard/install/#extra-romi-autossh","title":"Extra: Romi autossh","text":"<p>Locating a Romi device (ej. cablebot) on a network can be a dificult task depending on the network topology. Services like Dataplicity can solve this problem but have some disatvantages as not being free software, cost, vendor lock, etc.</p> <p>Having a server accesible via a public IP address is enough to make this work. Maintaining a persistent SSH reverse tunnel with autoSSH between the Romi device and the server will allow us to access the device from anywhere without knowing his IP address.</p>"},{"location":"Farmers%20Dashboard/install/#client-side-romi-device","title":"Client side (Romi device)","text":""},{"location":"Farmers%20Dashboard/install/#generate-a-rsa-key-withouth-passphrase","title":"Generate a rsa key (withouth passphrase)","text":"<pre><code>ssh-keygen\n</code></pre>"},{"location":"Farmers%20Dashboard/install/#copy-your-key-to-the-server-to-allow-passwordless-access","title":"Copy your key to the server to allow passwordless access.","text":"<p>The user must exist already on the server, we recomend creating a specific user for this task. <pre><code>ssh-copy-id user@server\n</code></pre></p>"},{"location":"Farmers%20Dashboard/install/#test-a-reverse-ssh-tunnel","title":"Test a reverse SSH tunnel","text":"<p>Select a specific port for each of the devices to use on the port-s:localhost:22 part of the command, so that port number (port-s) of the server will be linked to port 22 on the device. <pre><code>ssh -R port-s:localhost:22 [-p srv-ssh-port] user@server-with-public-ip\n</code></pre> Now try to log in from another computer with: <pre><code>ssh -J user@server -p port-s user@localhost\n</code></pre></p> <p>If that worked we can now setup the autossh to make the conection on boot and keep it alive.</p>"},{"location":"Farmers%20Dashboard/install/#install-autossh","title":"Install autossh","text":"<p>Depending on your linux package manager, ej. <code>sudo pacman -S autossh</code> or <code>sudo apt install autossh</code></p>"},{"location":"Farmers%20Dashboard/install/#test-autossh-reverse-tunneling","title":"Test autossh reverse tunneling","text":"<pre><code>autossh -M 0 -f -o \"ServerAliveInterval 45\" -o \"ServerAliveCountMax 2\" -N -R port-s:localhost:22 [-p srv-ssh-port] user@server-with-public-ip\n</code></pre> <p>Options:</p> <p>-M Specifies monitor port. May be overridden by environment variable AUTOSSH_PORT. 0 turns monitoring loop off. -f Run in background (autossh handles this, and does not pass it to ssh.) -N Do not execute a remote command.  This is useful for just forwarding ports. -R [bind_address:]port:host:hostport Specifies that connections to the given TCP port or Unix socket on the remote (server) host are to be forwarded to the local side.</p> <p>From the autossh manual page: </p> <p>Setting the monitor port to 0 turns the monitoring function off, and autossh will only restart ssh upon ssh's exit. For example, if you are using a recent version of OpenSSH, you may wish to explore using the ServerAliveInterval and ServerAliveCountMax options to have the SSH client exit if it finds itself no longer connected to the server. In many ways this may be a better solution than the monitoring port.</p> <p>So we are using ServerAliveInterval:</p> <p>Sets a timeout interval in seconds after which if no data has been received from the server, ssh(1) will send a message through the encrypted channel to request a response from the server.</p> <p>and ServerAliveCountMax:</p> <p>Sets the number of server alive messages (see below) which may be sent without ssh(1) receiving any messages back from the server.  If this threshold is reached while server alive messages are being sent, ssh will disconnect from the server, terminating the session.</p> <p>that means that after 45 seconds ssh by itself will try to communicate with the server, if it fails will try again in 45 more seconds and after two failures it will terminate the session, in wich case autossh will restart the connection.</p>"},{"location":"Farmers%20Dashboard/install/#starting-autossh-on-boot-with-systemd","title":"Starting autossh on boot with systemd","text":"<p>Create a new file <code>/etc/systemd/system/autossh.service</code> and add this to it, remember to set your port (port-s):</p> <pre><code>[Unit]\nDescription=AutoSSH service for port port-s\nAfter=network.target\n\n[Service]\nUser=user\nEnvironment=\"AUTOSSH_GATETIME=0\"\nExecStart=/usr/bin/autossh -M 0 -o ControlMaster=no -o \"ServerAliveInterval 45\" -o \"ServerAliveCountMax 2\" -N -i ~/.ssh/id_rsa -R port-s:localhost:22 [-p srv-ssh-port] user@server-with-public-ip\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Now enable and start the service:</p> <pre><code>sudo systemctl enable autossh\nsudo systemctl start autossh\n</code></pre> <p>From now autossh will be started at boot time and will keep the tunnel alive.</p> <p>Warning</p> <p>Only tested on Arch Linux.</p>"},{"location":"Farmers%20Dashboard/install/#server-side","title":"Server side","text":"<p>The simplest way is using the same user in the server and the clients, so if you haven't already created the user, do it.</p>"},{"location":"Farmers%20Dashboard/install/#no-password-logins-optional","title":"No Password logins (optional)","text":"<p>For extra security you can disable password login, only allowing logins with keys:</p> <p><pre><code>sudo vi /etc/ssh/sshd_config\n</code></pre> To do it system wide change <code>PasswordAuthentication yes</code> to <code>PasswordAuthentication no</code>.</p> <p>Or if you want to restrict password logins only for a specific user (eg. romi) comment the previous line: <code>#PasswordAuthentication no</code> and add this content to the file:</p> <pre><code>Match User user(romi)\n    PasswordAuthentication no\n</code></pre> <p>The inconvenience of this approach is that every time you want to add a new key (give rights to a new device) you will need to change this temporarily to allow password logins. Remember that after changing the <code>sshd_config</code> file you need to restart the service with <code>systemctl restart sshd</code>. </p>"},{"location":"Farmers%20Dashboard/install/#restrict-client-commands","title":"Restrict client commands","text":"<p>To avoid security risks in case a key on a romi-device has been compromised, we are going to restrict the commands that the client is able to execute to the minimal.</p> <p>This can be achieved by editing ~/.ssh/authorized_keys, by prefixing the desired key, e.g. as follows: <pre><code>command=\"\" ssh-rsa \u2026\n</code></pre></p> <p>This will allow any login with this specific key only to execute the command specified between the quotes, none in this example.</p>"},{"location":"Farmers%20Dashboard/install/#client-side-user-computer","title":"Client side (user computer)","text":""},{"location":"Farmers%20Dashboard/install/#generate-and-copy-rsa-key","title":"Generate and copy RSA key","text":"<p>If you haven't generated your ssh key do it with the <code>ssh-keygen</code> command and copy this key to the server and the romi device.</p>"},{"location":"Farmers%20Dashboard/install/#setup-connection-with-ssh-config-file","title":"Setup connection with ssh config file","text":"<p>Add a new entry in your <code>.ssh/config</code> with the following content: <pre><code>Host tunnel\n    Hostname server-with-public-ip\n    [port srv-ssh-port]\n    user user\n\nHost romi-device                                                      \n    Hostname localhost\n    port port-s\n    user user\n    ProxyJump tunnel   \n</code></pre> This will allow you to simply do <code>ssh romi-device</code> to log in your device.</p>"},{"location":"Farmers%20Dashboard/legacy/","title":"Wirebot","text":""},{"location":"Farmers%20Dashboard/legacy/#main-board-and-eletronics","title":"Main Board and Eletronics","text":""},{"location":"Farmers%20Dashboard/legacy/#raspberry-pi-packages-dependencies-and-configurations","title":"Raspberry Pi - packages, Dependencies and Configurations","text":"<ol> <li>Download the image Robotics Ubuntu+ROS Raspberry Pi Image (3B+ Support) that comes with Ubuntu 16.04 (LXDE), and ROS Kinetic.</li> <li>Copy the image to the SD card. Instructions here.</li> <li>Resizes the file system to fill the SD card before booting following this instructions.</li> <li>Acces to the  raspi-config utility:</li> </ol> <pre><code>$computer:~$sudo raspi-config\n</code></pre> <ul> <li> <p>Choose \"Expand root partition to fill SD card\" option:</p> </li> <li> <p>The Ubiquityrobotics images come up as a Wifi acces point. The SSID is ubiquityrobotXXXX where XXXX is part of the MAC address. Connect to the wifi hostopost and use folowing wifi password:</p> </li> </ul> <pre><code>robotseverywhere\n</code></pre> <ol> <li>Once connected, it is possible to log into the Pi with ssh ubuntu@10.42.0.1 with the following password of:</li> </ol> <pre><code>ubuntu\n</code></pre> <ol> <li>Desable the default robots and node runing on the pi.</li> </ol> <pre><code>$ ubuntu@ubiquityrobot.local:$sudo systemctl disable magni-base\n</code></pre>"},{"location":"Farmers%20Dashboard/legacy/#raspberry-pi-setting-up-the-wiredbot-to-the-network","title":"Raspberry Pi - Setting up the WIREDBOT to the Network","text":"<ol> <li>Open a new terminal window, and log in to the robot with ssh:</li> </ol> <p>ATENTION: The HOSTNAME for firts time is \u201cubiquityrobot.local\u201d.</p> <pre><code>$ computer:~$ssh ubuntu@ubiquityrobot.local\n</code></pre> <p>ATENTION: The password for firts time is \u201cubuntu\u201d.</p> <ol> <li>Change the hostname using pifi. Type the following command:</li> </ol> <pre><code>$ ubuntu@ubiquityrobot.local:~$sudo pifi set-hostname wiredbot\n</code></pre> <ol> <li>Reboot the Pi.</li> </ol> <pre><code>$ ubuntu@ubiquityrobot.local:~$sudo reboot\n</code></pre> <ol> <li>Log in to the robot with the new hostname \"wiredbot\":</li> </ol> <pre><code>$ computer:~$ssh ubuntu@wiredbot.local\n</code></pre> <ol> <li>Use pifi to list the nearby networks:</li> </ol> <pre><code>$ ubuntu@wiredbot:~$pifi list seen\n</code></pre> <p>ATENTION: Search for the network where the robots are connected.</p> <ol> <li>Swich to to the desire network by using the following command.</li> </ol> <pre><code>$ ubuntu@NEWHOSTNAME:~$sudo pifi add localNetwork password\n</code></pre> <p>ATTENTION: The keyword \"localNetwork\" on this documentation refert to the network the robot need to be connected. The keyword \"pass\" on this documentation refer to the password of the network.</p> <ol> <li>Reboot the Pi.</li> </ol> <pre><code>$ ubuntu@wiredbot:~$sudo reboot\n</code></pre> <ol> <li>Test the connectivity with the Pi. Open a new terminal window on a external on a diferent computer:</li> </ol> <p><pre><code>$ computer:~$ping wirebot.local\n</code></pre> TIP: Press control-c to stop the pinging ADVERTENCE: If something goes wrong, the PI will come back up as access point mode. Search on the network for the name ubiquityrobot, reboot and start over.</p> <ol> <li>Log into the PI by using:</li> </ol> <pre><code>$ computer:~$ssh ubuntu@wirebot.local\n</code></pre> <p>the output will be: <pre><code>The authenticity of host \u201810.0.0.113 (10.0.0.113)\u2019 can\u2019t be established. ECDSA key fingerprint is SHA256:sDDeGZzL8FPY3kMmvhwjPC9wH+mGsAxJL/dNXpoYnsc. Are you sure you want to continue connecting (yes/no)?\n</code></pre></p> <p>continue by wrinting:</p> <pre><code>$ computer:~$yes\n</code></pre> <p>the password is still.</p> <pre><code>ubuntu\n</code></pre> <ol> <li>Update and updagrade de Pi.</li> </ol> <pre><code>$ ubuntu@wiredbot:~$sudo apt-get update\n</code></pre> <pre><code>$ ubuntu@wiredbot:~$sudo apt-get upgrade\n</code></pre>"},{"location":"Farmers%20Dashboard/legacy/#ros-setting-up-the-ros-nodes-and-arduino-firmware","title":"ROS - Setting up the ROS NODES and Arduino Firmware.","text":"<ol> <li>Make sure you have installed the resent updates and updagrades.</li> </ol> <pre><code>$ ubuntu@wiredbot:~$sudo apt-get update\n</code></pre> <pre><code>$ ubuntu@wiredbot:~$sudo apt-get upgrade\n</code></pre> <ol> <li> <p>Point to the workspace folder for ros packages</p> </li> <li> <p>Clone the repository on the Pi, the romi/grlbl_serial into the /src folder of your catkin workspace and rebuild your workspace:</p> </li> </ol> <p><pre><code>$ ubuntu@wiredbot:~$cd ~/catkin_ws/src/\n</code></pre> <pre><code>$ ubuntu@wiredbot:~$git clone git@github.com:romi/grlbl_serial.git\n</code></pre> <pre><code>$ ubuntu@wiredbot:~$catkin_make\n</code></pre></p> <ol> <li>Clone the repository on the Pi, the romi/i2c_pca9685_driver  into the /src folder of your catkin workspace and rebuild your workspace:</li> </ol> <p><pre><code>$ ubuntu@wiredbot:~$cd ~/catkin_ws/src/\n</code></pre> <pre><code>$ ubuntu@wiredbot:~$git clone git@github.com:romi/i2c_pca9685_driver.git\n</code></pre> <pre><code>$ ubuntu@wiredbot:~$catkin_make\n</code></pre></p>"},{"location":"Farmers%20Dashboard/legacy/#wiring-diagram","title":"Wiring diagram.","text":"<ol> <li> <p>Schematics: </p> </li> <li> <p>List Part</p> </li> </ol> Item Description Quantity 0 Raspberry pi model 3b+ 1 1 Raspberry Pi Camera Module v2 1 2 16-Channel 12-bit - I2C interface - PCA9685 1 3 Arduino UNO 1 4 Arduino CNC Shield V3 1 5 A4988 Stepper Motor Driver 4 6 Nema 23 Unipolar 1.8deg 1 7 Survey3W Camera - Orange+Cyan+NIR (OCN, NDVI) 1 8 Survey3W HDMI PWM Trigger Cable 1 9 Survey3 Advanced GPS Receiver 1 10 12V Power Supply 1 11 Wires and general hardware -"},{"location":"Farmers%20Dashboard/legacy/#hardware-setup","title":"Hardware Setup.","text":"<p>1.Drawings * Assebly drawing - Top view  * Assebly drawing - Botton View </p> <ol> <li>List Part</li> </ol> Item Description Quantity 0 Aluminium Profile 20\u00d720 T-Slot 5 4 1 Idler Pulley Plate 6 2 Join Plat T 4 3 Corner connector 90 degree (V-Slot) 2 4 Gantry Plate V-Slot 20-80 2 5 3M Drop in Tee Nuts \u2013 Insert nuts 50 6 3M Allen Low Profile Screws 50 7 M8 Allen Screw - 45mm Long 6 8 Motor Mount Plate NEMA 23 1 9 Nylon Pulley And Wheel - 40 mm Diameter - 8 mm Bearing 6 10 Nema 23 stepper motor 1 11 P65 Weatherproof Enclosure/electrical enclosure box 2 12 5mm Shock Cord - Marine Grade Polyester Coated Rubber Rope -"},{"location":"Farmers%20Dashboard/legacy/#running-ros-node-path-planning","title":"Running ROS node - Path Planning","text":"<ol> <li>ROS Nodes Overview.</li> </ol> <ol> <li> <p>ROS Master - Run ROS Nodes over the raspberry PI.</p> </li> <li> <p>Log into the raspberry PI by using:</p> </li> </ol> <pre><code>$ computer:~$ssh ubuntu@wirebot.local\n</code></pre> <ul> <li>(OPTIONAL) Edit the path planning according to the dimensions of the field to scan and the desired length and amount of waypoints.</li> </ul> <p><pre><code>$ ubuntu@ubiquityrobot.local:~$sudo nano ~/catkin_ws/src/grlbl_serial/src/path_planning_action_client.py\n</code></pre> * Edit the path_planning_action_client.py by changing the variable movement_goal.xyz_position that is under the function def path_planning_client(). Here is an example of a Path planning that takes pictures of every 500mm in a distance of 10mts: <pre><code>    movement_goal.xyz_position = [\"{'x':0, 'y':0, 'z':500, 'delay':20}\",\n                                  \"{'x':0, 'y':0, 'z':1000, 'delay':20}\",\n                                  \"{'x':0, 'y':0, 'z':1500, 'delay':20}\",\n                                  \"{'x':0, 'y':0, 'z':2000, 'delay':20}\",\n                                  \"{'x':0, 'y':0, 'z':2500, 'delay':20}\",\n                                  \"{'x':0, 'y':0, 'z':3000, 'delay':20}\",\n                                  \"{'x':0, 'y':0, 'z':3500, 'delay':20}\",\n                                  \"{'x':0, 'y':0, 'z':4000, 'delay':20}\",\n                                  \"{'x':0, 'y':0, 'z':4500, 'delay':20}\",\n                                  \"{'x':0, 'y':0, 'z':5000, 'delay':20}\",\n                                  \"{'x':0, 'y':0, 'z':5500, 'delay':20}\",\n                                  \"{'x':0, 'y':0, 'z':6000, 'delay':20}\",\n                                  \"{'x':0, 'y':0, 'z':6500, 'delay':20}\",\n                                  \"{'x':0, 'y':0, 'z':7000, 'delay':20}\",\n                                  \"{'x':0, 'y':0, 'z':7500, 'delay':20}\",\n                                  \"{'x':0, 'y':0, 'z':8000, 'delay':20}\",\n                                  \"{'x':0, 'y':0, 'z':8500, 'delay':20}\",\n                                  \"{'x':0, 'y':0, 'z':9000, 'delay':20}\",\n                                  \"{'x':0, 'y':0, 'z':9500, 'delay':20}\"]\n</code></pre></p> <ul> <li>Start up the nodes and the ROS master by launching the path_planning_action_server_node node under the raspberry PI:</li> </ul> <pre><code>$ ubuntu@ubiquityrobot.local:~$roslaunch grlbl_serial path_planning_action_server_node.launch\n</code></pre> <ul> <li>(ADVERTENCE) If the ROS package is not under the autocomplete method of the terminal. The problem will be solve by sourcing the devel/setup.bash.</li> </ul> <p><pre><code>$ ubuntu@ubiquityrobot.local:~$source ~/catkin_ws/src/devel/setup.bash\n</code></pre> 3. ROS Slave - Run ROS Nodes over the Remote Computer.</p> <ul> <li>Start up the nodes by launching the path_planning_action_client_node node under the remote computer:</li> </ul> <pre><code>$ ubuntu@ubiquityrobot.local:~$roslaunch grlbl_serial path_planning_action_client_node.launch\n</code></pre> <ul> <li> <p>(OPTIONAL) This node as well can by launch over the raspberry PI. This can be done by lauching the node over a new terminal.</p> </li> <li> <p>By launching the previous ROS node on the WIREDBOT. The starting process of collecting photos from the Mapir camera and the Raspi Cam will be launch automatically according to the path planning instructions save on the path_planning_action_client.py file.</p> </li> </ul>"},{"location":"Farmers%20Dashboard/legacy/#saving-the-data-from-the-wiredbot-under-development","title":"Saving the data from the WIREDBOT. (UNDER-DEVELOPMENT)","text":"<ul> <li>Kepp running or re start  the node and the ROS master by launching the path_planning_action_server_node node under the raspberry PI:</li> </ul> <pre><code>$ ubuntu@ubiquityrobot.local:~$roslaunch grlbl_serial path_planning_action_server_node.launch\n</code></pre> <ul> <li>Publish a 1500us to the /i2c_pca9685_driver wiredbot_PWMValues/wiredbot_PWMValues mapir_control_pwm:</li> </ul> <pre><code>$ ubuntu@ubiquityrobot.local:~$rostopic pub -1 /i2c_pca9685_driver wiredbot_PWMValues/wiredbot_PWMValues \nint16 mapir_control_pwm 1500\nint16 motor_A_pwm\nint16 motor_B_pwm\n</code></pre> <ul> <li>Once ros is publishing the message mapir_control_pwm 1500us under the topic \\i2c_pca9685_driver\\wiredbot_PWMValues. The camera is ready to mount. On the raspberry PI. Mount the camera by using the following commands.</li> </ul> <p><pre><code>$ ubuntu@ubiquityrobot.local:~$mkdir /mapir\n$ ubuntu@ubiquityrobot.local:~$mkdir sudo mount -t vfat /dev/sdb2 /mapir\n$ ubuntu@ubiquityrobot.local:~$cd /mapir/DCMI/Photos\n</code></pre> *</p>"},{"location":"Farmers%20Dashboard/legacy/#image-gallery-valldaura","title":"Image Gallery - Valldaura:","text":"<ol> <li>Lettuce Think and Wirebot:</li> </ol> <ol> <li>Wirebot on the field:</li> </ol> <ol> <li>Wirebot on the field:</li> </ol>"},{"location":"Farmers%20Dashboard/legacy/#wirebot-3d-scans","title":"WIREBOT 3D Scans:","text":""},{"location":"Farmers%20Dashboard/motion/","title":"Motion system","text":"<p>The platform is battery powered  (12V LiPo 2Ah) and uses a brushless servo motor (DFROBOT FIT0441) combined with an optical cable tracker (ADNS-9800) to precisely move using a close loop system. It also contains built-in end stops switches as well as an inertial measurement unit to ensure it can safely operate autonomously. </p>"},{"location":"Farmers%20Dashboard/pipeline/","title":"Crops Insights Pipeline","text":""},{"location":"Farmers%20Dashboard/pipeline/#generating-orthomosaic-image","title":"Generating Orthomosaic Image","text":"<p>For the scope of this application, a rigorous system of collecting photographs of the fields was set, representing a first set of data to process; also to test different strategies of automatic navigation paths, by sending the same waypoints to the drones and to cable-bot. This set of instructions are basically following the rules of sweeping the entire area to then transform the data into an orthomosaic image which covers the whole crop. It\u2019s important that the images have around 50% to 70% vertical overlap and around 30% overlap on sides. </p> <p></p> <p>Each two pairs of images which have overlaps together are processed to find the same key points available between both (see fig 2) and following this pairing the images are geometrically corrected (orthorectified) such that the scale is uniform in all of them.</p> <p></p> <p>OpenDroneMap software/API was used to generate the orthomosaic image, the final outcome of this process is an aerial image of the whole crop which has a uniform scale. (see fig 3)</p> <p></p> <p>Frame Alignment Generated orthomosaic images could contain some extra parts such as extra borders, as well as random orientation. To be able to compare and analyze the maps generated from each scan overtime, we need to orient and crop them with similar boundaries and orientation. For this we include two processes of rotating the image if needed to straighten it, and as well, cropping the unnecessary parts in the borders to optimize the computation parts related to segmentation and future steps. The process of straightening the image starts by applying principal component analysis (PCA) to extract the axis line of the image, this axis line is then used to straighten the image. (see fig 4)</p> <p></p> <p>After aligning and cropping the orthomosaic images, they are stored in our database for the image segmentation task.</p> <p></p>"},{"location":"Farmers%20Dashboard/pipeline/#plant-detection","title":"Plant Detection","text":"<p>The collected orthomosaic images are generated with the pipeline explained above. In this section, we describe how the lettuces are detected and masked using Mask-RCNN, which is an instance segmentation algorithm. The generated output images show individual masks for each lettuce that is detected. The Mask-RCNN method has proven to be accurate (see the discussion of the segmentation methods in Section 1.3.6 - T6.3).  This method is able to separate lettuces from different kinds of plants or weeds, as well works well with different light and shadow conditions. </p> <p></p> <p>The orthomosaic images generated in step 4 are high quality large scale images. Training and Detection algorithms on these large scale images are computationally heavy therefore to apply the detection on orthomosaic images, first we divide the images to a smaller grid. The division and cell size of this grid is dependent on the crop scenario and can be defined manually by the user. The detection algorithm is applied on each cell (see Fig. 3.7).</p> <p> </p> <p>The results are merged back together to generate the overall detected image (see Fig. 3.8). In addition to the masked images, a json file containing the position and area (in the scale of image) of each detected plant is stored as well. In the next stages the masked images and the json files are used to catalog individual plants and monitor them through different scans.</p> <p></p>"},{"location":"Farmers%20Dashboard/pipeline/#tracking-individual-plants","title":"Tracking Individual Plants","text":"<p>Once we have segmented the scan image and obtained the list of individual plants, we have to identify each plant and track each individual throughout all the scans. We tested two methods to track individual plants. In the first method, we detect each plant in each image and extract the center point of each lettuce, the centers points from each scan are then compared together with Iterative Closest Point (ICP) algorithm to track the same lettuce in different scans. In the second method, we register all maps to a common frame using feature matching and detected plants at initial growth stage. The area around each plant is then considered in all images for measuring each plant size as approximated by the projected leaf area. </p> <p>Iterative closest point algorithm on the centers of lettuces This process uses the center points of lettuces in consecutive scans for the ICP method to find the same lettuces over different scans. However, this method has two major issues.</p> <ul> <li>Undetected lettuces in some images create unmatched points</li> <li>Changes in appearance of lettuces move their center.</li> </ul> <p> </p> <p>Registration Through Feature Matching In this process we use the position of the detected lettuces as well as the orthomosaic image generated in step 4, to track individual plants over different scans.  One of the main challenges in registering and tracking individual plants in this process is to have the same frame and coordinate system for all the images. This challenge is due to two main reasons. First, the orthomosaic images have different resolutions, as well the main frame for orthomosaic images is not exactly the same for all the images.  To create the same coordinate system for all the images, we use the first scan as our reference coordinate system and find the transformations between each two consecutive images using SURF feature matching methods available in OpenCV library which is based on the RANSAC algorithm. (Fig. 3.10)</p> <p></p> <p>For a series of images, the transformations between each two consecutive images gets combined with all the previous transformations to calculate the transformation to our reference image (first scan). Then all the images as well as the coordinates of plants from the json file are transformed to match the same coordinate system. (see Fig. 3.11)</p> <p></p>"},{"location":"Farmers%20Dashboard/pipeline/#creating-a-catalog-of-individual-plants","title":"Creating a Catalog of Individual Plants","text":"<p>Registration of images as well as plant\u2019s coordinates on the same coordinate system, allows for tracking of the same plant within different scans. This happens through clustering the plants\u2019 coordinates that are within a certain distance from each other (see Fig. 3.12).</p> <p>Next step is to Index the detected plant with the same ID over different scans. As well as creating a catalog of individual plants.</p> <p> </p>"},{"location":"Farmers%20Dashboard/pipeline/#leaf-area-monitoring-and-plant-growth-curves","title":"Leaf Area Monitoring and plant growth curves","text":"<p>The process of registering different scans over a common reference model results in having all the coordinates and detection images in the same scale. By knowing the scale of the reference image we can scale all the images to real size world coordinates. This is crucial for extracting the leaf area from 2D images. In order to extract the leaf area we calculate the amount of detected pixels (black) over the total amount of pixels in the image which is then multiplied by the scale factor of the image. (see Fig. 3.14)</p> <p></p> <p>The individual leaf area extracted in each scan, is stored together with the plant\u2019s index in a database. </p> <p></p>"},{"location":"Farmers%20Dashboard/pipeline/#creation-of-weed-maps","title":"Creation of weed maps","text":"<p>The results of the semantic segmentation can be used to map the weeds as well as the crops (Fig. 3.16). The resulting weed map provides an indicator on the areas where the pressure of the weeds is highest. These maps can be used in combination with the weeding Rover to prioritise the weeding activities. </p> <p></p>"},{"location":"Farmers%20Dashboard/power/","title":"Power","text":""},{"location":"Farmers%20Dashboard/power/#power_1","title":"Power","text":"<p>The power circuit has a main switch that avoids battery discharging during transportation or storing. Battery voltage is fed directly, a DC/DC voltage converter provides low voltage to the logic electronics subsystem. The battery voltage is feeding the camera module allowing high voltage motors. Both battery terminals are directly exposed to the charger station allowing the charge process during rest periods.</p> <p></p>"},{"location":"Farmers%20Dashboard/power/#power-distribution-pcb","title":"Power distribution PCB","text":""},{"location":"Farmers%20Dashboard/power/#dcdc-switching-regulator","title":"DC/DC switching regulator","text":"<p>To provide low voltage (5v) the OKI-78SR-5/1.5-W36E-C DC/DC switching regulator is used. It can provide 1.5A @ 5v and can stand inputs as high as 36v input. With this regulator we feed all the electronics contained in the carrier unit the camera module electronics use the same component to provide low voltage to its own electronics.</p>"},{"location":"Farmers%20Dashboard/power/#battery-module","title":"Battery module","text":""},{"location":"Farmers%20Dashboard/station/","title":"Charging Station","text":"<p>Size: 532mm x 240 x 805mm Weight: 3900gr (Without Access Point) Power Input - 120-240V AC, Output - 17V DC  </p> <p>The Charging Station module (CHAM) provides several features to the cablebot system:</p> <ul> <li>Rest position with minimal power consumption (motor can be disabled).</li> <li>Homing zero position with known precise geo position.</li> <li>Shelter from environmental hazards.</li> <li>Automatic charging via pogo pins contact connector.</li> <li>Optional WiFi access point.</li> </ul> <p>It allows the Carrier Module to park in a position that offers shelter against weather elements in addition to providing the necessary voltage for battery charging. A mechanical/magnetic clip system was designed to hold the CARM in a fixed rest position, once parked, the motor can be disabled to save power and a pogo pin contact connector enables charging. Alignment guides allow parking even under windy conditions.</p> <p>On homing the CARM obtains a geo-referenced fixed point from which precise position can be calculated for geotagging captured images. The CHAM holds an AC/DC power supply and an optional Wi-Fi access point.</p> <p>Installation ca be done directly on the cable with a 4 tensioning system that guarantees stability and allows correct alignment. A vertical fixed post or flat surface such as the wall of a building can also be used as holding structures.</p> <p>The carrier has three pogo pins that, via an interrupt signal, allows it to know when it is on home position and start the charging process.</p> <p>:q</p>"},{"location":"Farmers%20Dashboard/structure/","title":"Structure","text":"<p>The whole design uses a CNC folded aluminium-plastic sandwich panel designed to provide adequate outdoor resistance while being light-weight. The moving parts and other fixtures use custom 3D printed plastic (PLA/ABS). All the cabling and electronics integrate into the internal structure to minimize damage caused by environmental factors. The tension mechanism design allows it to move it from one cable to another as well to adapt to different cable tensions dynamically, and it uses a self-lubricated polymer to avoid maintenance. All the parts can be made on a Fab Lab or other rapid prototyping facility with no customized tooling required.</p> <p></p> <p>To facilitate documentation and assembly the Carrier Module is conceptually grouped in different parts:</p> <p>The hands hold the lateral pulleys supporting the module on the cable; the arms are the main symmetrical structure, joined by the central part of the sliding dynamic tension mechanism; the head is the moving part that holds the motor ant ist electronics; and finally the body that holds everything together and works as a bridge for the cabling between the different components.</p> <p></p>"},{"location":"Farmers%20Dashboard/structure/#hands","title":"Hands","text":"<p>The miniature switches used as end stops are held by a 3d printed piece that integrates a plastic hinge as trigger element. Integrated in the same housing are the 3d printed lateral pulleys held by a 608zz bearing.</p> <p></p> <p></p>"},{"location":"Farmers%20Dashboard/structure/#dynamic-tension","title":"Dynamic tension","text":"<p>The Farmers Dashboard carrier has a system the allows dynamic adaptation to different cable tensions, allowing a light motor torque to move even on high tension cables need to cover long distances and avoiding slips on low tension cables.</p> <p></p> <p>Three pieces of machined HDPE in sandwich form are used to allow smooth movement of the central part of the robot maintaining correct alignment.</p> <p></p> <p>To allow a bigger range of tensions pre-compression level of the springs can be adjusted by turning the three screws under the base piece. By turning the front screws the level of friction between the HDPE slides can also be regulated.</p> <p></p>"},{"location":"Farmers%20Dashboard/use/","title":"Installing and Operating the Cablebot","text":"<p>The Cablebot is a crucial component of the ROMI platform combining the Carrier module, the Charging Station and the Camera module into a single tool dedicated to the remote operation and autonomous scanning of crop beds.</p> <p>The Cablebot is under continous developement. For more detailed instructions refer to the development respository.</p>"},{"location":"Farmers%20Dashboard/use/#installation-process","title":"Installation Process","text":"<p>To install the Cablebot, follow these steps:</p> <ol> <li> <p>Prerequisites: Ensure that you have the necessary hardware components, including the Cablebot itself, its base station, and any additional accessories required for your specific use case. Also, make sure you have the required software tools and dependencies installed on your system.</p> </li> <li> <p>Physical Setup: Set up the Cablebot in the designated area on your farm. The installation manual provided with the Cablebot should offer detailed instructions on assembling and mounting the robot cable in your particular location (stand-alone, polytunnel, etc). Ensure the bot is properly secured and calibrated for optimal performance.</p> </li> </ol> <p></p> <ol> <li>Connectivity: Establish the necessary connectivity between the Cablebot and your WLAN (Wi-Fi 802.11n). This connection is crucial for communication and data exchange during operation. Refer to the documentation for the Cablebot for specific details on establishing a stable connection.</li> </ol>"},{"location":"Farmers%20Dashboard/use/#operating-the-cablebot","title":"Operating the Cablebot","text":"<p>Operating the Cablebot involves a series of steps to ensure its proper functioning and execution of tasks:</p> <ol> <li> <p>Power On: Ensure that the Cablebot is powered on and all systems are operational. Check the battery levels and make sure they are adequately charged before initiating any task.</p> </li> <li> <p>Calibration: Before deploying the Cablebot for any agricultural task, it is essential to calibrate its sensors and actuators. Calibration ensures accurate data collection and precise movements during operation. For manual control and navigation, the Cablebot can be operated using the off-the-sheld RC (Remote Control) included.  </p> </li> </ol> <p></p> <ol> <li> <p>Task Selection: Using the Farmers Dashboard interface, select the specific task you want the Cablebot to perform. The Farmers Dashboard provides an intuitive user interface that allows you to choose from a variety of predefined tasks or create custom tasks.</p> </li> <li> <p>Task Parameters: Depending on the selected task, you may need to input certain parameters such as the target location, or specific actions to be performed. Provide the necessary information through the Dashboard to guide the Cablebot correctly.</p> </li> <li> <p>Initiate Task: Once all the parameters are set, initiate the task and let the Cablebot autonomously carry it out based on your programmed task schedule.</p> </li> <li> <p>Real-time Monitoring: Throughout the task execution, you can monitor the Cablebot's progress in real-time using the Farmers Dashboard. Live camera feeds and status updates will keep you informed about its actions.</p> </li> </ol> <p></p> <ol> <li> <p>Data Collection: The Cablebot is equipped with various sensors to collect relevant data during its operations. This data can be valuable for analyzing farm conditions, crop health, and other important agricultural metrics.</p> </li> <li> <p>Task Completion: Once the Cablebot completes its task, it will either return to its base station or await further instructions, depending on the task's nature.</p> </li> <li> <p>Maintenance: Regularly inspect and maintain the Cablebot to ensure it remains in optimal working condition. Follow the provided maintenance guidelines to keep the robot running smoothly.</p> </li> <li> <p>Task Review and Analysis: After the Cablebot completes a task, review the data collected and analyze the results. The Farmers Dashboard might provide tools for data visualization and analysis to help you make informed decisions.</p> </li> </ol> <p>Some graphical user interfaces are currently under development. For more detailed instructions on the operation and control commands refer to the ongoing software installation guide at Farmers Dashboard Cablebot.</p>"},{"location":"Rover/","title":"The Romi Rover documentation","text":"<p>The documentation for the Romi Rover is split over the following files:</p> File Description Manual The user manual gives an overview of how to use the rover and what it can do. Configuration The doc details the configuration file and the scripts file of the rover. Hardware In the hardware section you can find (1) the drawings for the rover's frame (prototype V3), (2) the 3D designs of the printed components, and (3) the schema and components of the electronics. Software The software section gives an overview of the software of the rover. Librcom This document provides additional details of the <code>rcom</code> communication library that is used by the rover and that may be useful for other projects also. <p>Happy hacking!</p>"},{"location":"Rover/api-nodes/","title":"API of the nodes","text":"<p>The control software of the Romi Rover consists of number of nodes that communicate among each other. A number of topics are defined. These topics are a bit like an API.</p> <p>Besides the topics, the nodes also use different types of communication patterns. In most cases, the interaction with a node is similar to a remote procedure call. The client sends a JSON request over a messagelink (a websocket) to the node. The node replies with a JSON response message. In some cases, a simple HTTP request and response is used instead of a messagelink. In other cases, the node only braodcasts out JSON formatted events over a websocket. The types of communications are listed below.</p> Type Description service Uses the HTML request-response pattern controller Uses RPC over WebSocket streamer Transmits a continuous data flow over HTML messagehub Communicates over a WebSocket datahub Broadcasts messages over UDP <p>Different nodes can implement the same topic. For example, the nodes video4linux, picamera, fake_camera all implement the <code>camera</code> topic. However, only one of these nodes should be active at one time.</p> <p>A nodes can also implement several topics. For example, the motor controller implements both the <code>motorcontroller</code> and <code>encoders</code> topics.</p> <p>This document does not go into the details of messagelinks or how to do an HTTP request. Here we simply document all the available topics, the type of communication, and their messages they expect.</p> <p>In rcom, a communication end-point is identified by the combination of topic and type. So a streamer with the topic <code>camera</code> is distinct from the service with the same topic.</p> <p>The following table lists all the topics and the types that have been defined by Romi Rover. In the sections below you will find more details on each.</p> API Description Type camera Provides RGB images service and streamer camera_recorder Records a sequence of images to disk controller cnc Controls a XYZ motion device controller configuration Exports the configuration file service control_panel Controls the display and power relays controller encoders Broadcasts the encoder values of the wheels datahub fsdb Broadcasts events about newly created files messagehub gimbal Controls the camera mount controller motorcontroller Controls the wheel motors controller navigation Controls the displacement of the rover controller pose Broadcasts the position and orientation of the rover datahub proxy A web proxy to all the nodes and a web server for static pages service &amp; messagehub script_engine Executes scripts service tool_carrier Handles the mechanical weeding tool carrier controller"},{"location":"Rover/api-nodes/#camera","title":"camera","text":"<p>The existing implementations are: <code>video4linux</code>, <code>realsense</code>, <code>picamera</code>, and <code>fake_camera</code>.</p> <p>The camera combines two communication end-points interfaces. It has a service that provides single JPEG images. It also has a streamer interface that broadcasts a continuous stream of JPEG images encoded as a multipart response (the corresponding mimetype is \"multipart/x-mixed-replace\").</p> <p>The two handled URIs are: </p> <ul> <li> <p><code>/camera.jpg</code>: Use this URI to retrieve the latest RGB image from the camera service.</p> </li> <li> <p><code>/stream.html</code>: This URI to get a continuous, video-over-html stream from the camera streamer.</p> </li> </ul>"},{"location":"Rover/api-nodes/#camera_recorder","title":"camera_recorder","text":"<p>The camera_recorder is a controller that connects to a camera. Upon request, it will start recording the images of the camera to disk. It accepts to commands: start and stop.</p>"},{"location":"Rover/api-nodes/#start","title":"start","text":"<p>Start recording the images to disk.</p> <p>Example:</p> <pre><code>{\"command\": \"start\"}\n</code></pre>"},{"location":"Rover/api-nodes/#stop","title":"stop","text":"<p>Stop the recording.</p> <p>Example:</p> <pre><code>{\"command\": \"stop\"}\n</code></pre>"},{"location":"Rover/api-nodes/#configuration","title":"configuration","text":"<p>The <code>configuration</code> service allows nodes to obtain the settings of the rover. A simple HTML request with the name of the settings will return its associated value as a JSON-formatted object.</p> <p>Suppose the configuration file contains the following:</p> <pre><code>{\n\"menu\": {\n\"starters\": [\"velout\u00e9 de champignons\", \"tomato &amp; mozarella\"],\n\"mains\": [\"eggplant lasagna\", \"meatloaf with mashed potatoes\"],\n\"deserts\": [\"chocolate mousse\", \"panna cotta\", \"cr\u00e8me brul\u00e9e\"],\n}\n}\n</code></pre> <p>Then an HTTP request to the URI <code>/menu/starters</code> will return <code>[\"velout\u00e9 de champignons\", \"tomato &amp; mozarella\"]</code> in the body of the response.</p> <p>Implementation note: You can use the function <code>client_get</code> in the <code>rcom</code> API to obtain the value directly as a <code>json_object_t</code>:</p> <pre><code>json_object_t list = client_get(\"configuration\", \"menu/starters\");\n</code></pre>"},{"location":"Rover/api-nodes/#control_panel","title":"control_panel","text":"<p>The control_panel is the controller that interfaces with the physical control panel of the rover. It currently handles to commands: shutdown and display.</p>"},{"location":"Rover/api-nodes/#shutdown","title":"shutdown","text":"<p>Goes through the following steps: Asks the control panel to cut the power circuit (cuts the motors but not the controllers), then initiates the shutdown of the on-board computer, and requests the control panel to cut the power of the logical circuit (with a 5 seconds delay).</p> <p>Example:</p> <pre><code>{\"command\": \"shutdown\"}\n</code></pre>"},{"location":"Rover/api-nodes/#display","title":"display","text":"<p>Asks the control panel to display a message. The length of the message is currently limited to 16 characters. The message may not be displayed immediately, or may be skipped, if too many messages are sent.</p> <p>Example:</p> <pre><code>{\"command\": \"display\", \"message\": \"No network\"}\n</code></pre>"},{"location":"Rover/api-nodes/#cnc","title":"cnc","text":"<p>Existing implementations: <code>grbl</code>, <code>oquam</code>, and <code>fake_cnc</code>.</p> <p>The CNC has a JSON-over-WebSocket controller interface that accepts the commands documented below.  </p>"},{"location":"Rover/api-nodes/#moveto","title":"moveto","text":"<p>Move to a specified absolute position in meters.</p> <p>Example:</p> <pre><code>{\"command\": \"moveto\", \"x\": 0.4, \"y\": 0.4}\n{\"command\": \"moveto\", \"z\": -0.1}\n</code></pre>"},{"location":"Rover/api-nodes/#homing","title":"homing","text":"<p>Move to the home position, if the CNC has limit switches.</p> <p>Example:</p> <pre><code>{\"command\": \"homing\"}\n</code></pre>"},{"location":"Rover/api-nodes/#travel","title":"travel","text":"<p>The <code>travel</code> command serves to make the CNC travel a given path, defined by a list of points in absolute coordinates in meter.</p> <p>Example:</p> <pre><code>{\"command\": \"travel\", \"path\": [[0,0,0], [0.5,0,0], [0.5,0.5,0], [0,0.5,0], [0,0,0]]}\n</code></pre>"},{"location":"Rover/api-nodes/#spindle","title":"spindle","text":"<p>Starts or stops the spindle. A speed can be specified with a value between 0 (stopped) and 1 (maximum speed).</p> <p>Example:</p> <pre><code>{\"command\": \"spindle\", \"speed\": 1}\n</code></pre>"},{"location":"Rover/api-nodes/#encoders","title":"encoders","text":""},{"location":"Rover/api-nodes/#fsdb","title":"fsdb","text":""},{"location":"Rover/api-nodes/#gimbal","title":"gimbal","text":""},{"location":"Rover/api-nodes/#motor_controller","title":"motor_controller","text":""},{"location":"Rover/api-nodes/#navigation","title":"navigation","text":""},{"location":"Rover/api-nodes/#pose","title":"pose","text":""},{"location":"Rover/api-nodes/#proxy","title":"proxy","text":""},{"location":"Rover/api-nodes/#script_engine","title":"script_engine","text":""},{"location":"Rover/api-nodes/#tool_carrier","title":"tool_carrier","text":""},{"location":"Rover/configuration/","title":"Romi Rover configuration","text":"<p>The Romi Rover control software uses the following input files (see also the  Software documentation:</p> <ul> <li> <p>The configuration file: Most of the settings of the rover can be modified in this file.</p> </li> <li> <p>The script file: This file lists all the commands of the rover that   are actionnable by the user, and what these commands should do.</p> </li> </ul> <p>The configuration and script files are discussed in detail below.</p>"},{"location":"Rover/configuration/#configuration-file","title":"Configuration file","text":"<p>The rover control software is starting using the <code>romi-rover</code> command. This command takes the path to the main configuration as an argument. Example configutation files can be found in the <code>romi-rover/config</code> directory.</p> <p>The configuration data is formated in JSON and is a dictionnary of top-level section names and the values for that section. It's overall structure is:</p> <pre><code>{\n\"imager\": \"...\",\n\"navigation\": {\n},\n\"oquam\": {\n},\n\"ports\": {\n},\n\"user-interface\": {\n},\n\"weeder\": {\n}\n}\n</code></pre> <p>The <code>navigation</code> section contains the parameters related to the navigation, from the low-level settings of the motor drivers to the settings for the autonomous navigation. The <code>oquam</code> section contains all the parameters related to the CNC. The <code>ports</code> section lists all of the serial ports and their usage. The <code>user-interface</code> is used to configure such things as the controller to rover speed mapping. Finally, in the <code>weeder</code> section you can change the settings related to the weeding algorithm. We will discuss each of these sections below.</p>"},{"location":"Rover/configuration/#dimensions","title":"Dimensions","text":"<p>As much as possible, the configuration and script files uses meter for distances, seconds for time, m/s for speeds, m/s\u00b2 for acceleration.</p> <p>We deviate from the scientific standard for angles, for which we use degrees instead of radians.</p>"},{"location":"Rover/configuration/#the-navigation-section","title":"The navigation section","text":"<p>An example of a complete navigation section is shown below:</p> <pre><code>{\n\"navigation\": {\n\"rover\": {\n\"wheel-diameter\": 0.47,\n\"wheelbase\": 1.45,\n\"wheeltrack\": 1.35,\n\"maximum-speed\": 3,\n\"maximum-acceleration\": 0.5\n},\n\"brush-motor-driver\": {\n\"encoder-directions\": {\n\"left\": -1,\n\"right\": 1\n},\n\"encoder-steps\": 16000,\n\"maximum-signal-amplitude\": 500,\n\"pid\": {\n\"ki\": [ 3, 100 ],\n\"kp\": [ 100, 100 ]\n}\n},\n\"track-follower\": \"python\"\n}\n}\n</code></pre> <p>The <code>rover</code> section sets some of the physical parameters of the rover:</p> Name Description wheel-diameter The diameter of the motorized wheel, in meter (m) wheelbase The distance between the axes of the front wheel and the back wheel (m) wheeltrack The distance between the back wheels (front wheels), measured from the middle of the left wheel to the middle of the right wheel (m) maximum-speed The maximum allowed speed (m/s) maximum-acceleration The maximum allowed acceleration (m/s\u00b2) <p>The <code>brush-motor-driver</code> object has a number of settings for the firmware that controls the motors:.</p> Name Description encoder-directions This value is an array of two values, the first concerns the left wheel, the second concerns the right wheel. The values are either 1 or -1. In case the value is 1, the encoder values decrement when the wheel is turning forwards. In case the value is 1, the encoder values increment when the wheel is turning forwards. encoder-steps The number of encoder steps for a full turn of the wheel. This should take into account the number of steps of the encoder itself, plus the number of turns of the gearbox. maximum-signal-amplitude The maximum signal that the driver can send to the motor driver. Should be a value &gt; 0 and &lt;= 500; pid These are the values for the PI controller - a PID Controller without the 'D' - used by the driver. Two values are needed: the constants Kp and Ki. They are not specified as floating-point values but as a couple [numerator, denominator]. So [3, 100] means 3/100th. <p>Finally, the <code>track-follower</code> settings defines which algorithm should be used for the track following used for autonomous navigation. Three options are available at the time of writing:</p> Name Description odometry Use the encoders on the wheels. manual Expect manual input from the controller. python This is the line following algorithm currently being tested that follows a line of crops. The name will likely change when the algorithms is integrated in the main software."},{"location":"Rover/configuration/#the-oquam-section","title":"The Oquam section","text":"<pre><code>{\n\"oquam\": {\n\"cnc-range\": [[0, 1.0], [0, 0.75], [-0.4, 0]],\n\"path-maximum-deviation\": 0.01,\n\"path-slice-duration\": 0.02,\n\"stepper-settings\": {\n\"steps-per-revolution\": [200, 200, 200],\n\"microsteps\": [8, 8, 1],\n\"gears-ratio\": [1, 1, 1],\n\"displacement-per-revolution\": [0.04, 0.04, -0.0015],\n\"maximum-rpm\": [300, 300, 300],\n\"maximum-acceleration\": [0.3, 0.3, 0.03]\n}\n}\n}\n</code></pre> Name Description cnc-range The dimensions of the three axes of the CNC. The values must be provided as an array of three couples, for the x, y, and z axis. Each couple specifies the minimum and maximum position in meters. path-maximum-deviation This value, in meters, sets the maximum allowed deviation from the ideal path when the CNC is requested to trace a polygonal path. By allowing a small deviation, the CNC can maintain a given speed while assuring that the maximum accelerations (and thus forces) are respected. It allows for smoother path travelings. path-slice-duration A long path is sliced into small segments of constant speed. This variable sets the default duration of these segments. <p>The <code>stepper-settings</code> provide the information on the stepper motors that are used. The following six pieces of information are required:</p> Name Description steps-per-revolution The number os steps per revolution of the stepper motors, for the x, y, and z axis. microsteps If micro-stepping was enabled, the number of micro-steps for the x, y, and z axis (a value of 1 indicates no micro-stepping). gears-ratio If the stepper motors use gears, provide the gear ratio. 1 means no gearbox is used. A value of N means that N revolutions of the stepper motor are required for 1 revolution of the output axis, or that the driver has to send N times more steps to the stepper motor for the output axis to complete a revolution. displacement-per-revolution This value specifies by how much the CNC moves, in meter, for one revolution of the output axis of the motor + gearbox combination. This value is related to the size of the pulley that pulls on the belt. maximum-rpm The maximum speed of the stepper motors. The datasheets of the stepper motor generaly indicate this value in revolutions per minute (rpm). maximum-acceleration The maximum allowed acceleration, in m/s\u00b2, for each of the axes."},{"location":"Rover/configuration/#the-ports-section","title":"The ports section","text":"<p>This dictionnary lists which firmware drivers are available on what system ports. The rover uses mostly serial connections. The joystick uses a different device, however. The <code>rcdiscover</code> utility can be used to generate this list:</p> <pre><code>$ ./bin/rcdiscover path/to/config.json\n</code></pre> <p>The list has entries as follows:</p> <pre><code>{\n\"ports\": {\n\"oquam\": {\n\"port\": \"/dev/ttyACM5\",\n\"type\": \"serial\"\n}\n}\n}\n</code></pre> <p>First comes the name of the firmware that is accessible through this port. It tells the type of the port ('serial' or 'input-device'), and the device's path in the 'port' field).</p>"},{"location":"Rover/configuration/#the-user-interface-section","title":"The user-interface section","text":"<p>The user interface config contains the settings for the speed controller. There are two drive modes: fast and accurate. Each have their own settings:</p> <pre><code>{\n\"user-interface\": {\n\"speed-controller\": {\n\"accurate\": {\n\"speed-multiplier\": 0.2,\n\"direction-multiplier\": 0.05,\n\"use-speed-curve\": true,\n\"speed-curve-exponent\": 1,\n\"use-direction-curve\": true,\n\"direction-curve-exponent\": 1\n},\n\"fast\": {\n\"speed-multiplier\": 0.3,\n\"direction-multiplier\": 0.15,\n\"use-speed-curve\": true,\n\"speed-curve-exponent\": 1,\n\"use-direction-curve\": true,\n\"direction-curve-exponent\": 1\n}\n}\n}\n}\n</code></pre> <p>The speed controller sets the the speed of the left and right wheel and thus the radius of the turn that rover will follow.</p> <p>The two input values from the controller that set the speed and direction are mapped onto the range [-1, 1]. These normalized values are then used to determine the relative speed of the left and right wheel. The absolute speed of the rover is the product of the relative speed and the absolute speed given in the rover settings discussed above.</p> <p>The relative speed of the left wheel is:</p> <pre><code>speed_left = (speed_multiplier * speed + direction_multiplier * direction);\n</code></pre> <p>where <code>speed</code> and <code>direction</code> are the normalized inputs from the controller. </p> Name Description speed-multiplier This number, with a value between 0 and 1, determines the maximum relative speed when the the speed trigger is completely pressed by the user. direction-multiplier The number defines the maximum speed difference, and thus the maximum turning angle, between the left and right wheel. <p>In addition, it is possible to use an exponential curve to map the speed and direction values. This may give a more smooth behaviour when the rover starts from stand-still to higher speeds. A speed of zero still maps to zero, and a speed of one still maps to one, but in between the mapping follows a curve instead of a straight line. The curve is as follows:</p> <pre><code>x = ((exp(exponent * x) - 1.0) / (exp(exponent) - 1.0));\n</code></pre> Name Description use-speed-curve true of false: should the speed controller use the exponential curve to map the speed value? speed-curve-exponent The exponent of the speed curve. use-direction-curve true of false: should the speed controller use the exponential curve to map the direction value? direction-curve-exponent The exponent of the direction curve."},{"location":"Rover/configuration/#the-weeder-section","title":"The weeder section","text":"<p>The weeder sections contains the settings for the weeder tool. These are as follows:</p> <pre><code>{\n\"weeder\": {\n\"diameter-tool\": 0.04,\n\"imagecropper\": {\n\"workspace\": [\n306,\n210,\n985,\n745\n]\n},\n\"speed\": 0.8,\n\"z0\": -0.22\n}\n}\n</code></pre> Name Description diameter-tool The size of the weeding tool head, in meter. imagecropper.workspace The crop size and position the camera image to obtain the image of the workspace only. speed The relative speed of the weeding tool, between 0 and 1.0. This speed is multiplied by the maximum speed of the CNC to obtain the final speed of the weeding tool. z0 The position of the weeding tool when weeding."},{"location":"Rover/configuration/#script-file","title":"Script file","text":"<p>The script file consists of a JSON array of script objects:</p> <pre><code>[\n{\n\"id\": \"script1\",\n\"title\": \"The First Script\",\n\"script\": [\n//...\n]\n},\n//...\n]\n</code></pre> <p>The script objects have the following fields:</p> Name Value Required Description id String Required The internal name of the script title String Required The name to be shown to end-users script Array Required The list of actions to be taken <p>The script itself is an array that lists all the actions to be taken, one after the other. For example:</p> <pre><code>[\n{\n\"id\": \"script1\",\n\"title\": \"The First Script\",\n\"script\": [\n{\"action\": \"start_recording\"},\n{\"action\": \"move\", \"distance\": 8, \"speed\": 400},\n{\"action\": \"stop_recording\"},\n{\"action\": \"move\", \"distance\": -8, \"speed\": -400}\n]\n},\n//...\n]\n</code></pre> <p>Each action object has an <code>action</code> field. The list of available actions are:</p> Name Description start_recording Start recording the images of the top camera stop_recording Stop recording the images of the top camera move Move a given distance (at a given speed) hoe Start weeding homing Go to the home position (for rovers with a limit switch only) <p>The <code>move</code> action takes the following parameters:</p> Name Value Required Description distance Number (in meters) Required The distance to be travelled speed Number (-1000 &lt; speed &lt; 1000) Optional The speed <p>The <code>moveat</code> action takes a <code>speed</code> value, as in the <code>move</code> command above.</p>"},{"location":"Rover/developer/","title":"Developer Documentation","text":"<p>To download and compile the code, plase refer to the latest doc at the romi-rover-build-and-test code repository.</p>"},{"location":"Rover/hardware/","title":"Hardware Documentation","text":"<p>This document describes the hardware, both the mechanical parts and the electronics, of the Romi Rover for weeding. This document is detailing the third version of the prototype.</p> <p>The design files for the hardware can be found in a dedicated GitHub repository.</p>"},{"location":"Rover/hardware/#the-main-structure","title":"The main structure","text":"<p>The figure below gives an overview of the main components.</p> <p></p> <p>The documentation is split into two main parts: the rover and the weeding tool.</p> <p>For the rover, you will find the detailed drawings of frame in the next section. The mechanical components that we use are listed in the follow-up section. Then, we discuss the electronics and wiring.</p> <p>The weeding tool consists of a CNC, for which we use the 1000 mm XCarve. We replace the z-axis of the original CNC with a new arm adapted for the weeding task.</p> <p>You can find the discussion of the software in a dedicated page of the rover documentation.</p>"},{"location":"Rover/hardware/#the-rover","title":"The rover","text":""},{"location":"Rover/hardware/#the-frame","title":"The frame","text":"<p>The drawings of the frame are available as PDF files. The current version is prototype V3:</p> File Description Overview of the components The main frame. (fr: Ch\u00e2ssis) The left wheel module (fr: Roue motrice gauche) The right wheel module. It is a mirror of the left wheel module. (fr: Roue motrice droite) The arches for the rain and light cover. (fr: Toit) The caster wheel. (fr: Roue folle) In it's latest version, the caster wheel now has a straight fork. This drawing updates and replaces the fork in the previous drawing."},{"location":"Rover/hardware/#the-mechanical-components","title":"The mechanical components","text":"<p>The list of additional components to get the rover up on its feet are as follows. For the back wheels:</p> File Description Brushed DC motors, 24 V, minimum 200 W We are using wheel chair motors for now. We bought a set at Superdroid Robots. You may find similar motors elsewhere, such as on Aliexpress. Incremental encoders: The motors from Superdroid Robots can be purchased with encoders already added to them. If you want to buy them separately, we use the US Digital E2. Wheel shaft: Still from Superdroid Robots we use the wheel shaft that is adapted to the motors. For the back wheels we use wheels with traction lugs for farming equipment with a total diameter of roughly 41 cm diameter or 49 cm. Browsing the catalogs of farming wheels can be confusing because of the different standards and naming conventions, and because the tire and rim are often sold separately. We are using these 4.00-10 tires or 4.00-8 tires. The \"4\" indicates the width of the tire in inch, and the 8 or 10 indicates the diameter of the wheel rim in inch. <p>You'll notice that the our wheel rim has 4 holes while the wheel shaft has 5 holes... We ended up drilling new holes in the wheel shaft that aligned with those of the rim.</p> <p>For the front wheels:</p> File Description The front wheel The 25 mm axis. The adjusting rings. <p>The housing of the battery and the electronics are currently built from wood :) This is because we haven't found a good way, yet, to build waterproof housing inexpensively. Made-to-measure housings are quite expensive. Professional housings with the right size tend to be even more expensive, or they have features such as ribbed walls that make them unsuited for our needs. We will solve this issue in the next version of the prototype.</p> File Description Battery housing 20mm plywood, outside dimensions: WxHxD (width x height x depth) Electronics housing 10mm plywood, outside dimensions: WxHxD (width x height x depth)"},{"location":"Rover/hardware/#the-electronics-and-wiring","title":"The electronics and wiring","text":"<p>An overview of the wiring can be found in the following diagram. The diagram is quite large so you may want to open it in a separate window or download it and open it with Inkscape. Here's the GitHub link of this file.</p> <p></p> <p>The Meanwell DDR-30G-24 transformer converts the battery voltage level to a stable and clean 24V that powers the electronics. It is not powerfull enough to drive the motors (wheels, CNC, hoe) so these are draw there current directly from the battery.</p> <p>The Controllino Mini is a Programmable Logic Controller that manages the power supply to the motors. It uses three inputs to decide whether power should be supplied or not to the motors: the emergency switch, the on/off button, and the status pin of the Raspberry Pi.</p> <p>The Controllino does not provide the power directly to the motors. It uses the 24V relay from Finder to connect or disconnect the battery power. The power relay is designed to handle strong currents and has a protection against sparks and back-currents.</p> <p>We also added a 24V USB hub. It provides four additional USB ports and reduces the power that would be drawn from the USB ports of the Raspberry Pi without it since the hub takes its power from the 24V line.</p> <p>The brain of the rover is the Raspberry Pi 4. We added a HifiBerry shield so that the rover can output sound but also because it has a good 24V to 5V converter onboard. We designed a hat for the level shifter (needed to connect to the Controllino), the real-time clock (RTC), and the connector. You will find the details below.</p> <p>The components inside the electronics housing are fixed on a 50 cm long, 35 mm DIN rail. We also use distribution blocks and rail mounts:</p> File Description 50 cm DIN rail. Example We also use distribution blocks that clips on the DIN rail. Example. These mounting carriers from Wago are also very handy for connecting low-power wires: Wago 222 Series and the their classic splicing connector. Although we have not used them, the terminal blocks by Phoenix Contact may be worth a try. For the 3D printed housings that are clipsed on the DIN rail we use these \"35mm Rail Mounting Bracket\". <p>A more detailed overview of the wiring is shown in the following diagram (TODO: must be updated). Again, you can download the diagram and open it in Inkscape or in a separate browser window, for a more detailed view. </p> <p></p> <p>There are two separate power circuits:</p> <ol> <li>Logic circuit: This circuits powers the embedded PC and other control circuits. </li> <li>Power circuit: This circuit drives all the motors. This is the circuit that is cut when the security switch (the big red button) is pressed.  </li> </ol> <p></p> <p>The power circuits are controller by the Controllino according to the two start-up phases (the PC and the logic circuits start up first, then the motors are powered up). </p> Component Description Non-Latching, protection against sparks and back-current (TODO: be more precise) RS Online Farnell: button and housing The on-off switch Farnell"},{"location":"Rover/hardware/#the-display","title":"The display","text":"<p>The display is used to show status information from the control software. It also allows you to list the menu of actions that are programmed on the rover. The display is connected to the Raspberry Pi over USB. The associated firmware for the Uno is CrystalDisplay.</p> Component Description Arduino Uno We use the following shield with LCD screen: GM Electronic. There are similar clones available online."},{"location":"Rover/hardware/#the-navigation-module","title":"The navigation module","text":"<p>The navigation uses a differential wheel drive, with two motorized wheels in the back and two swivel caster in the front. This makes the control fairly straight-forward and the components are easy to source.</p> <p>The main components are shown below:</p> <p></p> Component Specifications Example Arduino Uno Shield The shield allows you to connect the wires securely. TODO: PCB design. <p>The associated firmware for the Uno is MotorController.</p> <p>TODO: 3D housing for motor controller</p> <p>The motor controller connects to the motor driver. The driver implements a standard H-bridge to control to power supplied to the motors (both forward and backward rotation). We use the Sabertooth 2x60A. There are other motor drivers out there that can be used also. The power input should be 24V and there should be able to drive a lot of current current (&gt; 15 A continuous, 40 A peak).</p> <p>Two PWM control signals are used from the Arduino to the driver, similar to RC input: one for the left and right motor.</p> <p></p> <p>The wiring diagram </p> <p></p> <p>TODO: DIN support for Sabertooth.</p>"},{"location":"Rover/hardware/#the-raspberry-pi","title":"The Raspberry Pi","text":"<p>We wrapped the Raspberri Pi 4 in ints own housing, together with some additional components. First, we have put a HifiBerry Amp2 hat on it. The HifiBerry is a high-end audio amplifier and you can connect two analog audio speakers to it. This is maybe a little extravagant because this board does not come cheap and its usage is currently still limited. Audio feedback is planned for the future, though. Also, we use the HifiBerry as a power adapter because it converts the 24V to the 5V needed by the Raspberri Pi. All this we probably be re-evaluated in upcoming versions.</p> <p>We also added a level-shifter, to communicate with the Controllino. We also added connector plugs and a ventilator to air the housing.</p> Component Link HifiBerry Amp2 Level-shifter: Sparkfun For the connectors, we use the following from Phoenix Contact: 1x 5 way, 1x 2 way, and 1x 4 way Ventilator 12/24V, 30 x 30 x 10 mm Amazon <p>We designed a PCB board to put all pieces together (TODO: files) and a housing to protect it all (TODO: file).</p> <p></p> <p>The software of the rover is available in Github and has its own documentation.</p>"},{"location":"Rover/hardware/#the-camera","title":"The camera","text":"<p>The camera uses the standard Raspberry Pi Zero 2 W together with the Raspberry Pi Camera V2. The software of the camera is part of the main repository and is also documented on the software page.</p> <p>The camera is powered by a 24 V line (plus ground) directly from the Meanwell power adaptor. We designed a PCB board to support the MP1584EN-based 24V-to-5V DC-DC buck converter to power the Pi Zero and we also add a RJ45 connection plug. This</p> <p>The 3D file of housing can be found here.</p> <p>TODO: PCB file</p> <p>The camera is up for a review because we want to switch to the Raspberry Pi HQ Camera.</p>"},{"location":"Rover/hardware/#the-steering","title":"The steering","text":"<p>The steering of the front wheels is currently performed by two NEMA 23 stepper motors with a 1:77 gearbox. The stepper motors are controlled by two standard stepper controller and an Arduino Uno. We also use two absolute rotary encoders from Opkon to adjust for errors.</p> <p>NOTE: at the time of writing, the steering controller has not been integrated into the electronics housing, yet.</p> <p>The coupling of the stepper motors and of the encoder onto the wheel axis is still provisional. We use a coupler to join the wheel axis to the motor. To connect the encoder to the axis, we use a GT2 belt and two 3D printed pulleys, one for the wheel axes and one for the encoder.</p> <p>It is all held together using 3D printed components and two laser-cut aluminium plates. It look roughly like this:</p> <p> </p> <p>The 3D files files are available at romi-rover-design/steering.</p> <p>TODO: link coupler, length GT2 belt</p>"},{"location":"Rover/hardware/#the-weeding-tool","title":"The weeding tool","text":""},{"location":"Rover/hardware/#the-cnc","title":"The CNC","text":"<p>A CNC is adapted for use in the rover. We are using the 1000 mm sized version of the X-Carve. We replaced the spindle that is normally used to carve wooden pieces, with a rotating weeding hoe. </p> <p>The newer X-Carve uses a custom design board for the control. However, we prefer using the older solution that combines an Arduino Uno with a gShield because it is smaller and more generic. It is possible to use other stepper drivers. The drivers must use the standard STEP/DIR control signals.  The associated firmware for the Uno is Oquam.</p> Component Specifications Example Arduino Uno Stepper drivers (3 steppers): gShield <p>For the wiring, you can still have a look at XCarve's older documentation on how to wire the controller boards:</p> <ul> <li>http://x-carve-instructions.inventables.com/xcarve2015/step10/ </li> <li>http://x-carve-instructions.inventables.com/xcarve2015/step14/ </li> </ul> <p>Notable, the following two diagrams are of interest:</p> <p></p> <p></p> <p>The yellow wire marked \"spindle\" in the image above is used to turn the weeding hoe on or off, as shown in the figure below (see also the figure in the section on the power circuit).</p> <p></p>"},{"location":"Rover/hardware/#the-z-axis","title":"The Z-axis","text":"<p>We replaced the default z-axis of the CNC with a longer and stronger axis to carry the weeding tool.</p> <p>The new axis uses standard mechanical elements together with laser-cut plates and 3D printed components. The 2D and 3D designas are available here here. You will find the list of additional components below.</p> <p>The gShield has a pin to turn the spindle on/off. This pin actuates the power relay shown in the list below to start/stop the motor.</p> Component Quantity Specifications Flange Bearing 8mm 2 Motedis KFL08 Trapezoidal threaded TR 8x1,5 630 mm Motedis GT2 pulley 20 teeth 8mm bore 1 Motedis Shaft 2 x 600 mm Motedis Shaft supports 4 Motedis Anti-backlash nut 1 Motedis Linear plain bearing 4 Motedis Linear bearings 4 Motedis Power relay 1 Sparkfun"},{"location":"Rover/hardware/#the-weeding-head","title":"The weeding head","text":"<p>We use a rather simple design for the weeding head. The support can be produced using a 3D printer and the four aluminium blades produced using a professional laser-cutting service. We have drawn a couple of alternative designs but these have not been tested, yet.</p>"},{"location":"Rover/librcom/","title":"rcom","text":"<p>rcom is light-weight C++ libary for inter-node communication. All data is sent over websockets and rcom provides an implementation of both server-side and client-side websockets.</p> <p>rcom offers a low-level API that can be used to build several communication patterns, such the publisher-subscriber pattern (pub-sub), or a message bus.</p> <p>rcom also offers a higher-level API that provides the remote procedure call pattern (RPC). We will discuss this API in more detail first. After that we will present the generic API.</p>"},{"location":"Rover/librcom/#installation","title":"Installation","text":"<p>The installation process follows the classical clone/cmake/make pattern:</p> <pre><code>$ git clone -b ci_dev https://github.com/romi/librcom.git\n$ cd librcom/\n$ mkdir build\n$ cd build\n$ cmake ..\n$ make\n</code></pre> <p>Then run the tests to make sure all is well:</p> <pre><code>$ ctest -V\n</code></pre> <p>To check the code coverage run:</p> <pre><code>$ make librcom_unit_tests_coverage\n$ firefox librcom/librcom_unit_tests_coverage/index.html </code></pre>"},{"location":"Rover/librcom/#using-rcom-for-remote-procedure-calls","title":"Using rcom for remote procedure calls","text":"<p>We will document how to use rcom through C++ API. However, it is possible to combine rcom with code writen in Python or Javascript, among other. We will provide some examples further below.</p>"},{"location":"Rover/librcom/#using-c","title":"Using C++","text":"<p>Suppose that you are writing an application called Madness that controls a bunch of happy monsters on the local network (whatever...). You design an interface called <code>IMonster</code>, as follows:</p> <pre><code>#include &lt;string&gt;\n#include &lt;iostream&gt;\n\nclass IMonster\n{\npublic:\nvirtual ~IMonster() = default;\nvirtual void jump_around() = 0;\nvirtual void gently_scare_someone(const std::string&amp; person_id) = 0;\nvirtual double get_energy_level() = 0;\n};\n</code></pre> <p>All the monsters of your application will derive from this interface, such as the <code>HappyMonster</code> below.</p> <pre><code>class HappyMonster : public IMonster\n{\nprotected:\nstd::string name_;\ndouble energy_;\npublic:\nHappyMonster(const std::string name);\n~HappyMonster() override = default;\nvoid jump_around() override;\nvoid gently_scare_someone(const std::string&amp; person_id) override;\ndouble get_energy_level() override;\n};\n\nHappyMonster::HappyMonster(const std::string name)\n: name_(name), energy_(1.0)\n{\n}\n\nvoid HappyMonster::jump_around()\n{\nstd::cout &lt;&lt; \"Jump around!\" &lt;&lt; std::endl;\n}\n\nvoid HappyMonster::gently_scare_someone(const std::string&amp; person_id)\n{\nstd::cout &lt;&lt; \"Hey \" &lt;&lt; person_id\n&lt;&lt; \", don't watch that. Watch this. \" &lt;&lt; \"This is the happy happy monster show.\"\n&lt;&lt; std::endl;\n}\n\ndouble HappyMonster::get_energy_level()\n{\nreturn energy_;\n}\n</code></pre> <p>You can now write a small application, create a monster, and have it do things.</p> <pre><code>int main(int argc, char** argv)\n{\nHappyMonster monster(\"Elmo\");\nmonster.gently_scare_someone(\"you\");\nreturn 0;\n}\n</code></pre> <p>The full code of this example is split over the following files: monster_simple.cpp, IMonster.h, and HappyMonster.h</p>"},{"location":"Rover/librcom/#the-client-side-application","title":"The client-side application","text":"<p>In the next step we will write a monster that lives in a remote application, either on the same machine but in a different process, or on a remote machine on the local network. We will write a new type of monster, called <code>RemoteMonster</code>.</p> <pre><code>#include \"rcom/RemoteStub.h\"\n#include \"rcom/RcomClient.h\"\n\nclass RemoteMonster : public IMonster, public rcom::RemoteStub\n{\npublic:\nRemoteMonster(std::unique_ptr&lt;rcom::IRPCClient&gt;&amp; client);\n~RemoteMonster() override = default;\nvoid jump_around() override;\nvoid gently_scare_someone(const std::string&amp; person_id) override;\ndouble get_energy_level() override;\n};\n</code></pre> <p>The new class inherits both from <code>IMonster</code> and <code>RemoteStub</code>. The latter is part of rcom. You can also see that the <code>RemoteMonster</code> constructor takes an instance of <code>IRPCClient</code> as an argument. This class represents the connection between the local application and the remote process. As you can see below, this pointer is passed to the constructor of <code>RemoteStub</code> who will use it to send and receive messages. Normally, you should not have to add arguments to the constructor or create additional member variables in the <code>RemoteMonster</code> class because it is just a stub that will forward all requests to the real implementation that lives in a remote process.</p> <pre><code>RemoteMonster::RemoteMonster(std::unique_ptr&lt;rcom::IRPCClient&gt;&amp; client)\n: RemoteStub(client)\n{\n}\n</code></pre> <p>We still have to implement the methods of our example class. They are shown below. </p> <pre><code>void RemoteMonster::jump_around()\n{\nbool success = execute_simple_request(\"jump-around\");\nif (!success) {\nstd::cout &lt;&lt; \"jump_around failed\" &lt;&lt; std::endl;\n}\n}\n\nvoid RemoteMonster::gently_scare_someone(const std::string&amp; person_id)\n{\nnlohmann::json params;\nparams[\"person-in\"] = person_id;\n\nbool success = execute_with_params(\"gently-scare-someone\", params);\nif (!success) {\nstd::cout &lt;&lt; \"gently_scare_someone failed\" &lt;&lt; std::endl;\n}\n}\n\ndouble RemoteMonster::get_energy_level()\n{\ndouble energy_level = -1.0;\nnlohmann::json result;\n\nbool success = execute_with_result(\"get-energy-level\", result);\nif (success) {\nenergy_level = result[\"energy-level\"];\n} else {\nstd::cout &lt;&lt; \"get_energy_level failed\" &lt;&lt; std::endl;\n}\n\nreturn energy_level;\n}\n</code></pre> <p>The implementation mostly calls upon the methods provided by <code>RemoteStub</code>:</p> <ul> <li>Use <code>execute_simple_request</code> for methods that don't take any   arguments and return no values.</li> <li>Use <code>execute_with_params</code> when the caller has to send arguments, but no   return value is expected.</li> <li>Use <code>execute_with_result</code> when there are no arguments but a   value is returned.  </li> <li>Finally, the generic method <code>execute</code> takes arguments for the remote   method and returns a value.</li> </ul> <p>Both the parameters and the return value are sent using the JSON format. The RemoteStub takes care of the encoding the data to a JSON string representation and parsing the incoming string to a C++ JSON data structure. For this rcom uses the JSON library by Niels Lohmann. Check out its documentation to get to know all its features.</p> <p>The various execute methods return <code>true</code> when the remote method was executed successfully and <code>false</code> when an error occured. They do not throw an exception. This leaves the choice up to you whether to throw an exception in response to a failed invokation or not. When an error occured, the <code>RemoteStub</code> will write a message with to the rcom logger. See more on the log system below.</p> <p>NOTE: The other functions, such as <code>RcomClient::create</code> below do throw exceptions.</p> <p>Here is the main function, again, rewriten for the use of the remote monster:</p> <pre><code>int main()\n{\ntry {\nauto client = rcom::RcomClient::create(\"elmo\", 10.0);\nRemoteMonster monster(client);        monster.gently_scare_someone(\"you\");\n} catch (std::exception&amp; e) {\nlog_error(\"main: '%s'\", e.what());\n}\nreturn 0;\n}\n</code></pre> <p>The function <code>rcom::RcomClient::create</code> establishes the connection to a remote object on the local network (or local machine) identified by \"elmo\". The second argument is a timeout for the connection. If \"elmo\" doesn't show up within 10 seconds, the application calls it quits.</p> <p>If the connection is established, it is passed to the <code>RemoteMonster</code> object. The application can then call the <code>IMonster</code> methods as if the remote monster was a normal, local object.</p> <p>The full code of the new version can be found in monster_client.cpp.</p>"},{"location":"Rover/librcom/#the-registry","title":"The registry","text":"<p>If you run the example application above, it will quit with the following error message:</p> <pre><code>ERROR: Socket::connect: failed to bind the socket\nERROR: Socket::Socket: Failed to connect to address 192.168.1.100:10101\nERROR: main: 'Socket: Failed to connect'\n</code></pre> <p>In order for the example above to find the \"elmo\" object, <code>rcom</code> uses another service called the <code>rcom-registry</code>. It is basically a directory service the maps identifiers to IP addresses. You will have to start the service separately:</p> <pre><code>$ ./bin/rcom-registry \nINFO: Registry server running at 192.168.1.100:10101.\n</code></pre> <p>If you run the example application again, it will still quit. This time, after 10 seconds, it will show the error message below:</p> <pre><code>WARNING: MessageLink::connect: Failed to obtain address for topic 'elmo'\nERROR: MessageLink: Failed to connect: elmo\nERROR: main: 'MessageLink: Failed to connect'\n</code></pre> <p>This is normal: we didn't implement and start the remote process, yet. We will look into that in the next session.</p>"},{"location":"Rover/librcom/#the-server-side-application","title":"The server-side application","text":"<p>The remote side - or server side - will receive requests coming from the application that was introduced above. These requests are sent as JSON strings. They have to be parsed and mapped to the methods of the actual C++ object that the remote client wants to address. For this, we will use an adaptor, as follows:</p> <pre><code>int main()\n{\ntry {\nstd::string name = \"elmo\";\nHappyMonster monster(name);\nMonsterAdaptor adaptor(monster);\nauto monster_server = rcom::RcomServer::create(name, adaptor);\n\nwhile (true) {\nmonster_server-&gt;handle_events();\nusleep(1000);\n}\n\n} catch (std::exception&amp; e) {\nlog_error(\"main: '%s'\", e.what());\n}\nreturn 0;\n}\n</code></pre> <p>The <code>MonsterAdaptor</code> instance sits in between the generic <code>RcomServer</code> object and the <code>HappyMonster</code> object. The server will handle incoming JSON requests and call the adapter. The adaptor must map the request to the Monster object. Any return values will be converted to JSON by the server and sent back.</p> <p>The key here is the adapter class. It looks as follows:</p> <pre><code>class MonsterAdaptor : public rcom::IRPCHandler\n{\nprotected:\nIMonster&amp; monster_;\n\npublic:\nMonsterAdaptor(IMonster&amp; monster);\n~MonsterAdaptor() override = default;\n\nvoid execute(const std::string&amp; method, nlohmann::json&amp; params,\nnlohmann::json&amp; result, rcom::RPCError&amp; status) override;\nvoid execute(const std::string&amp; method, nlohmann::json&amp; params,\nrcom::MemBuffer&amp; result, rcom::RPCError &amp;status) override;\n};\n</code></pre> <p>The two <code>execute</code> methods will be called by the server instance. The first one is for JSON text messages. The second one is for methods returning large binary data. The use of binary data will be discussed later.</p> <p>In our example, the <code>execute</code> method checks the value of the <code>method</code> argument and then dispatches the call to the appropriate methods on the \"real\" C++ object:</p> <pre><code>void MonsterAdaptor::execute(const std::string&amp; method, nlohmann::json&amp; params,\nnlohmann::json&amp; result, rcom::RPCError&amp; error)\n{\nerror.code = 0;\nif (method == \"jump-around\") {\nmonster_.jump_around();\n\n} else if (method == \"gently-scare-someone\") {\nstd::string id = params[\"person-id\"];\nmonster_.gently_scare_someone(id);\n\n} else if (method == \"get-energy-level\") {\nresult[\"energy-level\"] = monster_.get_energy_level();\n\n} else {\nerror.code = rcom::RPCError::kMethodNotFound;\nerror.message = \"Unknown method\";\n}\n}\n</code></pre> <p>That's it! The full code of this section can be found here: monster_server.cpp.</p>"},{"location":"Rover/librcom/#run-the-example","title":"Run the example","text":"<p>To run the example, you must first start the <code>rcom-registry</code>:</p> <pre><code>$ build/bin/rcom-registry\nINFO: Registry server running at 192.168.1.100:10101.\n</code></pre> <p>Then, in another shell, you start the server-side application that runs the remote object:</p> <pre><code>$ build/bin/monster_server\n</code></pre> <p>The rcom-registry console should display something like the message below. It shows that the remote server successfully registered with the \"elmo\" identifier.</p> <pre><code>INFO: RegistryServer: Received message: {\"request\": \"register\", \"topic\": \"elmo\", \"address\": \"192.168.1.100:45175\"}\nINFO: RegistryServer: Register topic 'elmo' at 192.168.1.100:45175\n</code></pre> <p>In a third shell, you can now start the client application:</p> <pre><code>$ build/bin/monster_client\n</code></pre> <p>This example application will quit almost immediately because it doesn't do anything other than send a simple message. The console of <code>monster_server</code> should show the following, though:</p> <pre><code>Hey you, don't watch that. Watch this. This is the happy happy monster show.\n</code></pre>"},{"location":"Rover/librcom/#returning-binary-data","title":"Returning binary data","text":"<p>To send binary data in the textual JSON format, it has to be encoded, for example, using the Base64 encoding. This can be quite a performance hit. For example, when the Raspberry Pi Zero has to transmit images, this encoding becomes a showstopper.</p> <p>So, it is therfore possible to return the data as a binary buffer. This is the reason for the second <code>execute</code> method in the adapter class discussed above.</p> <p>On the client side, you will have to do the following: </p> <pre><code>rcom::MemBuffer&amp; MyClass::call_method_with_binary_output(rcom::MemBuffer&amp; buffer)\n{\nnlohmann::json params;\nRPCError error;\n\nbuffer.clear();\nclient_-&gt;execute(\"method-id\", params, buffer, error);\n\nif (error.code != 0) {\n// ...\n}\n\nreturn buffer;\n}\n</code></pre> <p>In the example above, we don't use the <code>execute</code> methods of the stub but directly the <code>execute</code> method of the client connection maintained the stub.</p> <p>Currently, it is only possible to retrive binary data from the server. There is no method, yet, for sending a buffer of binary data to the server. If you have to send binary data, you will have to encode it and sending it as part of the JSON request.</p>"},{"location":"Rover/librcom/#the-generic-api","title":"The generic API","text":"<p><code>rcom</code> provides both server-side and client-side websockets. We'll call them client end-point and server end-points. A separate application, called 'rcom-registry' is a directory server that maintains the list of all server end-points. The rcom-registry application should be launched separately before any other application.</p> <p>The server end-points are identified using a topic, which is a free-form string. The topic should be unique for a given rcom-registry. Client end-points that want to communicate with a server first contact the rcom-registry to obtain the address of the server end-point. The address is simply a combination of IP address and port number. The client can then connect to the server end-point directly.</p> <p>An application can open several server end-points. And a single server end-point can handle many clients.</p> <p>The rcom library does not impose any format on the messages sent back and forth between the client and the server. Since the websocket standard makes a distinction between text-based, so does rcom. But under the hood, rcom is agnostic about the content of the messages.</p>"},{"location":"Rover/librcom/#the-logger","title":"The logger","text":"<p>By default, the rcom libray logs the internal messages, including error messages, to the console. If you are writing a large application, you probably want to redirect these messages to a file or a GUI window. In that case, you can subclass the <code>rcom::ILog</code> interface and inject it into the API functions discussed so far. For example, in the example discussed previously, we created a client connection to a remote object as follows:</p> <pre><code>int main()\n{\n// ...\nauto client = rcom::RcomClient::create(\"elmo\", 10.0);\n// ...\n}\n</code></pre> <p>This can be adapted as follows:</p> <pre><code>#include \"MyLog.h\"\n\nint main()\n{\n// ...\nauto log = std::make_shared&lt;MyLog&gt;();\nauto client = rcom::RcomClient::create(\"elmo\", 10.0, log);\n// ...\n}\n</code></pre> <p>The class <code>MyLog</code> implements the <code>rcom::Ilog</code> interface. It must handle the four types of messages that may be sent by the library as follows:</p> <pre><code>#include &lt;iostream.h&gt;\n#include &lt;rcom/ILog.h&gt;\n\nclass MyLog : public rcom::ILog\n{\npublic:\nMyLog() {}\n~MyLog() override = default;\n\nvoid error(const std::string&amp; message) override {\nstd::cout &lt;&lt; \"MyErr: \" &lt;&lt; message &lt;&lt; std::endl;\n}\n\nvoid warn(const std::string&amp; message) override {\nstd::cout &lt;&lt; \"MyWarn: \" &lt;&lt; message &lt;&lt; std::endl;\n}\n\nvoid info(const std::string&amp; message) override {\nstd::cout &lt;&lt; \"MyInfo: \" &lt;&lt; message &lt;&lt; std::endl;\n}\n\nvoid debug(const std::string&amp; message) override {\nstd::cout &lt;&lt; \"MyDebug: \" &lt;&lt; message &lt;&lt; std::endl;\n}\n};\n</code></pre> <p>Similarly, for the server-side, you can pass your own the <code>ILog</code> object:</p> <pre><code>#include \"MyLog.h\"\n\nint main()\n{\n// ...\nauto monster_server = rcom::RcomServer::create(name, adaptor, log);\n// ...\n}\n</code></pre>"},{"location":"Rover/librcom/#fixed-port","title":"Fixed port","text":""},{"location":"Rover/librcom/#no-registration","title":"No registration","text":""},{"location":"Rover/librcom/#security","title":"Security","text":""},{"location":"Rover/librcom/#specifying-the-address-of-the-registry","title":"Specifying the address of the registry","text":""},{"location":"Rover/librcom/#behind-a-web-server","title":"Behind a web server","text":""},{"location":"Rover/librcom/#http","title":"http","text":""},{"location":"Rover/librcom/#https","title":"https","text":""},{"location":"Rover/librcom/#connecting-from-javascript","title":"Connecting from Javascript","text":"<p>TODO: This section is work in progress (as is most of this documentation BTW).</p> <p>Connecting to a remote object from Javascript is a two-step process:</p> <ol> <li>Create a websocket to rcom-registry to obtain the address of the requested object.</li> </ol> <pre><code>function createRemoteMonster(name, registry)\n{\nvar registrySocket = new WebSocket('ws://' + registry + ':10101');\n\nregistrySocket.onopen = function (event) {\nvar request = { 'request': 'get', 'topic': name };\nregistrySocket.send(JSON.stringify(request));\n};\n\nregistrySocket.onmessage = function (event) {\nconsole.log(event.data);\nvar reply = JSON.parse(event.data);\nif (reply.success) {\nregistrySocket.close();\nmonster = new RemoteMonster(reply.address);\n}\n}\n}\n</code></pre> <ol> <li>Create a websocket to the remote object using the obtained address.</li> </ol> <pre><code>class RemoteMonster\n{\nconstructor(address) {\nthis.socket = new WebSocket('ws://' + address);\nthis.socket.onmessage = (event) =&gt; {\nthis.handleMessage(event.data);\n};\nthis.socket.onopen = (event) =&gt; {\n// ...\n};\n}\n\nhandleMessage(buffer) {\nvar response = JSON.parse(buffer);\nif (response.error) {\nthis.handleErrorMessage(response.error);\n} else if (response.method == 'get-energy-level') {\nconsole.log('RemoteMonster: Energy level ' + response['energy-level']\n}   }\n\nhandleErrorMessage(err) {\nconsole.log('RemoteMonster: Method: ' + response.method\n+ ', Error: ' + response.error.message);\n}  execute(method, params) {\nvar request = { 'method': method, 'params': params };\nvar s = JSON.stringify(request);\nthis.socket.send(s);\n}  jumpAround() {\nthis.execute('jump-around');\n}  gentlyScareSomeone(id) {\nthis.execute('gently-scare-someone', {'person-id': id}};\n}  getEnergyLevel() {\nthis.execute('get-energy-level');\n}  }\n</code></pre>"},{"location":"Rover/librcom/#connecting-from-python","title":"Connecting from Python","text":"<p>The <code>rcom</code> library provides some helper code to exchange data between Python code and rcom objects written in C++. At the current development stage, this Python code has only been used for prototyping during development. The code is not production ready but it may help to get started in your own projects.</p> <p>In the root directory of the rcom repository, you will find a directory called <code>python</code> that contains the Python <code>rcom</code> modules and some examples. You can install the rcom Python code and dependencies as follows:</p> <pre><code>$ cd python\n$ python3 setup.py install --user\n</code></pre>"},{"location":"Rover/librcom/#a-python-client-connecting-to-an-c-rcom-server","title":"A Python client connecting to an C++ rcom server","text":"<p>To run the example, start the rcom-registry server in a new shell:</p> <pre><code>$ bin/rcom-registry\n</code></pre> <p>In another shell, start the remote monster server that we discussed above:</p> <pre><code>$ bin/monster_server\n</code></pre> <p>Finally, run the Python client:</p> <pre><code>$ python3 examples/monster_client.py\n</code></pre> <p>The Python code looks as follows. First, we define a new class <code>RemoteMonster</code> that subclasses the <code>RcomClient</code> from the <code>rcom.rcom_client</code> module.</p> <pre><code>from rcom.rcom_client import RcomClient\n\nclass RemoteMonster(RcomClient):\n\n        def __init__(self, name, registry):\n            super().__init__(name, registry)\n\n        def jump_around(self):\n            self.execute('jump-around')\n\n        def gently_scare_someone(self, person_id):\n            self.execute('gently-scare-someone', {'person-id': person_id})\n\n        def get_energy_level(self):\n            answer = self.execute('get-energy-level')\n            return answer['energy-level']\n</code></pre> <p>TODO: The implementation still requires that you pass the IP address to the registry to the <code>RcomClient</code> instance. You can find the local IP address using this code snippet:</p> <pre><code>import socket\n\ndef get_local_ip():\n    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    s.connect((\"8.8.8.8\", 80))\n    ip = s.getsockname()[0]\n    s.close()\n    return ip\n</code></pre> <p>Calling the remote C++ object is now very straightforward:</p> <pre><code>monster = RemoteMonster('elmo', get_local_ip())\nmonster.jump_around()\nmonster.gently_scare_someone('you')\nenergy = monster.get_energy_level()\nprint(f'energy level is {energy}')\n</code></pre>"},{"location":"Rover/librcom/#overview-of-the-classes-and-code","title":"Overview of the classes and code","text":"<p><code>ILinux</code>, <code>Linux</code>, <code>MockLinux</code>: To facilitate unit testing, the system functions are abstracted in the <code>ILinux</code> interface. The <code>Linux</code> class provides the default implementation, and <code>MockLinux</code> the implementation used for testing.</p> <p>The interface <code>ISocket</code> defines a standard TCP/IP socket API. The class <code>Socket</code> is the default implementation of the API. Similarly, <code>IServerSocket</code> defines the API for a socket that accepts incoming connection. It's default implementation can be found in the <code>ServerSocket</code> class. Both <code>Socket</code> and <code>ServerSocket</code> actually share a lot of functionality. This functionality is grouped together in the class <code>BaseSocket</code>, which encapsulates the standard BSD socket interface. Both Both <code>Socket</code> and <code>ServerSocket</code> delegate most of the methods to <code>BaseSocket</code>.</p> <p>Websockets have there own API, defined in <code>IWebSocket</code>. This interface basically defines the methods to send or receive a message. The <code>WebSocket</code> class provides the default implementation. It uses an <code>ISocket</code> to send and receive data on the TCP/IP connection and then implements the websocket protocol as defined in RFC 6455.</p> <p>Most of the code doesn't create WebSockets directly but uses an instance of <code>ISocketFactory</code> to create them. Again, this facilitates the testing of the code by passing in a <code>MockSocketFactory</code>.</p> <p>The <code>WebSocketServer</code> implements a server that waits for incoming websocket connections and creates a new <code>ServerSideWebSocket</code> after a successful handshake. It also maintains the list of all open connections. This allows to send broadcast messages to all client connected to this server. The <code>handle_events</code> method should be called regularly to deal with the incoming connection requests.</p> <p>We distinguish between server-side and client-side websockets:</p> <ul> <li><code>ServerSideWebSocket</code>: The websocket created on the server-side in response to a new incoming connection.</li> <li><code>ClientSideWebSocket</code>: The websocket created by the client to connect to a <code>WebSocketServer</code>.</li> </ul> <p>Both inherit implementation from the <code>WebSocket</code> class.</p> <p>A <code>MessageHub</code> is like a <code>WebSocketServer</code> with the following additional functionality:</p> <ul> <li>It has a topic name.</li> <li>It registers the topic and its address to the remote registry.</li> </ul>"},{"location":"Rover/librcom/#rpc-classes","title":"RPC classes","text":"<p>IRPCHandler</p> <p>IRPCClient</p> <p>IRPCServer</p> <p>IMessageListener</p> <p>RcomClient</p> <p>RemoteStub</p> <p>RcomServer</p> <p>RcomMessageHandler</p>"},{"location":"Rover/librcom/#todo","title":"TODO","text":"<p>Remote access 4G router, set-up at the farm Managing an fleet of rovers 4G router with a solar panel queue management doc format messages describe format message for different actions: move, path, grab, ... c++ -&gt; Python</p>"},{"location":"Rover/manual/","title":"ROMI Rover User Manual","text":"<p>Draft v2 - Rover V3 - Summer 2022 </p> <p>This manual describes the usage of a fully assembled and ready-to-use rover.</p> <p>The information on how to build your own rover can be found in a separate document. Visit the ROMI web site at https://docs.romi-project.eu for more information.</p> <p>\u00a9 Sony Computer Science Laboratories - CC BY-SA 4.0 Licence</p>"},{"location":"Rover/manual/#acknowledgements","title":"Acknowledgements","text":"<p>ROMI has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 773875.</p>"},{"location":"Rover/manual/#short-description","title":"Short description","text":"<p>The ROMI Rover is a farming tool that assists vegetable farmers in maintaining vegetable beds free of weeds. It does this by regularly hoeing the surface of the soil and thus preventing small weeds from taking root. It can do this task mostly autonomously and requires only minor changes to the organisation of the farm. It is designed for vegetable beds between 70 cm and 110 cm wide (not including the passage ways) and for crops up to 50 cm high. It currently handles two types of crops, lettuce and carrots. The lettuce can be planted out in any layout, most likely in a quincunx pattern. In this configuration the rover uses a precision rotary hoe to clean the soil both between the rows and the plants. For carrots, the rover uses classical mechanical tools, such as stirrup hoe, to regularly clean the soil in between the rows. In this configuration, the carrots should be sown in line.</p> <p>A weekly passage of the robot should be sufficient to keep the population of weeds under control.</p> <p>In addition to weeding, the embedded camera can be used to collect images of the vegetable beds.</p> <p>The ROMI Rover is targeted at farms that grow small crops, such as lettuce and carrots, on a relatively small surface, between 200 m\u00b2 and 5 ha (Utilised Agricultural Area).</p>"},{"location":"Rover/manual/#technical-specifications","title":"Technical specifications","text":"<p>This section details the technical specifications, operating instructions and configuration of the current version of the Romi rover: V3.</p> Characteristic Value Dimensions Width: Adjustable between 1.42 m and 2 mLength: 1.8 mHeight: 1.44 mWith tool carrier: add 1 m to length Weight 140 kg Battery life 8 h (approx. dependent on use case) Charging time TODO Weeding speed Precision weeding: 235 m\u00b2/dayClassical weeding: 6400 m\u00b2/day Width vegetable beds Between 0.7 and 1.1  m Handled crops Precision weeding: LettuceClassical weeding: Carrots Turning space at end of bed 2.8 m (3.5 m with tool carrier)"},{"location":"Rover/manual/#functional-specifications-and-requirements","title":"Functional specifications and requirements","text":"<p>The following configuration is required for the use of the ROMI rover.</p> Image Price Features/Function ROMI rover with a remote control, a battery charger, and a protective cover 5000 \u20ac (estimate) - Prevents weed development- Takes image scans of the beds"},{"location":"Rover/manual/#overview-of-the-components","title":"Overview of the components","text":"<ul> <li>The control panel: The control panel provides a means to view status messages, and request the rover to perform a preconfigured action.</li> <li>The on/off switch: The on/off switch can be found on the electronics   housing.</li> </ul>"},{"location":"Rover/manual/#operation-instructions","title":"Operation instructions","text":""},{"location":"Rover/manual/#overview-of-the-rovers-usage","title":"Overview of the rover\u2019s usage","text":"<p>The basic usage of the rover is to position it on a vegetable bed and let the machine clean the top-soil with a rotating precision hoe. The rover must be taken to the field using the remote control. The robot currently expects the vegetables to be grown in \"beds\" of 0.7 m to 1.1 m wide.  The robot is designed for smaller market farms of less than 5 ha but the size of the farm depends on the number of rovers that you will use, and the amount of crop you want to cover. The rover can navigate autonomously along a bed if there is a clear line of crops that it can follow.</p> <p>Once the rover is positioned at the beginning of a bed, it hoes the surface of the soil so that small weeds cannot take root. It can perform this action along the entire length of the bed. Note that the rover cannot remove mature weeds that have already established themselves. It is therefore necessary to start with a vegetable bed that has been cleaned from all weeds. This can be done with various classical techniques to prepare the vegetable beds. Once the beds are clean, the rover can be used to maintain them as such.</p> <p>Two weeding methods are available. First, a precision weeding method in which the top-soil is turned over in between the rows (also called \"inter-row\") and between the plants (or \"intra-row\"). Second, a classical weeding method in which standard weeding tools are dragged behind the rover between rows of vegetables.</p> <p>For the precision weeding method, the rover uses a camera to detect the plants that are underneath the rover. It then moves the precision weeding tool over the surface whilst passing closely around the detected vegetables.</p> <p>Although the rover is autonomous for weeding a single bed, it is important to stay in proximity to the rover. A U-turn must also be manually performed at the end of the bed and the rover repositioned in line with the rails of the next bed.</p>"},{"location":"Rover/manual/#setting-up-the-vegetable-beds","title":"Setting up the vegetable beds","text":"<p>The use of the rover requires relatively flat beds. Precision weeding works best if the surface of the culture beds is planar. Ideally, the alleys between the beds should also be flat, to facilitate navigation of the rover. Otherwise, there is a risk that the tool will detach from the soil or that it will dig into the soil. There is no precise measure of how flat the beds should be but small holes in the ground should be avoided. The presence of stones should also be avoided. Small stones (approx. 1 cm) should not perturb the rover very much.</p> <p>NOTE: It is more convenient if the width of the vegetable beds are constant so that the width of the rover doesn't have to be adapted beds.</p>"},{"location":"Rover/manual/#setting-up-the-wi-fi-access-point","title":"Setting up the Wi-Fi access point","text":"<p>The use of a Wi-Fi access point is optional but strongly recommended. The rover must be connected to a Wi-Fi access point with Internet access for the following functionality:</p> <ul> <li>To automatically upload the images taken by the rover to the   Farmer\u2019s Dashboard web application.</li> <li>For remote maintenance.</li> </ul> <p>Both features are optional and can be left out when the rover is used for weeding only.</p> <p>However, if you decide to use an access point, it is important that the Wi-Fi signal is strong enough in all the zones where the rover will be used. If not, it may be impossible to connect to the rover\u2019s web interface with a phone or tablet to send instructions to the rover. It will still be possible to send instructions to the rover using the control panel (see \"Controlling the rover through the control panel\").</p> <p>The set-up of the Wi-Fi network is not part of the ROMI Rover package. In case of doubt, you should seek advice from a professional about the best solution for your premises. However, below, we briefly discuss several options.  Use an existing Wi-Fi router: If the zone where you wish to use the rover is adjacent to existing infrastructure (home, barn) and you have the possibility to install an Internet connection at the premises (ADSL modem over a phone line or any other solution), the Wi-Fi capabilities of the modem can be used to offer Internet access to the rover.</p> <p>Expand the reach of an existing Wi-Fi network: An existing Wi-Fi   can be extended to increase its reach using Wi-Fi range   extenders. They pick up and retransmit an existing Wi-Fi   signal. Most extenders require a standard power supply although some   can be powered using an USB battery. Using an Ethernet cable of up   to 100 metres long, it is possible to position a secondary access   point. Some of the Wi-Fi access points can be powered directly over   the Ethernet cable (PoE, Power over Ethernet) removing the need for   a power socket. It is also possible to send the network signal over   existing electricity cables using a technology called power-line   communication (PLC). Finally, there exist also long-range wireless   outdoor WiFi extenders that transmit the network between two   antennas designed for transmission over distances from a 100 metres   to over a kilometre.</p> <p>Install a GSM Wi-Fi router: If there is a good mobile phone signal   strength in the field, a GSM Wi-Fi router is a viable option. A GSM   Wi-Fi router connects to the Internet over a mobile data link (4G,   3G, HSDPA\u2026) and provides access to other devices over   Wi-Fi. Separate routers with good antennas can be purchased at   reasonable prices but generally require a power plug. Smaller,   USB-powered routers are available also and can be plugged directly   into a USB port inside the rover. A mobile phone configured as a   hotspot is an alternative solution (although with a smaller range   than a dedicated router with good antennas). The downside of this   option is that it requires a SIM card and a subscription with a   mobile network operator.</p> <p>Using a USB GSM modem: In contrast to the solution above, a USB   GSM modem is not a stand-alone router but, when plugged in, the   Raspberry Pi will see the modem as an additional network   interface. In this set-up, the rover remains the hotspot for the   Wi-Fi network but will route any Internet traffic through the GSM   modem. This solution may require additional changes to the network   configuration of the Raspberry Pi.</p>"},{"location":"Rover/manual/#charging-the-rover","title":"Charging the rover","text":"<p>The rover uses two 12 V Lithium batteries (the internal working voltage is 24 V). Use the supplied Victron Energy Blue Smart IP67 24V 5A Charger to reload the batteries. Plug 230 V side of the charger in a regular power plug. The 24 V side must be plugged into the POWER CHARGER plug on the battery box. The charger has LED indicators to show the status of the charging cycle. It is also possible to follow the status using a mobile phone using a Bluetooth connection. Check the official manual provided by Victron Energy charger for details.</p>"},{"location":"Rover/manual/#protection-cover","title":"Protection cover","text":"<p>The rover comes with a PVC protection cover. The cover must always be placed on the rover when the precision weeding is used. If the precision weeding is not used, it can be removed if there is no risk of the CNC becoming wet. The CNC, on its own, is not waterproof. If the CNC is removed, it is possible to use the rover without cover in light rain conditions (TODO: IP level?...)</p>"},{"location":"Rover/manual/#attendance-todo-regulations","title":"Attendance (TODO: regulations?)","text":"<p>IMPORTANT: The rover must be used only in the presence of an operator. The operator must be within a distance of XXX metres of the rover and must be able to reach the rover quickly in case of an emergency. The operator should carry the remote control with them at all times whilst the rover is on. This is in order to be able to recover the navigation control of the rover in all circumstances (TODO: add emergency button on the remote control or use certified controller for industrial use). The rover should not be used in proximity to people who have not been instructed to use the rover (TODO).  IMPORTANT: The rover must be used only during the day in good light conditions.</p>"},{"location":"Rover/manual/#storage","title":"Storage","text":"<p>The rover should be kept in a covered and dry space when not in use.</p>"},{"location":"Rover/manual/#emergency-button","title":"Emergency button","text":"<p>The emergency button on the back of the rover can be used to cut the power to the motors and CNC at any time. To cut the power, push the red button. To power up the motors, the button must be reactivated. This can be done by pulling the button out again.</p> <p>CAUTION: Before reactivating the button, make sure that the CNC and wheel motors are not moving.</p>"},{"location":"Rover/manual/#adjusting-the-width-of-the-rover","title":"Adjusting the width of the rover","text":"<p>The wheel-base of the rover can be adjusted to fit the variable width of the rails and beds. To adjust the width of the rover, loosen the four U-brackets that fix the wheel modules to the main frame and slide the wheel modules to the desired position. Ensure that the position of the modules is symmetric relative to the main frame.</p> <p>After a change to the width of the wheel-base, the CNC may have to be recalibrated if the weeding tool should work on a larger or smaller bed size (see \"Calibrating the CNC\")</p>"},{"location":"Rover/manual/#engagingdisengaging-the-motor-lock-levers-freewheeling-mode-and-drive-mode","title":"Engaging/disengaging the motor lock levers (freewheeling mode and drive mode)","text":"<p>The two wheel motors each have a lock lever that allows them to switch between freewheeling mode or motor drive mode. When the lock lever is in the horizontal position the wheels are freewheeling. In freewheeling mode, the robot can be moved simply by pushing it. Turn the lever 90\u00b0 into the vertical position - pointing to the ground - to switch the drive mode. In the drive mode, the wheels are powered by the motors and to move the rover you must use the remote control or the command interface.</p> <p>Mnemonic tip: When the handle points to the ground, the traction goes to the soil. When the handle is parallel to the axis, the traction \"stays\" in the axis.  CAUTION: Only switch to the drive mode when the rover is \"off\" to assure that the motors are powered off.</p> <p>Lock lever vertical: Motor drive mode </p> <p>Lock lever horizontal: Freewheeling mode </p>"},{"location":"Rover/manual/#control-panel-state-message","title":"Control panel state message","text":"<p>The display of the control panel is divided in two lines. The upper line shows current status of the rover:</p> <ul> <li> <p>Ready: The rover is ready for use. The on-board computer is     running and the motors can be powered up.</p> </li> <li> <p>Navigating: The rover is in navigation mode. Use the controller     to steer the rover.</p> </li> <li> <p>System Failure: The rover encountered an error from which it     cannot recover. Please restart the rover.</p> </li> </ul>"},{"location":"Rover/manual/#start-up-procedure","title":"Start-up procedure","text":"<p>If it is the first usage of the rover, you should go to the section \"First time configuration\".</p> <p>Before starting up, the rover should be in the following state:</p> <ul> <li>Verify that the emergency button is deactivated (pushed in).</li> <li>Verify that the on/off switch on the electronics housing is off.</li> </ul> <p>The start-up can now proceed:</p> <ol> <li>Engage the lock levers on the motor to put the motors into drive mode.</li> <li>Turn the on/off button on the electronic housing on. </li> <li>Activate the emergency by pulling it out.</li> <li>The rover should begin the start-up sequence.</li> <li>When the start-up is completed, the display will show \"Ready\". </li> <li>The motors of the wheels and the CNC are now powered up. </li> </ol> <p>You can now use the controller of the web interface to navigate the rover or send commands to the rover.</p>"},{"location":"Rover/manual/#shut-down-procedure","title":"Shut-down procedure","text":"<p>To turn off the rover, take the following steps:</p> <ol> <li>Deactivate the emergency button by pushing it in. This removes the power supply to the motor and CNC.</li> <li>Turn off the on/off button on the electronics housing</li> </ol>"},{"location":"Rover/manual/#switching-from-drive-to-freewheeling-mode","title":"Switching from drive to freewheeling mode","text":"<p>When the rover is in drive mode (motors powered on, the lock levers on the motor engaged/vertical), it is possible to go to freewheeling mode as follows:</p> <ol> <li>Turn off the power of the motors by pressing the red emergency button.</li> <li>Turn the motor lock levers in the horizontal position (disengaged).</li> </ol> <p>Once these steps are completed, you can move the rover by pushing it.</p>"},{"location":"Rover/manual/#switching-from-freewheeling-to-drive-mode-todo-this-is-not-safe-yet","title":"Switching from freewheeling to drive mode (TODO: this is not safe, yet)","text":"<p>To switch from freewheeling to drive mode, the rover\u2019s state should be \u201dOn\u201d, the user may then perform the following operations:</p> <ol> <li>Turn the motor lock levers in the vertical position (engaged).</li> <li>Pull the red security button to power the motors.</li> </ol> <p>CAUTION: Make sure that the speed and direction controllers of the remote control are in the neutral position.</p>"},{"location":"Rover/manual/#accessing-the-web-interface-of-the-rover","title":"Accessing the web interface of the rover","text":"<p>Adapt the Wi-Fi settings of the phone, tablet, or computer such that it connects to the same Wi-Fi network as the rover. The interface of the rover is accessible through a web browser. On the mobile device, open up your preferred web browser (see \"Supported web browsers\") and in the address field enter the following URL: http://romi-rover.local.</p> <p>To facilitate access to the interface in future uses, you can add the address to your bookmarks.</p>"},{"location":"Rover/manual/#using-the-controller","title":"Using the controller","text":"<p>The rover comes with the Sony DUALSHOCK\u00ae4 Wireless Controller. </p> <p></p> <p>The remote controller must be recharged using a micro USB cable that is plugged into the front of the controller. The controller is sensitive to the recharger. Make sure to use a quality recharger.</p>"},{"location":"Rover/manual/#controlling-the-rover-through-the-control-panel","title":"Controlling the rover through the control panel","text":"<p>You can send commands to the rover or activate the navigation mode using the controller as follows.</p>"},{"location":"Rover/manual/#send-a-command-to-start-an-action","title":"Send a command to start an action","text":"<p>The rover must be in the Ready state. Then press the Show Menu button.</p> <p>The name of the first task will appear on the bottom line of the display. Use the Next Menu and Previous Menu buttons to navigate in the list of possible tasks. To cancel and return to the main screen, press Cancel Button.</p> <p>Press the Select Button a second time to confirm the action, or press cancel to return to the menu screen.</p>"},{"location":"Rover/manual/#switching-to-navigation-mode","title":"Switching to navigation mode","text":"<p>XXX</p>"},{"location":"Rover/manual/#changing-the-list-of-rover-actions","title":"Changing the list of rover actions","text":"<p>The file uses the JSON format to describe the list of scripts and the associated sequences of actions. The general structure is as follows:</p> <pre><code>[\n{\n\"name\": \"move-forward\",\n\"display_name\": \"Forward\",\n\"script\": [\n{ \"action\": \"move\", \"distance\": 0.60 }\n]\n},\n{\n\"name\": \"move-backward\",\n\"display_name\": \"Backward\",\n\"script\": [\n{ \"action\": \"move\", \"distance\": -0.60 }\n]\n},\n{\n\"name\": \"scan\",\n\"display_name\": \"Scan\",\n\"script\": [\n{ \"action\": \"start_recording\" },\n{ \"action\": \"move\", \"distance\": 3.6 },\n{ \"action\": \"stop_recording\" }\n]\n}\n]\n</code></pre> <p>The file contains a list of scripts. Each script has a name that is used to identify the script, a display_name that is shown in the user interface, and a script field that consists of a list of actions. The list of available actions and their parameters is out of the scope of this manual. Please refer to the online documentation at https://docs.romi-project.eu/Rover/configuration/ for details.</p> <p>If you make modifications to the file, it is very important that the new content is a valid JSON file. If not, the rover will fail to load the file and no buttons will be shown in the user interface.</p>"},{"location":"Rover/software/","title":"Software Installation","text":""},{"location":"Rover/software/#overview","title":"Overview","text":"<p>This document describes how to run and compile the software for the ROMI Rover. </p>"},{"location":"Rover/software/#prerequisites","title":"Prerequisites","text":"<p>The software of the rover runs on Linux. It is not tied to a specific Linux distribution but we have tested it mostly on recent versions of Debian (includin Raspian) and Ubuntu.</p> <p>The software is mostly writen in C++ and depends on the following libraries:</p> <ul> <li> <p>rcom: An inter-process communication framework. It provides   real-time communication using UDP messages and high-level   communication based on web protocols (HTTP, Websockets). The code is   available on GitHub and separate   documentation is   available also.</p> </li> <li> <p>libromi: The library with thd base classes for the romi rover:   Code. This repository also   contains the firmware for most of the microcontrollers used in the   Rover, Plant Imager, and Cablebot. </p> </li> <li> <p>romi-rover: All of the apps for the Romi rover, including   <code>romi-rover</code>, <code>romi-camera</code>, and <code>romi-cablebot</code>.   Code</p> </li> </ul>"},{"location":"Rover/software/#installing-a-raspberry-pi-from-scratch","title":"Installing a Raspberry Pi from scratch","text":"<p>We use the Lite version of Raspbian. You can download it at https://www.raspberrypi.org/downloads/raspbian/. There are several ways to prepare the disk image for the RPi. Check the page at https://www.raspberrypi.org/documentation/installation/installing-images/ (there\u2019s lots of information available on this topic online) and follow the instructions that suit you best.</p> <p>Once you have the SD card, connect RPi to screen, keyboard and network (ethernet), power up the board and log in (user pi, password raspberry). The first thing you want to do is change some of the default settings using the raspi-config tool. In the console type:</p> <pre><code>$ sudo raspi-config\n</code></pre> <p>The list of settings that you may want to look at includes:</p> <pre><code>1 Change User Password\n2 Network Options\n   Hostname\n   WiFi\n4 Localisation Options\n   Change locales\n   Change keyboard layout\n5 Interfacing Options\n   Enable SSH\n8 Update\n</code></pre> <p>Next, create the user \u2018romi\u2019: </p> <pre><code>$ sudo adduser romi\n$ sudo adduser romi dialout\n$ sudo adduser romi video\n$ sudo adduser romi sudo\n</code></pre> <p>After that, quit the current session and login again as user \u2018romi\u2019.</p> <p>The nano text editor is installed by default but if you prefer anoher editor, now is a good time to install it:</p> <pre><code>$ sudo apt install emacs-nox (... or any editor you like :)\n</code></pre> <p>Install the developer tools: <pre><code>$ sudo apt install build-essential cmake git\n</code></pre></p> <p>Install the software dependencies:  <pre><code>$ sudo apt install libpng-dev libjpeg9-dev\n</code></pre> That's it. You should be ready.</p>"},{"location":"Rover/software/#installing-the-romi-rover-apps","title":"Installing the romi-rover apps","text":"<p>The Reaspberry Pi comes with a file calles <code>/etc/ld.so.preload</code> that preloads an library for all applications. This mechanism conflicts with our use the Address Sanitizer to detect memory errors. So you should hide the existing file:</p> <pre><code>$ mv /etc/ld.so.preload /etc/ld.so.preload.backup\n</code></pre> <p>The installation has it own documentation on github.</p>"},{"location":"Rover/software/#compiling-the-romi-camera","title":"Compiling the romi-camera","text":"<p>The <code>romi-camera</code> application is used in the Rover, Cablebot, and Plant Imager. Because the camera uses the Raspberry Pi Zero, which has less RAM memory than its larger siblings.</p> <p>The installation proceeds as described in the documentation of romi-rover-build-and-test. However, it is not necessary to compile all applications and limit this step to the camera app only:</p> <pre><code>$ mkdir build\n$ cd build/\n$ cmake ..\n$ make romi-camera\n</code></pre>"},{"location":"Rover/software/#starting-up-the-software-manually","title":"Starting up the software manually","text":"<p>As explained in the rcom documentation, the <code>rcom-registry</code> has to be started before the other applications. It can be started remotely using ssh as follows:</p> <pre><code>$ ssh romi@camera.local /home/romi/romi-rover-build-and-test/build/bin/romi-camera \\\n--registry &lt;IP-ADDRESS-REGISTRY&gt; --mode video --fps 5 --bitrate 12000000\n</code></pre> <p>In the second step, the <code>romi-camera</code> should be started up. The rcom c onnections use a time-out so there is some flexibility in the start-up order of the applications.</p> <pre><code>$ ./build/bin/rcom-registry\n</code></pre> <p>If you use any Python code, such as the Unet neural network for the image segmentation, the following script can be used: </p> <pre><code>$ python3 ./application/romi-python/main.py \\\n--model-path &lt;PATH-UNET-WEIGHTS&gt; </code></pre> <p>Lastly, the <code>romi-rover</code> application should be started.</p> <pre><code>$ ./build/bin/romi-rover --config $CONFIG_FILE \\\n--session $SESSION_DIR --script $SCRIPT_FILE\n</code></pre> <p>A more elaborate example can be found in this script. It should be called indirectly using the <code>intra-row-weeding-tbm</code> script:</p> <pre><code>$ cd scrips\n$ ./intra-row-weeding-tbm\n</code></pre>"},{"location":"Rover/software/#starting-the-apps-on-boot","title":"Starting the apps on boot","text":"<p>Currently we are still using the old rc.local mechanism. The file /etc/rc.local is no longer included in more recent Ubuntu versions. If <code>ls /etc/rc.local</code> returns an error, you will have to create the file as follows:</p> <pre><code>$ sudo nano /etc/rc.local\n</code></pre> <p>Copy the following contents:</p> <pre><code>#!/bin/sh -e\n#\n# rc.local\n#\n# This script is executed at the end of each multiuser runlevel.\n# Make sure that the script will \"exit 0\" on success or any other\n# value on error.\n#\n# In order to enable or disable this script just change the execution\n# bits.\n#\n# By default this script does nothing.\n\nexit 0\n</code></pre> <p>Finally, make the script executable. <pre><code>$ sudo chmod +x /etc/rc.local\n</code></pre></p> <p>To enable the apps on start-up, add the following line in /etc/rc.local, above the <code>exit 0</code> line:</p> <pre><code>sudo -u romi PATH-TO-STARTUP-SCRIPT &amp;\n</code></pre>"},{"location":"Rover/software/#rcom-interfaces","title":"Rcom interfaces","text":""},{"location":"Rover/software/#camera","title":"Camera","text":"<p>The CNC interface is exported by the <code>romi-camera</code> application.</p> Method Parameters Return Description camera:grab-jpeg-binary None A binary buffer with a JPEG-encoded image camera:set-value name: the name of the setting, value: the numerical value None camera:select-option name: the name of the option, value: the value as a string None"},{"location":"Rover/software/#cnc","title":"CNC","text":"<p>The CNC interface is exported by the <code>oquam</code> application.</p> Method Parameters Return Comments cnc-homing None None Starts the homing procedure that puts the CNC's arm in the home position cnc-moveto x, y, z: the position to move to, speed: the relative speed, as a fraction of the absolute speed None cnc-spindle speed: the speed, between 0 and 1 None cnc-travel path: a list of [x, y, z] points, speed: the relative speed None cnc-get-range None The dimensions of the CNC, as [[xmin, xmax], [ymin, ymax], [zmin, zmax]] cnc-helix xc, yc: the center point of the arc, alpha: the angle of the arc, z: to z-position to move to, speed: the relative speed None cnc-get-position None Returns the position as"},{"location":"Rover/UserManual/","title":"Index","text":"<p> <p></p> <p> </p> <p> Romi Rover   User Manual </p> <p>Draft - September 2020 </p> <p></p> <p></p>"},{"location":"Rover/UserManual/#contents","title":"Contents","text":"<p>This manual describes the usage of a fully assembled and ready to use rover. The information on how to build your own rover can be found in a separate document. Visit the ROMI web site at https://docs.romi-project.eu for more information.</p> <p>This manual consists of the following chapters:</p> <ul> <li>Copyright</li> <li>Legend</li> <li>Short Description</li> <li>Technical Specifications</li> <li>Functional Specifications and Requirements</li> <li>Operating Instructions</li> </ul>"},{"location":"Rover/UserManual/copyright/","title":"Copyright","text":""},{"location":"Rover/UserManual/copyright/#copyright","title":"Copyright","text":"<p>Copyright \u00a9 Sony Computer Science Laboratories</p>"},{"location":"Rover/UserManual/copyright/#license","title":"License","text":"<p>The documentation is available under the CC BY-SA 4.0 License</p>"},{"location":"Rover/UserManual/copyright/#acknowledgements","title":"Acknowledgements","text":"<p>ROMI has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 773875.</p>"},{"location":"Rover/UserManual/copyright/#credits","title":"Credits","text":"<p>TODO</p>"},{"location":"Rover/UserManual/copyright/#disclaimers","title":"Disclaimers","text":"<p>TODO</p>"},{"location":"Rover/UserManual/description/","title":"Description","text":""},{"location":"Rover/UserManual/description/#short-description","title":"Short Description","text":"<p>The ROMI Rover is a farming tool that assists vegetable farmers in maintaining the vegetable beds free from weeds. It does this by regularly hoeing the surface of the soil and thus preventing small weeds from taking roots. It can do this task mostly autonomously and requires only minor changes to the organization of the farm. It is designed for vegetable beds between 70 cm and 120 cm wide (not including the passage ways) and currently handles two types of crops, lettuce and carrots. The lettuce can be planted out in any layout, most likely in a quincunx pattern. In this configuration the rover uses a precision rotary hoe to clean the soil both between the rows and the plants. For the carrots, the rover uses classical mechanical tools, such as stirrup hoe, to regularly clean the soil in between the rows. In this configuration, the carrots should be sown in line. </p> <p>A weekly passage of the robot should be sufficient to keep the population of weeds under control. In addition to the weeding, the rover provides the following useful functions. It can draw quincunx patterns or straight lines in empty vegetable beds to speed up seeding and planting out. The embedded camera can be used to collect images of the vegetable beds. It can also be used as a motorized  tray.</p> <p>The ROMI Rover is targeted at farms that grow lettuce and carrots on a relatively small surface, between 200 m\u00b2 and 5 ha (Utilized Agricultural Area).</p>"},{"location":"Rover/UserManual/legend/","title":"Legend","text":"<p>The draft of the manual uses the following labels to indicate the status of the development:</p> <ul> <li>[v2] The feature refers to the second prototype and its implementation will be different in the third prototype. </li> <li>[v3] The feature refers to the third prototype and exists in a different form in the second prototype. </li> <li>[v2++] The feature will be implemented in the second prototype (deadline: December 2020)</li> <li>[v3++] The feature will be implemented in the third prototype (deadline: May 2021).</li> </ul>"},{"location":"Rover/UserManual/requirements/","title":"Functional Specifications and Requirements","text":"<p>The following configuration is required for the use of the ROMI rover.</p> Profile Price Features/Function Bed of vegetables  Width of the bed:  <ul> <li>min 0.7 m </li> <li>max 1.2 m  [v3]</li> </ul> Handled crops  <ol> <li>Lettuce: The lettuce can be planted out in any layout, most likely in a quincunx pattern   <li>Carrots: The carrots should be sown in line.   </li> ROMI rover with a remote control, a battery charger, and a protective cover  [v3]  5000 \u20ac (estimate) <ul> <li>Prevents weed development <li>Draw patterns for seeding and planting out <li>Takes image scans of the beds </li> Tool carrier Photo 1000 \u20ac (estimate) Used with the rover to carry the mechanical weeding tools (optional but needed in most configurations) Mechanical weeding tools (from Terrateck or other manufacturers) 400-1000\u20ac (estimate) The weeding tools that will be fixed on the tool carrier. Sold by other manufacturers, for example, by [Terrateck](https://www.terrateck.com) ([catalogue](https://www.terrateck.com/en/16-houes-pousse-pousse)) TOTAL cost of an equipped rover 6400 \u2013 7000 \u20ac Guides Photo <ul> <li>Stainless steel tubes (45x1.5): 3 \u20ac/m      <li>Tuyau irrigation PE HD 50mm : 4.4 \u20ac/m     The guides consist of either stainless metal tubes (preferred) or wooden boards. They facilitate the navigation of the rover along the bed so that it can operate accurately and reliably. A mobile phone Existing phone: 0\u20ac        Dedicated phone: 200 \u20ac     <ul> <li>Used to control the rover      <li>To browse the archived images of the online service         WiFi Rover as hotspot: 0 \u20ac/month Existing WiFi: 0 \u20ac/month GSM WiFi router: 120\u20ac (router) + 10 \u20ac/month (SIM card)     WiFi connectivity in the field     <ul> <li>To connect the mobile phone to the rover (required)      <li>To archives the images taken by the rover (optional)      <li>To provide remote assistance (optional)     Romi's Online Farmer's Dashboard [v3++] 1 \u20ac/month Monthly fee for the use of the Romi remote server (optional):    <ul> <li>To archive and browse the images taken by the rover       <li>To receive remote assistance     Training 70 \u20ac/day Two days of training for the rover (optional) Preprogramming according to the configuration of the farm 50 \u20ac Assistance to fine-tune the rover\u2019s configuration to the farm (optional) Maintenance costs 5 \u20ac/month/rover Maintenance contract with the ROMI association (or a local distributor) (optional)"},{"location":"Rover/UserManual/requirements/#examples-for-different-farm-sizes","title":"Examples for different farm sizes","text":"<p>For an urban farm of 5 000 m2 (lettuces or carrots)</p> <ul> <li>83 beds of 50 meters (beds of 80 cm wide, pathway of 40 cm between beds, 3320 m\u00b2 cultivated)</li> <li>4150 meters of guides (12 450 \u20ac\u2026)</li> <li>1 rover with tool carrier and mechanical weeding tools (7000 \u20ac)</li> </ul> <p>Investment costs: 7000 to 7510 + 12450 \u20ac + 16 \u20ac/months (optional: SIM card, remote maintenance, server archiving)</p> <p>For a vegetable farm of 2 ha</p> <ul> <li>124 beds of 100 meters (beds of 1.1 m wide, pathway of 0.50 m between beds, 13640 m\u00b2 cultivated)</li> <li>12 400 meters of guides (37.2 k\u20ac)</li> </ul> <p>Lettuces (180 000 plants, 54 k\u20ac-129 k\u20ac revenu):</p> <ul> <li>4 rovers</li> <li>Investment costs: 20.5 k\u20ac + 37.2 k\u20ac + 10 to 31 \u20ac/months (wifi and maintenance)</li> <li>Alternative solution: Plastic mulching film: 12 x 226,50 \u20ac = 2712 \u20ac/year [REF]</li> </ul> <p>Carrots (1.24M plants, 60-130 tonnes, 31k\u20ac-130k\u20ac revenue)</p> <ul> <li>1 rover</li> <li>Investment costs: 7000 to 7510 \u20ac + 49.6 k\u20ac + 10 to 16 \u20ac/months (wifi and maintenance)</li> </ul> <p>Sur carottes, l\u2019ensemble des op\u00e9rations de d\u00e9sherbage varie de 120 \u00e0 plus de 900 heures/ha. [http://itab.asso.fr/downloads/fiches-lpc/lpc-carotte.pdf] </p> <p>Prix [https://rnm.franceagrimer.fr/prix?CAROTTE&amp;12MOIS] </p> <p>For a vegetable farm of 5 ha</p> <ul> <li>XXX beds</li> <li>XXX meters of guides</li> <li>XXX rovers</li> </ul> <p>Investment costs: XXX \u20ac + \u20ac/months (wifi and maintenance)</p> <p>It has to be pointed out that in many countries, there are public financial supports for farmers which want to invest in robotics.</p>"},{"location":"Rover/UserManual/specifications/","title":"Technical Specifications","text":"Feature Value Size [v3] (Width x Length x Height) Min.: 1.45 m x 1.60 m x 1.46 m Max.: 1.70 m x 1.60 m x 1.46 m Weight [v2] 80 kg (estimate) Battery life [v3++] 8 h Charging time TODO Weeding speed Precision weeding: 600 m\u00b2/day [v3++]   (235 m\u00b2/day [v2]) Classical weeding 7200 m\u00b2/day [v3] (6400 m\u00b2/day [v2]) Width vegetable beds [v3] Min.: 0.70 m Max.: 1.2 m Handled crops Precision weeding: Lettuce Classical weeding: Carrots Turning space at end of bed 2 m (TODO: verify)"},{"location":"plant-3d-explorer/","title":"Getting started","text":"<p>This page will describe how to set up and start the plant visualizer.</p>"},{"location":"plant-3d-explorer/#dependencies","title":"Dependencies","text":"<p>This project is build using Node JS. As such, you need to install Node JS and npm (which should come with node).</p> <p>!!! important To make sure everything works as intended, check that your version of Node is at least 10, and your version of npm is at lease 6.</p>"},{"location":"plant-3d-explorer/#installing-packages-and-setting-up-the-environment","title":"Installing packages and setting up the environment","text":"<p>After making sure you have the right versions of Node and npm, you will need to clone the repository of the project. Simply run</p> <pre><code>git clone https://github.com/romi/plant-3d-explorer\n</code></pre> <p>The next step is to install everything the app needs with the following command:</p> <pre><code>npm install\n</code></pre> <p>If you want the app to use the remote server to fetch data, run the following command:</p> <pre><code>echo \"REACT_APP_API_URL='https://db.romi-project.eu'\" &gt; .env.local\n</code></pre> <p>If you don't, the app will use a local server at address <code>localhost:5000</code>. In order to do this, you need to get plantdb running.</p>"},{"location":"plant-3d-explorer/#starting-the-app","title":"Starting the app","text":"<p>To start a development server (used to develop, or simply test the app), run:</p> <pre><code>npm start\n</code></pre> <p>The app will then run at address <code>localhost:3000</code>.</p> <p>!!! warning Using Google Chrome (or Chromium Browser) is recommended as some problems have been encountered on Firefox due to some libraries we used.</p>"},{"location":"plant-3d-explorer/#more-commands","title":"More commands","text":"<p>To see a more detailed list of available commands, visit the GitHub repository.</p>"},{"location":"plant-3d-explorer/guide/","title":"Guide","text":""},{"location":"plant-3d-explorer/guide/#the-scan-list-page","title":"The scan list page","text":"<p>When first opening the app, you will be greeted with a home page displaying every available scans. A scan is defined as a folder containing acquisition of plant data (e.g. 2D RGB images, manual measures) and a set of 3D reconstructions and analysis (e.g. point cloud, mesh, automated measures,...) From this page, you can search for specific keywords, order the scans by their name, date, etc. In addition, scans can be filtered according to what data is actually available for each scan.</p> <p>Note</p> <p>If you're a developer, you can download the metadata and archives associated with each scan as well.</p>"},{"location":"plant-3d-explorer/guide/#the-viewer","title":"The viewer","text":"<p>Clicking the Open green button next to a scan (far right of the row corresponding to a scan) to open the actual viewer and explore available data.</p>"},{"location":"plant-3d-explorer/guide/#the-3d-reconstruction-and-the-3d-view-panel","title":"The 3D reconstruction and the 3D-view panel","text":"<p>The largest panel on the left displays the 3D view, allowing to observe the reconstructed plant and navigate around with basic mouse commands (right/left click and scroll). Check the question mark (?) help icon in the bottom right corner of this panel for more description of mouse control. Several icons appear on the top band of the 3D-view panel: they can simply be activated by clicking, hovering over provide information about their function.</p> <p>Feel free to experiment with the different features and tools of the 3D view.</p>"},{"location":"plant-3d-explorer/guide/#the-graphs","title":"The graphs","text":"<p>The right side consists of a measurement panels, typically displaying graphs of data related to the plant (either measurements provided as metadata or computed during the analysis). Help tooltips explain what those graphs correspond to. By default, phyllotaxis graphs (see below) are displayed. The green \"plus\" (+) icon allow uploading more measurement panels if available. On each graph, the top right corner cross button closes the panel.</p>"},{"location":"plant-3d-explorer/guide/#phyllotaxis-graphs","title":"Phyllotaxis graphs","text":"<p>Phyllotaxis graphs are sequences of either divergence angles (in degree) or internode length (in mm) measured between two consecutive lateral organs of the main stem, starting from the base to the shoot tip. Hovering a graph highlights a particular interval and display the order index of the two organs lateral organs bounding this interval. Interval hovering is synchronized between the two phyllotaxis graphs (divergence angles and internodes) if they are active. Synchronization also happen with the 3D-view panel if 'organ highlight' option is active: only the corresponding pair of organ remain visible. Changing organ colors for this pair in the 3D-view panel will also synchronize the colors in the graphs. Clicking on an interval in the graph allow to maintain the selection active. Download buttons at the top of each graph allow to get the data in CSV or TSV format.</p>"},{"location":"plant-3d-explorer/guide/#the-photo-carousel-and-camera-mode","title":"The photo carousel and camera mode","text":"<p>The entire bottom of the page is a line called the 'carousel': it contains images contained in the scan folder. Click on any of the images in the carousel, and the 3D-view panel will switch into the camera mode: the available 3D reconstructions will be superposed on the image, allowing to check the accuracy of the 3D reconstruction. The image of the carousel currently displayed in the 3D-View panel is boxed. Dragging this box left/rightwards changes the active image, and the 3D-view is synchronized immediately, allowing to dynamically navigate along the original camera path of acquisition. If the scan folder contain several images that have been made available for the visualizer, the source of images can be changed. The carousel will be populated by this new source and images can be displayed into the 3D viewer by activating the camera mode.</p>"},{"location":"plant-3d-explorer/guide/#reporting-bugs","title":"Reporting bugs","text":"<p>If you encounter some kind of unwanted behavior, or have a feature suggestion, head over to the GitHub repository and write an issue!</p>"},{"location":"plant_imager/","title":"Plant Phenotyping","text":"<p>Within the ROMI project, some work packages are oriented towards the development of a 3D plant phenotyping platform adapted to single potted plants. To achieve this goal, the team developed a suite of affordable open-source tools (hardware &amp; software) presented hereafter.</p> <p></p> <p>We aim at making our software architecture modular to ensure the required flexibility and adaptability to most of the robotic &amp; research applications from the ROMI project when possible.</p> <p>The flowchart belows indicate how the various modules and parts of the ROMI software interact:</p> %%{   init: {   \"theme\": \"base\",   \"flowchart\": {     \"htmlLabels\": true,     \"curve\": \"linear\"   },   \"themeVariables\": {     \"fontFamily\": \"Nunito Sans\",     \"primaryColor\": \"#00a960\",     \"background\": \"#ffffff\",     \"primaryTextColor\": \"#282828\",     \"secondaryColor\": \"#ffffff\",     \"tertiaryColor\": \"#ffffff\"     }   } }%%  flowchart LR     DB[(Database)]     P3DX(REACT API)     CLI(CLI)     Reconst&gt;Reconstruction]     Quantif&gt;Quantification]     LPy([LPy])     Blender([Blender])     Hardware[[Hardware]]      Terminal -.-&gt; CLI     subgraph romitask         CLI     end     subgraph plantdb         DB     end     subgraph plant-imager         subgraph VirtualPlantImager             LPy             Blender         end         subgraph PlantImager             Hardware         end     end     CLI --&gt; PlantImager     CLI --&gt; VirtualPlantImager     PlantImager --&gt; DB     VirtualPlantImager &lt;--&gt; DB     subgraph plant-3d-vision         Reconst         Quantif     end     plant-3d-vision &lt;--&gt; DB     CLI --&gt; plant-3d-vision     subgraph plant-3d-explorer         DB --&gt; P3DX     end     P3DX &lt;-.-&gt; Browser"},{"location":"plant_imager/#list-of-python-modules","title":"List of Python modules","text":"<p>Hereafter we list the Python module used in the Plant Imager project:</p> <p>plant-imager   to control the Plant Imager hardware, generates virtual plants and image them with the Virtual Plant Imager.</p> <p>plantdb  the database to host and serve images and processed data.</p> <p>plant-3d-explorer  to visualize the images and processed data.</p> <p>plant-3d-vision  to reconstruct the RGB images into 3D models and to quantify phylotaxis from 3D models</p> <p>romiseg  for semantic labelling of plants. </p> <p>romicgal  wrapper around CGAL for triangulation and skeletonization.</p> <p>romitask  defines the tasks and the command-line-interface (CLI) <code>romi_run_task</code>.</p> <p>dtw  to align sequences, notably angles and internodes obtained from phylotaxis measurements. </p>"},{"location":"plant_imager/#usage","title":"Usage","text":"<p>We provide tutorials explaining the usage of out various tools and how-to detailing the algorithms at work for each task.  </p> <p>Tutorials  How-to </p>"},{"location":"plant_imager/build_v2/","title":"Assembly instructions for the second version of the plant imager","text":"<p>This is the documentation for the second iteration of the plant imager hardware.</p>"},{"location":"plant_imager/build_v2/#assembly-overview","title":"Assembly overview","text":"<p>You will have to achieve the following steps:</p> <ol> <li>assemble the aluminium frame: instructions</li> <li>assemble the X-Carve CNC: instructions</li> <li>attach the CNC the to aluminium frame</li> <li>assemble one of the electronic controller:</li> <li>X-controller: instructions</li> <li>gShield hat : instructions</li> <li>wire the X-Carve CNC motors &amp; endstops to the controller</li> <li>assemble &amp; mount the custom gimbal: instructions</li> <li>choose a camera &amp; build the camera mount</li> <li>PiCamera: instructions</li> <li>Sony RX-0</li> <li>gPhoto2 compatible camera</li> <li>[OPTIONAL] add the LED bars inside</li> <li>wiring it all.</li> </ol>"},{"location":"plant_imager/build_v2/#open-hardware","title":"Open Hardware","text":"<p>We use \"widely available\" materials and provides all the required information, so you can reproduce and modify our work.</p> <p>If you want to modify it, please note the following important points:</p> <ol> <li>The width and depth of the aluminium frame is dependent of the CNC frame size, make sure the CNC frame fits inside the aluminium frame!</li> <li>The height of the aluminium frame will determine the maximum observable plant height.</li> </ol>"},{"location":"plant_imager/build_v2/#wiring-communication-overview","title":"Wiring &amp; communication overview","text":"<p>The plant imager electronics can be divided in two functional groups:</p> <ol> <li>the \"CNC group\", responsible for moving the camera around;</li> <li>the \"Gimbal &amp; Camera group\", allowing to rotate the camera around the z-axis to face the object (plant) to acquire.</li> </ol> <p>Note that the CNC group has 3 axis movement (X, Y &amp; Z), the gimbal add a forth one.</p> Overview of the plant imager's electronics. Note that the PiCamera act as a Wi-Fi hotspot to which the controller computer is registered.  <p>The two groups communicate with the main controller by USB. Note that since the gimbal &amp; camera are located at the end of the arm on the z-axis, the USB cable powering &amp; controlling them are going through the cable rails with those dedicated to the axis motors. This may create interference, so you will need a (long) properly shielded USB cable!</p>"},{"location":"plant_imager/build_v2/alu_frame/","title":"Assembly instructions for the aluminium frame v2","text":"<p>Important</p> <p>All units are in millimeters!</p>"},{"location":"plant_imager/build_v2/alu_frame/#bom","title":"BOM","text":"<p>To assemble the aluminium frame you will need:</p> <ul> <li>4x 1800mm 3030 profiles (A)</li> <li>4x 1200mm 3030 profiles (B)</li> <li>4x 1007mm 3030 profiles (C)</li> <li>16x brackets (30x60) for 3030 profiles</li> <li>64x M6 T-nuts</li> <li>64x M6x12 screws</li> </ul> <p>Note that:</p> <ul> <li>Parts A defines the height of the enclosure;</li> <li>Parts B &amp; C depends on the external size of the chosen CNC frame!</li> </ul>"},{"location":"plant_imager/build_v2/alu_frame/#assembly-instructions","title":"Assembly instructions","text":"<p>Start by assembling the smallest sides of the frame using two A parts &amp; two C parts per side. To ease the build, lay them flat on the floor.</p> <ol> <li> <p>Prepare all A parts with their brackets at the right position:</p> <p> Positioning the 30x60 brackets on the 1800mm 3030 profiles (A). </p> </li> <li> <p>Add two C parts per side to construct the \"small sides\". Make sure they are horizontal.</p> </li> <li> <p>Then add the two lower B parts &amp; finally the two upper B parts.</p> <p> Assembled aluminium frame schematic &amp; isometric view. </p> </li> </ol>"},{"location":"plant_imager/build_v2/alu_frame/#optional","title":"Optional","text":"<p>Add wood corner brackets to rigidify the structure.</p> Assembled aluminium frame with wood corners schematic. Isometric view of assembled aluminium frame with wood corners."},{"location":"plant_imager/build_v2/cnc_calibration/","title":"CNC setup &amp; calibration","text":"<p>After building &amp; wiring the CNC and fitting it into the aluminium frame it is required to calibrate some Grbl software parameters like the homing directions or the acceleration rates.</p>"},{"location":"plant_imager/build_v2/cnc_calibration/#using-grbl","title":"Using Grbl","text":"<p>Except if you are familiar with Grbl, if you want to know more, have a look at the official Grbl wiki.</p>"},{"location":"plant_imager/build_v2/cnc_calibration/#connect-to-the-arduino","title":"Connect to the Arduino","text":"<p>Then you can use picocom to connect to the arduino:</p> <pre><code>picocom /dev/ttyACM0 -b 115200\n</code></pre> <p>Note</p> <p>See here how to find the right USB port.</p>"},{"location":"plant_imager/build_v2/cnc_calibration/#getting-help","title":"Getting help","text":"<p>You can type <code>$</code> (and press enter) to get help. You should not see any local echo of the <code>$</code> and enter. Grbl should respond with: <pre><code>[HLP:$$ $# $G $I $N $x=val $Nx=line $J=line $SLP $C $X $H ~ ! ? ctrl-x]\nok\n</code></pre></p>"},{"location":"plant_imager/build_v2/cnc_calibration/#accessing-the-saved-configuration","title":"Accessing the saved configuration","text":"<p>To access the saved configuration, type <code>$$</code> to obtain the parameter values. For example our config is: <pre><code>$0=10\n$1=255\n$2=0\n$3=5\n$4=0\n$5=0\n$6=0\n$10=1\n$11=0.020\n$12=0.002\n$13=0\n$20=1\n$21=0\n$22=1\n$23=0\n$24=25.000\n$25=5000.000\n$26=250\n$27=1.000\n$30=12000\n$31=0\n$32=0\n$100=40.000\n$101=40.000\n$102=188.947\n$110=8000.000\n$111=8000.000\n$112=1000.000\n$120=100.000\n$121=100.000\n$122=50.000\n$130=780.000\n$131=790.000\n$132=150.000\n</code></pre></p> <p>Note</p> <p>See the official Grbl wiki for details &amp; meaning of parameter.</p>"},{"location":"plant_imager/build_v2/cnc_calibration/#setup-calibration","title":"Setup &amp; calibration","text":"<p>Now we will:</p> <ol> <li>verify the cnc respond correctly to homing instruction</li> <li>calibrate if needed</li> </ol>"},{"location":"plant_imager/build_v2/cnc_calibration/#homing-the-x-carve","title":"Homing the X-carve","text":"<p>Once you are sure that everything is connected properly (especially the limit switches) you can try to \"home\" the X-Carve manually using <code>$H</code> in the previous terminal connected to Grbl.</p> <p>Warning</p> <p>Be ready to use the emergency stop button in case the axes move in the opposite direction of your limit switches!</p> <p>Note</p> <p>If you don't see any response in the terminal when you type the commands, it is perfectly normal!</p>"},{"location":"plant_imager/build_v2/cnc_calibration/#change-homing-direction","title":"Change homing direction:","text":"<p>Parameters named <code>$3</code> control the homing direction.</p> <p>From the official wiki:</p> <p>Quote</p> <p>By default, Grbl assumes that the axes move in a positive direction when the direction pin signal is low, and a negative direction when the pin is high.</p> <p>To configure the homing direction, you simply need to send the value for the axes you want to invert using the table below.</p> Setting Value Mask Invert X Invert Y Invert Z 0 00000000 N N N 1 00000001 Y N N 2 00000010 N Y N 3 00000011 Y Y N 4 00000100 N N Y 5 00000101 Y N Y 6 00000110 N Y Y 7 00000111 Y Y Y <p>For example, if want to invert the Y axis direction only, you'd send <code>$3=2</code> to Grbl, and the setting should now read <code>$3=2 (dir port invert mask:00000010)</code></p>"},{"location":"plant_imager/build_v2/cnc_calibration/#edit-the-acceleration-rates","title":"Edit the acceleration rates","text":"<p>Parameters named <code>$120</code>, <code>$121</code> &amp; <code>$123</code> control the axes acceleration in mm/second/second.</p> <p>From the official wiki:</p> <p>Quote</p> <p>Simplistically, a lower value makes Grbl ease slower into motion, while a higher value yields tighter moves and reaches the desired feed rates much quicker. Much like the max rate setting, each axis has its own acceleration value and are independent of each other. This means that a multi-axis motion will only accelerate as quickly as the lowest contributing axis can.</p> <p>Since the Z-axis arm is long and have a \"heavy weight\" (gimbal + camera) at its lower end, it is probably a good to keep low values to avoid blurry image due to shaking!</p> <p>To determine optimal values, the official wiki is clear, you will have to run some tests:</p> <p>Quote</p> <p>Again, like the max rate setting, the simplest way to determine the values for this setting is to individually test each axis with slowly increasing values until the motor stalls. Then finalize your acceleration setting with a value 10-20% below this absolute max value. This should account for wear, friction, and mass inertia. We highly recommend that you dry test some G-code programs with your new settings before committing to them. Sometimes the loading on your machine is different when moving in all axes together.</p> <p>For example, you can send <code>$120=100.0</code> to set the X axis acceleration rate to 100 mm/second\u00b2.</p> <p>Note</p> <p>Deceleration rates may also create shaking!</p>"},{"location":"plant_imager/build_v2/cnc_communication/","title":"Communicating with the CNC","text":""},{"location":"plant_imager/build_v2/cnc_communication/#connect-to-the-arduino-uno","title":"Connect to the Arduino UNO","text":"<p>First you need to find which USB port your arduino is connected to.</p> <p>To do so, you can use <code>dmesg</code>:</p> <ol> <li>make sure the usb cable from the arduino is unplugged</li> <li>run <code>dmesg -w</code> in a terminal</li> <li>connect the usb and see something like:</li> </ol> <pre><code>[70480.940181] usb 1-2: new full-speed USB device number 31 using xhci_hcd\n[70481.090857] usb 1-2: New USB device found, idVendor=2a03, idProduct=0043, bcdDevice= 0.01\n[70481.090862] usb 1-2: New USB device strings: Mfr=1, Product=2, SerialNumber=220\n[70481.090865] usb 1-2: Product: Arduino Uno\n[70481.090868] usb 1-2: Manufacturer: Arduino Srl            [70481.090871] usb 1-2: SerialNumber: 554313131383512001F0\n[70481.093408] cdc_acm 1-2:1.0: ttyACM0: USB ACM device\n</code></pre> <p>Important</p> <p>The important info here is <code>ttyACM0</code>!</p> <p>Then you can use picocom to connect to the arduino:</p> <pre><code>picocom /dev/ttyACM0 -b 115200\n</code></pre> <p>Note</p> <p><code>-b 115200</code> is the baud rate of the connection, read the picocom man page for more info.</p> <p>Once connected you should see something like:</p> <pre><code>picocom v2.2\n\nport is        : /dev/ttyACM0\nflowcontrol    : none\nbaudrate is    : 115200\nparity is      : none\ndatabits are   : 8\nstopbits are   : 1\nescape is      : C-a\nlocal echo is  : no\nnoinit is      : no\nnoreset is     : no\nnolock is      : no\nsend_cmd is    : sz -vv\nreceive_cmd is : rz -vv -E\nimap is        : \nomap is        : \nemap is        : crcrlf,delbs,\n\nType [C-a] [C-h] to see available commands\n\nTerminal ready\n\nGrbl 1.1f ['$' for help]\n[MSG:'$H'|'$X' to unlock]\n</code></pre> <p>This mean you now have access to a Grbl terminal (<code>Grbl 1.1f</code>) to communicate, notably send instructions, to the CNC!</p>"},{"location":"plant_imager/build_v2/cnc_communication/#troubleshooting","title":"Troubleshooting","text":""},{"location":"plant_imager/build_v2/cnc_communication/#serial-access-denied","title":"Serial access denied","text":"<p>Look here if you can not communicate with the scanner using usb.</p>"},{"location":"plant_imager/build_v2/cnc_electronics/","title":"Wiring of the CNC electronics","text":""},{"location":"plant_imager/build_v2/cnc_electronics/#x-controller-assembly","title":"X-Controller assembly","text":"<p>If you ordered the X-Controller, follow the official assembly instructions here.</p> <p>Note</p> <p>This is what we used in our second and third version of the phenotyping station hardware.</p> <p>Warning</p> <p>We replaced the default <code>Grbl</code> firmware by <code>Oquam</code>, our own implementation. See here for the instructions on how to flash this firmware to the X-Controller.</p>"},{"location":"plant_imager/build_v2/cnc_electronics/#gshield-assembly","title":"gShield assembly","text":"<p>If you have the recent version, follow the official wiring instructions here.</p> <p>Here is link to the post 2015 version of the \"wiring\" instructions.</p> <p>Here is link to the post 2015 version of the \"electronic assembly\" instructions.</p> <p>Note</p> <p>This is what we used in our first version of the phenotyping station hardware.</p>"},{"location":"plant_imager/build_v2/cnc_electronics/#bom","title":"BOM","text":"<p>If you are familiar with the Arduino world, the electronic is pretty straightforward:</p> <ul> <li>Arduino UNO (official buy here)</li> <li>Synthetos gShield (buy here)</li> <li>Power converter 220V ac. - 24V dc. (buy @Farnell)</li> <li>Emergency stop button (buy @Farnell)</li> </ul> <p>Optional:</p> <ul> <li>24V fan</li> <li>220V power cord</li> </ul> <p>Note</p> <p>Before 2017 X-Carve shipped 400W power units, now they use a 320W unit. The link is for a 350W unit.</p>"},{"location":"plant_imager/build_v2/cnc_electronics/#wiring-instructions","title":"Wiring instructions","text":""},{"location":"plant_imager/build_v2/cnc_electronics/#wire-the-stepper-cable-to-the-gshield","title":"Wire the Stepper Cable to the gShield","text":"<p>Once you\u2019ve determined which stepper cable belongs to which axis, you can wire them into the gShield. First loosen all the screws on the gShield (they will jump a thread when they are fully loose, but they won\u2019t come out of the terminal blocks.)</p> <p>The gShield is marked \"X,\" \"Y,\" and \"Z\". Wire the stepper cable according to the markings on the shield and order your wires (from left to right) black, green, white, red.</p> <p></p> <p>Check out this diagram for clarification.</p> <p></p>"},{"location":"plant_imager/build_v2/cnc_electronics/#mount-the-gshield","title":"Mount the gShield","text":"<p>Now push the gShield onto the Arduino. There are pins on the gShield that go into the headers of the Arduino.</p> <p></p>"},{"location":"plant_imager/build_v2/cnc_electronics/#24v-fan-mount-optional","title":"24V Fan mount (optional)","text":"<p>If you want, you can add a 24V fan to cool the shield. Uses the 24V pins on the gShield as shown in the picture below:</p> <p></p> <p>Loosen the screws in the power terminal of the gShield and insert the red twisted pair into <code>Vmot</code> and the black twisted pair into <code>GND</code>.</p>"},{"location":"plant_imager/build_v2/cnc_electronics/#connect-limit-switches-to-gshield","title":"Connect Limit Switches to gShield","text":"<p>Crimp the white ends of each limit switch wire pair. The order of the white wires from left to right is X, Y, Z. The first, sixth, and eighth slots are left EMPTY.</p> <p></p> <p>Pin mapping:</p> <ul> <li><code>D9</code>: x-limit (red)</li> <li><code>D10</code>: y-limit (red)</li> <li><code>D12</code>: z-limit (red)</li> <li><code>GND</code>: ground (all 3)</li> </ul>"},{"location":"plant_imager/build_v2/cnc_electronics/#power-the-gshield","title":"Power the gShield","text":"<p>Loosen the screws in the power terminal of the gShield and insert the red twisted pair into <code>Vmot</code> and the black twisted pair into <code>GND</code>. This is similar to the optional 24V fan.</p> <p>Note</p> <p>This will also power the Arduino.</p>"},{"location":"plant_imager/build_v2/cnc_frame/","title":"Assembly instructions for the CNC frame","text":"<p>We chose the X-Carve 1000 mm as basis for the CNC gantry, feel free to choose another one but beware to check the aluminium frame still fit!</p> X-Carve 1000mm."},{"location":"plant_imager/build_v2/cnc_frame/#bom","title":"BOM","text":"<p>To build the CNC you will need:</p> <ul> <li>X-Carve 1000mmm (US buy here; EU   buy here)</li> </ul> <p>Note</p> <p>We chose the X-Carve because initially you could customize your order and for example not order the work area (wood plate). Apparently it is not possible anymore!</p>"},{"location":"plant_imager/build_v2/cnc_frame/#instructions","title":"Instructions","text":"<p>Follow the official build instructions for the X-Carve here.</p> <p>You should follow these steps:</p> <ul> <li>Gantry - Side Plates</li> <li>Gantry - X-Carriage</li> <li>Gantry - Z-Axis</li> <li>Assemble Gantry</li> <li>Rails</li> <li>Belting</li> </ul> <p>Note</p> <p>Do NOT follow the steps indicating how to:</p> <ul> <li>build the work area</li> <li>fix the spindle</li> <li>fix the gantry and rails over the work area (wood plate)</li> </ul> <p>Once you followed the official instructions, attach the CNC to the aluminium frame before continuing with the next section: wiring the CNC electronics!</p>"},{"location":"plant_imager/build_v2/gimbal_communication/","title":"Communication &amp; control of the custom gimbal","text":""},{"location":"plant_imager/build_v2/gimbal_communication/#connect-to-the-feather-m0","title":"Connect to the Feather M0","text":"<p>First you need to find which USB port your Feather M0 is connected to.</p> <p>To do so, you can use <code>dmesg</code>:</p> <ol> <li>make sure the usb cable from the Feather M0 is unplugged</li> <li>run <code>dmesg -w</code> in a terminal</li> <li>connect the usb and see something like: <pre><code>[70481.093408] cdc_acm 1-2:1.0: ttyACM1: USB ACM device\n</code></pre></li> </ol> <p>Important</p> <p>The important info here is <code>ttyACM1</code>!</p> <p>Then you can use picocom to connect to the Feather M0: <pre><code>picocom /dev/ttyACM1 -b 115200\n</code></pre></p>"},{"location":"plant_imager/build_v2/gimbal_communication/#usage","title":"Usage","text":"<p>Send plain text-formatted commands for the position or speed commands. See the command list below:</p> <ul> <li><code>x</code>: set velocity (unit: RPM)</li> <li><code>X</code>: set target position (units: degrees)</li> <li><code>c</code>: start the calibration of the encoder</li> <li><code>C</code>: print results of the calibration</li> <li><code>v</code>: print target velocity</li> <li><code>p</code>: print PWM value</li> </ul>"},{"location":"plant_imager/build_v2/gimbal_communication/#troubleshooting","title":"Troubleshooting","text":""},{"location":"plant_imager/build_v2/gimbal_communication/#serial-access-denied","title":"Serial access denied","text":"<p>Look here if you can not communicate with the scanner using usb.</p>"},{"location":"plant_imager/build_v2/gimbal_setup/","title":"Assembly instructions for the custom gimbal","text":""},{"location":"plant_imager/build_v2/gimbal_setup/#bom","title":"BOM","text":"<p>The bill of material is short:</p> <ul> <li>Adafruit Feather M0</li> <li>Custom made ROMI gimbal controller hat</li> </ul>"},{"location":"plant_imager/build_v2/gimbal_setup/#sources","title":"Sources","text":"<p>The sources of the custom-made gimbal controller are available here.</p> <p>The source code to compile &amp; upload with Arduino IDE to the Feather M0 is here.</p> <p>The bracket for the picamera that also hold the M0 &amp; the hat should be 3D printed. The design is available here.</p>"},{"location":"plant_imager/build_v2/gimbal_setup/#resources","title":"Resources","text":"<p>An introduction to the Adafruit Feather M0 Basic is available on Adafruit's website here.</p>"},{"location":"plant_imager/build_v2/picamera_setup/","title":"PiCamera setup","text":"<p>Warning</p> <p>This is a work in progress!</p>"},{"location":"plant_imager/build_v2/picamera_setup/#getting-started","title":"Getting started","text":"<p>To set up the PiCamera you first need to install a fresh OS on your Raspberry Pi Zero W.</p>"},{"location":"plant_imager/build_v2/picamera_setup/#burn-a-new-raspberry-pi-os-lite-image","title":"Burn a new Raspberry Pi OS Lite image","text":"<p>We recommend the Raspberry Pi Imager tool to burn a new Raspberry Pi OS (32-bit) Lite. You can find it here.</p> <p>Or you can download an official ZIP here.</p>"},{"location":"plant_imager/build_v2/picamera_setup/#accessing-the-pizero","title":"Accessing the PiZero","text":""},{"location":"plant_imager/build_v2/picamera_setup/#a-enable-ssh-remote-access","title":"A - Enable SSH remote access","text":"<p>After installing the Raspberry Pi OS, add an empty file <code>ssh</code> in the <code>boot</code> partition to enable SSH access.</p> <p>Warning</p> <p>As we will later use the Wi-Fi from the PiZero to create an Access Point, we recommend to use the ethernet port to SSH in the PiZero.</p>"},{"location":"plant_imager/build_v2/picamera_setup/#b-use-local-access","title":"B - Use local access","text":"<p>Otherwise, use a screen and keyboard to access the PiZero.</p>"},{"location":"plant_imager/build_v2/picamera_setup/#raspberry-pi-os-setup","title":"Raspberry Pi OS setup","text":"<p>Before installing the software, you have to configure some Raspberry Pi OS settings &amp; change the password.</p>"},{"location":"plant_imager/build_v2/picamera_setup/#change-the-password","title":"Change the password","text":"<p>For security reasons, you have to change the <code>pi</code> user password because the default is known by everyone!</p> <p>Use the <code>raspi-config</code> tool to do it:</p> <pre><code>sudo raspi-config\n</code></pre> <p>Warning</p> <p>The default password is <code>raspberry</code>!</p>"},{"location":"plant_imager/build_v2/picamera_setup/#edit-the-timezone-locales","title":"Edit the timezone &amp; locales","text":"<p>Change the timezone to get the right time from the PiZero clock! You may also change the locales to suits your needs.</p>"},{"location":"plant_imager/build_v2/picamera_setup/#create-the-wi-fi-access-point","title":"Create the Wi-Fi Access Point","text":"<p>We will now create the AP using command lines following the tutorial from: https://learn.sparkfun.com/tutorials/setting-up-a-raspberry-pi-3-as-an-access-point/all.</p> <p>You may also be interested in doing this with a graphical interface, and we would highly recommend raspap-webgui.</p>"},{"location":"plant_imager/build_v2/picamera_setup/#install-packages","title":"Install Packages","text":"<p>To install the required packages, enter the following into the console:</p> <pre><code>sudo apt-get -y install hostapd dnsmasq\n</code></pre>"},{"location":"plant_imager/build_v2/picamera_setup/#set-static-ip-address","title":"Set Static IP Address","text":"<p>Edit the <code>dhcpcd.conf</code> file:</p> <pre><code>sudo nano /etc/dhcpcd.conf\n</code></pre> <p>At the bottom of the file, add:</p> <pre><code>denyinterfaces wlan0\n</code></pre> <p>Save and exit by pressing Ctrl+X and Y when asked.</p> <p>Next, we need to tell the Raspberry Pi to set a static IP address for the Wi-Fi interface. Open the interfaces file with the following command:</p> <pre><code>sudo nano /etc/network/interfaces\n</code></pre> <p>At the bottom of that file, add the following:</p> <pre><code>auto lo\niface lo inet loopback\n\nauto eth0\niface eth0 inet dhcp\n\nallow-hotplug wlan0\niface wlan0 inet static\n    address 192.168.0.1\n    netmask 255.255.255.0\n    network 192.168.0.0\n    broadcast 192.168.0.255\n</code></pre>"},{"location":"plant_imager/build_v2/picamera_setup/#configure-hostapd","title":"Configure Hostapd","text":"<p>We need to set up hostapd to tell it to broadcast a particular SSID and allow Wi-Fi connections on a certain channel. Edit the hostapd.conf file (this will create a new file, as one likely does not exist yet) with this command: <pre><code>sudo nano /etc/hostapd/hostapd.conf\n</code></pre> Enter the following into that file. Feel fee to change the ssid (Wi-Fi network name) and the wpa_passphrase (password to join the network) to whatever you'd like. You can also change the channel to something in the 1-11 range (if channel 6 is too crowded in your area). <pre><code>interface=wlan0\ndriver=nl80211\nssid=romi_hotspot\nhw_mode=g\nchannel=6\nieee80211n=1\nwmm_enabled=1\nht_capab=[HT40][SHORT-GI-20][DSSS_CCK-40]\nmacaddr_acl=0\nauth_algs=1\nignore_broadcast_ssid=0\nwpa=2\nwpa_key_mgmt=WPA-PSK\nwpa_passphrase=raspberry\nrsn_pairwise=CCMP\n</code></pre></p> <p>Save and exit by pressing Ctrl+X and Y when asked.</p> <p>Unfortunately, <code>hostapd</code> does not know where to find this configuration file, so we need to provide its location to the hostapd startup script.</p> <p>Open <code>/etc/default/hostapd</code>:</p> <pre><code>sudo nano /etc/default/hostapd\n</code></pre> <p>Find the line #DAEMON_CONF=\"\" and replace it with:</p> <pre><code>DAEMON_CONF=\"/etc/hostapd/hostapd.conf\"\n</code></pre> <p>Save and exit by pressing Ctrl+X and Y when asked.</p>"},{"location":"plant_imager/build_v2/picamera_setup/#configure-dnsmasq","title":"Configure Dnsmasq","text":"<p>Dnsmasq will help us automatically assign IP addresses as new devices connect to our network as well as work as a translation between network names and IP addresses. The <code>.conf</code> file that comes with Dnsmasq has a lot of good information in it, so it might be worthwhile to save it (as a backup) rather than delete it. After saving it, open a new one for editing:</p> <pre><code>sudo mv /etc/dnsmasq.conf /etc/dnsmasq.conf.bak\nsudo nano /etc/dnsmasq.conf\n</code></pre> <p>In the blank file, paste in the text below.</p> <pre><code>interface=wlan0 \nlisten-address=192.168.0.1\nbind-interfaces \nserver=8.8.8.8\ndomain-needed\nbogus-priv\ndhcp-range=192.168.0.100,192.168.0.200,24h\n</code></pre>"},{"location":"plant_imager/build_v2/picamera_setup/#test-wi-fi-connection","title":"Test Wi-Fi connection","text":"<p>Restart the Raspberry Pi using the following command:</p> <pre><code>sudo reboot\n</code></pre> <p>After your Pi restarts (no need to log in), you should see <code>romi_hotspot</code> appear as a potential wireless network from your computer.</p> <p>Connect to it (the network password is raspberry, unless you changed it in the <code>hostapd.conf</code> file). Then try to SSH it with:</p> <pre><code>ssh pi@192.138.0.1\n</code></pre>"},{"location":"plant_imager/build_v2/picamera_setup/#install-python3-pip","title":"Install Python3 &amp; pip","text":"<p>You will need the Python3 interpreter (Python&gt;=3.6) to run the PiCamera server &amp; <code>pip</code> to install the PiCamera package.</p> <pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y\nsudo apt install python3 python3-pip\n</code></pre> <p>This will update the package manager, upgrade the system libraries &amp; install Python3 &amp; pip.</p>"},{"location":"plant_imager/build_v2/picamera_setup/#install-the-picamera-package","title":"Install the PiCamera package","text":"<p>Once you have <code>pip</code> you can install the <code>picamera</code> Python3 package:</p> <pre><code>pip3 install picamera\n</code></pre> <p>Note</p> <p>We do not create an isolated environment in this case since the sole purpose of the PiZero will be to act as a responsive image server.</p>"},{"location":"plant_imager/build_v2/picamera_setup/#camera-serve-python-code","title":"Camera serve Python code","text":"<p>To capture and serve the images from the PiCamera, we use this Python script:</p> <p>To upload it to your PiZero, from a terminal:</p> <pre><code>wget https://gist.github.com/jlegrand62/c24e454922f0cf203d6f9ed49f95ecc1/raw/c4cda40ff56984188dca928693852f3f7a317fa4/picamera_server.py\n</code></pre> <p>Now (test) start the server with:</p> <pre><code>python3 picamera_server.py\n</code></pre> <p>Todo</p> <p>Explain how to execute <code>python3 picamera_server.py</code> command at PiZero boot.</p>"},{"location":"plant_imager/build_v2/troubleshooting/","title":"Troubleshooting","text":""},{"location":"plant_imager/build_v2/troubleshooting/#serial-access-denied","title":"Serial access denied","text":"<p>If you get an error about permission access:</p> <ol> <li> <p>Check in what group you are with:     <pre><code>groups ${USER}\n</code></pre></p> </li> <li> <p>If you are not in <code>dialout</code>:     <pre><code>sudo gpasswd --add ${USER} dialout\n</code></pre></p> </li> <li> <p>Then log out and back in to see changes!</p> </li> </ol>"},{"location":"plant_imager/build_v3/","title":"Assembly instructions for the third version of the Plant Imager","text":"<p>This is the documentation for the third iteration of the plant imager hardware. It aims at improving the overall quality of the build and to fix some limitations of the second iteration.</p> The Plant Imager v3 assembled at the RDP laboratory."},{"location":"plant_imager/build_v3/#rationale","title":"Rationale","text":"<p>Notable limitations of the second version of the plant imager hardware:</p> <ul> <li>unused z-axis motor in absence of a tilt motor</li> <li>camera powered by batteries (limited to RX-0)</li> <li>unstable control of the gimbal (due to usb cable length)</li> <li>wobbling camera arm and enclosure</li> <li>poor surrounding isolation, even when wrapped with opaque fabric</li> <li>no indication of camera tilt</li> </ul> <p>Change list in this third version:</p> <ul> <li>conversion of the z-axis to pan-axis control</li> <li>manual z-axis control thanks to vertical ruler on arm and arrowhead on manual gimbal mount</li> <li>manual tilt control thanks to a 360\u00b0 protractor and arrowhead on manual gimbal mount</li> <li>possibility to add a second picamera</li> <li><code>oquam</code> firmware instead of <code>Grbl</code></li> <li>complete wooden enclosure of the plant imager</li> </ul>"},{"location":"plant_imager/build_v3/#assembly-overview","title":"Assembly overview","text":"<p>You will have to achieve the following steps:</p> <ol> <li>assemble the aluminium frame: instructions</li> <li>assemble the X-Carve CNC, except Z-axis motor &amp; arm: instructions</li> <li>attach the CNC to the aluminium frame: instructions</li> <li>add the wooden enclosure &amp; the LED bars inside: instructions</li> <li>assemble the X-Carve controller and attach it to the enclosure (left wooden panel at the same height as the X-Carve)</li> <li>flash <code>oquam</code> firmware in place of <code>Grbl</code> on the X-Carve controller: instructions</li> <li>wire the X-Carve CNC motors &amp; endstops to the controller (except z-axis motor and endstop)</li> <li>assemble the arm: instructions</li> <li>attach the arm to the CNC X-Carriage: instructions</li> <li>assemble the manual gimbal: instructions</li> <li>assemble the picamera: instructions</li> <li>attach the picamera to the gimbal then to the pan arm</li> <li>wire the pan encoder as z-endstop: instructions</li> </ol>"},{"location":"plant_imager/build_v3/#open-hardware","title":"Open Hardware","text":"<p>We use \"widely available\" materials and provides all the required information, so you can reproduce and modify our work.</p> <p>If you want to modify it, please note the following important points:</p> <ol> <li>The width and depth of the aluminium frame is dependent of the CNC frame size, make sure the CNC frame fits inside the aluminium frame!</li> <li>The height of the aluminium frame will determine the maximum observable plant height.</li> </ol>"},{"location":"plant_imager/build_v3/#wiring-communication-overview","title":"Wiring &amp; communication overview","text":"<p>Todo</p> <p>Update this section!</p>"},{"location":"plant_imager/build_v3/alu_frame/","title":"Aluminium frame assembly","text":"<p>Important</p> <p>All units are in millimeters!</p>"},{"location":"plant_imager/build_v3/alu_frame/#bom","title":"BOM","text":"ID Name Qty Off the shelf / Custom Material Manufactureur Serial number Link 11 Profil 40x40x2000 4 Off the shelf Aluminium Bosch Rexroth 3842993120 rs-online.com 12 Profil 40x40x1200 5 Off the shelf Aluminium Bosch Rexroth 13 Profil 40x40x1006 6 Off the shelf Aluminium Bosch Rexroth 14 Profil 40x40x160 4 Off the shelf Aluminium Bosch Rexroth 15 Profil 40x40x100 4 Off the shelf Aluminium Bosch Rexroth 22 HINGED_FOOT 4 Off the shelf Bosch Rexroth 3842352061 rs-online.com 23 DAMPING_RING_D44 4 Off the shelf Bosch Rexroth 3842521817 rs-online.com 25 brackets_ce-sb3680-8 30 Off the shelf Bosch Rexroth 3 842 529 005 faure-technologies.com 26 Ecrou de but\u00e9e pour rainure de 10 mm - Taraudage M8 220 Off the shelf Bosch Rexroth 3 842 345 081 faure-technologies.com 27 Vis \u00e0 t\u00eate rectangulaire pour rainure de 10 mm - M8x25 220 Off the shelf Bosch Rexroth 3 842 528 718 faure-technologies.com"},{"location":"plant_imager/build_v3/alu_frame/#assembly","title":"Assembly","text":"<p>Assemble the frame to obtains the following structure:</p> <p></p> <p>Note</p> <p>The 4 wall mountings, made of parts 15 &amp; 25, are optional.</p> <p>We suggest that you start by assembling the smaller sides, laying them on a flat floor to help you get a square structure. Make sure that everything is square and properly tightened before moving to the next step!</p>"},{"location":"plant_imager/build_v3/boms/","title":"Bills of materials","text":""},{"location":"plant_imager/build_v3/boms/#frame-enclosure","title":"Frame &amp; Enclosure","text":"<p>To assemble the aluminium frame &amp; enclosure you will need the following materials:</p> ID Name Qty Off the shelf / Custom Material Manufactureur Serial number Link 1 Planche 1 1 Custom Wood pdf 2 Planche 2 1 Custom Wood pdf 3 Planche 3 1 Custom Wood pdf 4 Planche 4 1 Custom Wood pdf 5 Planche 5 1 Custom Wood pdf 6 Magnet bracket 2 Custom PLA stl 7 LED Strip U Channel 4 Off the shelf amazon.fr 8 LED Ribbon 1 Off the shelf amazon.fr 9 LED Dimmer 1 Off the shelf amazon.fr 10 Convertisseur 24-12V 1 Off the shelf amazon.fr 11 Profil 40x40x2000 4 Off the shelf Bosch Rexroth 3842993120 rs-online.com 12 Profil 40x40x1200 5 Off the shelf 13 Profil 40x40x1006 6 Off the shelf 14 Profil 40x40x160 4 Off the shelf 15 Profil 40x40x100 4 Off the shelf 16 Profil 30x30x2000 2 Off the shelf Bosch Rexroth 3842990720/2000 rs-online.com 17 Profil 30x30x1210 3 Off the shelf 18 STEEL_BASE 4 Off the shelf Bosch Rexroth 3 842 542 667 faure-technologies.com 19 HINGE 3 Off the shelf Bosch Rexroth 3 842 543 331 faure-technologies.com 20 MAGNET_CATCH 2 Off the shelf Bosch Rexroth 3 842 516 165 faure-technologies.com 21 STRAP_SHAPED_HANDLE 1 Off the shelf Bosch Rexroth 3 842 525 766 faure-technologies.com 22 HINGED_FOOT 4 Off the shelf Bosch Rexroth 3842352061 rs-online.com 23 DAMPING_RING_D44 4 Off the shelf Bosch Rexroth 3842521817 rs-online.com 24 BRACKET_30X60 8 Off the shelf Bosch Rexroth 3842523541 faure-technologies.com 25 brackets_ce-sb3680-8 30 Off the shelf Bosch Rexroth 3 842 529 005 faure-technologies.com 26 Ecrou de but\u00e9e pour rainure de 10 mm - Taraudage M8 220 Off the shelf Bosch Rexroth 3 842 345 081 faure-technologies.com 27 Vis \u00e0 t\u00eate rectangulaire pour rainure de 10 mm - M8x25 220 Off the shelf Bosch Rexroth 3 842 528 718 faure-technologies.com 28 Vis \u00e0 t\u00eate rectangulaire pour rainure de 8 mm - M6x20 20 Off the shelf Bosch Rexroth 3 842 523 921 faure-technologies.com 29 Ecrou de but\u00e9e pour rainure de 8 mm - Taraudage M6 20 Off the shelf Bosch Rexroth 3 842 523 925 faure-technologies.com 30 M12x20 Hexagonal Head 4 Off the shelf RS PRO 192-5513 rs-online.com <p>You can find the PDF with the BOM and assembly instructions here.</p>"},{"location":"plant_imager/build_v3/boms/#pan-arm","title":"Pan arm","text":"<p>To assemble the pan arm you will need the following materials:</p> ID Name Qty Off the shelf / Custom Material Manufacturer Serial number Link 1 PLI004 1 Custom Aluminium pdf 2 PLI006 1 Custom Steel Ewellix Makers in Motion LJM12X600ESSC2 pdf, rs-online.com 3 PLI007-3 1 Custom Aluminium pdf 4 PLI009 1 Custom Aluminium pdf 5 PLI011 1 Custom Aluminium pdf 6 PLI012 1 Custom Aluminium pdf 7 PLI016 1 Custom Aluminium pdf 8 Cable Zip 3 Custom PLA stl 9 Dozenal_ruler_30cm 1 Custom PMMA RS PRO 434-295 dxf, rs-online.com 10 PROFILE_D_ETAYAGE_20x40x300 1 Off the shelf Bosch Rexroth R987501076 rs-online.com 11 Slip Ring ROB-13063 1 Off the shelf SparkFun ROB-13063 mouser.fr 12 Nema 23 - 45mm 1 Off the shelf Stepperonline 23HS18-2004H omc-stepperonline.com 13 Convertisseur 24-5V 1 Off the shelf amazon.com 14 Huco 047_20 1 Off the shelf Huco 047.20.3535 rs-online.com 15 CUI_DEVICES_AMT132Q-0048-1200 1 Off the shelf CUI Devices AMT132Q-V eu.mouser.com 16 AMT Cable 1 Off the shelf CUI Devices AMT-18C-3-036 eu.mouser.com 17 AMT Viewpoint Programming Cable 1 Off the shelf CUI Devices AMT-PGRM-18C eu.mouser.com 18 SKF_P 12 TF 2 Off the shelf SKF P 12 TF rs-online.com 19 MQCL-12-A 1 Off the shelf Ruland MQCL-12-A rs-online.com 20 M8x16 Socket Head 4 Off the shelf RS PRO 529-703 rs-online.com 21 M5x10 Shoulder Screw 2 Off the shelf RS PRO 292-271 rs-online.com 22 M3x12 Set Screw 1 Off the shelf RS PRO 137-736 rs-online.com 23 M3x20 Socket Head 2 Off the shelf RS PRO 293-319 rs-online.com 24 M4x12 Socket Head 2 Off the shelf RS PRO 281-035 rs-online.com 25 M5x18 Socket Head 4 Off the shelf Vis express 8420501818 vis-express.fr 26 M5x16 Flat Head 4 Off the shelf RS PRO 232-8423 rs-online.com 27 M6x40 Flat Head 2 Off the shelf RS PRO 917-6238 rs-online.com 28 M5x8 Socket Head 2 Off the shelf RS PRO 660-4633 rs-online.com 29 M5x8 Button Head 3 Off the shelf RS PRO 822-9114 rs-online.com 30 Bosch nut 5 Off the shelf Bosch Rexroth 3842523142 rs-online.com 31 M3 Locknut 2 Off the shelf RS PRO 524-281 rs-online.com 32 M4 Locknut 2 Off the shelf RS PRO 524-304 rs-online.com 33 M5 Locknut 10 Off the shelf RS PRO 122-4371 rs-online.com <p>You can find the PDF with the BOM and assembly instructions here.</p>"},{"location":"plant_imager/build_v3/boms/#manual-gimbal","title":"Manual gimbal","text":"<p>To assemble the manual gimbal you will need the following materials:</p> ID Name Qty Off the shelf / Custom Material Manufacturer Serial number Link 1 MG004 1 Custom Aluminium pdf 2 MG009 1 Custom Aluminium pdf 3 MG010 1 Custom Aluminium pdf 4 MG012 1 Custom Aluminium pdf 5 MG011 1 Custom PMMA RS PRO 434-295 pdf, dxf, rs-online.com 6 MG014 1 Custom PMMA dxf 7 Mounting Plate Rpi Zero Hq Camera 1 Custom PMMA dxf 8 Protractor dial deg 1 Custom PMMA dxf 9 Spacer M5x8 2 Off the shelf Vis express 8470100818 vis-express.fr 10 Spacer M5x20 1 Off the shelf Vis express 8470102018 vis-express.fr 11 Bouton de serrage M5 3 Off the shelf RS PRO 771-673 rs-online.com 12 M5x40 Socket Head 1 Off the shelf RS PRO 293-353 rs-online.com 13 M5x30 Socket Head 2 Off the shelf RS PRO 290-130 rs-online.com 14 M5x12 Socket Head 2 Off the Shelf RS PRO 281-079 rs-online.com 15 M4x16 Flat Head 2 Off the Shelf RS PRO 171-837 rs-online.com 16 M4x12 Socket Head 1 Off the shelf RS PRO 281-035 rs-online.com 17 M3x8 Socket Head 2 Off the Shelf RS PRO 280-997 rs-online.com 18 Bosch nut 2 Off the Shelf Bosch Rexroth 3842523142 rs-online.com 19 Washer M4 L 1 Off the Shelf Vis express 3534000402 vis-express.fr 20 IGUS GTM-0818-020 2 Off the Shelf IGUS GTM-0818-020 amazon.fr 21 RaspberryPiZeroW 1 Off the Shelf kubii.fr 22 Raspberry Pi HQ Camera 1 Off the Shelf kubii.fr 23 6mm CS-mount lens 1 Off the Shelf kubii.fr 24 Ribbon cable 1 Off the Shelf kubii.fr 25 5mm M2.5 nylon screws 8 Off the Shell thepihut.com 26 M2.5 nylon nuts 8 Off the Self <p>You can find the PDF with the BOM and assembly instructions here.</p>"},{"location":"plant_imager/build_v3/boms/#x-carve-cnc","title":"X-Carve CNC","text":"<p>To assemble the X-Carve CNC, you will need to buy one (1000mm):</p> <ul> <li>US buy here</li> <li>EU buy here</li> </ul>"},{"location":"plant_imager/build_v3/boms/#picamera","title":"PiCamera","text":"<p>To assemble a SINGLE PiCamera HQ, you will need the following list of materials:</p> ID Name Qty Off the shelf / Custom Material Manufacturer Serial number Link 7 Mounting Plate Rpi Zero Hq Camera 1 Custom PMMA dxf 21 RaspberryPiZeroW 1 Off the Shelf kubii.fr 22 Raspberry Pi HQ Camera 1 Off the Shelf kubii.fr 23 6mm CS-mount lens 1 Off the Shelf kubii.fr 24 Ribbon cable 1 Off the Shelf kubii.fr 25 5mm M2.5 nylon screws 8 Off the Shell thepihut.com <p>Note</p> <p>This is contained in the \"Manual gimbal\" BOM.</p>"},{"location":"plant_imager/build_v3/boms/#plant-table","title":"Plant table","text":"<p>To put our plant at camera height, we use a simple adjustable piano stand from RockJam, findable on Amazon</p>"},{"location":"plant_imager/build_v3/cnc_arm/","title":"Mounting the pan arm","text":""},{"location":"plant_imager/build_v3/cnc_arm/#bom","title":"BOM","text":"<ul> <li>4x M5x16 flat head screw</li> <li>4x M5 nuts.</li> </ul>"},{"location":"plant_imager/build_v3/cnc_arm/#assembly","title":"Assembly","text":"<p>Mount the pan arm to the X carriage of the X-carve CNC as follows:</p> <p></p> <p>Note</p> <p>Use the holes ot the x-carriage &amp; arm mounting plate.</p>"},{"location":"plant_imager/build_v3/cnc_communication/","title":"Communicating with <code>Oquam</code>","text":""},{"location":"plant_imager/build_v3/cnc_communication/#finding-the-usb-port","title":"Finding the USB port","text":"<p>First you need to find which USB port your X-Controller board is connected to.</p> <p>To do so, you can use <code>dmesg</code>:</p> <ol> <li>make sure the USB cable from the X-Controller board is unplugged from your computer</li> <li>run <code>dmesg -w</code> in a terminal</li> <li>connect the usb to your computer and see something like:     <pre><code>[42063.157605] usb 1-2: new full-speed USB device number 7 using xhci_hcd\n[42063.313985] usb 1-2: New USB device found, idVendor=0403, idProduct=6001, bcdDevice= 6.00\n[42063.313991] usb 1-2: New USB device strings: Mfr=1, Product=2, SerialNumber=3\n[42063.313995] usb 1-2: Product: X-Controller\n[42063.313997] usb 1-2: Manufacturer: Inventables\n[42063.314000] usb 1-2: SerialNumber: XCONTROLLER6CFIRF\n[42063.317878] ftdi_sio 1-2:1.0: FTDI USB Serial Device converter detected\n[42063.317954] usb 1-2: Detected FT232RL\n[42063.318915] usb 1-2: FTDI USB Serial Device converter now attached to ttyUSB0\n</code></pre></li> </ol> <p>Important</p> <p>The important info here is <code>ttyUSB0</code>!</p>"},{"location":"plant_imager/build_v3/cnc_communication/#serial-connection","title":"Serial connection","text":""},{"location":"plant_imager/build_v3/cnc_communication/#with-picocom","title":"With <code>picocom</code>","text":"<p>Then you can use picocom to connect to the board:</p> <pre><code>picocom /dev/ttyUSB0 -b 115200 --omap crcrlf --echo\n</code></pre> <p>Note</p> <p><code>-b 115200</code> is the baud rate of the connection, read the picocom man page for more info. <code>--omap crcrlf</code> is mapping the serial output from <code>CR</code> to <code>CR+LF</code>. <code>--echo</code> allows you to see what you are typing.</p> <p>Once connected you should see something like:</p> <pre><code>picocom v3.1\n\nport is        : /dev/ttyUSB0\nflowcontrol    : none\nbaudrate is    : 115200\nparity is      : none\ndatabits are   : 8\nstopbits are   : 1\nescape is      : C-a\nlocal echo is  : yes\nnoinit is      : no\nnoreset is     : no\nhangup is      : no\nnolock is      : no\nsend_cmd is    : sz -vv\nreceive_cmd is : rz -vv -E\nimap is        : \nomap is        : crcrlf,\nemap is        : crcrlf,delbs,\nlogfile is     : none\ninitstring     : none\nexit_after is  : not set\nexit is        : no\n\nType [C-a] [C-h] to see available commands\nTerminal ready\n</code></pre> <p>This mean you now have access to a <code>Oquam</code> terminal to communicate, notably send instructions, to the CNC!</p>"},{"location":"plant_imager/build_v3/cnc_communication/#with-the-serial-monitor-of-the-arduino-ide","title":"With the serial monitor of the <code>Arduino IDE</code>","text":"<p>Open the <code>Arduino IDE</code>, go to <code>Tools &gt; Serial Monitor</code> or use the keyboard shortcut Ctrl+Shift+M. Do not forget to select <code>Both NL &amp; CR</code> instead of the default <code>Newline</code>.</p>"},{"location":"plant_imager/build_v3/cnc_communication/#troubleshooting","title":"Troubleshooting","text":""},{"location":"plant_imager/build_v3/cnc_communication/#serial-access-denied","title":"Serial access denied","text":"<p>If you get an error about permission access:</p> <ol> <li> <p>Check in what <code>groups</code> you are with:     <pre><code>groups ${USER}\n</code></pre></p> </li> <li> <p>If you are not in <code>dialout</code>:     <pre><code>sudo gpasswd --add ${USER} dialout\n</code></pre></p> </li> <li> <p>Then log out and back in to see changes!</p> </li> </ol>"},{"location":"plant_imager/build_v3/cnc_electronics/","title":"Wiring of the X-Controller","text":"X-Controller."},{"location":"plant_imager/build_v3/cnc_electronics/#bom","title":"BOM","text":"<p>To assemble the X-Controller, you will need to buy one:</p> <ul> <li>US buy here</li> <li>EU buy here</li> </ul>"},{"location":"plant_imager/build_v3/cnc_electronics/#assembly","title":"Assembly","text":"<p>If you ordered the X-Controller, follow the official assembly instructions here.</p> <p>Note</p> <p>This is what we used in our first and third version of the plant imager hardware.</p> <p>Warning</p> <p>We replaced the default <code>Grbl</code> firmware by <code>Oquam</code>, our own implementation. See here for the instructions on how to flash this firmware to the X-Controller.</p>"},{"location":"plant_imager/build_v3/cnc_electronics/#change-micro-stepping","title":"Change micro-stepping","text":"<p>By default, the X-Controller board is set to 8x microstepping for X &amp; Y axes and to 2x microstepping for the Z axis.</p> <p>DIP switches: - 8x microstepping with switches 1, 3, and 4 in the 'ON' position. - 2x microstepping with only switches 2 and 4 'ON'.</p> <p>You can use these values later in the Oquam <code>config.json</code> (see the related section), but this will limit the precision of the axes.</p> <p>We would thus strongly recommend to change them to 16x microstepping by setting the DIP switches 1, 2 &amp; 4 in the 'ON' position and 3 to 'OFF'.</p>"},{"location":"plant_imager/build_v3/cnc_electronics/#troubleshooting","title":"Troubleshooting","text":""},{"location":"plant_imager/build_v3/cnc_electronics/#parts","title":"Parts","text":"<p>If you burn the X-Controller main board, you can buy a new one here.</p>"},{"location":"plant_imager/build_v3/cnc_frame/","title":"Assembly instructions for the X-Carve CNC frame","text":"<p>We chose the X-Carve 1000 mm as basis for the CNC gantry, feel free to choose another one but beware to check the aluminium frame still fit!</p> X-Carve 1000mm."},{"location":"plant_imager/build_v3/cnc_frame/#bom","title":"BOM","text":"<p>To build the CNC you will need:</p> <ul> <li>X-Carve 1000mmm (US buy here; EU   buy here)</li> </ul> <p>Note</p> <p>We chose the X-Carve because initially you could customize your order and for example not order the work area (wood plate). Apparently it is not possible anymore!</p>"},{"location":"plant_imager/build_v3/cnc_frame/#assembly","title":"Assembly","text":"<p>Follow the official assembly instructions for the X-Carve here.</p> <p>You should follow these steps:</p> <ul> <li>Gantry - Side Plates</li> <li>Gantry - X-Carriage</li> <li>Assemble Gantry</li> <li>Rails</li> <li>Belting</li> </ul> <p>Note</p> <p>Do NOT follow the steps indicating how to:</p> <ul> <li>mount the Z-axis</li> <li>build the work area</li> <li>fix the spindle</li> <li>fix the gantry and rails over the work area (wood plate)</li> </ul>"},{"location":"plant_imager/build_v3/comm_overview/","title":"Plant Imager communications","text":""},{"location":"plant_imager/build_v3/comm_overview/#overview","title":"Overview","text":"<p>We use a Raspberry Pi 4 as a \"main controller\" to:</p> <ol> <li>attach a touchscreen to display and interact with the Plant Imager user interface</li> <li>control the CNC (with <code>Oquam</code>) over USB</li> <li>create an access point</li> <li>control the PiCamera(s)</li> </ol> <p>It thus acts as an Access Point (a.k.a. Hotspot) for the other devices, notably for the PiCamera(s) (Pi Zero W).</p> Overview of the connectivity for the Plant Imager v3. <p>Note</p> <p>The Ethernet connection is not mandatory, you may want to isolate this from the internet for security reasons!</p>"},{"location":"plant_imager/build_v3/comm_overview/#access-point","title":"Access point","text":"<p>As the above schematic illustrate, in the following steps you will create a local network.</p> <p>This will require to choose:</p> <ul> <li>an SSID, that is the name of the network the other devices will connect to</li> <li>a password, to restrict access to this network</li> </ul> <p>For the sake of clarity, we will use:</p> <ul> <li>SSID: <code>Plant Imager</code></li> <li>password: <code>my_secret_password!</code></li> </ul> <p>Warning</p> <p>Change the password as it is not so secret anymore, and use a strong one!</p>"},{"location":"plant_imager/build_v3/comm_overview/#raspberrypi-os","title":"RaspberryPi OS","text":"<p>As the above schematic illustrate, in the following steps you will set up several Operating Systems.</p> <p>This will require to choose:</p> <ul> <li>a username, that is the name of the (admin) user</li> <li>a password, to restrict access to this device</li> </ul> <p>For the sake of clarity, we will use:</p> <ul> <li>username: <code>romi</code></li> <li>password: <code>my_raspberry!</code></li> </ul> <p>Warning</p> <p>Change the password as it is not so secret anymore, and use a strong one!</p>"},{"location":"plant_imager/build_v3/enclosure/","title":"Enclosure assembly","text":"<p>You can find the PDF with the BOM and assembly instructions for the enclosure here.</p> <p>Note</p> <p>You may want to change the orientation of the LED bars to horizontal to reduce shading by the CNC gantry.</p>"},{"location":"plant_imager/build_v3/flash_rpi_os/","title":"Flashing RaspberryPi OS","text":"<p>To flash the OS on the microSD, have a look at the official Raspberry Pi OS instructions here.</p> <p>We strongly advise to use the Raspberry Pi Imager as it will allow you to set the hostname, create a user and its password prior to booting the RPi.</p> <p>If you have access to <code>snaps</code>, use the <code>rpi-imager</code> package from the store: <pre><code>sudo snap install rpi-imager\n</code></pre> This will guarantee to get the latest version of the <code>imager</code> tool with the configurable Advanced options!</p> <p>Here is a screenshot of the Advanced options for the Raspberry Pi Imager v1.7.3:</p> <p></p> <p>This will allow you to save a lot of time and efforts in the following steps, especially you will not have to connect a screen and keyboard to the device!</p>"},{"location":"plant_imager/build_v3/flashing_oquam/","title":"Flashing Oquam on X-Controller","text":"<p>The ROMI project developed the <code>Oquam</code> firmware as an alternative to <code>Grbl</code>. To flash it on the X-Controller follow these instructions.</p> <p>You will need the Arduino IDE software. On Ubuntu, with <code>snap</code>, do:</p> <pre><code>snap install arduino\n</code></pre> <p>For other OS, have a look at the official Arduino software website here.</p>"},{"location":"plant_imager/build_v3/flashing_oquam/#download-the-sources","title":"Download the sources","text":"<p>Start by downloading the <code>libromi</code> sources from the ROMI GitHub repository:</p> <pre><code>git clone https://github.com/romi/libromi.git\n</code></pre> <p>Currently, the software is developed under the <code>encoder_z</code> branch:</p> <pre><code>git checkout encoder_z\n</code></pre>"},{"location":"plant_imager/build_v3/flashing_oquam/#flashing-the-firmware","title":"Flashing the firmware","text":""},{"location":"plant_imager/build_v3/flashing_oquam/#create-a-zip-archive-of-the-arduino-libraries","title":"Create a ZIP archive of the Arduino libraries","text":"<p>You will need to add the <code>RomiSerial</code> as extra library to the Arduino IDE. It is located under <code>libromi/arduino_libraries/RomiSerial</code>.</p> <p>To create a ZIP archive: <pre><code>zip -r RomiSerial.zip RomiSerial\n</code></pre></p>"},{"location":"plant_imager/build_v3/flashing_oquam/#open-the-arduino-ide","title":"Open the Arduino IDE","text":"<p>Open the <code>libromi/firmware/Oquam/Oquam.ino</code> sketch using <code>File &gt; Open...</code> or with the Ctrl + O shortcut. Then browse and select the INO file.</p>"},{"location":"plant_imager/build_v3/flashing_oquam/#add-the-romiserial-library","title":"Add the <code>RomiSerial</code> library","text":"<p>Add the <code>RomiSerial.zip</code> library using the <code>Sketch &gt; Include Library &gt; Add .ZIP Library...</code> menu. Then browse and select the ZIP file.</p>"},{"location":"plant_imager/build_v3/flashing_oquam/#select-the-board","title":"Select the board","text":"<p>Select the right board to upload to using the <code>Tools &gt; Board</code> menu and select <code>Arduino UNO</code>.</p>"},{"location":"plant_imager/build_v3/flashing_oquam/#power-up-the-x-controller","title":"Power up the X-Controller","text":"<p>Connect the power cord and start the X-Controller.</p>"},{"location":"plant_imager/build_v3/flashing_oquam/#select-the-usb-port","title":"Select the USB port","text":"<p>Plug the USB cable and select the right USB port to upload to using <code>Tools &gt; Port</code> menu.</p> <p>See find the USB port to determine which port the X-Controller is connected to.</p>"},{"location":"plant_imager/build_v3/flashing_oquam/#flash-the-firmware","title":"Flash the firmware","text":"<p>You may now flash the firmware with the  icon below the main menu.</p> <p>Wait for the IDE to indicate <code>Done uploading</code> on the status bar at the bottom right of the IDE window. Then you can unplug safely!</p>"},{"location":"plant_imager/build_v3/flashing_oquam/#troubleshooting","title":"Troubleshooting","text":""},{"location":"plant_imager/build_v3/flashing_oquam/#find-the-usb-port","title":"Find the USB port","text":"<p>To find which USB port your arduino board is connected to, you can use <code>dmesg</code>:</p> <ol> <li>make sure the usb cable is unplugged</li> <li>run <code>dmesg -w</code> in a terminal</li> <li>connect the usb (from the arduino board to the computer) and you should see something like:</li> </ol> <pre><code>[14818.631347] usb 1-4: new full-speed USB device number 7 using xhci_hcd\n[14818.787048] usb 1-4: New USB device found, idVendor=0403, idProduct=6001, bcdDevice= 6.00\n[14818.787062] usb 1-4: New USB device strings: Mfr=1, Product=2, SerialNumber=3\n[14818.787069] usb 1-4: Product: X-Controller\n[14818.787073] usb 1-4: Manufacturer: Inventables\n[14818.787077] usb 1-4: SerialNumber: XCONTROLLER6CFIRF\n[14818.847148] usbcore: registered new interface driver usbserial_generic\n[14818.847155] usbserial: USB Serial support registered for generic\n[14818.851163] usbcore: registered new interface driver ftdi_sio\n[14818.851179] usbserial: USB Serial support registered for FTDI USB Serial Device\n[14818.851367] ftdi_sio 1-4:1.0: FTDI USB Serial Device converter detected\n[14818.851402] usb 1-4: Detected FT232RL\n[14818.851860] usb 1-4: FTDI USB Serial Device converter now attached to ttyUSB0\n</code></pre> <p>Important</p> <p>The important info here is <code>ttyUSB0</code>!</p>"},{"location":"plant_imager/build_v3/flashing_oquam/#serial-access-denied","title":"Serial access denied","text":"<p>If you get an error about permission access:</p> <ol> <li> <p>Check you selected the right USB port. If yes, proceed to the next steps.</p> </li> <li> <p>To see which groups your <code>$USER</code> belongs to:     <pre><code>groups ${USER}\n</code></pre>     If you see the <code>dialout</code> group there, go back to checking the USB port!</p> </li> <li> <p>If you are not in <code>dialout</code>:     <pre><code>sudo gpasswd --add ${USER} dialout\n</code></pre></p> </li> <li> <p>Then log out for your session, and then log back in to see changes (with <code>groups ${USER}</code>).</p> </li> </ol>"},{"location":"plant_imager/build_v3/frame_cnc/","title":"Enclosure &amp; CNC assembly","text":"<p>You may now put the X-Carve CNC into the enclosure as follows:</p> <p></p> <p>Note</p> <p>While the image above shows the CNC mounted in the aluminium frame with the wooden enclosure, it might be more practical to only mount the wooden panels AFTER mounting the CNC.</p>"},{"location":"plant_imager/build_v3/manual_gimbal/","title":"Manual gimbal","text":"CAD - Manual gimbal with 360\u00b0 protractor &amp; PiCamera."},{"location":"plant_imager/build_v3/manual_gimbal/#bom","title":"BOM","text":"<p>To assemble a SINGLE manual gimbal, you will need the following list of materials:</p> ID Name Qty Off the shelf / Custom Material Manufacturer Serial number Link 1 MG004 1 Custom Aluminium pdf 2 MG009 1 Custom Aluminium pdf 3 MG010 1 Custom Aluminium pdf 4 MG012 1 Custom Aluminium pdf 5 MG011 1 Custom PMMA RS PRO 434-295 pdf, dxf, rs-online.com 6 MG014 1 Custom PMMA dxf 8 Protractor dial deg 1 Custom PMMA dxf 9 Spacer M5x8 2 Off the shelf Vis express 8470100818 vis-express.fr 10 Spacer M5x20 1 Off the shelf Vis express 8470102018 vis-express.fr 11 Bouton de serrage M5 3 Off the shelf RS PRO 771-673 rs-online.com 12 M5x40 Socket Head 1 Off the shelf RS PRO 293-353 rs-online.com 13 M5x30 Socket Head 2 Off the shelf RS PRO 290-130 rs-online.com 14 M5x12 Socket Head 2 Off the Shelf RS PRO 281-079 rs-online.com 15 M4x16 Flat Head 2 Off the Shelf RS PRO 171-837 rs-online.com 16 M4x12 Socket Head 1 Off the shelf RS PRO 281-035 rs-online.com 17 M3x8 Socket Head 2 Off the Shelf RS PRO 280-997 rs-online.com 18 Bosch nut 2 Off the Shelf Bosch Rexroth 3842523142 rs-online.com 19 Washer M4 L 1 Off the Shelf Vis express 3534000402 vis-express.fr 20 IGUS GTM-0818-020 2 Off the Shelf IGUS GTM-0818-020 amazon.fr"},{"location":"plant_imager/build_v3/manual_gimbal/#assembly","title":"Assembly","text":"<p>You can find the PDF with the BOM and assembly instructions for the manual gimbal here.</p> Manual gimbal with 360\u00b0 protractor &amp; PiCamera."},{"location":"plant_imager/build_v3/pan_arm/","title":"Pan arm assembly","text":"Pan arm - Front (CAD) Pan arm - Back (CAD)"},{"location":"plant_imager/build_v3/pan_arm/#bom","title":"BOM","text":"ID Name Qty Off the shelf / Custom Material Manufacturer Serial number Link 1 PLI004 1 Custom Aluminium pdf 2 PLI006 1 Custom Steel Ewellix Makers in Motion LJM12X600ESSC2 pdf, rs-online.com 3 PLI007-3 1 Custom Aluminium pdf 4 PLI009 1 Custom Aluminium pdf 5 PLI011 1 Custom Aluminium pdf 6 PLI012 1 Custom Aluminium pdf 7 PLI016 1 Custom Aluminium pdf 8 Cable Zip 3 Custom PLA stl 9 Dozenal_ruler_30cm 1 Custom PMMA RS PRO 434-295 dxf, rs-online.com 10 PROFILE_D_ETAYAGE_20x40x300 1 Off the shelf Bosch Rexroth R987501076 rs-online.com 11 Slip Ring ROB-13063 1 Off the shelf SparkFun ROB-13063 mouser.fr 12 Nema 23 - 45mm 1 Off the shelf Stepperonline 23HS18-2004H omc-stepperonline.com 13 Convertisseur 24-5V 1 Off the shelf amazon.com 14 Huco 047_20 1 Off the shelf Huco 047.20.3535 rs-online.com 15 CUI_DEVICES_AMT132Q-0048-1200 1 Off the shelf CUI Devices AMT132Q-V eu.mouser.com 16 AMT Cable 1 Off the shelf CUI Devices AMT-18C-3-036 eu.mouser.com 17 AMT Viewpoint Programming Cable 1 Off the shelf CUI Devices AMT-PGRM-18C eu.mouser.com 18 SKF_P 12 TF 2 Off the shelf SKF P 12 TF rs-online.com 19 MQCL-12-A 1 Off the shelf Ruland MQCL-12-A rs-online.com 20 M8x16 Socket Head 4 Off the shelf RS PRO 529-703 rs-online.com 21 M5x10 Shoulder Screw 2 Off the shelf RS PRO 292-271 rs-online.com 22 M3x12 Set Screw 1 Off the shelf RS PRO 137-736 rs-online.com 23 M3x20 Socket Head 2 Off the shelf RS PRO 293-319 rs-online.com 24 M4x12 Socket Head 2 Off the shelf RS PRO 281-035 rs-online.com 25 M5x18 Socket Head 4 Off the shelf Vis express 8420501818 vis-express.fr 26 M5x16 Flat Head 4 Off the shelf RS PRO 232-8423 rs-online.com 27 M6x40 Flat Head 2 Off the shelf RS PRO 917-6238 rs-online.com 28 M5x8 Socket Head 2 Off the shelf RS PRO 660-4633 rs-online.com 29 M5x8 Button Head 3 Off the shelf RS PRO 822-9114 rs-online.com 30 Bosch nut 5 Off the shelf Bosch Rexroth 3842523142 rs-online.com 31 M3 Locknut 2 Off the shelf RS PRO 524-281 rs-online.com 32 M4 Locknut 2 Off the shelf RS PRO 524-304 rs-online.com 33 M5 Locknut 10 Off the shelf RS PRO 122-4371 rs-online.com"},{"location":"plant_imager/build_v3/pan_arm/#assembly","title":"Assembly","text":"<p>You can find the PDF with the BOM and assembly instructions for the arm here.</p> Pan arm with two PiCamera - Front Pan arm with two PiCamera - Back"},{"location":"plant_imager/build_v3/pan_electronics/","title":"Pan arm electronics","text":""},{"location":"plant_imager/build_v3/pan_electronics/#pan-motor","title":"Pan motor","text":"<p>We replaced the original z-axis stepper motor by an hollow shaft stepper motor from stepperonline <code>23HS18-2004H</code>. Full specifications can be accessed here.</p> <p>Wiring of <code>23HS18-2004H</code>:</p> A+ A- B+ B- red red/white green/white green <p>The original motor was <code>KL23H251-28-4AP</code> from Automation Technology. Full specifications can be accessed here.</p> <p>Wiring of <code>KL23H251-28-4AP</code>:</p> Phase A+ A- B+ B- Pin A C B D Color white red green black <p>Thus, the re-wiring looks like this:</p> <ul> <li>X-Carve white wire -&gt; HS stepper red wire</li> <li>X-Carve red wire -&gt; HS stepper red/white wire</li> <li>X-Carve green wire -&gt; HS stepper green/white wire</li> <li>X-Carve black wire -&gt; HS stepper green wire</li> </ul>"},{"location":"plant_imager/build_v3/pan_electronics/#incremental-rotary-encoder","title":"Incremental rotary encoder","text":"<p>The encoder has 4096 steps and this is too fine for the arduino to see. From our tests, it seems very stable at 360.</p>"},{"location":"plant_imager/build_v3/pan_electronics/#change-the-steps-resolution","title":"Change the steps resolution","text":"<ul> <li>get the AMT Cable &amp; the AMT Viewpoint Programming Cable (parts 16 &amp; 17 from the BOM list)</li> <li>load the ATM Viewpoint software (Windows only)</li> <li>set the resolution to <code>360</code>.</li> <li>make sure you then align the <code>0</code> again after you change resolution.</li> </ul> <p>If that does not work:</p> <ul> <li>Drink a beer</li> <li>Try again</li> <li>Repeat as necessary.</li> </ul>"},{"location":"plant_imager/build_v3/pan_electronics/#testing-with-an-arduino","title":"Testing with an Arduino","text":""},{"location":"plant_imager/build_v3/pan_electronics/#wiring","title":"Wiring","text":"<p>To test the encoder with an Arduino UNO, wire the encoder a follows:</p> <ul> <li>Encoder Pin 12 Z+ (blue) -&gt; Arduino Pin 11</li> <li>Encoder Pin 13 Z- (blue &amp; white) -&gt; Arduino GND</li> <li>Encoder Pin 04 GND (green &amp; Brown) -&gt; Arduino GND</li> <li>Encoder Pin 06 +5V (red) -&gt; Arduino +5</li> </ul> <p>You can also wire two switches to PIN9 &amp; GND and to PIN10 and GND of the UNO. It is also possible to wire just one switch and swap PINs during the homing procedure, but beware of shortcuts.</p>"},{"location":"plant_imager/build_v3/pan_electronics/#testing","title":"Testing","text":"<p>To test the encoder, we recommend to use the serial monitor of the Arduino IDE as you can manually select <code>Both NL &amp; CR</code> as EOL.</p> <ol> <li>Pull the Arduino script <code>libromi/firmware/Oquam/Oquam.ino</code> from the <code>encoder_z</code> branch of the <code>libromi</code> repository.</li> <li>Then build / upload, refers to the detailed flashing instructions.</li> <li>Using the serial monitor of the Arduino IDE, deactivate the X &amp; Y axes (disables axes <code>0</code> &amp; <code>1</code>, and enables <code>2</code>) by sending:     <pre><code>#h[-1,-1,2]:xxxx\n</code></pre></li> <li>Then test the homing procedure for the theta axis by sending:     <pre><code>#H:xxxx\n</code></pre>     While the board is waiting for homing, turn the encoder, once you hit the <code>0</code> you will see a \"homing complete\" message.</li> <li>Now, let's try to home all axis. So activate all of them with:     <pre><code>#h[0,1,2]:xxxx\n</code></pre></li> <li>Then test the homing procedure for all axes by sending:     <pre><code>#H:xxxx\n</code></pre>     While the board is waiting for homing, press the X switch, then the Y switch and finally turn the encoder.     When you release the switches you will see a \"homing complete\" message.     Again, with the encoder, once you hit the <code>0</code> you will see a \"homing complete\" message.</li> </ol> <p>Important</p> <p>The <code>RomiSerial</code> expect both \"newline\" (NL) AND \"carriage return\" (CR)! So you have to select <code>Both NL &amp; CR</code> instead of the default <code>Newline</code> in the serial monitor of the Arduino IDE.</p>"},{"location":"plant_imager/build_v3/pan_electronics/#x-carve-controller","title":"X-Carve controller","text":""},{"location":"plant_imager/build_v3/pan_electronics/#wiring_1","title":"Wiring","text":"<p>Simply wire the rotary encoder Z+/Z- pins to the red/black Z-limit wires:</p> <ul> <li>Encoder Pin 12 Z+ (blue w/ white stripe) -&gt; red Z-limit wire of the X-Carve controller (PIN12, not PIN11 as with grblShield!)</li> <li>Encoder Pin 13 Z- (white w/ blue stripe) -&gt; black Z-limit wire of the X-Carve controller</li> <li>Encoder Pin 04 GND (green w/ red strip) -&gt; black wire (-) of the 24/5V DC converter</li> <li>Encoder Pin 06 +5V (red w/ green stripe) -&gt; red wire (+) of the 24/5V DC converter</li> </ul>"},{"location":"plant_imager/build_v3/pan_electronics/#test","title":"Test","text":"<p>With the <code>Oquam</code> firmware flashed on the X-Controller (instructions here) and the correct wiring:</p> <ol> <li>Open the Arduino IDE</li> <li>Plug the USB from the X-Controller to your computer</li> <li>Select the right USB port (assume <code>ttyUSB0</code>)</li> <li>Open the serial monitor of the Arduino IDE, select <code>Both NL &amp; CR</code>, send:     <pre><code>#h[0,1,2]:xxxx\n#H:xxxx\n</code></pre>     The X-Carve should home X &amp; Y axes, then you turn the encoder and when it hits home it should stop the homing process.</li> </ol>"},{"location":"plant_imager/build_v3/picamera/","title":"PiCamera HQ","text":"PiCamera."},{"location":"plant_imager/build_v3/picamera/#bom","title":"BOM","text":"<p>To assemble a SINGLE PiCamera HQ, you will need the following list of materials:</p> ID Name Qty Off the shelf / Custom Material Manufacturer Serial number Link 7 Mounting Plate Rpi Zero Hq Camera 1 Custom PMMA dxf 21 RaspberryPiZeroW 1 Off the Shelf kubii.fr 22 Raspberry Pi HQ Camera 1 Off the Shelf kubii.fr 23 6mm CS-mount lens 1 Off the Shelf kubii.fr 24 Ribbon cable 1 Off the Shelf kubii.fr 25 5mm M2.5 nylon screws 8 Off the Shell thepihut.com"},{"location":"plant_imager/build_v3/picamera/#assembly","title":"Assembly","text":"<p>Todo</p> <p>Write assembly instructions!</p>"},{"location":"plant_imager/build_v3/picamera_soft/","title":"PiCamera - Software setup","text":""},{"location":"plant_imager/build_v3/picamera_soft/#flash-raspbian-lite-os","title":"Flash Raspbian Lite OS","text":"<p>The procedure to flash the Raspberry Pi OS on the microSD card AND to set up the necessary options is:</p> <ol> <li>Open the <code>Imager</code> app</li> <li>Choose the OS, we use the <code>Raspberry Pi OS Lite (32bit)</code> as we have an PiZW</li> <li>Select the storage device (microSD)</li> <li>Click the \"Advanced options\" icon (bottom right)</li> <li>\"Set hostname\" to <code>picamera</code>, add a number or another indication if you plan to have more than one </li> <li>\"Enable SSH\" to \"Use password authentication\"</li> <li>\"Set username and password\" to <code>romi</code> and <code>my_raspberry!</code></li> <li>\"Configure wireless LAN\" to:<ul> <li>SSID: <code>Plant Imager</code></li> <li>password: <code>my_secret_password!</code></li> <li>Set the \"Wireless LAN country\"</li> </ul> </li> <li>Change the \"locale settings\" to match yours</li> <li>Finally, hit the \"Write\" button to flash the OS to the microSD.</li> </ol> <p>Important</p> <p>We use the 32bit version with the Raspberry Pi OS Lite NOT Desktop!.</p> <p>Important</p> <p>Do NOT forget to replace the (not so) secret password by the one you used!</p> <p>You should end up with something like this:</p> <p></p>"},{"location":"plant_imager/build_v3/picamera_soft/#manually-setting-the-advanced-options","title":"Manually setting the advanced options","text":"<p>Hereafter we show how to manually define the advanced options after flashing the OS without defining any. This requires to boot the RPi with a screen and keyboard.</p> <p>Warning</p> <p>There is NO NEED to do this if you have followed the previous instructions on how to configure the advanced options prior to flashing the microSD card!</p> <p>The first time you boot your RPi with your new image, you can follow the steps of the \"Welcome\" screen:</p> <ul> <li>Select a country, language, keyboard layout, timezone.</li> <li>Connect to a Wi-Fi network, here our access point:</li> <li>SSID: <code>Plant Imager</code></li> <li>password: <code>my_secret_password!</code></li> <li>Change the default user <code>pi</code> and password <code>raspberry</code> to:</li> <li>user: <code>romi</code> </li> <li>password: <code>my_raspberry!</code>.</li> <li>(Optional) Update packages to their newest version, this OBVIOUSLY requires an internet connexion.</li> </ul> <p>Important</p> <p>Do NOT forget to replace the (not so) secret password by the one you used!</p>"},{"location":"plant_imager/build_v3/picamera_soft/#1-set-the-hostname","title":"1. Set the <code>hostname</code>","text":"<p>We strongly advise to give a specific <code>hostname</code> to each device to avoid having the all named <code>raspberrypi</code>.</p> <p>Important</p> <p>RFCs mandate that a hostname's labels may contain only the ASCII letters 'a' through 'z' (case-insensitive), the digits '0' through '9', and the hyphen. Hostname labels cannot begin or end with a hyphen. No other symbols, punctuation characters, or blank spaces are permitted.</p> <p>Choose an option, then reboot the RPi!</p> raspi-configcommand-line <p>It is possible to set the <code>hostname</code> with the <code>raspi-config</code> tool, in a terminal: <pre><code>sudo raspi-config\n</code></pre> Then move to <code>1 System Options &gt; S4 Hostname</code>. Enter the desired hostname, e.g. <code>picamera</code>, and hit <code>&lt;OK&gt;</code>.</p> <p>It is possible to set the <code>hostname</code> manually by changing <code>/etc/hostname</code> &amp; <code>/etc/hosts</code> with: <pre><code>export NEW_HNAME=\"picamera\"\nsudo sed \"s/raspberrypi/$NEW_HNAME/\" /etc/hostname\nsudo sed \"s/raspberrypi/$NEW_HNAME/\" /etc/hosts\n</code></pre></p>"},{"location":"plant_imager/build_v3/picamera_soft/#2-enable-ssh","title":"2. Enable SSH","text":"<p>Enabling SSH allows to easily connect to all the devices (connected to the <code>Plant Imager</code> Access Point) using a terminal.</p> <p>Warning</p> <p>This represents a security risk if you do not change the default user <code>pi</code> and password <code>raspberry</code> or use a weak password!</p> raspi-configcommand-line <p>It is possible to enable SSH with the <code>raspi-config</code> tool, in a terminal: <pre><code>sudo raspi-config\n</code></pre> Then move to <code>5 Interfacing Options &gt; P2 SSH</code>, and select <code>&lt;Yes&gt;</code>.</p> <p>It is possible to enable SSH with <code>systemctl</code>, in a terminal: <pre><code>sudo systemctl enable ssh    \nsudo systemctl start ssh    </code></pre></p> <p>Once the <code>picamera</code> device is connected to a network, you can SSH to this device using a machine connected to the same network. You will need the IP address of the device you want to connect to. It is accessible in a terminal with: <pre><code>hostname -l\n</code></pre></p>"},{"location":"plant_imager/build_v3/picamera_soft/#3-set-username-and-password","title":"3. Set username and password","text":"<p>If at the first boot you did NOT change the default <code>pi</code> user &amp; <code>raspberry</code> password you can still do it as follows:</p> <ol> <li>Create the user <code>romi</code> with:     <pre><code>sudo adduser romi\n</code></pre>     This will also create the home directory for this user and ask for a password.</li> <li>Add this user to <code>dialout</code>, <code>video</code> &amp; <code>sudo</code> groups with:     <pre><code>sudo adduser romi dialout\nsudo adduser romi video\nsudo adduser romi sudo\n</code></pre></li> <li>Remove the default <code>pi</code> user with:     <pre><code>sudo deluser --remove-home pi\n</code></pre></li> </ol>"},{"location":"plant_imager/build_v3/picamera_soft/#4-configure-wireless-lan","title":"4. Configure wireless LAN","text":"raspi-configcommand-line <p>It is possible to configure the wireless LAN with the <code>raspi-config</code> tool, in a terminal: <pre><code>sudo raspi-config\n</code></pre> Then move to <code>1 System options &gt; S2 Wireless LAN</code>, then enter your SSID <code>Plant Imager</code> and password <code>my_secret_password!</code>.</p> <p>You can edit the <code>wpa_supplicant.conf</code> file to add the <code>Plant Imager</code> network information. Copy/paste the next lines in your favorite editor, change the <code>psk</code>, then copy/paste this in the terminal: <pre><code>cat &lt;&lt; EOF &gt;&gt; /etc/wpa_supplicant/wpa_supplicant.conf\nnetwork={\n  ssid=\"Plant Imager\"\n  psk=\"my_secret_password!\"\n}\nEOF\n</code></pre> This will add the lines 2 to 5 to the end of the <code>wpa_supplicant.conf</code> file.</p> <p>Important</p> <p>Do NOT forget to replace the (not so) secret password by the one you used!</p>"},{"location":"plant_imager/build_v3/picamera_soft/#5-change-the-locale-settings","title":"5. Change the locale settings","text":"raspi-configcommand-line <p>It is possible to change the locale settings with the <code>raspi-config</code> tool, in a terminal: <pre><code>sudo raspi-config\n</code></pre> Then move to <code>5 Localisation Options</code>, then set <code>L1 Locale</code>, <code>L2 Timezone</code>, <code>L3 Keyboard</code> and <code>L4 WLAN country</code>.</p> <p>It is possible to change the locale settings with the <code>dpkg-reconfigure</code> tool, in a terminal: <pre><code>sudo dpkg-reconfigure locales\n</code></pre> Then follow the instructions in the terminal prompt. </p>"},{"location":"plant_imager/build_v3/picamera_soft/#romi-software","title":"ROMI software","text":"<p>We will now set up the necessary ROMI software to enable the camera and make the PiCamera device communicate with the main controller. Notably to be able to capture the images upon request by the Plant Imager main controller.</p>"},{"location":"plant_imager/build_v3/picamera_soft/#install-requirements","title":"Install requirements","text":"<p>To install the requirements, in a terminal simply run: <pre><code>sudo apt install build-essential cmake git libpng-dev libjpeg9-dev\n</code></pre></p> <p>We also deactivate something useless in our case (dynamic linker): <pre><code>sudo mv /etc/ld.so.preload /etc/ld.so.preload.bak\n</code></pre></p>"},{"location":"plant_imager/build_v3/picamera_soft/#clone-the-sources","title":"Clone the sources","text":"<p>To clone the sources from the ROMI GitHub repository, simply run: <pre><code>git clone --branch ci_dev --recurse-submodules https://github.com/romi/romi-rover-build-and-test.git\n</code></pre></p> <p>Note</p> <p>The <code>--recurse-submodules</code> option will automatically initialize and update each submodule in the repository.</p>"},{"location":"plant_imager/build_v3/picamera_soft/#compile-the-sources","title":"Compile the sources","text":"<p>Then move to the cloned <code>romi-rover-build-and-test</code> directory and compile the <code>romi-camera</code> app with: <pre><code>cd romi-rover-build-and-test\nmkdir build\ncd build\ncmake ..\nmake romi-camera\n</code></pre></p>"},{"location":"plant_imager/build_v3/picamera_soft/#test-the-romi-camera-app","title":"Test the <code>romi-camera</code> app","text":"<p>Assuming:</p> <ul> <li>everything went well in the previous step</li> <li>you are connected to the <code>Plant Imager</code> network</li> <li>you want to register a camera named <code>camera-top</code></li> <li>it has an HQ (v2) camera lens</li> </ul> <p>You can test the <code>romi-camera</code> app with: <pre><code>./bin/romi-camera --registry 10.10.10.1 --topic camera-top --camera-version hq --mode still --width 2028 --height 1520 &amp;\n</code></pre></p>"},{"location":"plant_imager/build_v3/picamera_soft/#automatically-enable-the-romi-camera-app-at-startup","title":"Automatically enable the <code>romi-camera</code> app at startup","text":"<p>To activate the <code>romi-camera</code> app at startup, we add the previous command to <code>/etc/rc.local</code> as follows: <pre><code>cat &lt;&lt; EOF &gt;&gt; /etc/rc.local\n\nsudo -u romi /home/romi/romi-rover-build-and-test/build/bin/romi-camera --registry 10.10.10.1 --topic camera-top --camera-version hq --mode still --width 2028 --height 1520 &amp;\nEOF\n</code></pre></p> <p>This assumes you have created a romi user, if not, change the <code>romi</code>:</p> <ul> <li>in <code>sudo -u romi</code> to the correct username</li> <li>in <code>/home/romi</code> to the correct username</li> </ul> <p>Todo</p> <p>The use of <code>rc.local</code> is obsolete... use <code>systemd</code> instead!</p>"},{"location":"plant_imager/build_v3/plantimager_controller/","title":"Plant Imager controller","text":""},{"location":"plant_imager/build_v3/plantimager_controller/#bom","title":"BOM","text":"<p>The bill of material is quite simple:</p> <ul> <li>Raspberry Pi 4B 4GB or 8GB</li> <li>Power supply, 5.1 V, 3.0 A, USB Type-C like this</li> <li>7\" TFT LCD TouchScreen 800x480 like this</li> <li>7\" TouchScreen case like this</li> <li>a microSD card, with a minimum of 8Go</li> </ul>"},{"location":"plant_imager/build_v3/plantimager_controller/#flash-raspberry-pi-os","title":"Flash Raspberry Pi OS","text":"<p>The procedure to install and configure is as follows:</p> <ol> <li>Open the <code>Imager</code> app</li> <li>Choose the OS, we use the <code>Raspberry Pi OS (64bit)</code> as we have an RPi4</li> <li>Select the storage device (microSD)</li> <li>Click the \"Advanced options\" icon (bottom right)</li> <li>\"Set hostname\" to <code>plant-imager</code>, add a number or another indication if you plan to have more than one </li> <li>Do NOT \"Enable SSH\", except if you know what you are doing!</li> <li>\"Set username and password\" to <code>romi</code> and <code>my_raspberry!</code></li> <li>Do NOT \"Configure wireless LAN\", except the \"Wireless LAN country\", as we will later create an Access Point!</li> <li>Change the \"locale settings\" to match yours</li> <li>Finally, hit the \"Write\" button to flash the OS to the microSD.</li> </ol> <p>Important</p> <p>We use the 64bit version with the Raspberry Pi Desktop NOT \"Lite\" as we want to use it to display a user interface.</p> <p>Important</p> <p>Do NOT forget to replace the (not so) secret password by the one you used!</p> <p>You should end up with something like this:</p> <p></p>"},{"location":"plant_imager/build_v3/plantimager_controller/#manually-setting-the-advanced-options","title":"Manually setting the advanced options","text":"<p>Hereafter we show how to manually define the advanced options after flashing the OS without defining any. This requires to boot the RPi with a screen and keyboard.</p> <p>Warning</p> <p>There is NO NEED to do this if you have followed the previous instructions on how to configure the advanced options prior to flashing the microSD card!</p> <p>The first time you boot your RPi with your new image, you can follow the steps of the \"Welcome\" screen:</p> <ul> <li>Select a country, language, keyboard layout, timezone.</li> <li>Do NOT connect to a Wi-Fi network!</li> <li>Change the default user <code>pi</code> and password <code>raspberry</code> to:</li> <li>user: <code>romi</code> </li> <li>password: <code>my_raspberry!</code>.</li> <li>(Optional) Update packages to their newest version, this OBVIOUSLY requires an internet connexion.</li> </ul> <p>Important</p> <p>Do NOT forget to replace the (not so) secret password by the one you used!</p>"},{"location":"plant_imager/build_v3/plantimager_controller/#1-set-the-hostname","title":"1. Set the <code>hostname</code>","text":"<p>We strongly advise to give a specific <code>hostname</code> to each device to avoid having the all named <code>raspberrypi</code>.</p> <p>Important</p> <p>RFCs mandate that a hostname's labels may contain only the ASCII letters 'a' through 'z' (case-insensitive), the digits '0' through '9', and the hyphen. Hostname labels cannot begin or end with a hyphen. No other symbols, punctuation characters, or blank spaces are permitted.</p> <p>Choose an option, then reboot the RPi!</p> raspi-configcommand-line <p>It is possible to set the <code>hostname</code> with the <code>raspi-config</code> tool, in a terminal: <pre><code>sudo raspi-config\n</code></pre> Then move to <code>1 System Options &gt; S4 Hostname</code>. Enter the desired hostname, e.g. <code>plant-imager</code>, and hit <code>&lt;OK&gt;</code>.</p> <p>It is possible to set the <code>hostname</code> manually by changing <code>/etc/hostname</code> &amp; <code>/etc/hosts</code> with: <pre><code>export NEW_HNAME=\"plant-imager\"\nsudo sed \"s/raspberrypi/$NEW_HNAME/\" /etc/hostname\nsudo sed \"s/raspberrypi/$NEW_HNAME/\" /etc/hosts\n</code></pre></p>"},{"location":"plant_imager/build_v3/plantimager_controller/#2-enable-ssh","title":"2. Enable SSH","text":"<p>Enabling SSH allows to connect to the <code>plant-imager</code> device from any machine connected to the <code>Plant Imager</code> Access Point.</p> <p>Warning</p> <p>This represents a security risk if you do not change the default user <code>pi</code> and password <code>raspberry</code> or use a weak password!</p> raspi-configcommand-line <p>It is possible to enable SSH with the <code>raspi-config</code> tool, in a terminal: <pre><code>sudo raspi-config\n</code></pre> Then move to <code>5 Interfacing Options &gt; P2 SSH</code>, and select <code>&lt;Yes&gt;</code>.</p> <p>It is possible to enable SSH with <code>systemctl</code>, in a terminal: <pre><code>sudo systemctl enable ssh    \nsudo systemctl start ssh    </code></pre></p> <p>Once the <code>plant-imager</code> device will reboot, you can SSH to this device using a machine connected to the same network. You will need the IP address of the <code>plant-imager</code> device. It is accessible in a terminal with: <pre><code>hostname -l\n</code></pre> It should be <code>10.10.10.1</code>.</p>"},{"location":"plant_imager/build_v3/plantimager_controller/#3-set-username-and-password","title":"3. Set username and password","text":"<p>If at the first boot you did NOT change the default <code>pi</code> user &amp; <code>raspberry</code> password you can still do it as follows:</p> <ol> <li>Create the user <code>romi</code> with:     <pre><code>sudo adduser romi\n</code></pre>     This will also create the home directory for this user and ask for a password.</li> <li>Add this user to <code>dialout</code>, <code>video</code> &amp; <code>sudo</code> groups with:     <pre><code>sudo adduser romi dialout\nsudo adduser romi video\nsudo adduser romi sudo\n</code></pre></li> <li>Remove the default <code>pi</code> user with:     <pre><code>sudo deluser --remove-home pi\n</code></pre></li> </ol>"},{"location":"plant_imager/build_v3/plantimager_controller/#rotate-the-7-touchscreen","title":"Rotate the 7\" touchscreen","text":"<p>To wire the 7\" touchscreen you can follow these instructions. Some people says that there is no need to wire SCL &amp; SDA... Do what you want!</p> <p>If you are using the 7\" touchscreen with the case we described in the BOM, you may notice it is rotated by 180\u00b0. To rotate the 7\" touchscreen, with an RPi 4 and the latest Raspberry OS:</p> <ol> <li>Hit the raspberry main menu</li> <li>Got to <code>Preference</code> &gt; <code>Screen Configuration</code></li> <li>On the <code>Screen Layout Editor</code> menu bar, hit <code>Configure</code>, select <code>Screens</code> &gt; <code>DSI-1</code> &gt; <code>Orientation</code> and select <code>inverted</code>.</li> <li>Hit the \"Green check\" icon  to validate!</li> <li>It should ask you to reboot to apply changes.</li> </ol>"},{"location":"plant_imager/build_v3/plantimager_controller/#romi-software","title":"ROMI software","text":""},{"location":"plant_imager/build_v3/plantimager_controller/#install-oquam","title":"Install <code>Oquam</code>","text":""},{"location":"plant_imager/build_v3/plantimager_controller/#install-the-system-requirements","title":"Install the system requirements","text":"<p>To install the system requirements, simply run: <pre><code>sudo apt install build-essential cmake git libpng-dev libjpeg-dev\n</code></pre></p>"},{"location":"plant_imager/build_v3/plantimager_controller/#clone-the-sources","title":"Clone the sources","text":"<p>To clone the sources from the ROMI GitHub repository, simply run: <pre><code>git clone --branch ci_dev --recurse-submodules https://github.com/romi/romi-rover-build-and-test.git\n</code></pre></p> <p>Note</p> <p>The <code>--recurse-submodules</code> option will automatically initialize and update each submodule in the repository.</p>"},{"location":"plant_imager/build_v3/plantimager_controller/#compile-the-sources","title":"Compile the sources","text":"<p>Then move to the cloned directory and compile the <code>oquam</code> app with: <pre><code>cd romi-rover-build-and-test\nmkdir build\ncd build\ncmake ..\nmake oquam\n</code></pre></p>"},{"location":"plant_imager/build_v3/plantimager_controller/#configure-the-controller-to-act-as-a-hotspot","title":"Configure the controller to act as a hotspot","text":"<p>To configure the RPi4 to act as a hotspot you may use the <code>network-hotspot-setup.sh</code> CLI, as root: <pre><code>sudo bash tests-hardware/network-hotspot-setup.sh --wlan wlan0 --ssid \"Plant Imager\" --pwd \"my_secret_password!\"\n</code></pre></p> <p>This will:</p> <ul> <li>install the required dependencies</li> <li>configure the routing tables</li> <li>set the name of the SSID</li> <li>set the password to use to connect to the hotspot</li> </ul> <p>Important</p> <p>Change the password as it is not so secret anymore!</p>"},{"location":"plant_imager/build_v3/plantimager_controller/#configure-rcom","title":"Configure <code>rcom</code>","text":"<pre><code>cd romi-rover-build-and-test\ngit submodule init  # if not done yet (empty 'rcom' directory)\ngit submodule update\ncd build\ncmake ..\nmake rcom-registry\n</code></pre> <p>Start <code>rcom-registry</code>: <pre><code>./bin/rcom-registry\n</code></pre></p> <p>Power the Picamera, you should see a message when it connects to the AP!</p>"},{"location":"plant_imager/build_v3/plantimager_controller/#configure-live-preview","title":"Configure live-preview","text":"<p>You have to install and configure <code>appache2</code>.</p> <p>Let's start by installing the requirements: <pre><code>sudo apt install apache2\n</code></pre></p> <p>Change <code>DocumentRoot</code> in <code>/etc/apache2/sites-enabled/000-default.conf</code> to <code>/home/romi/romi-rover-build-and-test/applications/romi-monitor/</code></p> <p>Also add: <pre><code>        &lt;Directory /home/romi/romi-rover-build-and-test/applications/romi-monitor/&gt;\n                   Options Indexes\n                   AllowOverride None\n                   Require all granted\n        &lt;/Directory&gt;\n</code></pre> Then restart the <code>apache2.service</code> with <code>systemctl</code>: <pre><code>sudo systemctl restart apache2.service\n</code></pre></p> <p>Finally, on the controller, open a web browser (Chromium on the RPi) at <code>10.10.10.1/camera.html</code>. Then scroll down to hit the \"connect\" icon. If the PiCamera is ON, you should now see something!</p> <p>It's now time to adjust the lens! GOOD LUCK!</p>"},{"location":"plant_imager/build_v3/plantimager_controller/#install-the-plant-imager-library","title":"Install the <code>plant-imager</code> library","text":""},{"location":"plant_imager/build_v3/plantimager_controller/#install-system-requirements","title":"Install system requirements","text":"<p>You will need <code>python3</code> and <code>pip</code>, that can be easily installed as follows: <pre><code>sudo apt update\nsudo apt install python3 python3-pip\n</code></pre></p>"},{"location":"plant_imager/build_v3/plantimager_controller/#add-the-users-private-bin-to-the-path","title":"Add the user's private <code>bin</code> to the <code>$PATH</code>","text":"<p>We have to add a few lines to the <code>.bashrc</code> file so the locally installed Python library are available.</p> <p>To do so, simply copy/paste the following lines in the terminal: <pre><code>cat&lt;&lt;EOF &gt;&gt; ~/.bashrc\n\n# set PATH so it includes user's private bin if it exists\nif [ -d \"$HOME/.local/bin\" ] ; then\n    PATH=\"$HOME/.local/bin:$PATH\"\nfi\nEOF\n</code></pre></p>"},{"location":"plant_imager/build_v3/plantimager_controller/#clone-the-sources_1","title":"Clone the sources","text":"<p>To clone the <code>plant-imager</code> sources from the ROMI GitHub repository, simply run: <pre><code>cd  # Move back to the user home directory\ngit clone --branch dev_lyon --recurse-submodules https://github.com/romi/plant-imager.git\n</code></pre></p> <p>Note</p> <p>The <code>--recurse-submodules</code> option will automatically initialize and update each submodule in the repository.</p>"},{"location":"plant_imager/build_v3/plantimager_controller/#install-the-sources","title":"Install the sources","text":"<p>Then move to the cloned directory and install the <code>plant-imager</code> Python library and its submodules (<code>plantdb</code> &amp; <code>romitask</code>) with: <pre><code># Don't forget to activate the environment!\ncd plant-imager\n# Install `plantdb` requirements &amp; sources from submodules:\npython -m pip install -r ./plantdb/requirements.txt\npython -m pip install -e ./plantdb/\n# Install `romitask` sources from submodules:\npython -m pip install -r ./romitask/requirements.txt\npython -m pip install -e ./romitask/\n# Install `plant-imager`:\npython -m pip install -e .\n</code></pre></p>"},{"location":"plant_imager/build_v3/plantimager_controller/#test-the-installation","title":"Test the installation","text":"<p>To test the installation of the Python libraries you can run: <pre><code>python -c \"import plantdb\"\npython -c \"import romitask\"\npython -c \"import plantimager\"\n</code></pre></p> <p>If no error message is returned, you are all set with this part, well done!</p>"},{"location":"plant_imager/build_v3/registry_setup/","title":"Configure the ROMI devices","text":"<p>We now need to configure the devices.</p>"},{"location":"plant_imager/build_v3/registry_setup/#configure-the-romi-cameras","title":"Configure the ROMI camera(s)","text":"<ol> <li>Power the Plant Imager controller (RPi 4)</li> <li>Power the X-Carve, this should also power the romi-camera(s) (RPi Z W)</li> <li>Plug the USB of the X-Controller to the Plant Imager controller (RPi 4)</li> <li>In a terminal (RPi 4):     <pre><code>cd ~romi-rover-build-and-test/\n./build/bin/rcdiscover tests-hardware/20-plant-imager/config.json\n./build/bin/rcom-registry\n</code></pre></li> </ol> <p>Note</p> <p>The Plant Imager controller should start the AP at boot. The romi-camera(s) should automatically connect to the AP.</p>"},{"location":"plant_imager/build_v3/registry_setup/#configure-the-plant-imager-controller","title":"Configure the Plant Imager controller","text":"<p>We have to define the <code>config.json</code> to use with Oquam.</p>"},{"location":"plant_imager/build_v3/registry_setup/#tune-the-camera-optics","title":"Tune the camera optics","text":"<pre><code>cd ~romi-rover-build-and-test/\nfirefox applications/romi-monitor/camera.html &amp; #check the camera topic name and the registry IP\ncd ~plant-imager/\npython3 /preview/preview.py --registry IP-of-the-registry\nromi_run_task --config config/hardware_scan_v3.toml Scan ~/romi_db/dir-name/\n</code></pre>"},{"location":"plant_imager/build_v3/resources/","title":"Resources","text":""},{"location":"plant_imager/build_v3/resources/#file-formats","title":"File formats","text":"<ul> <li>a <code>dfx</code> file is a data file saved in a format developed by Autodesk and used for CAD (computer-aided design) vector image files, such as AutoCAD documents. DXF files are similar to .DWG files but are more compatible with other programs since they are ASCII (text) based fileinfo.com.</li> <li>an <code>stl</code> file is a 3D model saved in the stereolithography file format developed by 3D Systems. It contains plain text or binary data that describes a set of triangular facets, which comprise a model. STL files are quite common, and they can be opened in many CAD and 3D modeling programs fileinfo.com.</li> <li>a <code>step</code> file is a 3D model file formatted in STEP (Standard for the Exchange of Product Data), an ISO standard exchange format. It contains three-dimensional model data saved in a text format recognized by multiple computer-assisted design (CAD) programs wikipedia.</li> <li>an <code>f3z</code> file is a Zip-compressed archive used by Autodesk Fusion 360, a 3D CAD industrial and mechanical design tool fileinfo.com.</li> <li>an <code>SLDASM</code> file is a 3D assembly created by SolidWorks CAD software. SLDASM files are comprised of SolidWorks part (.SLDPRT) files, which users combine to form assemblies fileinfo.com.</li> <li>an <code>smg</code> file is a 3D CAD model saved in the SolidWorks Composer file format. It contains a 3D assembly, which may include one or more parts or sub-assemblies. SMG files also store document properties, which include information about the coordinate system and textures of the assembly. fileinfo.com.</li> </ul>"},{"location":"plant_imager/build_v3/resources/#cad-models","title":"CAD &amp; Models","text":""},{"location":"plant_imager/build_v3/resources/#pan-arm","title":"Pan arm","text":"<ul> <li>f3z file</li> <li>SLDASM file</li> <li>smg file</li> <li>step file</li> </ul>"},{"location":"plant_imager/build_v3/resources/#frame-enclosure","title":"Frame &amp; enclosure","text":"<ul> <li>f3z file</li> <li>SLDASM file</li> <li>smg file</li> <li>step file</li> </ul>"},{"location":"plant_imager/build_v3/resources/#manual-gimbal","title":"Manual gimbal","text":"<ul> <li>f3z file</li> <li>SLDASM file</li> <li>smg file</li> <li>step file</li> </ul>"},{"location":"plant_imager/build_v3/resources/#plant-imager","title":"Plant Imager","text":"<ul> <li>f3z file</li> <li>SLDASM file</li> <li>smg file</li> <li>step file</li> </ul>"},{"location":"plant_imager/build_v3/resources/#bom-assembly-instructions","title":"BOM &amp; assembly instructions","text":"<p>All bills of material are detailed here.</p>"},{"location":"plant_imager/build_v3/resources/#pan-arm_1","title":"Pan arm","text":"<p>You can find the PDF with the BOM and assembly instructions here.</p>"},{"location":"plant_imager/build_v3/resources/#frame-enclosure_1","title":"Frame &amp; enclosure","text":"<p>You can find the PDF with the BOM and assembly instructions here.</p>"},{"location":"plant_imager/build_v3/resources/#manual-gimbal_1","title":"Manual gimbal","text":"<p>You can find the PDF with the BOM and assembly instructions here.</p>"},{"location":"plant_imager/build_v3/resources/#plant-imager_1","title":"Plant Imager","text":"<p>You can find the PDF with the BOM and assembly instructions here.</p>"},{"location":"plant_imager/developer/colmap_cli/","title":"COLMAP CLI","text":""},{"location":"plant_imager/developer/colmap_cli/#feature-extraction","title":"Feature extraction","text":"<p>Perform feature extraction for a set of images.</p> <p><pre><code>colmap feature_extractor -h\n</code></pre> <pre><code>COLMAP 3.8 (Commit 31df46c on 2022-03-05 with CUDA)\n\nOptions can either be specified via command-line or by defining\nthem in a .ini project file passed to `--project_path`.\n\n  -h [ --help ] \n  --random_seed arg (=0)\n  --log_to_stderr arg (=0)\n  --log_level arg (=2)\n  --project_path arg\n  --database_path arg\n  --image_path arg\n  --camera_mode arg (=-1)\n  --image_list_path arg\n  --descriptor_normalization arg (=l1_root)\n                                        {'l1_root', 'l2'}\n  --ImageReader.mask_path arg\n  --ImageReader.camera_model arg (=SIMPLE_RADIAL)\n  --ImageReader.single_camera arg (=0)\n  --ImageReader.single_camera_per_folder arg (=0)\n  --ImageReader.single_camera_per_image arg (=0)\n  --ImageReader.existing_camera_id arg (=-1)\n  --ImageReader.camera_params arg\n  --ImageReader.default_focal_length_factor arg (=1.2)\n  --ImageReader.camera_mask_path arg\n  --SiftExtraction.num_threads arg (=-1)\n  --SiftExtraction.use_gpu arg (=1)\n  --SiftExtraction.gpu_index arg (=-1)\n  --SiftExtraction.max_image_size arg (=3200)\n  --SiftExtraction.max_num_features arg (=8192)\n  --SiftExtraction.first_octave arg (=-1)\n  --SiftExtraction.num_octaves arg (=4)\n  --SiftExtraction.octave_resolution arg (=3)\n  --SiftExtraction.peak_threshold arg (=0.0066666666666666671)\n  --SiftExtraction.edge_threshold arg (=10)\n  --SiftExtraction.estimate_affine_shape arg (=0)\n  --SiftExtraction.max_num_orientations arg (=2)\n  --SiftExtraction.upright arg (=0)\n  --SiftExtraction.domain_size_pooling arg (=0)\n  --SiftExtraction.dsp_min_scale arg (=0.16666666666666666)\n  --SiftExtraction.dsp_max_scale arg (=3)\n  --SiftExtraction.dsp_num_scales arg (=10)\n</code></pre></p>"},{"location":"plant_imager/developer/colmap_cli/#matchers","title":"Matchers","text":"<p>Perform feature matching after performing feature extraction.</p>"},{"location":"plant_imager/developer/colmap_cli/#exhaustive-matcher","title":"Exhaustive matcher","text":"<p>If the number of images in your dataset is relatively low (up to several hundreds), this matching mode should be fast enough and leads to the best reconstruction results. Here, every image is matched against every other image, while the block size determines how many images are loaded from disk into memory at the same time.</p> <p><pre><code>colmap exhaustive_matcher -h\n</code></pre> <pre><code>COLMAP 3.8 (Commit 31df46c on 2022-03-05 with CUDA)\n\nOptions can either be specified via command-line or by defining\nthem in a .ini project file passed to `--project_path`.\n\n  -h [ --help ] \n  --random_seed arg (=0)\n  --log_to_stderr arg (=0)\n  --log_level arg (=2)\n  --project_path arg\n  --database_path arg\n  --SiftMatching.num_threads arg (=-1)\n  --SiftMatching.use_gpu arg (=1)\n  --SiftMatching.gpu_index arg (=-1)\n  --SiftMatching.max_ratio arg (=0.80000000000000004)\n  --SiftMatching.max_distance arg (=0.69999999999999996)\n  --SiftMatching.cross_check arg (=1)\n  --SiftMatching.max_error arg (=4)\n  --SiftMatching.max_num_matches arg (=32768)\n  --SiftMatching.confidence arg (=0.999)\n  --SiftMatching.max_num_trials arg (=10000)\n  --SiftMatching.min_inlier_ratio arg (=0.25)\n  --SiftMatching.min_num_inliers arg (=15)\n  --SiftMatching.multiple_models arg (=0)\n  --SiftMatching.guided_matching arg (=0)\n  --SiftMatching.planar_scene arg (=0)\n  --ExhaustiveMatching.block_size arg (=50)\n</code></pre></p>"},{"location":"plant_imager/developer/colmap_cli/#sequential-matcher","title":"Sequential matcher","text":"<p>This mode is useful if the images are acquired in sequential order, e.g., by a video camera. In this case, consecutive frames have visual overlap and there is no need to match all image pairs exhaustively. Instead, consecutively captured images are matched against each other. This matching mode has built-in loop detection based on a vocabulary tree, where every N-th image (<code>loop_detection_period</code>) is matched against its visually most similar images (<code>loop_detection_num_images</code>). Note that image file names must be ordered sequentially (e.g., <code>image0001.jpg</code>, <code>image0002.jpg</code>, etc.). The order in the database is not relevant, since the images are explicitly ordered according to their file names. Note that loop detection requires a pre-trained vocabulary tree, that can be downloaded from https://demuc.de/colmap/.</p> <p><pre><code>colmap sequential_matcher -h\n</code></pre> <pre><code>COLMAP 3.8 (Commit 31df46c on 2022-03-05 with CUDA)\n\nOptions can either be specified via command-line or by defining\nthem in a .ini project file passed to `--project_path`.\n\n  -h [ --help ] \n  --random_seed arg (=0)\n  --log_to_stderr arg (=0)\n  --log_level arg (=2)\n  --project_path arg\n  --database_path arg\n  --SiftMatching.num_threads arg (=-1)\n  --SiftMatching.use_gpu arg (=1)\n  --SiftMatching.gpu_index arg (=-1)\n  --SiftMatching.max_ratio arg (=0.80000000000000004)\n  --SiftMatching.max_distance arg (=0.69999999999999996)\n  --SiftMatching.cross_check arg (=1)\n  --SiftMatching.max_error arg (=4)\n  --SiftMatching.max_num_matches arg (=32768)\n  --SiftMatching.confidence arg (=0.999)\n  --SiftMatching.max_num_trials arg (=10000)\n  --SiftMatching.min_inlier_ratio arg (=0.25)\n  --SiftMatching.min_num_inliers arg (=15)\n  --SiftMatching.multiple_models arg (=0)\n  --SiftMatching.guided_matching arg (=0)\n  --SiftMatching.planar_scene arg (=0)\n  --SequentialMatching.overlap arg (=10)\n  --SequentialMatching.quadratic_overlap arg (=1)\n  --SequentialMatching.loop_detection arg (=0)\n  --SequentialMatching.loop_detection_period arg (=10)\n  --SequentialMatching.loop_detection_num_images arg (=50)\n  --SequentialMatching.loop_detection_num_nearest_neighbors arg (=1)\n  --SequentialMatching.loop_detection_num_checks arg (=256)\n  --SequentialMatching.loop_detection_num_images_after_verification arg (=0)\n  --SequentialMatching.loop_detection_max_num_features arg (=-1)\n  --SequentialMatching.vocab_tree_path arg\n</code></pre></p>"},{"location":"plant_imager/developer/colmap_cli/#spatial-matcher","title":"Spatial matcher","text":"<p>This matching mode matches every image against its spatial nearest neighbors. Spatial locations can be manually set in the database management. By default, COLMAP also extracts GPS information from EXIF and uses it for spatial nearest neighbor search. If accurate prior location information is available, this is the recommended matching mode.</p> <p><pre><code>colmap spatial_matcher -h\n</code></pre> <pre><code>COLMAP 3.8 (Commit 31df46c on 2022-03-05 with CUDA)\n\nOptions can either be specified via command-line or by defining\nthem in a .ini project file passed to `--project_path`.\n\n  -h [ --help ] \n  --random_seed arg (=0)\n  --log_to_stderr arg (=0)\n  --log_level arg (=2)\n  --project_path arg\n  --database_path arg\n  --SiftMatching.num_threads arg (=-1)\n  --SiftMatching.use_gpu arg (=1)\n  --SiftMatching.gpu_index arg (=-1)\n  --SiftMatching.max_ratio arg (=0.80000000000000004)\n  --SiftMatching.max_distance arg (=0.69999999999999996)\n  --SiftMatching.cross_check arg (=1)\n  --SiftMatching.max_error arg (=4)\n  --SiftMatching.max_num_matches arg (=32768)\n  --SiftMatching.confidence arg (=0.999)\n  --SiftMatching.max_num_trials arg (=10000)\n  --SiftMatching.min_inlier_ratio arg (=0.25)\n  --SiftMatching.min_num_inliers arg (=15)\n  --SiftMatching.multiple_models arg (=0)\n  --SiftMatching.guided_matching arg (=0)\n  --SiftMatching.planar_scene arg (=0)\n  --SpatialMatching.is_gps arg (=1)\n  --SpatialMatching.ignore_z arg (=1)\n  --SpatialMatching.max_num_neighbors arg (=50)\n  --SpatialMatching.max_distance arg (=100)\n</code></pre></p>"},{"location":"plant_imager/developer/colmap_cli/#mapper","title":"Mapper","text":"<p>Sparse 3D reconstruction / mapping of the dataset using SfM after performing feature extraction and matching. <pre><code>colmap mapper -h\n</code></pre> <pre><code>COLMAP 3.8 (Commit 31df46c on 2022-03-05 with CUDA)\n\nOptions can either be specified via command-line or by defining\nthem in a .ini project file passed to `--project_path`.\n\n  -h [ --help ] \n  --random_seed arg (=0)\n  --log_to_stderr arg (=0)\n  --log_level arg (=2)\n  --project_path arg\n  --database_path arg\n  --image_path arg\n  --input_path arg\n  --output_path arg\n  --image_list_path arg\n  --Mapper.min_num_matches arg (=15)\n  --Mapper.ignore_watermarks arg (=0)\n  --Mapper.multiple_models arg (=1)\n  --Mapper.max_num_models arg (=50)\n  --Mapper.max_model_overlap arg (=20)\n  --Mapper.min_model_size arg (=10)\n  --Mapper.init_image_id1 arg (=-1)\n  --Mapper.init_image_id2 arg (=-1)\n  --Mapper.init_num_trials arg (=200)\n  --Mapper.extract_colors arg (=1)\n  --Mapper.num_threads arg (=-1)\n  --Mapper.min_focal_length_ratio arg (=0.10000000000000001)\n  --Mapper.max_focal_length_ratio arg (=10)\n  --Mapper.max_extra_param arg (=1)\n  --Mapper.ba_refine_focal_length arg (=1)\n  --Mapper.ba_refine_principal_point arg (=0)\n  --Mapper.ba_refine_extra_params arg (=1)\n  --Mapper.ba_min_num_residuals_for_multi_threading arg (=50000)\n  --Mapper.ba_local_num_images arg (=6)\n  --Mapper.ba_local_function_tolerance arg (=0)\n  --Mapper.ba_local_max_num_iterations arg (=25)\n  --Mapper.ba_global_use_pba arg (=0)\n  --Mapper.ba_global_pba_gpu_index arg (=-1)\n  --Mapper.ba_global_images_ratio arg (=1.1000000000000001)\n  --Mapper.ba_global_points_ratio arg (=1.1000000000000001)\n  --Mapper.ba_global_images_freq arg (=500)\n  --Mapper.ba_global_points_freq arg (=250000)\n  --Mapper.ba_global_function_tolerance arg (=0)\n  --Mapper.ba_global_max_num_iterations arg (=50)\n  --Mapper.ba_global_max_refinements arg (=5)\n  --Mapper.ba_global_max_refinement_change arg (=0.00050000000000000001)\n  --Mapper.ba_local_max_refinements arg (=2)\n  --Mapper.ba_local_max_refinement_change arg (=0.001)\n  --Mapper.snapshot_path arg\n  --Mapper.snapshot_images_freq arg (=0)\n  --Mapper.fix_existing_images arg (=0)\n  --Mapper.init_min_num_inliers arg (=100)\n  --Mapper.init_max_error arg (=4)\n  --Mapper.init_max_forward_motion arg (=0.94999999999999996)\n  --Mapper.init_min_tri_angle arg (=16)\n  --Mapper.init_max_reg_trials arg (=2)\n  --Mapper.abs_pose_max_error arg (=12)\n  --Mapper.abs_pose_min_num_inliers arg (=30)\n  --Mapper.abs_pose_min_inlier_ratio arg (=0.25)\n  --Mapper.filter_max_reproj_error arg (=4)\n  --Mapper.filter_min_tri_angle arg (=1.5)\n  --Mapper.max_reg_trials arg (=3)\n  --Mapper.local_ba_min_tri_angle arg (=6)\n  --Mapper.tri_max_transitivity arg (=1)\n  --Mapper.tri_create_max_angle_error arg (=2)\n  --Mapper.tri_continue_max_angle_error arg (=2)\n  --Mapper.tri_merge_max_reproj_error arg (=4)\n  --Mapper.tri_complete_max_reproj_error arg (=4)\n  --Mapper.tri_complete_max_transitivity arg (=5)\n  --Mapper.tri_re_max_angle_error arg (=5)\n  --Mapper.tri_re_min_ratio arg (=0.20000000000000001)\n  --Mapper.tri_re_max_trials arg (=1)\n  --Mapper.tri_min_angle arg (=1.5)\n  --Mapper.tri_ignore_two_view_tracks arg (=1)\n</code></pre></p>"},{"location":"plant_imager/developer/colmap_cli/#point-triangulator","title":"Point triangulator","text":"<p><pre><code>colmap point_triangulator -h\n</code></pre> <pre><code>COLMAP 3.8 (Commit 31df46c on 2022-03-05 with CUDA)\n\nOptions can either be specified via command-line or by defining\nthem in a .ini project file passed to `--project_path`.\n\n  -h [ --help ] \n  --random_seed arg (=0)\n  --log_to_stderr arg (=0)\n  --log_level arg (=2)\n  --project_path arg\n  --database_path arg\n  --image_path arg\n  --input_path arg\n  --output_path arg\n  --clear_points arg (=0)               Whether to clear all existing points \n                                        and observations\n  --Mapper.min_num_matches arg (=15)\n  --Mapper.ignore_watermarks arg (=0)\n  --Mapper.multiple_models arg (=1)\n  --Mapper.max_num_models arg (=50)\n  --Mapper.max_model_overlap arg (=20)\n  --Mapper.min_model_size arg (=10)\n  --Mapper.init_image_id1 arg (=-1)\n  --Mapper.init_image_id2 arg (=-1)\n  --Mapper.init_num_trials arg (=200)\n  --Mapper.extract_colors arg (=1)\n  --Mapper.num_threads arg (=-1)\n  --Mapper.min_focal_length_ratio arg (=0.10000000000000001)\n  --Mapper.max_focal_length_ratio arg (=10)\n  --Mapper.max_extra_param arg (=1)\n  --Mapper.ba_refine_focal_length arg (=1)\n  --Mapper.ba_refine_principal_point arg (=0)\n  --Mapper.ba_refine_extra_params arg (=1)\n  --Mapper.ba_min_num_residuals_for_multi_threading arg (=50000)\n  --Mapper.ba_local_num_images arg (=6)\n  --Mapper.ba_local_function_tolerance arg (=0)\n  --Mapper.ba_local_max_num_iterations arg (=25)\n  --Mapper.ba_global_use_pba arg (=0)\n  --Mapper.ba_global_pba_gpu_index arg (=-1)\n  --Mapper.ba_global_images_ratio arg (=1.1000000000000001)\n  --Mapper.ba_global_points_ratio arg (=1.1000000000000001)\n  --Mapper.ba_global_images_freq arg (=500)\n  --Mapper.ba_global_points_freq arg (=250000)\n  --Mapper.ba_global_function_tolerance arg (=0)\n  --Mapper.ba_global_max_num_iterations arg (=50)\n  --Mapper.ba_global_max_refinements arg (=5)\n  --Mapper.ba_global_max_refinement_change arg (=0.00050000000000000001)\n  --Mapper.ba_local_max_refinements arg (=2)\n  --Mapper.ba_local_max_refinement_change arg (=0.001)\n  --Mapper.snapshot_path arg\n  --Mapper.snapshot_images_freq arg (=0)\n  --Mapper.fix_existing_images arg (=0)\n  --Mapper.init_min_num_inliers arg (=100)\n  --Mapper.init_max_error arg (=4)\n  --Mapper.init_max_forward_motion arg (=0.94999999999999996)\n  --Mapper.init_min_tri_angle arg (=16)\n  --Mapper.init_max_reg_trials arg (=2)\n  --Mapper.abs_pose_max_error arg (=12)\n  --Mapper.abs_pose_min_num_inliers arg (=30)\n  --Mapper.abs_pose_min_inlier_ratio arg (=0.25)\n  --Mapper.filter_max_reproj_error arg (=4)\n  --Mapper.filter_min_tri_angle arg (=1.5)\n  --Mapper.max_reg_trials arg (=3)\n  --Mapper.local_ba_min_tri_angle arg (=6)\n  --Mapper.tri_max_transitivity arg (=1)\n  --Mapper.tri_create_max_angle_error arg (=2)\n  --Mapper.tri_continue_max_angle_error arg (=2)\n  --Mapper.tri_merge_max_reproj_error arg (=4)\n  --Mapper.tri_complete_max_reproj_error arg (=4)\n  --Mapper.tri_complete_max_transitivity arg (=5)\n  --Mapper.tri_re_max_angle_error arg (=5)\n  --Mapper.tri_re_min_ratio arg (=0.20000000000000001)\n  --Mapper.tri_re_max_trials arg (=1)\n  --Mapper.tri_min_angle arg (=1.5)\n  --Mapper.tri_ignore_two_view_tracks arg (=1)\n</code></pre></p>"},{"location":"plant_imager/developer/colmap_cli/#model-alignment","title":"Model alignment","text":"<p>Align/geo-register model to coordinate system of given camera centers.</p> <p><pre><code>colmap model_aligner -h\n</code></pre> <pre><code>COLMAP 3.8 (Commit 31df46c on 2022-03-05 with CUDA)\n\nOptions can either be specified via command-line or by defining\nthem in a .ini project file passed to `--project_path`.\n\n  -h [ --help ] \n  --random_seed arg (=0)\n  --log_to_stderr arg (=0)\n  --log_level arg (=2)\n  --project_path arg\n  --input_path arg\n  --output_path arg\n  --database_path arg\n  --ref_images_path arg\n  --ref_is_gps arg (=1)\n  --merge_image_and_ref_origins arg (=0)\n  --transform_path arg\n  --alignment_type arg (=custom)        {plane, ecef, enu, enu-plane, \n                                        enu-plane-unscaled, custom}\n  --min_common_images arg (=3)\n  --estimate_scale arg (=1)\n  --robust_alignment arg (=1)\n  --robust_alignment_max_error arg (=0)\n</code></pre></p>"},{"location":"plant_imager/developer/colmap_cli/#bundle-adjustment","title":"Bundle adjustment","text":"<p>Run global bundle adjustment on a reconstructed scene, e.g. when a refinement of the intrinsics is needed or after running the <code>image_registrator</code>.</p> <p><pre><code>colmap bundle_adjuster -h\n</code></pre> <pre><code>COLMAP 3.8 (Commit 31df46c on 2022-03-05 with CUDA)\n\nOptions can either be specified via command-line or by defining\nthem in a .ini project file passed to `--project_path`.\n\n  -h [ --help ] \n  --random_seed arg (=0)\n  --log_to_stderr arg (=0)\n  --log_level arg (=2)\n  --project_path arg\n  --input_path arg\n  --output_path arg\n  --BundleAdjustment.max_num_iterations arg (=100)\n  --BundleAdjustment.max_linear_solver_iterations arg (=200)\n  --BundleAdjustment.function_tolerance arg (=0)\n  --BundleAdjustment.gradient_tolerance arg (=0)\n  --BundleAdjustment.parameter_tolerance arg (=0)\n  --BundleAdjustment.refine_focal_length arg (=1)\n  --BundleAdjustment.refine_principal_point arg (=0)\n  --BundleAdjustment.refine_extra_params arg (=1)\n  --BundleAdjustment.refine_extrinsics arg (=1)\n</code></pre></p>"},{"location":"plant_imager/developer/colmap_cli/#model-analyzer","title":"Model analyzer","text":"<p>Print statistics about reconstructions.</p> <p><pre><code>colmap model_analyzer -h\n</code></pre> <pre><code>COLMAP 3.8 (Commit 31df46c on 2022-03-05 with CUDA)\n\nOptions can either be specified via command-line or by defining\nthem in a .ini project file passed to `--project_path`.\n\n  -h [ --help ] \n  --random_seed arg (=0)\n  --log_to_stderr arg (=0)\n  --log_level arg (=2)\n  --project_path arg\n  --path arg\n</code></pre></p>"},{"location":"plant_imager/developer/conda/","title":"Conda","text":"<p>Recipes to build conda packages can be found here.</p> <p>Follow these instructions to build conda packages.</p> <p>Warning</p> <p>Conda packages should be built from the <code>base</code> environment. <pre><code>conda activate base\n</code></pre></p>"},{"location":"plant_imager/developer/conda/#requirements","title":"Requirements","text":""},{"location":"plant_imager/developer/conda/#install-conda-build","title":"Install <code>conda-build</code>:","text":"<p>Install <code>conda-build</code>, in the <code>base</code> environment, to be able to build conda package:</p> <pre><code>conda install conda-build\n</code></pre> <p>WARNING: For macOS, follow these instructions to install the required <code>macOS 10.9 SDK</code>.</p>"},{"location":"plant_imager/developer/conda/#optional-install-anaconda-client","title":"Optional - Install <code>anaconda-client</code>:","text":"<p>To be able to upload your package on anaconda cloud you need to install <code>anaconda-client</code>:</p> <pre><code>conda install anaconda-client\n</code></pre>"},{"location":"plant_imager/developer/conda/#build-conda-packages","title":"Build conda packages:","text":""},{"location":"plant_imager/developer/conda/#build-lettucethink","title":"Build <code>lettucethink</code>:","text":"<p>Using the given recipe, it is easy to build the <code>lettucethink-python</code> conda package:</p> <pre><code>cd conda_recipes/\nconda build lettucethink/ --user romi-eu\n</code></pre>"},{"location":"plant_imager/developer/conda/#build-plantdb","title":"Build <code>plantdb</code>:","text":"<p>Using the given recipe, it is easy to build the <code>plantdb</code> conda package:</p> <pre><code>cd conda_recipes/\nconda build plantdb/ -c romi-eu -c open3d-admin --user romi-eu\n</code></pre>"},{"location":"plant_imager/developer/conda/#build-plant3dvision","title":"Build <code>plant3dvision</code>:","text":"<p>Using the given recipe, it is easy to build the <code>plant3dvision</code> conda package:</p> <pre><code>cd conda_recipes/\nconda build plant3dvision/ -c romi-eu -c conda-forge -c open3d-admin --user romi-eu\n</code></pre>"},{"location":"plant_imager/developer/conda/#build-romi-plantviz","title":"Build <code>romi-plantviz</code>:","text":"<p>Using the given recipe, it is easy to build the <code>romi-plantviz</code> conda package:</p> <pre><code>cd conda_recipes/\nconda build romi-plantviz/ -c romi-eu -c conda-forge --user romi-eu\n</code></pre>"},{"location":"plant_imager/developer/conda/#optional-build-dirsync-package","title":"Optional - Build <code>dirsync</code> package:","text":"<p>To build <code>dirsync</code> you have to install <code>hgsvn</code>:</p> <pre><code>sudo apt install hgsvn </code></pre> <p>Using the given recipe, it is easy to build the <code>dirsync</code> conda package:</p> <pre><code>cd conda_recipes\nconda build dirsync/recipe/ --user romi-eu\n</code></pre>"},{"location":"plant_imager/developer/conda/#optional-build-opencv-python-package","title":"Optional - Build <code>opencv-python</code> package:","text":"<p>To build <code>opencv-python</code> you have to install <code>qt4-qmake</code>:</p> <pre><code>sudo apt install qt4-qmake qt4-default\n</code></pre> <p>Using the given recipe, it is easy to build the <code>opencv-python</code> conda package:</p> <pre><code>cd conda_recipes\nconda build opencv-python/ -c conda-forge --user romi-eu\n</code></pre>"},{"location":"plant_imager/developer/conda/#conda-useful-commands","title":"Conda useful commands:","text":""},{"location":"plant_imager/developer/conda/#purge-built-packages","title":"Purge built packages:","text":"<pre><code>conda build purge\n</code></pre>"},{"location":"plant_imager/developer/conda/#clean-cache-unused-packages","title":"Clean cache &amp; unused packages:","text":"<pre><code>conda clean --all\n</code></pre>"},{"location":"plant_imager/developer/git_docReview/","title":"Contributing to docs","text":"<p>From romi-robots-docs, we incorporate changes using a typical git workflow with commits and pull requests. The documentation is generated using MkDocs.</p>"},{"location":"plant_imager/developer/git_docReview/#objectives","title":"Objectives","text":"<p>At the end of this tutorial, you should be able to:</p> <ul> <li>create content for romi-robots-docs</li> <li>review modifications suggested for romi-robots-docs by you colleagues</li> </ul>"},{"location":"plant_imager/developer/git_docReview/#prerequisite","title":"Prerequisite","text":"<ol> <li> <p>Install git</p> </li> <li> <p>Install Mkdocs </p> </li> </ol>"},{"location":"plant_imager/developer/git_docReview/#1-install-git","title":"1. Install git","text":"<ul> <li>Install git</li> <li>Example in Linux system with a Debian-based distribution (e.g. Ubuntu):   <pre><code>git --version #verify that you have git installed: it should return the version (e.g. git version 2.25.1)\n#if git is not installed:\n$ sudo apt install git-all\n</code></pre></li> <li>git clone romi-robots-docs</li> </ul> <pre><code>git clone https://github.com/romi/romi-robots-docs.git\ncd romi-robots-docs  #enter the cloned repository so that all git actions are available \n</code></pre>"},{"location":"plant_imager/developer/git_docReview/#2-install-mkdocs","title":"2. Install mkdocs","text":"<ul> <li>install command (e.g. on Linux, see the specific documentation for more detailed instructions )</li> </ul> <pre><code>sudo apt install mkdocs\nsudo pip3 install mkdocs-material #this extension of mkdocs is required\n</code></pre> <p>Note: if the second installation fails, you may consider installing or updating pip3:</p> <pre><code>sudo apt-get install python3-pip\n</code></pre> <p>Then re-run gain the command: <code>sudo pip3 install mkdocs-material</code></p>"},{"location":"plant_imager/developer/git_docReview/#step-by-step-tutorial","title":"Step-by-step tutorial","text":"<p>if you only review an existing branch (without adding or modifying content), go directly to steps 8/9.</p>"},{"location":"plant_imager/developer/git_docReview/#1-create-your-local-branch","title":"1. Create your local branch","text":"<p>The default branch is <code>master</code>, that directly incorporate all changes. There is no dev branch. Never ever work on <code>master</code>: create a local branch to make changes, using the following commands:</p> <pre><code>git checkout master  # go to master\ngit pull  # update it with last changes\ngit checkout -b my_branch  # create local branch `my_branch` (it will derived from the last master)\ngit push --set-upstream origin my_branch  # attach branch `my_branch` to `origin/my_branch`. GitHub login/password will be asked for.\n</code></pre>"},{"location":"plant_imager/developer/git_docReview/#2-visualize-your-changes-locally-on-a-web-browser-in-an-interactive-manner","title":"2. Visualize your changes locally on a web browser in an interactive manner","text":"<pre><code>mkdocs serve  # reads the mkdocs.yml file to generate the web page.\n</code></pre> <p>The terminal gives you information. The programs starts by building the documentation. As soon as you can read:</p> <p>[I 210323 08:58:22 server:335] Serving on http://127.0.0.1:8000</p> <p>..., you can connect your favorite web browser by copy-pasting the url or just (ctrl+click) on the address to open it in your default browser.</p> <p>The program <code>mkdocs serve</code> constantly watches for changes and refreshes the build as soon as they are detected, as indicated by the terminal:</p> <p>[I 210323 08:58:22 handlers:62] Start watching changes INFO    -  Start watching changes</p> <p>[I 210323 08:58:22 handlers:64] Start detecting changes INFO    -  Start detecting changes Since the refresh is very rapid upon changes, you can then see in live the effect of you modifications.</p> <p>In the terminal, possible issues are listed (INFO and WARNING),pointing to problems that should be fixed:</p> <p>In this case, pages should be added in the <code>nav</code> section of the <code>mkdocs.yml</code> file (see later point 4.). In the interactive browser, you cannot see and display pages that does not exist in the \"nav\" configuration.</p> <ul> <li>internal hyperlink issues (WARNING):</li> </ul> <p>WARNING - Documentation file 'xxx/xxxx/file1.md' contains a link to 'xxx/yyy/otherfile.md' which is not found in the documentation files.</p> <p>In this case, check and modify the hyperlink in file1.md to provide good redirection to otherfile.md</p>"},{"location":"plant_imager/developer/git_docReview/#3-adding-images-in-the-content","title":"3. Adding images in the content","text":"<ul> <li>store the image files in assets/. You can also directly provide html address for third party images if you are sure that the link will be stable over time.</li> <li>To have more flexibility and options for images layout, use the html command in your markdown file:</li> </ul> <pre><code>&lt;img src=\"/assets/images/my_image.png\" alt=\"name_displayed_if_error\" width=\"600\" style=\"display:block; margin-left: auto; margin-right: auto;\"&gt; # here the style centers the picture\n</code></pre>"},{"location":"plant_imager/developer/git_docReview/#4-modify-the-navigation-in-mkdocsyml","title":"4. Modify the navigation in mkdocs.yml","text":"<p>Open <code>mkdocs.yml</code> at the root of <code>romi-robots-docs</code> repo.</p> <p>Some changes must be reported in this file in the <code>nav</code> section: when you create a new page (a new <code>file.md</code>) or a new directory, or modify the name of an existing file.md/directory.</p> <p>In the <code>nav</code> section, you can also enter the name given to pages in the menu.</p>"},{"location":"plant_imager/developer/git_docReview/#5-commit-your-changes","title":"5. Commit your changes","text":"<p>This follow the classical git commit procedure:</p> <pre><code>git status  #list all files affected by changes\ngit add/restore/rm &lt;file&gt;  #do as many action as listed in red by the previous command\n(git status)  #optional: verify that all changes are staged and ready for commit)\ngit commit -m \"my awesome commit\"\ngit push  # push modification to `origin/my_branch`\n(git log)  #optional: verify that your commit is recorded\n</code></pre>"},{"location":"plant_imager/developer/git_docReview/#6-merge-your-working-branch-with-current-master-rebase-may-be-needed","title":"6. Merge your working branch with current master: rebase may be needed","text":"<p>Assuming that your working branch is called <code>my_branch</code></p> <pre><code>git checkout master  #switch to master\ngit pull  #update in case changes were made in the meantime: now you have the latest master branch\ngit checkout my_branch  #switch again to the branch to merge\ngit rebase master  # rebase `master` branch onto `my_branch`\n</code></pre> <p>if the last command indicates conflicts, it means that <code>master</code> and <code>my_branch</code> have diverged during your work. For each files listed by git:</p> <ul> <li>fix the conflicts by directly editing the file</li> <li>stage your changes with:</li> </ul> <pre><code>git add file1\n</code></pre> <ul> <li>continue the rebase with:</li> </ul> <pre><code>git rebase --continue\n</code></pre> <p>Finally, once all conflicts have been resolved and changes staged, Push the rebasing to remote central repo:</p> <pre><code>git push -f origin my_branch  #-f (force) implies that login/password will be asked for.\n</code></pre>"},{"location":"plant_imager/developer/git_docReview/#7-prepare-a-pr-on-github-webpage","title":"7. Prepare a PR on GitHub webpage","text":"<ul> <li>Go to the distant romi repository : https://github.com/romi/romi-robots-docs</li> <li>select you branch and prepare a PR: open a pull request (green button), enter a brief text to explain the modifications, assign reviewers (in the right column   of the page), and press the green button 'create pull request'.</li> </ul>"},{"location":"plant_imager/developer/git_docReview/#8-test-a-distant-branch-eg-for-a-pull-request-review-from-your-colleagues","title":"8. test a distant branch (e.g. for a pull request review from your colleagues)","text":"<pre><code>git checkout test_branch  #switch to 'test_branch'. /!\\ do not switch to origin/test_branch since your working locally\ngit fetch  #Download objects and refs from the remote branch on /origin\ngit pull  #Incorporates changes from the remote repository into the current local `test_branch`\nmkdocs serve  #serve the docs website on the local server\n</code></pre> <p>You can view the display, test the links, etc... You can also create a new branch from it to modify it.</p>"},{"location":"plant_imager/developer/git_docReview/#9-make-your-review-on-github-web-interface","title":"9. make your review on GitHub web interface","text":"<p>Comment the pull requests (PR), file by file. Point to issues if any.</p>"},{"location":"plant_imager/developer/git_docReview/#10-use-the-project-board-kanban-type","title":"10. Use the project board (Kanban type)","text":"<p>Go to: https://github.com/orgs/romi/projects/10</p> <ul> <li>link your PR to existing issues</li> <li>move the corresponding note to the appropriate column (To do / In progress / Test / Done)</li> </ul>"},{"location":"plant_imager/developer/git_docReview/#troubleshooting","title":"TROUBLESHOOTING","text":""},{"location":"plant_imager/developer/git_docReview/#warning-messages","title":"Warning messages","text":"<p>They points to files reported in the mkdocs.yml but not existing in the current documentation. This is not an rugent issue sine the doculentation can be built despite these raised warnings. Consider modifying the indicated files.</p>"},{"location":"plant_imager/developer/git_docReview/#fatal-errors-occuring-after-executing-mkdocs-serve","title":"Fatal errors occuring after executing <code>mkdocs serve</code>","text":"<ul> <li>module 'materialx.emoji' <p>cannot find module 'materialx.emoji' (No module named 'materialx')</p> </li> </ul> <p>solution: execute <code>sudo pip3 install mkdocs-material</code></p> <ul> <li>'gitsnippet' <p>ERROR    -  Config value: 'plugins'. Error: The \"gitsnippet\" plugin is not installed</p> </li> </ul> <p>solution: execute <code>pip install mkdocs-gitsnippet-plugin</code></p> <ul> <li>'decorator' <p>ERROR    -  The 'decorator' distribution was not found and is required by the application</p> </li> </ul> <p>solution: execute <code>pip install decorator</code></p> <ul> <li>'wcwidth' <p>ERROR    - The 'wcwidth' distribution was not found and is required by prompt-toolkit</p> </li> </ul> <p>solution: execute <code>pip install wcwidth</code></p>"},{"location":"plant_imager/developer/git_submodules_workflow/","title":"Git submodules in plant-3d-vision","text":"<p>We make use of git submodules in the <code>plant-3d-vision</code> repository to tightly control the version of the other ROMI libraries used as dependencies. Hereafter we detail how to manage those submodules, especially how to update them.</p>"},{"location":"plant_imager/developer/git_submodules_workflow/#getting-started","title":"Getting started","text":""},{"location":"plant_imager/developer/git_submodules_workflow/#clone-the-sources","title":"Clone the sources","text":"<p>If you are joining the project start by cloning the sources:</p> <pre><code>git clone https://github.com/romi/plant-3d-vision.git\n</code></pre>"},{"location":"plant_imager/developer/git_submodules_workflow/#initialize-the-submodules","title":"Initialize the submodules","text":"<p>If you just cloned the repository or if the submodules folders (<code>romitask</code>, <code>romicgal</code>, <code>plantdb</code>...) are empty, you have to initialize the submodules in the <code>plant-3d-vision</code> folder with:</p> <pre><code>cd plant-3d-vision\ngit submodule init\ngit submodule update\n</code></pre> <p>You should now have submodules folders (<code>romitask</code>, <code>romicgal</code>, <code>plantdb</code>...) filled with the contents for the associated \"fixed commit\".</p> <p>To know the latest commit associated to a submodule, move to its folder and look-up the <code>git log</code>:</p> <pre><code>cd plant-3d-vision/&lt;submodule_root_dir&gt;\ngit log\n</code></pre> <p>Tips</p> <p>Press key <code>q</code> to quit the log.</p>"},{"location":"plant_imager/developer/git_submodules_workflow/#integrate-the-modifications-of-a-submodule","title":"Integrate the modifications of a submodule","text":"<p>For the sake of clarity, let's assume you have worked on the <code>dtw</code> submodule, integrated your changes in the branch  <code>&lt;my_branch&gt;</code> (usually <code>main</code>) and you want to integrate these modifications to <code>plant-3d-vision</code>.</p> <p>On way to do it is this: <pre><code>cd plant-3d-vision/\ngit pull\n\n# It would be better to create an integration branch...\n\ncd dtw  # could be another submodule than `dtw`\ngit checkout &lt;my_branch&gt;  # usually `main` or `dev`/`develop`\ngit pull\ngit log  # check this is indeed the last commit that you want to integrate\n\ncd ..\ngit status  # should see `modifi\u00e9 :         dtw (nouveaux commits)`\n\ngit add dtw\ngit status  # should show added `dtw` in green\ngit commit -m \"update dtw submodule\"\ngit push\n</code></pre></p>"},{"location":"plant_imager/developer/git_submodules_workflow/#update-the-plant-3d-vision-integration-branch","title":"Update the plant-3d-vision integration branch","text":"<p>From the <code>plant-3d-vision</code> folder, checkout the <code>dev</code> branch (integration branch):</p> <pre><code>git checkout dev\ngit fetch\ngit pull\ngit submodule update\ngit status\n</code></pre>"},{"location":"plant_imager/developer/git_submodules_workflow/#update-the-submodule-branch","title":"Update the submodule branch","text":"<p>To check the latest commit of a submodule do:</p> <pre><code>git log\n</code></pre> <p>To get the commit short hash:</p> <pre><code>git rev-parse --short=8 HEAD\n</code></pre> <p>To update a submodule branch:</p> <pre><code>cd &lt;submodule_root_dir&gt;\ngit checkout &lt;submodule_branch&gt;  # or commit short hash\ngit status\ngit log\n</code></pre>"},{"location":"plant_imager/developer/git_submodules_workflow/#commit-the-changes","title":"Commit the changes","text":"<p>You may now commit the changes to <code>plant-3d-vision</code>:</p> <pre><code>git commit -m \"Update romicgal submodule\"\n</code></pre>"},{"location":"plant_imager/developer/git_workflow/","title":"Git Workflow","text":"<p>Many workflows are possible when using a version control system like <code>git</code>. To clarify how we use it in the ROMI project we hereafter details our choices and show ho to performs the most basic tasks to participate in the development of the ROMI libraries.</p>"},{"location":"plant_imager/developer/git_workflow/#rules","title":"Rules","text":"<p>Here are some very important rules, be sure to understand them first!</p> <ol> <li>NEVER EVER work on <code>master</code> or <code>dev</code>, always on a branch!</li> <li>ALWAYS rebase your destination branch onto the one you want to merge before doing it!</li> <li><code>dev</code> is the integration branch, <code>master</code> is the release branch</li> </ol>"},{"location":"plant_imager/developer/git_workflow/#clone-configure-the-repository","title":"Clone &amp; configure the repository","text":"<p>It all starts by cloning the repository you want to contribute to, e.g. <code>plant3dvision</code>:</p> <pre><code>git clone https://github.com/romi/plant3dvision.git  # clone the repository\n</code></pre> <p>To use all possible git actions on this repository ('repo'), go the location of this local clone</p> <pre><code>cd plant3dvision  #the repo is cloned at the point where you executed the previous command (git clone). If you moved the clone repo, prefix with path like: cd path/to/yourcloned/plant3dvision\n</code></pre>"},{"location":"plant_imager/developer/git_workflow/#create-development-branch","title":"Create development branch","text":"<p>To contribute to development you have to create a branch on which you will work.</p> <ol> <li>Let's start by pulling the latest developments by updating our local <code>dev</code> branch     <pre><code>git checkout dev  # switch to your -local- `dev` branch\ngit fetch  # fetch changes from remote (`origin/dev`)\ngit pull  # pull changes (if any) from remote to local \n</code></pre></li> <li>Then create your new branch <code>&lt;my_branch&gt;</code> and set tracking to remote central repo (<code>origin/&lt;my_branch&gt;</code>)     <pre><code>git checkout -b &lt;my_branch&gt;  # create local branch `&lt;my_branch&gt;`\ngit push --set-upstream origin &lt;my_branch&gt;  # attach local branch `&lt;my_branch&gt;` to remote `origin/&lt;my_branch&gt;`. Login/password will be asked for.\n</code></pre></li> </ol> <p>Note</p> <p>Setting the branch tracking can be done later, even after committing changes to local repository!</p>"},{"location":"plant_imager/developer/git_workflow/#work-on-your-modifications","title":"Work on your modifications","text":"<p>We advise to use a proper IDE (like PyCharm or Atom) with an integrated or plugin based git tool for this part, as manually adding a lot of files can be time-consuming. Overall you will benefit from a nicer and faster integration of this particular step.</p> <p>Nevertheless, for the sake of clarity, hereafter we detail how to do that with the <code>git</code> command-line interface.</p>"},{"location":"plant_imager/developer/git_workflow/#tracking-new-files","title":"Tracking new files","text":"<p>If you create a new file, you will have to tell git to track its changes with:</p> <pre><code>git add &lt;my_new_file.py&gt;\n</code></pre>"},{"location":"plant_imager/developer/git_workflow/#adding-changes-to-local-repository","title":"Adding changes to local repository","text":"<p>After editing your files (e.g. <code>&lt;my_file1.py&gt;</code> <code>&lt;my_file2.py&gt;</code>), tell git to validate the changes to these files by adding them to the list of tracked changes with:</p> <pre><code>git add &lt;my_file1.py&gt; &lt;my_file2.py&gt;\n</code></pre> <p>Then commit them to your local repository:</p> <pre><code>git commit -m \"This is my awesome commit!\"\n</code></pre>"},{"location":"plant_imager/developer/git_workflow/#pushing-changes-to-remote-repository","title":"Pushing changes to remote repository","text":"<p>Once you are satisfied with the state of your work, you can push the locally committed changes to the remote central repository:</p> <pre><code>git push  # push modification to `origin/&lt;my_branch&gt;`\n</code></pre> <p>Important</p> <p>Try to do this <code>add/commit/push</code> sequence as often as you can!!</p>"},{"location":"plant_imager/developer/git_workflow/#how-do-i-not-forget-changes-for-committing","title":"How do I not forget changes for committing ?","text":"<p>Commits that affect only a limited number of files are preferable to track changes and history. However, some work require to modify several files. Especially when working with an IDE allowing easy exploring and modifications of all the repository content, you may forget some changes you did.</p> <p>To quickly identify all current files with uncommitted changes in your current branch, just simply check with <code>git status</code> on your local branch.</p> <p>The terminal lists in a red color all files requiring an action (git add, git restore, git remove): modified files, deleted files, newly added files.</p> <p>Note</p> <p>after acting on listed red files, typing <code>git status</code> should turn all previous files in green. Your branch is ready for committing.</p> <p>Then just proceed as above with <code>git commit</code> and <code>git push</code>.</p>"},{"location":"plant_imager/developer/git_workflow/#check-your-commit-has-been-pushed","title":"check your commit has been pushed","text":"<p>After <code>git push</code>, you can get the list of pushed commits related to your current branch with <code>git log</code> (press <code>q</code> to exit the list in the terminal).</p>"},{"location":"plant_imager/developer/git_workflow/#prepare-your-work-for-merging","title":"Prepare your work for merging","text":"<p>Once you are ready for creating a \"Pull Request\", let's update (rebase in git) our branch with potential remote changes (<code>origin/dev</code>) since branching occurred.</p> <p>Important</p> <p>Start with step 1 &amp; 2 and performs step 3 only if the branch where you are trying to integrate your work have diverged!</p> <ol> <li>Get the latest version of <code>origin/dev</code>:     <pre><code>git checkout dev  # checkout your local `dev` branch\ngit fetch  # fetch remote changes\ngit pull  # pull remote changes (if any) to local \n</code></pre></li> <li>Rebase of <code>origin/dev</code> onto <code>&lt;my_branch&gt;</code>:     <pre><code>git checkout &lt;my_branch&gt;  # checkout the branch to rebase\ngit rebase dev  # rebase `dev` branch onto `&lt;my_branch&gt;`\n</code></pre></li> <li>If <code>dev</code> has diverged during your work:<ol> <li>if you have conflicts:<ol> <li>fix them using an IDE</li> <li>say to git that conflicts are resolved (e.g. for <code>&lt;my_file1.py&gt;</code>):     <pre><code>git add &lt;my_file1.py&gt;\n</code></pre></li> <li>continue rebase until all changes are applied:     <pre><code>git rebase --continue  # to finish rebase\n</code></pre></li> </ol> </li> <li>push all changes (your rebased modifications) to the remote repository:     <pre><code>git push -f origin &lt;my_branch&gt;\n</code></pre></li> </ol> </li> </ol> <p>Warning</p> <p>Using the <code>-f</code> option is necessary after a rebase as local and remote are now different (as show by a <code>git status</code>). This will force push the  changes done after rebasing.</p> <p>Then you can create your \"Pull Request\" from this latest commit using the GitHub interface. Don't forget to add reviewer. Now if you have a CI job checking the instability and performing tests, you may have to wait it all goes well before merging.</p>"},{"location":"plant_imager/developer/git_workflow/#finalization-delete-integrated-branches","title":"Finalization: delete integrated branches.","text":"<p>Check if you find all yours commits on <code>origin/dev</code> (either on GitHub interface of using <code>git log</code> in this branch), if yes:</p> <pre><code>git branch --delete my_branch  # delete local development branch\ngit push origin :my_branch  # delete development branch on origin\n</code></pre> <p>Note</p> <p><code>git branch -a</code> lists all the local branches first (with the current branch in green), followed by the remote-tracking branches in red. <code>git branch</code> only lists your local branches.</p>"},{"location":"plant_imager/developer/git_workflow/#revert-a-commit","title":"Revert a commit","text":"<p>You can revert the last commit with:</p> <pre><code>git reset HEAD^\n</code></pre>"},{"location":"plant_imager/developer/git_workflow/#update-the-project-board-kanban-type","title":"Update the project board (Kanban type)","text":"<p>Go to: https://github.com/orgs/romi/projects</p> <ul> <li>choose the project board corresponding to your pull request (PR)</li> <li>link your PR to existing issues</li> <li>move the corresponding note to the appropriate column</li> </ul>"},{"location":"plant_imager/developer/pipeline_repeatability/","title":"Evaluating the repeatability of a reconstruction and quantification pipeline","text":""},{"location":"plant_imager/developer/pipeline_repeatability/#objective","title":"Objective","text":"<p>The reconstruction and quantification pipeline aims to convert a series of RGB images (output of the plant-imager) into a 3D object, here a plant, with the ultimate goal to obtain quantitative phenotypic information.</p> <p>It is possible to create multiple pipelines as they are composed of a sequence of different tasks, each having a specific function. Some algorithms used in these tasks may be stochastic, hence their output might vary even tough we provide the same input. As it can impact the subsequent quantification, it is of interest to be able to identify the sources of variability and to quantify them.</p> <p>Mainly, two things can be evaluated: * using a dedicated metric (e.g. chamfer distance on point-clouds), quantify the differences in the outputs of a repeated task * quantify the final repercussion on the extracted phenotypic traits</p>"},{"location":"plant_imager/developer/pipeline_repeatability/#prerequisite","title":"Prerequisite","text":"<ul> <li> <p>Create and activate an isolated Python environment (see the procedure here )</p> </li> <li> <p>Install romi <code>plant-3d-vision</code> (from source or using a docker image) &amp; read install procedure</p> </li> </ul>"},{"location":"plant_imager/developer/pipeline_repeatability/#cli-overview","title":"CLI overview","text":"<p>The <code>robustness_evaluation</code> script has been developed to quantify variability in the reconstruction and quantification pipeline. It may be used to test the variability of a specific task or of the full  reconstruction (and quantification) pipeline.</p> <p>Basically it compares outputs of a task given the same input (previous task output or acquisition output depending on the mode) on a fixed parameterizable number of replicates.</p> <p><pre><code>robustness_evaluation -h\n</code></pre> <pre><code>usage: robustness_evaluation [-h] [-n REPLICATE_NUMBER] [-s SUFFIX] [-f] [-np] [-db TEST_DATABASE] [--models MODELS]\n                             [--log-level {CRITICAL,FATAL,ERROR,WARN,WARNING,INFO,DEBUG,NOTSET}]\n                             scan task config_file\n\nRobustness evaluation of the Reconstruction &amp; Quantification pipelines.\n\nEvaluating the repeatability of a Reconstruction &amp; Quantification (R&amp;Q) pipeline is made as follows:\n 1. duplicate the selected scan dataset in a temporary folder (and clean it from previous R&amp;Q if necessary)\n 2. run the R&amp;Q pipeline up to the previous task of the selected task to evaluate, if any\n 3. copy/replicate this result to a new database (append a replicate id to the dataset name)\n 4. run the task to evaluate for each replicated dataset\n 5. compare the directories of the task to evaluate pair by pair\n 6. apply the comparison metrics for the task to evaluate, as defined in `robustness_evaluation.json` \n\nPlease note that:\n - Directory comparisons are done at the scale of the files generated by the selected task.\n - We use metrics to get a quantitative comparison on the output of the task.\n - It is possible to create fully independent repetitions by running the whole R&amp;Q pipeline using `-f`.\n - In order to use the ML-based R&amp;Q pipeline, you will have to:\n   1. create an output directory\n   2. use the `--models` argument to copy the CNN trained models\n\npositional arguments:\n  scan                  Scan dataset to use for repeatability evaluation.\n  task                  Task to test, should be in: AnglesAndInternodes, ClusteredMesh, Colmap, CurveSkeleton,\n                        ExtrinsicCalibration, IntrinsicCalibration, Masks, OrganSegmentation, PointCloud,\n                        Segmentation2D, Segmentation2d, SegmentedPointCloud, TreeGraph, TriangleMesh, Undistorted,\n                        Voxels\n  config_file           Path to the pipeline TOML configuration file.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -n REPLICATE_NUMBER, --replicate_number REPLICATE_NUMBER\n                        Number of replicate to use for repeatability evaluation. Defaults to `30`.\n  -s SUFFIX, --suffix SUFFIX\n                        Suffix to append to the created database folder.\n  -f, --full_pipe       Run the whole Reconstruction &amp; Quantification pipeline on each replicate independently.\n  -np, --no_pipeline    Do not run the pipeline, only compare tasks outputs.\n  -db TEST_DATABASE, --test_database TEST_DATABASE\n                        test database location to use. Use at your own risks!\n  --models MODELS       Models database location to use with ML pipeline.\n  --log-level {CRITICAL,FATAL,ERROR,WARN,WARNING,INFO,DEBUG,NOTSET}\n                        Set message logging level. Defaults to `INFO`.\n\nDetailed explanations here: https://docs.romi-project.eu/plant_imager/developer/pipeline_repeatability/\n</code></pre> The metrics used are the same as the ones for an evaluation against a ground-truth</p>"},{"location":"plant_imager/developer/pipeline_repeatability/#step-by-step-tutorial","title":"Step-by-step tutorial","text":""},{"location":"plant_imager/developer/pipeline_repeatability/#1-test-a-single-task","title":"1. Test a single task","text":"<p>Example with the task <code>TriangleMesh</code> task, whose goal is to compute a mesh from a point-cloud: <pre><code>robustness_evaluation /path/db/my_scan TriangleMesh plant-3d-vision/config/pipeline.toml -n 10\n</code></pre> To summarize, the <code>pipeline.toml</code> configuration indicate the following order of tasks: <pre><code>ImagesFilesetExists -&gt; Colmap -&gt; Undistorted -&gt; Masks -&gt; Voxels -&gt; PointCloud -&gt; TriangleMesh\n</code></pre></p> <p>The call to <code>robustness_evaluation</code>, as previously defined, should result in the following folder structure: <pre><code>path/\n\u251c\u2500\u2500 20210628123840_eval_TriangleMesh/\n\u2502   \u251c\u2500\u2500 my_scan_0/\n\u2502   \u251c\u2500\u2500 my_scan_1/\n\u2502   \u251c\u2500\u2500 my_scan_2/\n\u2502   \u251c\u2500\u2500 my_scan_3/\n\u2502   \u251c\u2500\u2500 my_scan_4/\n\u2502   \u251c\u2500\u2500 my_scan_5/\n\u2502   \u251c\u2500\u2500 my_scan_6/\n\u2502   \u251c\u2500\u2500 my_scan_7/\n\u2502   \u251c\u2500\u2500 my_scan_8/\n\u2502   \u251c\u2500\u2500 my_scan_9/\n\u2502   \u251c\u2500\u2500 filebyfile_comparison.json\n\u2502   \u251c\u2500\u2500 romidb\n\u2502   \u2514\u2500\u2500 TriangleMesh_comparison.json\n\u2514\u2500\u2500 db/\n    \u251c\u2500\u2500 my_scan/\n    \u2514\u2500\u2500 romidb\n</code></pre> The scan datasets <code>my_scan_*</code> are identical up to <code>PointCloud</code> as they result from copies of the same temporary folder. Then the <code>TriangleMesh</code> task is run separately on each one. Quantitative results, using the appropriate metric(s), are in the <code>TriangleMesh_comparison.json</code> file.</p>"},{"location":"plant_imager/developer/pipeline_repeatability/#2-independent-tests","title":"2. Independent tests","text":"<p>If the goal is to evaluate the impact of stochasticity through the whole pipeline in the output of the <code>TriangleMesh</code> task, you should perform independent tests (run the whole pipeline each time) using the <code>-f</code> parameter: <pre><code>robustness_evaluation /path/db/my_scan TriangleMesh plant-3d-vision/config/pipeline.toml -n 10 -f\n</code></pre></p> <p>This will yield a similar folder structure: <pre><code>path/\n\u251c\u2500\u2500 20210628123840_eval_TriangleMesh/\n\u2502   \u251c\u2500\u2500 my_scan_0/\n\u2502   \u251c\u2500\u2500 my_scan_1/\n\u2502   \u251c\u2500\u2500 my_scan_2/\n\u2502   \u251c\u2500\u2500 my_scan_3/\n\u2502   \u251c\u2500\u2500 my_scan_4/\n\u2502   \u251c\u2500\u2500 my_scan_5/\n\u2502   \u251c\u2500\u2500 my_scan_6/\n\u2502   \u251c\u2500\u2500 my_scan_7/\n\u2502   \u251c\u2500\u2500 my_scan_8/\n\u2502   \u251c\u2500\u2500 my_scan_9/\n\u2502   \u251c\u2500\u2500 filebyfile_comparison.json\n\u2502   \u251c\u2500\u2500 romidb\n\u2502   \u251c\u2500\u2500 TriangleMesh_comparison.json\n\u2514\u2500\u2500 db/\n    \u251c\u2500\u2500 my_scan/\n    \u2514\u2500\u2500 romidb\n</code></pre></p> <p>Note</p> <p>To run tests on an existing database the <code>-db</code> parameter is configurable but be careful with it!</p>"},{"location":"plant_imager/developer/tutorial_template/","title":"How to make a good romi tutorial ?","text":"<ul> <li>Format: starts with \"How\", is a question, should be focused to a user-oriented question</li> <li>content: try to build a streamline procedure. If the procedure is too complicated (like different case scenarios, if you need to explain 'if you want to ... and/or if you want to...'), consider splitting your tutorial in as many tutorials as needed to obtain a streamline procedure.</li> </ul> <p>Here a suggested sections for our romi tutorials:</p>"},{"location":"plant_imager/developer/tutorial_template/#objectives","title":"Objectives","text":"<p>At the end of this tutorial, you should be able to:</p> <ul> <li>write a good tutorial</li> <li>review others' tutorials</li> </ul>"},{"location":"plant_imager/developer/tutorial_template/#prerequisite","title":"Prerequisite","text":"<ul> <li>Installations</li> <li>link(s) to other tutorials</li> </ul>"},{"location":"plant_imager/developer/tutorial_template/#short-theoretical-primer","title":"Short theoretical primer","text":"<p>(if needed)</p>"},{"location":"plant_imager/developer/tutorial_template/#step-by-step-tutorial","title":"Step-by-step tutorial","text":""},{"location":"plant_imager/developer/tutorial_template/#1-this-is-step-1","title":"1. This is step 1","text":"<p>For non-computer experts, please remember to provide detailed information each time a command line is needed:</p> <ul> <li>explain if a new terminal should be open or not ;</li> <li>precise from where the command(s) should be executed ;</li> <li>indicate whether a particular isolated environment should be activated (e.g. conda environment) ;</li> <li>give precise and detailed command lines needed (make sure that all commands are given)</li> </ul> <pre><code>cd my repo  #precise from where the command should be executed\ncommand 1  #code comments are welcome !\ncommand 2  # this is for....\n</code></pre> <p>Provide ideas of the expected results to check that the commands were successfully run. examples:</p> <p>a new folder/object is created</p> <p>the terminal says \"useful information\"</p>"},{"location":"plant_imager/developer/tutorial_template/#2-this-is-step-2","title":"2. This is step 2","text":""},{"location":"plant_imager/developer/user_case_scenarios/","title":"User case scenarios for the plant scanner","text":"<p>(or the biologists wish list!)</p> <p>Last edited on 7 Nov 2018.</p> <p>We here describe the scenario followed by a biologist experimenter to acquire and reconstruct the plant architecture using the phenotyping station or 3D scanner.</p>"},{"location":"plant_imager/developer/user_case_scenarios/#minimal-scenario-command-line-interface-cli","title":"Minimal scenario: command line interface (CLI)","text":"<p>We here define a minimal scenario using simple CLI to develop and test the workflow.</p> <ol> <li>the experimenter prepares the plant if needed;</li> <li>the plant is placed (upright) at the centre of the phenotyping station;</li> <li>the experimenter checks the JSON file defining biological metadata</li> <li>the experimenter starts a \"circular scan\" using a CLI:<ol> <li>images are acquired and saved locally (computer controlling the 3D scanner);</li> <li>they are later organised using the Database API;</li> <li>the reconstruction is automatically started after the previous step;</li> </ol> </li> </ol>"},{"location":"plant_imager/developer/user_case_scenarios/#final-scenario-graphical-user-interface-gui","title":"Final scenario: graphical user interface (GUI)","text":"<p>This is the final scenario we want to set up.</p> <ol> <li>the experimenter prepares the plant if needed;</li> <li>the plant is placed (upright) at the centre of the phenotyping station;</li> <li>the experimenter login to the GUI, if not done already;</li> <li>under the \"metadata\" tab, the experimenter defines the biological metadata related to its plan and should be able to check the used hardware metadata:<ol> <li>he or she defines the biological metadata using predefined fields and values (should be possible to add more);</li> <li>he or she validates by clicking a \"save\" button;</li> </ol> </li> <li>under the \"acquisition\" tab, the experimenter defines the acquisition method and settings (the type of scan, number of images, ...) and initiate the acquisition:<ol> <li>he or she defines the acquisition settings using predefined fields and values (should be possible to add more);</li> <li>he or she starts this step by clicking an \"acquire\" button;</li> </ol> </li> <li>under the \"reconstruction\" tab, the experimenter can access a list of datasets (he owns or accessible to him) and initiate reconstruction(s):<ol> <li>he or she can select one dataset (or more?),</li> <li>he or she selects a 3D reconstruction method,</li> <li>he or she defines its settings,</li> <li>he or she starts this step by clicking a \"reconstruct\" button;</li> </ol> </li> <li>under the \"quantification\" tab, the experimenter can access a list of 3D structures (he owns or accessible to him) and initiate quantification(s):<ol> <li>he or she can select one 3D structure (or more?),</li> <li>he or she selects a quantification method,</li> <li>he or she defines its settings,</li> <li>he or she starts this step by clicking a \"quantify\" button;</li> </ol> </li> </ol> <p>We would probably need an \"OMERO\" tab to:</p> <ol> <li>select/change the \"group\" to which the dataset should be sent to;</li> <li>change the URL and port (change the used OMERO database);</li> <li>change the user logged to the OMERO database;</li> </ol>"},{"location":"plant_imager/developer/vpi_reference_api/","title":"Virtual Plant Imager API","text":"<p>Once you start the <code>romi_virtualplantimager</code> Python script in Blender, you will have a Flask server running and listening to HTTP requests.</p> <p>Hereafter we list the GET/POST requests that can be made and give some examples.</p>"},{"location":"plant_imager/developer/vpi_reference_api/#start-the-blender-server","title":"Start the Blender server","text":"<p>There are many ways to do this, but the simplest &amp; fastest option is to use the Docker image <code>roboticsmicrofarms/virtualplantimager</code>.</p> <p>Note</p> <p>We make use of the <code>$DB_LOCATION</code> environment variable. If not done yet, you may want to set it with <code>export DB_LOCATION=/path/to/database</code>. Or replace it with the path to use.</p>"},{"location":"plant_imager/developer/vpi_reference_api/#start-a-container","title":"Start a container","text":"Docker CLI<code>run.sh</code> script <p>Start a container and open a bash shell: <pre><code>docker run -it --gpus all roboticsmicrofarms/virtualplantimager:latest -v $DB_LOCATION:/myapp/db bash\n</code></pre></p> <p>Start a container and open a bash shell: <pre><code>./docker/virtualplantimager/run.sh -db $DB_LOCATION\n</code></pre></p>"},{"location":"plant_imager/developer/vpi_reference_api/#start-the-blender-flask-server","title":"Start the Blender Flask server","text":"<p>Then start the Blender server (listening to port <code>9001</code>) with: <pre><code>romi_bpy plant-imager/bin/romi_virtualplantimager -- --port 9001\n</code></pre></p> <p>When the server is up and running you should get something like: <pre><code> * Serving Flask app 'romi_virtualplantimager'\n * Running on all addresses (0.0.0.0)\n * Running on http://127.0.0.1:9001\n * Running on http://172.17.0.2:9001\n</code></pre></p> <p>Note</p> <p>The first HTTP address is accessible from within the container. The second HTTP address is accessible from the host running the container.</p>"},{"location":"plant_imager/developer/vpi_reference_api/#test-the-server","title":"Test the server","text":"From the hostFrom the container <p>You may now use a web browser to submit a <code>/hello_world</code> request at <code>http://172.17.0.2:9001</code>.</p> <p>To do so, just copy/paste <code>http://172.17.0.2:9001/hello_world</code> to the URL bar.</p> <p>You may use Python to submit a request at <code>http://172.0.0.1:9001</code>.</p> <p>For example, you may get info by submitting a <code>/hello_world</code> request as follows:</p> <ol> <li>Open a new shell in the running container with (do not forget to replace the <code>CONTAINER_ID</code>): <pre><code>docker exec -it CONTAINER_ID bash\n</code></pre></li> <li>Then use Python to send a GET request: <pre><code>python -c \"import requests\nres = requests.get('http://127.0.0.1:9001/hello_world')\nprint(res.content.decode())\"\n</code></pre></li> </ol> <p>You should get a JSON response similar to this: <pre><code>\"Hello World!\"\n\"I am a Flask server named 'romi_virtualplantimager'.\"\n\"I run Blender 2.93.16 built on 2023-03-21.\"\n\"I run Python 3.9.16.\"\n</code></pre></p>"},{"location":"plant_imager/developer/vpi_reference_api/#reference","title":"Reference","text":""},{"location":"plant_imager/developer/vpi_reference_api/#objects","title":"Objects","text":"<ul> <li><code>/objects</code> (GET): retrieve the list of <code>obj</code> files in the data folder that can be loaded.</li> <li><code>/load_object/&lt;object_id&gt;</code> (GET) load the given object in the scene. Takes a translation vector as URL parameters (<code>dx</code>, <code>dy</code>, <code>dz</code>)</li> </ul>"},{"location":"plant_imager/developer/vpi_reference_api/#classes","title":"Classes","text":"<ul> <li><code>/classes</code> (GET): retrieve the list of classes.</li> </ul>"},{"location":"plant_imager/developer/vpi_reference_api/#backgrounds","title":"Backgrounds","text":"<ul> <li><code>/backgrounds</code> (GET): retrieve the list of <code>hdr</code> files in the <code>hdri</code> folder that can be loaded.</li> <li><code>/load_background/&lt;background_id&gt;</code> (GET) load the given background in the scene.</li> </ul>"},{"location":"plant_imager/developer/vpi_reference_api/#camera","title":"Camera","text":"<ul> <li><code>/camera_intrinsics</code> (POST): set camera intrinsics. Keys: <code>width</code>, <code>height</code>, <code>focal</code></li> <li><code>/camera_pose</code> (POST): set camera pose. Keys: <code>tx</code>, <code>ty</code>, <code>tz</code>, <code>rx</code>, <code>ry</code>, <code>rz</code></li> </ul>"},{"location":"plant_imager/developer/vpi_reference_api/#rendering","title":"Rendering","text":"<ul> <li><code>/render</code> (GET): gets the rendering of the scene</li> <li><code>/render_class/&lt;class_id&gt;</code> (GET) renders the scene, with everything transparent except the given class</li> </ul> <p>Todo</p> <p>Missing endpoints.</p>"},{"location":"plant_imager/developer/vpi_reference_api/#examples","title":"Examples","text":"<p>Using a browser to send HTTP requests is not too convenient. Instead, you may use httpie to send HTTP commands from a terminal.</p> <p>To easily adapt to other configurations, we define the <code>$VPI_HOST</code> &amp; <code>$VPI_PORT</code> variables. For example, matching the example given above, we define: <pre><code>export VPI_HOST='172.17.0.2'\nexport VPI_PORT='9001'\n</code></pre></p>"},{"location":"plant_imager/developer/vpi_reference_api/#setup-camera","title":"Setup camera","text":"<pre><code>http -f post \"http://$VPI_HOST:$VPI_PORT/camera_intrinsics width=1920 height=1080 focal=35\"\n</code></pre>"},{"location":"plant_imager/developer/vpi_reference_api/#load-arabidopsis_0-obj","title":"Load <code>arabidopsis_0</code> OBJ","text":"<pre><code>http get \"http://$VPI_HOST:$VPI_PORT/load_object/arabidopsis_0.obj?dx=10&amp;dy=20&amp;dz=1\"\n</code></pre>"},{"location":"plant_imager/developer/vpi_reference_api/#load-old-tree-in-the-park-background","title":"Load \"old tree in the park\" background","text":"<pre><code>http get \"http://127.0.0.1:$VPI_PORT/load_background/old_tree_in_city_park_8k.hdr\"\n</code></pre>"},{"location":"plant_imager/developer/vpi_reference_api/#move-camera","title":"Move camera","text":"<pre><code>http -f post \"http://$VPI_HOST:$VPI_PORT/camera_pose tx=-60 ty=0 tz=50 rx=60 ry=0 rz=-90\"\n</code></pre>"},{"location":"plant_imager/developer/vpi_reference_api/#render-scene-and-download-image","title":"Render scene and download image","text":"<pre><code>http --download get \"http://$VPI_HOST:$VPI_PORT/render\"\n</code></pre>"},{"location":"plant_imager/developer/vpi_reference_api/#render-only-leaves","title":"Render only leaves","text":"<pre><code>http --download get \"http://$VPI_HOST:$VPI_PORT/render_class/Color_7\"\n</code></pre>"},{"location":"plant_imager/developer/how_to/create_new_evaluation_task/","title":"How-to create a new ROMI evaluation task","text":"<p>In order to evaluate the reconstruction and quantification tasks accuracy, we offer the possibility to also create evaluation tasks. The idea is to use a digital twin to generates the expected outcome of a task and use it as ground truth to challenge the reconstruction task.</p> <p>To do so, you will have to create two tasks:</p> <ul> <li>ground truth task: it should generate the expected outcome of the evaluated task from the digital twin;</li> <li>evaluation task: it will compare the output of the evaluated task against the ground truth.</li> </ul> <p>For example, the <code>Voxels</code> task has a <code>VoxelGroundTruth</code> task and a <code>VoxelEvaluation</code> task.</p>"},{"location":"plant_imager/developer/how_to/create_new_evaluation_task/#ground-truth-task","title":"Ground truth task","text":"<p>Ground truth tasks should be defined in <code>plant-3d-vision/plant3dvision/tasks/ground_truth.py</code>. It should inherit from <code>RomiTask</code> and define a <code>run</code> method exporting the ground truth later use as reference in the evaluation task.</p> <p>Warning</p> <p>Do not forget to reference the task in <code>romitask/romitask/modules.py</code>.</p>"},{"location":"plant_imager/developer/how_to/create_new_evaluation_task/#evaluation-task","title":"Evaluation task","text":"<p>Evaluation tasks should be defined in <code>plant-3d-vision/plant3dvision/tasks/evaluation.py</code>. The evaluation task that you will write should inherit from <code>EvaluationTask</code> that defines:</p> <ul> <li>the <code>requires</code> method to use an <code>upstream_task</code> and <code>ground_truth</code>;</li> <li>the <code>output</code> method to create the corresponding evaluation dataset</li> <li>the <code>evaluate</code> method that you should override;</li> <li>the <code>run</code> method that call <code>evaluate</code> and save the results as a JSON file.</li> </ul> <p>Warning</p> <p>Do not forget to reference the task in <code>romitask/romitask/modules.py</code>.</p>"},{"location":"plant_imager/developer/how_to/create_new_task/","title":"How-to create a new ROMI task","text":"<p>We hereafter details how you can make your own algorithms available to the ROMI reconstruction and analysis pipeline by creating a task and registering it as an available module.</p> <p>For the sake of clarity and to illustrate how-to create a ROMI task from scratch, in this guide we will assume you want to add something quite different from what is already there.</p> <p>Important</p> <p>ROMI task usually have a semantic meaning and for example, the task <code>AnglesAndInternodes</code> may take several types of object in input (mesh, point-cloud &amp; skeletons) but always output the JSON file with the obtained measures. So, to decide if you have to create a new task or add your algorithm to an existing task, following this rule should help: at a given step of the pipeline, if the output change, this is a NEW task!</p>"},{"location":"plant_imager/developer/how_to/create_new_task/#add-your-algorithm-to-plant3dvision","title":"Add your algorithm to <code>plant3dvision</code>","text":"<p>You first have to add a file (or append to an existing one), e.g. named <code>algo.py</code>, under the <code>plant-3d-vision/plant3dvision</code> directory.</p> <p>Let's assume the previously added file has a main function called <code>my_algo</code> like this:</p> <pre><code>def my_algo(data, *params, **kwargs):\n    # Do something to data with given parameters to return transformed data `out_data`\n    return out_data, error\n</code></pre> <p>It has:</p> <ul> <li>data input(s) (e.g. images, point clouds, meshes, ...) that will often be the output of a previous task in the pipeline</li> <li>parameter(s), specific to the algorithm you want to add</li> <li>output(s), the transformed dataset that will often be the input of a following task in the pipeline</li> </ul>"},{"location":"plant_imager/developer/how_to/create_new_task/#create-a-romi-task","title":"Create a ROMI task","text":""},{"location":"plant_imager/developer/how_to/create_new_task/#dependency-to-luigi","title":"Dependency to <code>luigi</code>","text":"<p>We use <code>luigi</code> to manage the pipeline execution and handle requirements &amp; tasks dependencies. To create a task you will thus have to create a new Python class <code>MyTask</code> inheriting from the <code>RomiTask</code> class and creates a few methods and at least a <code>run</code> method used by <code>luigi</code>.</p>"},{"location":"plant_imager/developer/how_to/create_new_task/#dependency-to-plantdb","title":"Dependency to <code>plantdb</code>","text":"<p>To manage the files, inputs and outputs, we use the <code>plantdb</code> package implementing a local file system database written in pure python. It provides classes and methods that simplifies and normalize the creation and use of the tasks outputs and inputs.</p>"},{"location":"plant_imager/developer/how_to/create_new_task/#new-romitask-template","title":"New <code>RomiTask</code> template","text":"<p>You will create a new python file <code>my_task.py</code> in the <code>tasks</code> submodule: <code>plant-3d-vision/plant3dvision/tasks/my_task.py</code></p> <pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\"\"\"\nBriefly describe your module here.\n\"\"\"\n\nimport luigi\n\nfrom plantdb import RomiTask\nfrom plantdb import io\nfrom plant3dvision.log import logger  # Use this as logging method\nfrom plant3dvision.tasks.proc3d import SegmentedPointCloud\n\n# Now import your main method:\nfrom plant3dvision.algo import my_algo\n\n\ndef MyTask(RomiTask):\n\"\"\"My algorithm is the best!\n\n    Attributes\n    ----------\n    upstream_task : luigi.TaskParameter\n        Upstream task that will provides the data to your algorithm, here `SegmentedPointCloud`.\n    param1 : luigi.FloatParameter\n        An example float parameter parsed from the TOML config file.\n        Set to `2.0` by default.\n    param2 : luigi.IntParameter\n        An example float parameter parsed from the TOML config file.\n        Set to `5` by default.\n    log : luigi.BoolParameter\n        An example boolean parameter.\n\n    \"\"\"\n    # No need to write an `__init__` section, declare your class attributes as task parameters:\n    upstream_task = luigi.TaskParameter(default=SegmentedPointCloud)\n    param1 = luigi.FloatParameter(default=2.0)\n    param2 = luigi.IntParameter(default=5)\n    log = luigi.BoolParameter(default=False)\n\n    def requires(self):\n\"\"\"Used by luigi to check you task dependencies.\"\"\"\n        # By default a RomiTask requires a luigi.TaskParameter called `upstream_task`.\n        # So no need to declare this method if you don't requires more than one upstream task!\n        # Else you can override with something like (should be of type `luigi.TaskParameter`!):\n        #return [self.upstream_task1(), self.upstream_task1()]\n        pass\n\n    def run(self):\n\"\"\"Called by luigi, it will run your algorithm.\n\n        Usually consist of 3 steps:\n        1. Get the input(s) data from the previous task, eg. images or point clouds\n        2. Run you algorithm on input data\n        3. Save the result(s) of your method, eg. as a JSON file\n\n        Notes\n        -----\n        The parameters for your algorithms have been declared at class instantiation!\n        \"\"\"\n        # -1- Get the input(s) data from the previous task\n        # To access the single file output of the upstream task use:\n        uptask_input_file = self.input_file()\n        # Read it with the proper reader, here a point-cloud reader (SegmentedPointCloud):\n        in_data = io.read_point_cloud(uptask_input_file)\n\n        # -2- Run you algorithm on input data\n        out_data, error = my_algo(in_data)\n        # Use example for boolean parameter &amp; logger with 'info' level\n        if self.log:\n            logger.info(\"My task ran perfectly!\")\n\n        # -3- Write a single output (eg. a JSON file)...\n        # Create the output `File` object\n        task_output_file = self.output_file()\n        # Write a JSON file with your method results\n        io.write_json(task_output_file, out_data)\n        # Add metadata to your file, eg. some error measure you don't want to include in the main output file:\n        task_output_file.set_metadata(\"my_error\", error)\n</code></pre> <p>The corresponding TOML configuration file (<code>my_pipeline.toml</code>) controlling your task behaviour would look like this:</p> <pre><code>[MyTask]\nupstream_task='SegmentedPointCloud'\nparam1=6.0\nparam2=3\nlog=true\n</code></pre> <p>Note</p> <p>You may need to add methods to read and write data, this should be done in the <code>plantdb</code> library using the <code>plantdb/plantdb/io.py</code> file!</p>"},{"location":"plant_imager/developer/how_to/create_new_task/#multiple-io-for-a-task","title":"Multiple I/O for a task","text":"<p>Your method (or the upstream task) may produce a set of object you want to save as separates files. In such case, use <code>Filset</code> objects.</p> <p>For example to output multiple JSON files:</p> <pre><code>list_of_jsonifyable = [...]\ntask_output_fs = self.output().get()\nfor i, json_data in enumerate(list_of_jsonifyable):\n    f = task_output_fs.create_file(f\"my_json_{i}\")  # no extension!\n    io.write_json(f, json_data)\n    # Add some metadata to this `File` object\n    f.set_metadata(\"foo\", f\"bar{i}\")\n</code></pre>"},{"location":"plant_imager/developer/how_to/create_new_task/#test-your-task","title":"Test your task","text":"<p>You should now be able to test your newly created task <code>MyTask</code> with <code>romi_run_task</code>:</p> <pre><code>romi_run_task MyTask /path/to/dataset --config /path/to/my_pipeline.toml --module plant3dvision.tasks.my_algo\n</code></pre> <p>Using the <code>--module</code> option you can test your task without registering it.</p>"},{"location":"plant_imager/developer/how_to/create_new_task/#register-your-task","title":"Register your task","text":"<p>Once you are satisfied, you can add it to <code>romitask/modules.py</code> by referring to the task class name &amp; its python module location:</p> <pre><code>MODULES = {\n    # ...\n    \"MyTask\": \"plant3dvision.tasks.my_algo\",\n    # ...\n}\n</code></pre>"},{"location":"plant_imager/developer/how_to/create_new_task/#use-your-newly-created-task","title":"Use your newly created task","text":"<p>Finally, you should now be able to use your newly created task <code>MyTask</code> with <code>romi_run_task</code>:</p> <pre><code>romi_run_task MyTask /path/to/dataset --config /path/to/my_pipeline.toml\n</code></pre> <p>Warning</p> <p>Use of absolute path is highly recommended as you may experience some difficulties from <code>luigi</code> otherwise!</p>"},{"location":"plant_imager/developer/how_to/import_files/","title":"How-to import files in ROMI database","text":""},{"location":"plant_imager/developer/how_to/import_files/#importing-external-images-as-a-dataset","title":"Importing external images as a dataset","text":"<p>In order to be able to use external images, i.e. images that were not acquired with the software &amp; hardware developed by ROMI, we provide tools to import them as a scan dataset in the ROMI database.</p> <p>One example could be a set of pictures (of a plant) acquired with your phone that you would like to reconstruct and maybe analyse with our software.</p> <p>To do so, you may use the <code>romi_import_folder</code> or <code>romi_import_file</code> executables from <code>plantdb</code>.</p> <p>For example, you have a set of 10 RGB pictures named <code>img_00*.jpg</code> in a folder <code>my_plant/</code> that you would like to import as <code>outdoor_plant_1</code> in a romi database located under <code>/data/romi/db</code>.</p> <p>First you have to move the pictures to an <code>\u00ecmages</code> sub-directory &amp; create a <code>metadata.json</code> describing the object under study:</p> <pre><code>cd my_plant\nmkdir images\nmv *.jpg images/.\ntouch metadata.json\n</code></pre> <p>An example of a <code>metadata.json</code>:</p> <pre><code>{\n\"object\": {\n\"age\": \"N/A\",\n\"culture\": \"N/A\",\n\"environment\": \"outdoor\",\n\"experiment_id\": \"romi demo outdoor plant\",\n\"object\": \"plant\",\n\"plant_id\": \"Chirsuta_1\",\n\"sample\": \"whole plant\",\n\"species\": \"Cardamine hirsuta\",\n\"stock\": \"WT\",\n\"treatment\": \"none\"\n}\n}\n</code></pre> <p>To summarize you now should have the following folder structure:</p> <pre><code>my_plant/\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 img_001.jpg\n\u2502   \u251c\u2500\u2500 [...]\n\u2502   \u2514\u2500\u2500 img_010.jpg\n\u2514\u2500\u2500 metadata.json\n</code></pre> <p>Then you can perform the 'import to the database' operation with <code>romi_import_folder</code>:</p> <pre><code>romi_import_folder my_plant/\u00ecmages/ /data/romi/db/outdoor_plant_1 --metadata my_plant/metadata.json\n</code></pre> <p>That's it! Your manual acquisition is ready to be used by the <code>romi_run_task</code> tool for reconstruction.</p>"},{"location":"plant_imager/docker/","title":"Docker containers for ROMI","text":"<p>The official dockerhub repository for the ROMI project is roboticsmicrofarms.</p>"},{"location":"plant_imager/docker/#list-of-docker-containers","title":"List of docker containers","text":"<p>We hereafter list the docker containers, their availability and provides link to their location &amp; usage instructions:</p> <ul> <li> plantdb is available here and explanations there</li> <li> plantimager is not available yet and explanations there</li> <li> plant-3d-vision is available there</li> <li> plant-3d-explorer is available here and   explanations there</li> </ul>"},{"location":"plant_imager/docker/#use-cases-with-docker-compose","title":"Use cases with docker-compose","text":"<p>In this section we reference the \"real-life\" use cases of our software.</p>"},{"location":"plant_imager/docker/#use-the-plant-3d-explorer-on-a-local-database-directory","title":"Use the plant 3d explorer on a local database directory","text":"<p>The easiest way to use the plant-3d-explorer on a local database directory without installing the ROMI libraries (and their dependencies) is to use the pre-built docker image and add a docker-compose YAML recipe. See here for more details.</p>"},{"location":"plant_imager/docker/#getting-started-with-docker","title":"Getting started with docker","text":"<p>In order to be able to use the ROMI docker images you have to install <code>docker-ce</code> and <code>nvidia-docker2</code>.</p>"},{"location":"plant_imager/docker/#installing-docker","title":"Installing docker","text":"<p>To install <code>docker-ce</code>, please refer to the official documentation.</p>"},{"location":"plant_imager/docker/#install-nvidia-docker","title":"Install nvidia-docker","text":"<p>To install <code>nvidia-docker2</code>, please refer to the official documentation.</p>"},{"location":"plant_imager/docker/#dockerhub","title":"DockerHub","text":""},{"location":"plant_imager/docker/#romi-repository","title":"ROMI repository","text":"<p>The Docker hub repository for the ROMI project is here: https://hub.docker.com/orgs/roboticsmicrofarms.</p>"},{"location":"plant_imager/docker/#colmap","title":"Colmap","text":"<p>Docker images for the Colmap open source project: https://hub.docker.com/r/colmap/colmap</p> <p>nvidia/cuda with Colmap - (compatible with Driver Version: 418.67 CUDA Version: 10.1) https://hub.docker.com/r/geki/colmap</p>"},{"location":"plant_imager/docker/colmap/","title":"Colmap docker image","text":"<p>As we want the possibility to choose the Python version we use, for example 3.8, and the provided docker image are based on Ubuntu 18.04 that ship Python 3.6 &amp; 3.7, we need to create our own <code>Dockerfile</code>.</p> <p>This has been done in <code>docker/colmap/Dockerfile</code> with:</p> <ul> <li>Python 3.8</li> <li>Colmap 3.7</li> </ul>"},{"location":"plant_imager/docker/colmap/#build-the-docker-image","title":"Build the docker image","text":"<p>To build the docker image, use the <code>Dockerfile</code> in <code>docker/colmap/</code>:</p> <pre><code>image_name=\"roboticsmicrofarms/colmap\"\nimage_version=\"3.7\"\ndocker build -t=\"$image_name:$image_version\" docker/colmap/.\ndocker tag \"$image_name:$image_version\" \"$image_name:latest\"\n</code></pre>"},{"location":"plant_imager/docker/colmap/#test-the-container","title":"Test the container","text":"<p>Let's test the image we just created!</p>"},{"location":"plant_imager/docker/colmap/#start-a-container","title":"Start a container","text":"<p>You can start by creating a running container with:</p> <pre><code>docker run -it --gpus all \\\n-v /tmp/integration_tests/2019-02-01_10-56-33/:/tmp/ \\\nroboticsmicrofarms/colmap:3.7\n</code></pre> <p>Try to call the colmap executable to get the version number with:</p> <pre><code>colmap -v\n</code></pre> <p>As we also installed Python, try to call it after activating the <code>venv</code> with: <pre><code>. /venv/bin/activate\npython -V\n</code></pre></p>"},{"location":"plant_imager/docker/colmap/#get-a-test-dataset","title":"Get a test dataset","text":"<p>To further test the built image, let's try to use colmap on a typical set of data. If you do not have your own dataset, we provide a test dataset that you can download (to the temporary folder) as follows:</p> <pre><code>cd /tmp\nwget https://db.romi-project.eu/models/test_db.tar.gz\ntar -xf test_db.tar.gz\n</code></pre>"},{"location":"plant_imager/docker/colmap/#extract-images-poses","title":"Extract images poses","text":"<p>We use the \"poses\" (camera locations) provided in the images' metadata (JSON file associated to each images) by the plant-imager to create a <code>poses.txt</code> file containing each image coordinates: <pre><code>import os\nimport json\n\nposefile = open(f\"/tmp/poses.txt\", mode='w')\n# - Try to get the pose from each file metadata:\nfor i, file in enumerate(sorted(os.listdir(\"/tmp/metadata/images\"))):\n    with open(f\"/tmp/metadata/images/{file}\", mode='r') as f:\n        jdict = json.load(f)\n    # print(jdict)\n    try:\n        p = jdict['approximate_pose']\n    except KeyError:\n        p = jdict['pose']  # backward compatibility, should work for provided test dataset\n    s = '%s %d %d %d\\n' % (file.split('.')[0] + \".jpg\", p[0], p[1], p[2])\n    posefile.write(s)\n\nposefile.close()\n</code></pre></p>"},{"location":"plant_imager/docker/colmap/#test-colmap-tools","title":"Test colmap tools","text":"<p>You can test that the colmap tools are working properly by calling them as follows:</p> <pre><code>DATASET_PATH=/tmp\n\ncolmap feature_extractor \\\n--database_path $DATASET_PATH/database.db \\\n--image_path $DATASET_PATH/images\n\ncolmap exhaustive_matcher \\\n--database_path $DATASET_PATH/database.db\n\nmkdir $DATASET_PATH/sparse\n\ncolmap mapper \\\n--database_path $DATASET_PATH/database.db \\\n--image_path $DATASET_PATH/images \\\n--output_path $DATASET_PATH/sparse\n\ncolmap model_aligner \\\n--ref_images_path $DATASET_PATH/poses.txt \\\n--input_path $DATASET_PATH/sparse/0 \\\n--output_path $DATASET_PATH/sparse/0 \\\n--ref_is_gps 0 \\\n--robust_alignment_max_error 10\n\ncolmap model_converter \\\n--input_path $DATASET_PATH/sparse/0 \\\n--output_path $DATASET_PATH/sparse/0/sparse.ply \\\n--output_type PLY\n\nmkdir $DATASET_PATH/dense\n\ncolmap image_undistorter \\\n--image_path $DATASET_PATH/images \\\n--input_path $DATASET_PATH/sparse/0 \\\n--output_path $DATASET_PATH/dense \\\n--output_type COLMAP \\\n--max_image_size 2000\n\ncolmap patch_match_stereo \\\n--workspace_path $DATASET_PATH/dense \\\n--workspace_format COLMAP \\\n--PatchMatchStereo.geom_consistency true\n\ncolmap stereo_fusion \\\n--workspace_path $DATASET_PATH/dense \\\n--workspace_format COLMAP \\\n--input_type geometric \\\n--output_path $DATASET_PATH/dense/fused.ply\n</code></pre>"},{"location":"plant_imager/docker/colmap/#test-geometric-pipeline","title":"Test geometric pipeline","text":"<p>If you have <code>plant-3d-vision</code> installed on your machine, you can further test the colmap image with the reconstruction pipelines using our test scripts and datasets.</p>"},{"location":"plant_imager/docker/colmap/#test-it-on-real-data","title":"Test it on real data","text":"<pre><code>export COLMAP_EXE=\"roboticsmicrofarms/colmap\"\n./tests/check_geom_pipe.sh --tmp\n</code></pre>"},{"location":"plant_imager/docker/colmap/#test-it-on-virtual-data","title":"Test it on virtual data","text":"<pre><code>export COLMAP_EXE=\"roboticsmicrofarms/colmap\"\n./tests/check_geom_pipe.sh --tmp --virtual\n</code></pre>"},{"location":"plant_imager/docker/colmap/#test-it-on-your-real-data","title":"Test it on your (real) data","text":"<pre><code>export COLMAP_EXE=\"roboticsmicrofarms/colmap\"\n./tests/check_geom_pipe.sh --tmp --database /path/to/my/dataset\n</code></pre>"},{"location":"plant_imager/docker/colmap/#upload-the-built-image","title":"Upload the built image","text":"<p>Once you have checked the obtained image leads to functional containers, you can upload the image to docker hub!</p> <p>Start by login in to docker hub with: <pre><code>docker login\n</code></pre></p> <p>Then you can upload with: <pre><code>docker push \"$image_name\"\n</code></pre></p>"},{"location":"plant_imager/docker/docker_compose/","title":"Docker compose","text":"<p>In the following sections we will propose several use cases combining docker images thanks to <code>docker-compose</code>.</p> <p>Note</p> <p>You need <code>docker-compose</code> installed, see here.</p>"},{"location":"plant_imager/docker/docker_compose/#database-plant-3d-explorer","title":"Database &amp; plant 3D explorer","text":"<p>To use your own local database, we provide a docker compose recipe that:</p> <ol> <li>start a database container using <code>roboticsmicrofarms/plantdb</code></li> <li>start a plant-3d-explorer container using <code>roboticsmicrofarms/plant-3d-explorer</code></li> </ol>"},{"location":"plant_imager/docker/docker_compose/#use-pre-built-docker-image","title":"Use pre-built docker image","text":"<p>You can use the pre-built images <code>plantdb</code> &amp; <code>plantviewer</code>, accessible from the ROMI dockerhub, to easily test &amp; use the <code>plant 3d explorer</code> with your own database 1.</p> <p>The <code>docker-compose.yml</code> look like this:</p> <pre><code>version: '3'\nservices:\ndb:\nimage: \"roboticsmicrofarms/plantdb\"\nvolumes:\n- ${ROMI_DB}:/myapp/db\nexpose:\n- \"5000\"\nhealthcheck:\ntest: \"exit 0\"\nviewer:\nimage: \"roboticsmicrofarms/plant-3d-explorer\"\ndepends_on:\n- db\nenvironment:\nREACT_APP_API_URL: http://172.21.0.2:5000\nports:\n- \"3000:3000\"\n</code></pre> <p>From the root directory of <code>plant-3d-explorer</code> containing the <code>docker-compose.yml</code> in a terminal:</p> <pre><code>export ROMI_DB=&lt;path/to/db&gt;\ndocker-compose up -d </code></pre> <p>Important</p> <p>Do not forget to set the path to the database.</p> <p>Warning</p> <p>If you have other containers running it might not work since it assumes the plantdb container will have the <code>172.21.0.2</code> IP address!</p> <p>To stop the containers:</p> <pre><code>docker-compose stop\n</code></pre> <p>Note</p> <p>To use local builds, change the <code>image</code> YAML parameter to match your images names &amp; tag.</p>"},{"location":"plant_imager/docker/docker_compose/#force-local-builds","title":"Force local builds","text":"<p>To force builds at compose startup, for development or debugging purposes, use the <code>build</code> YAML parameter instead of <code>image</code> 2. It is possible to keep the <code>image</code> YAML parameter to tag the built images 3.</p> <p>The <code>docker-compose.yml</code> should look like this:</p> <pre><code>version: '3'\nservices:\ndb:\nbuild: ../plantdb/.\nimage: db:debug\nvolumes:\n- ${ROMI_DB}:/myapp/db\nexpose:\n- \"5000\"\nhealthcheck:\ntest: \"exit 0\"\nviewer:\nbuild: ../developer\nimage: viewer:debug\ndepends_on:\n- db\nenvironment:\nREACT_APP_API_URL: http://172.21.0.2:5000\nports:\n- \"3000:3000\"\n</code></pre> <p>Warning</p> <p>This assumes that you have to <code>plantdb</code> repository cloned next to the one of <code>plant-3d-explorer</code>.</p> <ol> <li> <p>https://docs.docker.com/compose/compose-file/#image \u21a9</p> </li> <li> <p>https://docs.docker.com/compose/gettingstarted/ \u21a9</p> </li> <li> <p>https://docs.docker.com/compose/compose-file/#build \u21a9</p> </li> </ol>"},{"location":"plant_imager/docker/plant-3d-vision_docker/","title":"Docker container for ROMI plantinterpreter","text":"<p>Important</p> <p>An existing local database directory is required, it will be mounted at container startup. To see how to create a local database directory, look here.</p>"},{"location":"plant_imager/docker/plant-3d-vision_docker/#use-pre-built-docker-image","title":"Use pre-built docker image","text":"<p>Assuming you have a valid ROMI database directory under <code>/data/ROMI/DB</code>, you can easily download and start the pre-built <code>roboticsmicrofarms/plant-3d-vision</code> docker image with:</p> <pre><code>export ROMI_DB=/data/ROMI/DB\ndocker run --runtime=nvidia --gpus all \\\n--env PYOPENCL_CTX='0' \\\n-v $ROMI_DB:/myapp/db \\\n-it roboticsmicrofarms/plant-3d-vision:latest\n</code></pre> <p>This should start the latest pre-built <code>roboticsmicrofarms/plant-3d-vision</code> docker image in interactive mode. The database location inside the docker container is <code>/myapp/db</code>.</p> <p>Note</p> <p><code>-v $ROMI_DB:/myapp/db</code> performs a bind mount to enable access to the local database by the docker image. See the official documentation.</p>"},{"location":"plant_imager/docker/plant-3d-vision_docker/#build-docker-image","title":"Build docker image","text":"<p>We provide a convenience bash script to ease the build of <code>roboticsmicrofarms/plant-3d-vision</code> docker image. You can choose to use this script OR to \"manually\" call the <code>docker build</code> command.</p>"},{"location":"plant_imager/docker/plant-3d-vision_docker/#provided-convenience-buildsh-script","title":"Provided convenience <code>build.sh</code> script","text":"<p>To build the image with the provided build script, from the root directory:</p> <pre><code>./docker/build.sh\n</code></pre> <p>You can also pass some options, use <code>./docker/build.sh -h</code> to get more details about usage, options and default values.</p> <p>Tips</p> <p>To be sure to always pull the latest parent image, you may add the <code>--pull</code> option!</p>"},{"location":"plant_imager/docker/plant-3d-vision_docker/#manually-call-the-docker-build-command","title":"Manually call the <code>docker build</code> command","text":"<p>To build the image, from the root directory:</p> <pre><code>export VTAG=\"latest\"\ndocker build -t roboticsmicrofarms/plant-3d-vision:$VTAG .\n</code></pre>"},{"location":"plant_imager/docker/plant-3d-vision_docker/#publish-docker-image","title":"Publish docker image","text":"<p>Push it on docker hub:</p> <pre><code>docker push roboticsmicrofarms/plantdb:$VTAG\n</code></pre> <p>This requires a valid account &amp; token on dockerhub!</p>"},{"location":"plant_imager/docker/plant-3d-vision_docker/#usage","title":"Usage","text":""},{"location":"plant_imager/docker/plant-3d-vision_docker/#requirements","title":"Requirements","text":"<p>To run it, you need to have a valid local ROMI database, look here for instructions and here for an example database.</p>"},{"location":"plant_imager/docker/plant-3d-vision_docker/#starting-the-plant-3d-vision-docker-image","title":"Starting the <code>plant-3d-vision</code> docker image","text":""},{"location":"plant_imager/docker/plant-3d-vision_docker/#provided-runsh-script","title":"Provided <code>run.sh</code> script","text":"<p>To start the container, in interactive mode, with the provided run script in <code>plant3dvision/docker</code>, use:</p> <pre><code>./run.sh\n</code></pre> <p>You can also pass some options, use <code>./run.sh -h</code> to get more details about usage and options, notably to mount a local romi database.</p>"},{"location":"plant_imager/docker/plant-3d-vision_docker/#nvidia-gpu-test","title":"NVIDIA GPU test","text":"<p>To make sure the started container will be able to access the host GPU, use:</p> <pre><code>./run.sh  --gpu_test\n</code></pre>"},{"location":"plant_imager/docker/plant-3d-vision_docker/#pipelines-tests","title":"Pipelines tests","text":"<p>To performs test reconstructions, you have several possibilities:</p> <ul> <li>test the geometric pipeline:     <pre><code>./run.sh  --geom_pipeline_test\n</code></pre></li> <li>test the machine learning pipeline:     <pre><code>./run.sh  --ml_pipeline_test\n</code></pre></li> <li>test both pipelines:     <pre><code>./run.sh  --pipeline_test\n</code></pre></li> </ul> <p>Note</p> <p>This use test data &amp; test models (for ML) provided with <code>plant3dvision</code> in <code>plant3dvision/tests/testdata</code>.</p>"},{"location":"plant_imager/docker/plant-3d-vision_docker/#manually","title":"Manually","text":"<p>Assuming you have a valid ROMI database directory under <code>/data/ROMI/DB</code>, you can start the <code>roboticsmicrofarms/plant-3d-vision</code> docker image with:</p> <pre><code>export ROMI_DB=/data/ROMI/DB\ndocker run --runtime=nvidia --gpus all \\\n--env PYOPENCL_CTX='0' \\\n-v $ROMI_DB:/myapp/db \\\n-it roboticsmicrofarms/plant-3d-vision:$VTAG bash\n</code></pre> <p>This should start the built <code>roboticsmicrofarms/plant-3d-vision</code> docker image in interactive mode. The database location inside the docker container is <code>~/db</code>.</p> <p>Note that:</p> <ul> <li>you are using the docker image <code>roboticsmicrofarms/plant-3d-vision:$VTAG</code></li> <li>you mount the host directory <code>$ROMI_DB</code> \"inside\" the running container in the <code>~/db</code> directory</li> <li>you activate all GPUs within the container with <code>--gpus all</code></li> <li>declaring the environment variable <code>PYOPENCL_CTX='0'</code> select the first CUDA GPU capable</li> <li><code>-it</code> &amp; <code>bash</code> returns an interactive bash shell</li> </ul> <p>You may want to name the running container (with <code>--name &lt;my_name&gt;</code>) if you \"demonize\" it (with <code>-d</code>).</p>"},{"location":"plant_imager/docker/plant-3d-vision_docker/#executing-a-romi-task","title":"Executing a ROMI task","text":"<p>Once you are inside the running docker container, you may call the ROMI tasks.</p> <pre><code>romi_run_task AnglesAndInternodes db/&lt;my_scan_000&gt;/ --config plant3dvision/configs/original_pipe_0.toml\n</code></pre> <p>Note</p> <p>You may have to source the <code>.profile</code> file before calling <code>romi_run_task</code>.</p> <p>You can give a command to execute at container start-up using the <code>-c, --cmd</code> option. For example:</p> <pre><code>export ROMI_DB=/data/ROMI/DB\n./run.sh -p $ROMI_DB -u scanner -c \"source .profile &amp;&amp; romi_run_task AnglesAndInternodes db/&lt;my_scan_000&gt;/ --config plant3dvision/configs/original_pipe_0.toml\"\n</code></pre> <p>Important</p> <p><code>source .profile</code> is important to add <code>.local/bin/</code> to the <code>$PATH</code> environment variable. If you don't do this, you might not be able to access the <code>romi_run_task</code> binary from bash in the docker container.</p>"},{"location":"plant_imager/docker/plant3dexplorer_docker/","title":"Docker container for ROMI <code>plant 3d explorer</code>","text":"<p>The plant visualizer is a webapp that dialog with the database to display images &amp; some quantitative traits.</p> <p>It is based on Ubuntu 18.04.</p> <p>Note that we tag the different versions, the default is to use the latest, but you can also specify a specific version by changing the value of the environment variable <code>$VTAG</code>, e.g. <code>export VTAG=\"2.1\"</code>. Look here for a list of available tags: https://hub.docker.com/repository/docker/roboticsmicrofarms/plantviewer</p>"},{"location":"plant_imager/docker/plant3dexplorer_docker/#requirements","title":"Requirements","text":"<p>The docker image does not contain any plant scans and does not come with a working ROMI local database.</p> <p>To quickly create an example DB you can use:</p> <pre><code>wget https://db.romi-project.eu/models/test_db.tar.gz\ntar -xf test_db.tar.gz\n</code></pre> <p>This will create a <code>integration_tests</code> folder with a ready to use test database.</p> <p>To create a local ROMI database:</p> <ul> <li>python package install, look here.</li> <li><code>plantdb</code> docker image, look here.</li> <li>docker-compose YAML recipe (start both a <code>plantdb</code> &amp; a <code>plant-3d-explorer</code> docker image connected to the db), look here.</li> </ul>"},{"location":"plant_imager/docker/plant3dexplorer_docker/#use-pre-built-docker-image","title":"Use pre-built docker image","text":"<p>You can easily download and start the pre-built <code>plant-3d-explorer</code> docker image with:</p> <pre><code>docker run -p 3000:3000 roboticsmicrofarms/plant-3d-explorer:$VTAG\n</code></pre> <p>By default, the docker image will create a container pointing toward the official ROMI database https://db.romi-project.eu.</p> <p>To change that, e.g. to a local running database at '0.0.0.0', do 1:</p> <pre><code>docker run --env REACT_APP_API_URL='0.0.0.0' -p 3000:3000 roboticsmicrofarms/plant-3d-explorer:$VTAG\n</code></pre>"},{"location":"plant_imager/docker/plant3dexplorer_docker/#build-docker-image","title":"Build docker image","text":"<p>To build the image, from the <code>plant-3d-explorer</code> root directory, run:</p> <pre><code>export VTAG=\"latest\"\ndocker build -t roboticsmicrofarms/plant-3d-explorer:$VTAG .\n</code></pre> <p>To start the container using the built image:</p> <pre><code>docker run -p 3000:3000 roboticsmicrofarms/plant-3d-explorer:$VTAG\n</code></pre> <p>Once it's up and running, you should be able to access the viewer using a browser here: http://localhost:3000/</p> <p>Note</p> <p>If you omit the <code>-p 3000:3000</code> you can still access the interface using the docker ip, something like http://172.17.0.2:3000/</p> <p>Important</p> <p>Use <code>chrome</code> as <code>firefox</code> has some issues with the used JavaScript libraries!</p>"},{"location":"plant_imager/docker/plant3dexplorer_docker/#publish-docker-image","title":"Publish docker image","text":"<p>To push it on the <code>roboticsmicrofarms</code> docker hub:</p> <pre><code>docker push roboticsmicrofarms/plant-3d-explorer:$VTAG\n</code></pre> <p>This requires a valid account and token on dockerhub!</p> <ol> <li> <p>https://docs.docker.com/engine/reference/commandline/run/#set-environment-variables--e---env---env-file \u21a9</p> </li> </ol>"},{"location":"plant_imager/docker/plantdb_docker/","title":"Docker container for ROMI database","text":"<p>Important</p> <p>An existing local database directory is required, it will be mounted at container startup. To see how to create a local database directory, look here.</p>"},{"location":"plant_imager/docker/plantdb_docker/#use-pre-built-docker-image","title":"Use pre-built docker image","text":"<p>Assuming you have a valid ROMI database directory under <code>/data/ROMI/DB</code>, you can easily download and start the pre-built <code>plantdb</code> docker image with:</p> <pre><code>export ROMI_DB=/data/ROMI/DB\ndocker run -it -p 5000:5000 \\\n-v $ROMI_DB:/myapp/db \\\nroboticsmicrofarms/plantdb:latest\n</code></pre> <p>You should be able to access it here: http://localhost:5000/</p> <p>Note</p> <p><code>-v $ROMI_DB:/myapp/db</code> performs a bind mount to enable access to the local database by the docker image. See the official documentation.</p>"},{"location":"plant_imager/docker/plantdb_docker/#build-docker-image","title":"Build docker image","text":"<p>We provide a convenience bash script to ease the build of <code>plantdb</code> docker image. You can choose to use this script OR to \"manually\" call the <code>docker build</code> command.</p>"},{"location":"plant_imager/docker/plantdb_docker/#provided-convenience-buildsh-script","title":"Provided convenience <code>build.sh</code> script","text":"<p>To build the image with the provided build script, from the <code>plantdb/docker</code> directory:</p> <pre><code>./build.sh\n</code></pre> <p>You can also pass some options, use <code>./build.sh -h</code> to get more details about usage, options and default values.</p>"},{"location":"plant_imager/docker/plantdb_docker/#manually-call-the-docker-build-command","title":"Manually call the <code>docker build</code> command","text":"<p>To build the image, from the <code>plantdb</code> root directory:</p> <pre><code>export VTAG=\"latest\"\ndocker build -t roboticsmicrofarms/plantdb:$VTAG .\n</code></pre> <p>You can use the following optional arguments:</p> <ul> <li><code>--build-arg USER_NAME=&lt;user&gt;</code>: change the default user in container;</li> <li><code>--build-arg PLANTDB_BRANCH=&lt;git_branch&gt;</code>: change the cloned git branch from <code>plantdb</code>.</li> </ul>"},{"location":"plant_imager/docker/plantdb_docker/#publish-docker-image","title":"Publish docker image","text":"<p>Push it on docker hub:</p> <pre><code>docker push roboticsmicrofarms/plantdb:$VTAG\n</code></pre> <p>This requires a valid account &amp; token on dockerhub!</p>"},{"location":"plant_imager/docker/plantdb_docker/#usage","title":"Usage","text":""},{"location":"plant_imager/docker/plantdb_docker/#requirements","title":"Requirements","text":"<p>To run it, you need to have a valid local ROMI database, look here for instructions and here for an example database.</p>"},{"location":"plant_imager/docker/plantdb_docker/#starting-the-plantdb-docker-image","title":"Starting the <code>plantdb</code> docker image","text":""},{"location":"plant_imager/docker/plantdb_docker/#provided-runsh-script","title":"Provided <code>run.sh</code> script","text":"<p>To start the container with the provided run script in <code>plantdb/docker</code>, use:</p> <pre><code>./run.sh\n</code></pre> <p>You can also pass some options, use <code>./run.sh -h</code> to get more details about usage and options.</p>"},{"location":"plant_imager/docker/plantdb_docker/#manually","title":"Manually","text":"<p>Assuming you extracted it in your home folder (<code>/home/$USER/integration_tests</code>), you can start the <code>plantdb</code> docker image with:</p> <pre><code>docker run -it -p 5000:5000 -v /home/$USER/integration_tests:/myapp/db plantdb:$VTAG\n</code></pre> <p>In both cases, you should see something like:</p> <pre><code>n scans = 2\n * Serving Flask app \"romi_scanner_rest_api\" (lazy loading)\n * Environment: production\n   WARNING: This is a development server. Do not use it in a production deployment.\n   Use a production WSGI server instead.\n * Debug mode: off\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n</code></pre> <p>Tip</p> <p><code>-v /home/$USER/integration_tests:/myapp/db</code> performs a bind mount to enable access to the local database by the docker image. See the official Docker documentation.</p>"},{"location":"plant_imager/docker/plantdb_docker/#accessing-the-rest-api","title":"Accessing the REST API","text":"<p>Once it's up, you should be able to access the REST API here: http://localhost:5000/</p> <p>To access the REST API, open your favorite browser and use URLs to access:</p> <ul> <li>the list of all scans: http://0.0.0.0:5000/scans</li> <li>the '2018-12-17_17-05-35' dataset: http://0.0.0.0:5000/scans/2018-12-17_17-05-35</li> </ul> <p>You should see JSON formatted text.</p>"},{"location":"plant_imager/docker/plantimager_docker/","title":"Docker container for ROMI plant-imager","text":""},{"location":"plant_imager/docker/virtualplantimager_docker/","title":"Docker container for ROMI virtual plant imager","text":""},{"location":"plant_imager/docker/virtualplantimager_docker/#objective","title":"Objective","text":"<p>The following sections aim to show you how to build the docker image and run the corresponding container of the Virtual Plant Imager</p>"},{"location":"plant_imager/docker/virtualplantimager_docker/#prerequisites","title":"Prerequisites","text":"<p>In addition to having docker installed in your system, you must also install the nvidia gpu drivers, nvidia-docker (v2.0) and nvidia-container-toolkit.</p> <p>This docker image has been tested successfully on: <code>docker --version=19.03.6 | nvidia driver version=450.102.04 | CUDA version=11.0</code></p>"},{"location":"plant_imager/docker/virtualplantimager_docker/#building-the-docker-image","title":"Building the Docker image","text":"<p>In this repository, you will find a script <code>build.sh</code> in the <code>docker</code> directory.</p> <p><pre><code>    git clone https://github.com/romi/plant-imager.git\n    cd plant-imager/\n    cd docker/\n    ./build.sh\n</code></pre> This will create by default a docker image <code>plantimager:latest</code>. Inside the docker image, a user is created and named as the one currently used by your system. If you want more build options (specific branches, tags...etc), type <code>./build.sh --help</code>.</p>"},{"location":"plant_imager/docker/virtualplantimager_docker/#running-the-docker-container","title":"Running the docker container","text":"<p>In the docker directory, you will find also a script named <code>run.sh</code>.</p> <p>To show more options, type <code>./run.sh --help</code></p>"},{"location":"plant_imager/docker/virtualplantimager_docker/#pre-requisites","title":"Pre-requisites","text":"<p>For clarity let us defines some variables here:</p> <ul> <li><code>ROMI_DB</code>: the ROMI database root directory (should contain a <code>plantdb</code> file);</li> <li><code>ROMI_CFG</code>: the directory containing the ROMI configurations (TOML files);</li> </ul> <p>To defines these variable, in a terminal:</p> <pre><code>export ROMI_DB=/data/ROMI/DB\nexport ROMI_CFG=/data/ROMI/configs\n</code></pre>"},{"location":"plant_imager/docker/virtualplantimager_docker/#get-an-example-archive-with-arabidopsis-model","title":"Get an example archive with arabidopsis model","text":"<p>Download &amp; extract the example archive at the root directory of the romi database:</p> <pre><code>wget --progress=bar -P $ROMI_DB https://media.romi-project.eu/data/vscan_data.tar.xz\ntar -C $ROMI_DB/ -xvJf $ROMI_DB/vscan_data.tar.xz\n</code></pre>"},{"location":"plant_imager/docker/virtualplantimager_docker/#toml-config","title":"TOML config","text":"<p>Use the following configuration, replacing <code>&lt;my_vscan&gt;</code> with the name of the virtual scan dataset to create, e.g. <code>vscan_007</code>.</p> <pre><code>[ObjFileset]\nscan_id = \"&lt;my_vscan&gt;\"\n\n[HdriFileset]\nscan_id = \"vscan_data\"\n\n[LpyFileset]\nscan_id = \"vscan_data\"\n\n[PaletteFileset]\nscan_id = \"vscan_data\"\n\n[ScanPath]\nclass_name = \"Circle\"\n\n[ScanPath.kwargs]\ncenter_x = -2\ncenter_y = 3\nz = 34.17519302880196\ntilt = 8\nradius = 30\nn_points = 72\n\n[VirtualScan]\nobj_fileset = \"ObjFileset\"\nuse_palette = true\nuse_hdri = true\nload_scene = false\nscene_file_id = \"pot\"\nrender_ground_truth = true\n\n[VirtualScan.scanner]\nwidth = 896\nheight = 896\nfocal = 24\nflash = true\nadd_leaf_displacement = true\n\n[Voxels]\ntype = \"averaging\"\nvoxel_size = 0.05\n</code></pre>"},{"location":"plant_imager/docker/virtualplantimager_docker/#virtual-scan-of-a-model-plant","title":"Virtual scan of a model plant","text":""},{"location":"plant_imager/docker/virtualplantimager_docker/#start-the-docker-container","title":"Start the docker container","text":"<p>Use the <code>roboticsmicrofarms/plantimager</code> docker image:</p> <pre><code>export ROMI_DB=/data/ROMI/DB\nexport ROMI_CFG=/data/ROMI/configs\n\ndocker run --runtime=nvidia --gpus all \\\n-v $ROMI_DB:/myapp/db \\\n-v $ROMI_CFG:/myapp/configs \\\n-it roboticsmicrofarms/plantimager:latest bash\n</code></pre>"},{"location":"plant_imager/docker/virtualplantimager_docker/#initialize-a-scan-dataset","title":"Initialize a scan dataset","text":"<p>Use the <code>romi_import_folder</code> tool to import the required <code>data</code> into a new scan dataset, e.g. <code>vscan_007</code>:</p> <pre><code>romi_import_folder ~/db/vscan_data/data/ ~/db/vscan_007/ --metadata ~/db/vscan_data/files.json\n</code></pre>"},{"location":"plant_imager/docker/virtualplantimager_docker/#start-a-virtualscan-romi-task","title":"Start a <code>VirtualScan</code> romi task","text":"<pre><code>cd plantimager/bin\nromi_run_task VirtualScan ~/db/vscan_007 --config ~/plantimager/config/vscan_obj.toml\n</code></pre>"},{"location":"plant_imager/explanations/","title":"General explanations &amp; concepts","text":"<p>Hereafter, we introduce the general philosophy of the project, provide some high level explanations and give key concepts to understand the work we have done around the Plant Imager.</p>"},{"location":"plant_imager/explanations/#aim-of-the-plant-imager-project","title":"Aim of the Plant Imager project","text":"<p>Manual phenotyping, apart from bringing you closer to nature, is quite fastidious and time-consuming, and we should be better off leaving this kind of tasks to machines designed to accomplish this task.</p> <p>The endgame of the Plant Imager project is thus to be able to serve as a phenotyping platform, not only for Arabidopsis thaliana, but for -potentially- any plant that can be grown in lab conditions (meaning not trees for example).</p>"},{"location":"plant_imager/explanations/#general-idea","title":"General idea","text":"<p>We sought at solving the automatic phenotyping problem as follows:</p> <ol> <li>take pictures of a plant with a robot</li> <li>use photogrammetry algorithms to reconstruct a 3D model of the plant</li> <li>use algorithms to extract the phenotype of the plant </li> </ol>"},{"location":"plant_imager/explanations/#research-oriented-user-story","title":"Research oriented user story","text":"<ol> <li>The user put a plant inside the Plant Imager and perform an acquisition, that return a set of RGB images.</li> <li>These images are automatically uploaded to a database.</li> <li>The user use a pre-defined workflow to reconstruct the plant and quantify its traits.</li> <li>The user can access the acquisitions, reconstructions &amp; quantitative data by connecting to a visualization server.</li> </ol> <p>Ideally:</p> <ol> <li>There is a GUI to guide the user for the acquisition process and to gather the maximum amount of metadata like authorship, plant information and so on.</li> <li>The database is accessible remotely.</li> <li>The reconstruction might be run on a distant server, again via a GUI.</li> <li>The visualization server is accessible remotely.</li> </ol>"},{"location":"plant_imager/explanations/#general-philosophy","title":"General philosophy","text":"<p>Our state of mind is that of Open Science, meaning to produce technology and knowledge accessible to and reproducible by everyone.</p> <p>To that respect, we imposed ourselves the following \"constrains\":</p> <ul> <li>use an open-source programming language, libraries and algorithms (or at least free of charges)</li> <li>use affordable and \"off the shelves\" hardware and equipments</li> <li>publish and promote our work, so it may be reused by the community</li> </ul>"},{"location":"plant_imager/explanations/#how-it-worked-so-far","title":"How it worked so far...","text":"<p>As for most technological development strategy, we decided to \"start simple\" and try to \"complexify over time\".</p> <p>We thus chose a simple plant model to reconstruct, like Arabidopsis thaliana, as it has small leaves that are mostly located at the base of the plant. This was to avoid the \"occlusion problem\", where large leaves would occlude the rest from the camera.</p> <p>Also, we started with one RGB camera, even tough we tested several (including a depth-sensing camera), and a simple circular path around the plant to take pictures of it.</p> <p>We have hand-picked and aggregated state-of-the-art technologies that met our requirements. Some notable examples are:</p> <ul> <li>Raspberry Pi and Arduino boards to build our robot</li> <li>the <code>luigi</code> library as a workflow manager</li> <li><code>Colmap</code> as a Structure from Motion solver</li> </ul> <p>We developed the missing pieces of software and designed a few algorithm along the way.</p> <p>We now have a modular and flexible software design that you may learn about in the next section.</p>"},{"location":"plant_imager/explanations/general_design/","title":"Overview of the modules interactions","text":"<p>The following figure shows a use case of the ROMI modules, and the way they interact, to design an efficient plant phenotyping platform used in research.</p> <p></p>"},{"location":"plant_imager/explanations/general_design/#plantdb","title":"PlantDB","text":"<p>It implements our database system and is totally independent of the rest since it could be uses in other parts of the ROMI project (Rover, Cable bot, ...) through the abstract class <code>DB</code> or even the local database class <code>FSDB</code>.</p>"},{"location":"plant_imager/explanations/general_design/#plant-imager","title":"Plant Imager","text":"<p>It requires a physical connection to the hardware (<code>pyserial</code>) to control. It also needs an active ROMI database (PlantDB) to export acquired datasets (plant images).</p>"},{"location":"plant_imager/explanations/general_design/#virtual-plant-imager","title":"Virtual Plant Imager","text":"<p>It requires a connection to an active ROMI database (PlantDB) to export generated datasets (virtual plant images). In case of machine learning methods, the database would also provide training datasets and trained models.</p>"},{"location":"plant_imager/explanations/general_design/#plant-3d-vision","title":"Plant 3D Vision","text":"<p>It requires connection to an active ROMI database (PlantDB) to import the dataset to process and export the results. Two plant reconstruction approaches are available:</p> <ol> <li>Geometry based: try to infer the plant's geometry using structure from motion algorithms and space carving to first reconstruct a point cloud.</li> <li>Machine learning based: try to infer the plant's geometry using semantic (organ) segmentation of pictures and space carving to first reconstruct a labelled point cloud.</li> </ol> <p>Then meshing and skeletonization finally enables to extract the plant's phyllotaxis.</p>"},{"location":"plant_imager/explanations/general_design/#plant-3d-explorer","title":"Plant 3D Explorer","text":"<p>It requires an active ROMI database (PlantDB) with datasets to browse and represent.</p>"},{"location":"plant_imager/explanations/geometric_workflow/","title":"The geometry based workflow","text":"<p>The first workflow we introduce is the one we called geometry based workflow as it transform the obtained point cloud, after reconstruction, to extract a semantic skeleton (called tree graph) and use it to compute the fruit orientation in space. From there we can fulfill our goal: to estimate the fruits' successive angles and internodes lengths.</p>"},{"location":"plant_imager/explanations/geometric_workflow/#reconstruction-part","title":"Reconstruction part","text":""},{"location":"plant_imager/explanations/geometric_workflow/#aim","title":"Aim","text":"<p>The aim of the reconstruction part is to reconstruct a 3D model of the plant, here a point cloud, from a set of RGB images.</p>"},{"location":"plant_imager/explanations/geometric_workflow/#general-idea","title":"General idea","text":"<p>We sought at combining structure from motion with space carving to obtain a quick and reliable 3D reconstruction of the plant.</p> <p>The rational is mainly in two part:</p> <ol> <li>We have to use structure from motion to get accurate estimate of the true camera positions because of the uncertainties from the motors (see open-loop-design).</li> <li>We use space carving, instead of multiview stereo (as in the second part of Colmap reconstruction pipeline), because we want a fast reconstruction of a small portion of the scene.</li> </ol> <p>The second step is fast because we use a simple linear filter to detect the plant position, and we select a small region where there is only the plant to reconstruct.</p> <p>However, without a precise and repeatable acquisition procedure with the Plant Imager, you might have to often change the bounding-box manually, which breaks the full automation of the reconstruction procedure.</p>"},{"location":"plant_imager/explanations/geometric_workflow/#overview","title":"Overview","text":"Reconstruction part of the geometry based workflow as defined in geom_pipe_real.toml configuration &amp; default parameters."},{"location":"plant_imager/explanations/geometric_workflow/#details","title":"Details","text":"<ol> <li>We start with the <code>Colmap</code> task to estimate both intrinsic and extrinsic parameters using a structure from motion algorithm.</li> <li>The camera intrinsics are used by the <code>Undistorted</code> task with a <code>SIMPLE_RADIAL</code> model to fix the original RGB images.</li> <li>Then the <code>Masks</code> task detect the plant position in each image and create a binary mask for each.</li> <li>This is later used by the <code>Voxels</code> task, in combination with the camera extrinsics (also called camera poses), to perform the space carving of a 3D volume. This reconstructs the volume occupied by the plant in the selected portion of the scene.</li> <li>Finally, this is turned into a point cloud describing the envelope of the reconstructed plant structure by the <code>PointCloud</code> task.</li> </ol>"},{"location":"plant_imager/explanations/geometric_workflow/#quantification-part","title":"Quantification part","text":""},{"location":"plant_imager/explanations/geometric_workflow/#aim_1","title":"Aim","text":"<p>The aim of the quantification part is estimate the fruits' successive angles and internodes lengths, from the 3D point cloud.</p>"},{"location":"plant_imager/explanations/geometric_workflow/#general-idea_1","title":"General idea","text":"<p>We sought at extracting the skeleton of the plant to be able to estimate the organs direction, here the fruits or leaves, thanks to a single trajectory of points describing it.</p>"},{"location":"plant_imager/explanations/geometric_workflow/#overview_1","title":"Overview","text":"Quantification part of the geometry based workflow as defined in geom_pipe_real.toml configuration &amp; default parameters."},{"location":"plant_imager/explanations/geometric_workflow/#details_1","title":"Details","text":"<ol> <li>We start by transforming the point cloud into a triangular mesh with the <code>TriangleMesh</code> task.</li> <li>From there we can extract the plant skeleton thanks to the <code>CurveSkeleton</code> task.</li> <li>We then augment this structure with \"biological meaning\" to this skeleton by defining the root of the tree, labelling the points as fruits or main stem using the <code>TreeGraph</code> task.</li> <li>Finally, the <code>AnglesAndInternodes</code> task will compute the fruits direction and branching points, allowing us to estimate the successive angles and internode lengths between the fruits.</li> </ol>"},{"location":"plant_imager/explanations/ml_workflow/","title":"The machine-learning based workflow","text":"<p>The second workflow we introduce is the one we called machine-learning based workflow as we make use of a trained CNN to predict organ types and locations on our original RGB images. This information is then projected on the point cloud, so we can compute the fruit orientation in space. From there we can fulfill our goal: to estimate the fruits' successive angles and internodes lengths.</p>"},{"location":"plant_imager/explanations/ml_workflow/#reconstruction-part","title":"Reconstruction part","text":""},{"location":"plant_imager/explanations/ml_workflow/#aim","title":"Aim","text":"<p>The aim of the reconstruction part is to reconstruct a 3D model of the plant, here a point cloud, from a set of RGB images.</p>"},{"location":"plant_imager/explanations/ml_workflow/#general-idea","title":"General idea","text":"<p>As for the geometry based workflow, we sought at combining structure from motion with space carving to obtain a quick and reliable 3D reconstruction of the plant.</p> <p>However, we added an extra layer of complexity with the prediction of organs types and locations instead of the simple &amp; fast linear filter. This in turn improve the automation level of the reconstruction procedure as the plant is automatically identified in the scene. There is thus no need for manual definition of the scene bounding-box.</p>"},{"location":"plant_imager/explanations/ml_workflow/#overview","title":"Overview","text":"Reconstruction part of the machine-learning based workflow as defined in ml_pipe_real.toml configuration &amp; default parameters."},{"location":"plant_imager/explanations/ml_workflow/#details","title":"Details","text":"<ol> <li>We start with the <code>Colmap</code> task to estimate both intrinsic and extrinsic parameters using a structure from motion algorithm.</li> <li>The camera intrinsics are used by the <code>Undistorted</code> task with a <code>SIMPLE_RADIAL</code> model to fix the original RGB images.</li> <li>Then the <code>Segmentation2D</code> task performs semantic labelling of the plant organs in each image and create a binary mask for each image and organ type.</li> <li>This is later used by the <code>Voxels</code> task, in combination with the camera extrinsics (also called camera poses), to perform the space carving of a 3D volume. This reconstructs the volume occupied by the plant in the selected portion of the scene.</li> <li>Finally, this is turned into a point cloud describing the envelope of the reconstructed plant structure by the <code>PointCloud</code> task.</li> </ol>"},{"location":"plant_imager/explanations/ml_workflow/#quantification-part","title":"Quantification part","text":""},{"location":"plant_imager/explanations/ml_workflow/#aim_1","title":"Aim","text":"<p>The aim of the quantification part is estimate the fruits' successive angles and internodes lengths, from the 3D point cloud.</p>"},{"location":"plant_imager/explanations/ml_workflow/#general-idea_1","title":"General idea","text":"<p>We sought at projecting the CNN predictions about organ types on the 3D point cloud to know the exact position of each fruit. From there we could estimate the fruit directions, thanks to oriented bounding-box for fruits directions and the mean skeleton for the branching point to the main stem.  </p>"},{"location":"plant_imager/explanations/ml_workflow/#overview_1","title":"Overview","text":"Quantification part of the machine-learning based workflow as defined in ml_pipe_real.toml configuration &amp; default parameters."},{"location":"plant_imager/explanations/ml_workflow/#details_1","title":"Details","text":"<ol> <li>We start by projecting the CNN predictions about organ types and locations to the 3D point cloud with the <code>SegmentedPointcloud</code> task.</li> <li>From there individualize each organ thanks to the <code>OrganSegmentation</code> task.</li> <li>Finally, the <code>AnglesAndInternodes</code> task will compute the fruits direction and branching points, allowing us to estimate the successive angles and internode lengths between the fruits.</li> </ol> <p>The <code>ClusteredMesh</code> task is here to generate a labelled triangular mesh that can be visualized, but is not necessary to the previous quantification workflow. Note that this task could be used in place of the <code>OrganSegmentation</code> task, and it could be later used by <code>AnglesAndInternodes</code>.</p>"},{"location":"plant_imager/explanations/strengths_limits/","title":"Known strengths and limitations","text":"<p>The way we designed the Plant Imager and the software has strengths but also some limitations. We try to cover those we are aware of in the following section.</p>"},{"location":"plant_imager/explanations/strengths_limits/#open-loop-design","title":"Open loop design","text":""},{"location":"plant_imager/explanations/strengths_limits/#the-problem","title":"The problem","text":"<p>The motors we use function in an open loop manner, that is: we send and series of instructions to the motors, telling them how many steps we want and in which direction to performs them... and we hope for the best.</p> <p>Indeed, contrary to a feedback loop design, there is no way of knowing if those instruction were followed accurately. This open loop design might seem unfortunate or inadequate but this is actually a lot cheaper and widely used in robotics.</p> <p>As there is a user standing next to the Plant Imager you might expect her/him to notice any mistake made by the robot. This is simply not true and not a good approach to tackling the problem.</p>"},{"location":"plant_imager/explanations/strengths_limits/#the-visual-proof","title":"The visual proof","text":"<p>I have performed a series of 5 repetitions of the very same acquisitions, without changing a thing between each of them. To my knowledge, everything went fine, but this is what I got when I cycled through the pictures taken at the same position (the first one):</p> <p></p> <p>As you can see, most of the images looks exactly the same, and if it was not for the frame indicator on the top right we would not notice the change. However, we can clearly see that one frame, <code>Sangoku_40_1</code>, has a small offset compared to the others.</p>"},{"location":"plant_imager/explanations/strengths_limits/#the-solution","title":"The solution","text":"<p>As the accuracy of the camera positions is not perfect, we have to resort to Structure from Motion algorithm to compensate for those imperfections.</p>"},{"location":"plant_imager/explanations/strengths_limits/#intrinsic-calibration-procedure","title":"Intrinsic calibration procedure","text":""},{"location":"plant_imager/explanations/strengths_limits/#the-problem_1","title":"The problem","text":"<p>Every camera need a lens to capture the luminous signals. However, the cheaper the camera, the cheaper the lens and the more likely it will induce deformations to the image due to imperfections.</p>"},{"location":"plant_imager/explanations/strengths_limits/#the-solution_1","title":"The solution","text":"<p>We developed an intrinsic calibration procedure to compensate for lens aberrations. We use OpenCV and a ChArUco board to accurately estimate and correct them.</p>"},{"location":"plant_imager/explanations/task_masks/","title":"Masks","text":"<p>Here we show the effects of the <code>type</code> of filter to use in the <code>Mask</code> task.</p>"},{"location":"plant_imager/explanations/task_masks/#method-linear","title":"Method <code>linear</code>","text":""},{"location":"plant_imager/explanations/task_masks/#theory","title":"Theory","text":"<p>The equation for the linear filter method (LF) is as follows: LF= c_r*R + c_g*G + c_b*B, with c_r, c_g &amp; c_b a coefficient in [0., 1.] to apply to the corresponding channel.</p> <p>Then we apply a high-pass threshold to binarize the image and create a mask locating the plant in the image. Finally, we use a binary dilation to enlarge the masked area. </p>"},{"location":"plant_imager/explanations/task_masks/#example-code","title":"Example &amp; code","text":"<p>Here is an example of what the <code>linear</code> filter does when used in combination with a binarization step and a dilation step, as in the <code>Masks</code> task:</p> <p></p> <p>The source code for this figure is as follows:</p> <pre><code>import matplotlib.pyplot as plt\nfrom imageio.v3 import imread\nfrom plant3dvision import test_db_path\nfrom plant3dvision.proc2d import linear, dilation\npath = test_db_path()\nimg = imread(path.joinpath('real_plant/images/00000_rgb.jpg'))\nrgb_coefs = [0.1, 1., 0.1]\nfilter_img = linear(img, rgb_coefs)  # apply `linear` filter\nthreshold = 0.3\nmask = filter_img &gt; threshold  # convert to binary mask using threshold\nradius = 2\ndilated_mask = dilation(mask, radius)  # apply a dilation to binary mask\nfig, axes = plt.subplots(2, 2, figsize=(8, 7))\naxes[0, 0].imshow(img)\naxes[0, 0].set_title(\"Original image\")\naxes[0, 1].imshow(filter_img, cmap='gray')\naxes[0, 1].set_title(\"Mask image (linear filter)\")\naxes[1, 0].imshow(mask, cmap='gray')\naxes[1, 0].set_title(f\"Binary mask image (threshold={threshold})\")\naxes[1, 1].imshow(dilated_mask, cmap='gray')\naxes[1, 1].set_title(f\"Dilated binary mask image (radius={radius})\")\n[ax.set_axis_off() for ax in axes.flatten()]\nplt.tight_layout()\n</code></pre> <p>Important</p> <p>Pay attention to the values used for <code>rgb_coefs</code>, <code>threshold</code> and <code>dilation</code>.</p>"},{"location":"plant_imager/explanations/task_masks/#method-excess_green","title":"Method <code>excess_green</code>","text":""},{"location":"plant_imager/explanations/task_masks/#theory_1","title":"Theory","text":"<p>This filter was published in: Woebbecke, D. M., Meyer, G. E., Von Bargen, K., &amp; Mortensen, D. A. (1995). Color indices for weed identification under various soil, residue, and lighting conditions. Transactions of the ASAE, 38(1), 259-269.</p> <p>The equation for the excess green method (EG) is as follows: EG= 2*g-r-b, with:</p> <ul> <li>r = R/(R+G+B)</li> <li>g = G/(R+G+B)</li> <li>b = B/(R+G+B)</li> <li>Then we apply a high-pass threshold to binarize the image and create a mask locating the plant in the image. Finally, we use a binary dilation to enlarge the masked area. </li> </ul>"},{"location":"plant_imager/explanations/task_masks/#example-code_1","title":"Example &amp; code","text":"<p>Here is an example of what the <code>excess_green</code> filter does when used in combination with a binarization step and a dilation step, as in the <code>Masks</code> task:</p> <p></p> <p>The source code for this figure is as follows:</p> <pre><code>import matplotlib.pyplot as plt\nfrom imageio.v3 import imread\nfrom plant3dvision import test_db_path\nfrom plant3dvision.proc2d import excess_green, dilation\npath = test_db_path()\nimg = imread(path.joinpath('real_plant/images/00000_rgb.jpg'))\nfilter_img = excess_green(img)  # apply `excess_green` filter\nthreshold = 0.3\nmask = filter_img &gt; threshold  # convert to binary mask using threshold\nradius = 2\ndilated_mask = dilation(mask, radius)  # apply a dilation to binary mask\nfig, axes = plt.subplots(2, 2, figsize=(8, 7))\naxes[0, 0].imshow(img)\naxes[0, 0].set_title(\"Original image\")\naxes[0, 1].imshow(filter_img, cmap='gray')\naxes[0, 1].set_title(\"Mask image (excess green filter)\")\naxes[1, 0].imshow(mask, cmap='gray')\naxes[1, 0].set_title(f\"Binary mask image (threshold={threshold})\")\naxes[1, 1].imshow(dilated_mask, cmap='gray')\naxes[1, 1].set_title(f\"Dilated binary mask image (radius={radius})\")\n[ax.set_axis_off() for ax in axes.flatten()]\nplt.tight_layout()\n</code></pre> <p>Important</p> <p>Pay attention to the values used for <code>threshold</code> and <code>dilation</code>.</p>"},{"location":"plant_imager/explanations/task_segmentation2d/","title":"Segmentation of images","text":"<p>The segmentation of an image consists in assigning a label to each of its pixels. For the 3d reconstruction of a plant, we need at least the segmentation of the images into 2 classes: plant and background. For a reconstruction with semantic labeling of the point cloud, we will need a semantic segmentation of the images giving one label for each organ type (e.g. {leaf, stem,pedicel, flower, fruit}). The figure below shows the binary and multi-class segmentations for a virtual plant.</p> Example image of virtual arabidopsis (left) with binary (middle) and multi-class segmentation (right)."},{"location":"plant_imager/explanations/task_segmentation2d/#binary-segmentation","title":"Binary segmentation","text":"<p>The binary segmentation of an image into plant and background is performed with the following command:</p> <pre><code>romi_run_task Masks scan_id --config myconfig.toml </code></pre> <p>with upstream task being ImagesFilesetExists when processing the raw RGB images or Undistorded when processing images corrected using the intrinsic parameters of the camera. The task takes this set of images as an input and produce one binary mask for each image.</p> <p>There are 2 methods available to compute indices for binary segmentation: Excess Green Index and Linear SVM. For each method, we provide an example configuration file in the Index computation section.</p>"},{"location":"plant_imager/explanations/task_segmentation2d/#index-computation","title":"Index computation","text":""},{"location":"plant_imager/explanations/task_segmentation2d/#linear-support-vector-machine-svm","title":"Linear support vector machine (SVM)","text":"<p>A linear combination of R, G and B is used to compute the index for pixel (i,j):</p>  S_{ij}=w_0 R_{ij} + w_1 G_{ij} +w_2 B_{ij}  <p>where w is the parameters vector specified in the configuration file. A simple vector, like w=(0,1,0) may be used for example.</p> <p>Alternatively, you can train an SVM to learn those weights, and the threshold to be provided in the configuration file. For this, we consider you have a sample image and a ground truth binary mask. A ground truth may be produced using a manual annotation tool like LabelMe.</p> <p>Using for example a list of N randomly selected pixels as X_{train} (array of size [N,3]) and their corresponding labels as Y_{train} (array of size N), a  linear SVM is trained using</p> <pre><code>from sklearn import svm\n\nX_train, Y_train = ...\nclf = svm.SVC(kernel='linear')\nclf.fit(X_train, y_train)\n</code></pre> <p>the weights can then be retrieved as clf.coef_ and the threshold as -clf.intercept_</p>"},{"location":"plant_imager/explanations/task_segmentation2d/#configuration-file","title":"Configuration file","text":"<pre><code>[Masks]\nupstream_task = \"ImagesFilesetExists\" # other option \"Undistorted\"\ntype = \"linear\"\nparameters = \"[0,1,0]\"\nthreshold = 0.5\n</code></pre>"},{"location":"plant_imager/explanations/task_segmentation2d/#excess-green","title":"Excess green","text":"<p>This segmentation method is assuming the plant is green and the background is not. It has no parameter but it may be less robust than the linear SVM.</p> <p>We compute the normalized RGB values x \\in {r,g,b} for each pixel (i,j):</p>  x_{ij} = \\frac{X_{ij}}{R_{ij}+G_{ij}+B_{ij}}  <p>where X \\in {R, G, B} is the red, green or blue image</p> <p>Then, the green excess index is computed as:</p>  \\operatorname{ExG}=2g-r-b"},{"location":"plant_imager/explanations/task_segmentation2d/#configuration-file_1","title":"Configuration file","text":"<pre><code>[Masks]\nupstream_task = \"ImagesFilesetExists\" # other option \"Undistorted\"\ntype = \"excess_green\"\nthreshold = 0.2\n</code></pre>"},{"location":"plant_imager/explanations/task_segmentation2d/#multi-class-segmentation","title":"Multi-class segmentation","text":"<p>The Segmentation2D task performs the semantic segmentation of images using a deep neural network (DNN). The command to run this task is:</p> <pre><code>romi_run_task Segmentation2D scan_id my_config.toml\n</code></pre> <p>This will produce a series of binary masks, one for each class on which the network was trained.</p> Generic encoder/decoder architecture for semantic segmentation (U-net). <p>The architecture of the network is inspired from the U-net 1, with a ResNet encoder 2. It consists in encoding and decoding pathways with skip connections between the 2. Along the encoding pathways, there is a sequence of convolutions and the image signal is upsampled along the decoding pathway.</p> <p>The network is trained for segmenting images of a size (S_x,S_y) which is not necessarily the image size of the acquired images. Those parameters Sx and Sy should be provided in the configuration file. The images will be cropped to (S_x,S_y) before being fed to the DNN and it is then resized to the original size as an output of the task.</p>"},{"location":"plant_imager/explanations/task_segmentation2d/#configuration-file_2","title":"Configuration File","text":"<pre><code>[Segmentation2D]\nmodel_id = \"Resnetdataset_gl_png_896_896_epoch50\"  # no default value\nSx = 896 Sy = 896\nthreshold = 0.01\n</code></pre>"},{"location":"plant_imager/explanations/task_segmentation2d/#dnn-model","title":"DNN model","text":"<p>The neural architecture weights are obtained through training on an annotated dataset (see How to train a DNN for semantic segmentation). Those weights should be stored in the database (at <code>&lt;database&gt;/models/models</code>) and the name of the weights file should be provided as the model_id parameter in the configuration. You can use our model trained on virtual arabidopsis here</p>"},{"location":"plant_imager/explanations/task_segmentation2d/#binarization","title":"Binarization","text":"<p>A binary mask m is produced from the index or from the output of the DNN, I, by applying a threshold \\theta on I for each pixel (i,j):</p> \\begin{equation} m_{ij} = \\begin{cases} 255 &amp; \\text{if $I_{ij}&gt;\\theta$}\\\\ 0 &amp; \\text{otherwise} \\end{cases}        \\end{equation} <p>This threshold may be chosen empirically, or it may be learnt from annotated data (see linear SVM section).</p>"},{"location":"plant_imager/explanations/task_segmentation2d/#dilation","title":"Dilation","text":"<p>If the integer dilation parameter is non-zero a morphological dilation is applied to the image using the function binary_dilation from the skimage.morphology module.</p> <p>The dilation parameter sets the number of times binary_dilation is iteratively applied. For a faithful reconstruction this parameter should be set to 0 but in practice you may want to have a coarser point cloud. This is true when your segmentation is not perfect, dilation will fill the holes or when the reconstructed mesh is broken because the point-cloud is too thin.</p>"},{"location":"plant_imager/explanations/task_segmentation2d/#working-with-data-from-the-virtual-scanner","title":"Working with data from the virtual scanner","text":"<p>When working with data generated with the virtual scanner, the images folder contains multiple channels corresponding to the various class for which images were generated (stem, flower, fruit, leaf, pedicel). You have to select the rgb channel using the query parameter.</p>"},{"location":"plant_imager/explanations/task_segmentation2d/#configuration-file_3","title":"Configuration File","text":"<pre><code>[Masks]\ntype = \"excess_green\"\nthreshold = 0.2\nquery = \"{\\\"channel\\\":\\\"rgb\\\"}\"\n</code></pre> <ol> <li> <p>Ronneberger, O., Fischer, P., &amp; Brox, T. (2015, October). U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention (pp. 234-241). Springer, Cham.\u00a0\u21a9</p> </li> <li> <p>Zhang, Z., Liu, Q., &amp; Wang, Y. (2018). Road extraction by deep residual u-net. IEEE Geoscience and Remote Sensing Letters, 15(5), 749-753.\u00a0\u21a9</p> </li> </ol>"},{"location":"plant_imager/how_to/","title":"How To","text":"<p>To get a deeper understanding of what each task does, we made some notebooks.</p> <ul> <li>Colmap - From RGB images to camera poses</li> <li>Undistorted - Undistort RGB images using camera model</li> <li>Masks - From RGB images to masks</li> <li>Voxels - From mask images and camera coordinates to volume</li> <li>PointCloud - From carved volume to point cloud</li> <li>TriangleMesh - From point cloud to triangular mesh</li> <li>CurveSkeleton - From triangular mesh to curved skeleton</li> <li>TreeGraph - From curved skeleton to tree graph</li> <li>AnglesAndInternodes - Extract fruits angles and internodes distance from a tree graph</li> </ul> <p>You have to download them and execute them by calling <code>jupyter notebook</code> in the conda environment with the <code>plant-3d-vision</code> module installed.</p>"},{"location":"plant_imager/install/","title":"Installing the ROMI software","text":""},{"location":"plant_imager/install/#use-cases","title":"Use cases","text":"<p>In the following subsections we will detail how to install ROMI software for a few usage cases:</p> <ol> <li>Create a database server here.</li> <li>Plant scans acquisition using the ROMI plant scanner to a database here.</li> <li>Plant reconstruction pipelines from existing plant scans in a database here.</li> <li>Virtual plant creation (3D modelling of plant architecture with LPY), virtual scan (mimic plant scanner with blender) &amp; reconstruction (same as 2.) here.</li> <li>Create a web server hosting the plant 3d explorer GUI here.</li> </ol> <p>Note</p> <p>You can find docker images for use cases #1, #3 &amp; #5 in the dockerhub repository of the ROMI project here.</p>"},{"location":"plant_imager/install/#general-requirements","title":"General requirements","text":""},{"location":"plant_imager/install/#cloning-sources","title":"Cloning sources","text":"<p>To clone the git repository, you will need:</p> <ul> <li><code>git</code></li> <li><code>ca-certificates</code></li> </ul> <p>Start with these system dependencies:</p> <pre><code>sudo apt-get install git ca-certificates\n</code></pre>"},{"location":"plant_imager/install/#downloading-from-urls","title":"Downloading from URLs","text":"<p>Sometimes the documentation will provide commands with <code>wget</code> to download archives or other types of files, here is the command line to install it if you do not have it:</p> <pre><code>sudo apt install wget\n</code></pre>"},{"location":"plant_imager/install/#creating-isolated-python-environments","title":"Creating isolated Python environments","text":"<p>Important</p> <p>We recommend using <code>conda</code> to create isolated environments as some packages, like <code>openalea.lpy</code>, are available as <code>conda</code> packages but not from <code>pip</code> and can be tricky to install from sources!</p> <p>Follow this link to learn how to install <code>miniconda3</code> &amp; create isolated Python environments with <code>conda</code>.</p> <p>If you have no idea why you should use isolated Python environments, here is a quote from the official Python documentation:</p> <p>Quote</p> <p><code>venv</code> (for Python 3) and <code>virtualenv</code> (for Python 2) allow you to manage separate package installations for different projects. They essentially allow you to create a \"virtual\" isolated Python installation and install packages into that virtual installation. When you switch projects, you can simply create a new virtual environment and not have to worry about breaking the packages installed in the other environments. It is always recommended using a virtual environment while developing Python applications.</p>"},{"location":"plant_imager/install/#list-of-sources","title":"List of sources","text":"<p>For the ROMI projects, several libraries have been developed in various languages and made available on GitHub. Here is a list of the important repositories for the plant scanner project:</p> <ul> <li><code>plantdb</code>: the database module is accessible here;</li> <li><code>plantimager</code>: the scanner interface, and the virtual scanner is accessible here;</li> <li><code>plant3dvision</code>: the computer vision algorithms to reconstruct the plants is accessible here;</li> <li><code>romiseg</code>: the ML-based plant segmentation models is accessible here;</li> <li><code>plant-3d-explorer</code>: the Node JS web viewer for plant scan, reconstruction and quantification is accessible here</li> </ul> <p>Additionally, we also have:</p> <ul> <li><code>romicgal</code>: some CGAL bindings used for skeletonization &amp; meshing is accessible here</li> <li><code>bldc_featherwing</code>: the controller for BLDC motor on a feather wing is accessible here</li> </ul>"},{"location":"plant_imager/install/create_env/","title":"Creating isolated Python environments","text":"<p>You can use <code>venv</code> or <code>conda</code> to create isolated Python environments.</p> <p>Warning</p> <p>Some ROMI libraries have dependencies relying on specific Python versions. Make sure that the isolated environment you create match these requirements!</p>"},{"location":"plant_imager/install/create_env/#isolated-environments-with-venv","title":"Isolated environments with <code>venv</code>","text":""},{"location":"plant_imager/install/create_env/#requirements","title":"Requirements","text":"<p>Python 3 &amp; <code>pip</code> are required. On Debian-like OS, use the following command to install them:</p> <pre><code>sudo apt-get install python3 python3-pip\n</code></pre> <p>For more details &amp; explanations, follow this official guide to learn how to install packages using pip and virtual environments.</p>"},{"location":"plant_imager/install/create_env/#environment-creation","title":"Environment creation","text":"<p>To create a new environment, named <code>plant_imager</code>, use <code>python3</code> and the <code>venv</code> module:</p> <pre><code>python3 -m venv plant_imager\n</code></pre> <p>Note</p> <p>This will create a <code>plant_imager</code> folder in the current working directory and place the \"environment files\" there! We thus advise to gather all your environment in a common folder like <code>~/envs</code>.</p> <p>To activate it:</p> <pre><code>source plant_imager/bin/activate\n</code></pre>"},{"location":"plant_imager/install/create_env/#usage","title":"Usage","text":"<p>Now you can easily install Python packages, for example <code>NumPy</code>, as follow:</p> <pre><code>pip3 install numpy\n</code></pre> <p>Note</p> <p>Use <code>deactivate</code> or kill terminal to leave it!</p> <p>You can now use this environment to install the ROMI software &amp; dependencies.</p>"},{"location":"plant_imager/install/create_env/#isolated-environments-with-miniconda","title":"Isolated environments with <code>miniconda</code>","text":""},{"location":"plant_imager/install/create_env/#requirements_1","title":"Requirements","text":"<p>In this case you do not need Python to be installed on your system, all you need it to install <code>miniconda3</code>. You can download the latest <code>miniconda3</code> version with:</p> <pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n</code></pre> <p>On Debian-like OS, use the following command to install it:</p> <pre><code>bash Miniconda3-latest-Linux-x86_64.sh\n</code></pre> <p>For more details &amp; explanations, follow this official guide to learn how to install <code>miniconda</code>.</p>"},{"location":"plant_imager/install/create_env/#environment-creation_1","title":"Environment creation","text":"<p>To create a new conda environment, named <code>plant_imager</code> with Python 3.7:</p> <pre><code>conda create --name plant_imager python==3.7\n</code></pre> <p>To activate it:</p> <pre><code>conda activate plant_imager\n</code></pre>"},{"location":"plant_imager/install/create_env/#usage_1","title":"Usage","text":"<p>Now you can now easily install Python packages, for example <code>NumPy</code>, as follow:</p> <pre><code>conda install numpy\n</code></pre> <p>Note</p> <p>Use <code>conda deactivate</code> or kill terminal to leave it!</p> <p>You can now use this environment to install the ROMI software &amp; dependencies.</p>"},{"location":"plant_imager/install/plant3dexplorer_setup/","title":"Install the ROMI <code>Plant 3d explorer</code>","text":"<p>To follow this guide you should have a <code>conda</code> environment, see here. For the sake of clarity it will be called <code>Plant 3d explorer</code>.</p> <p>Note</p> <p>If you do not want the hassle of having to create environment &amp; install python libraries, there is a pre-built docker image, with usage instructions here.</p>"},{"location":"plant_imager/install/plant3dexplorer_setup/#pre-requisite","title":"Pre-requisite","text":"<p>The <code>Plant 3d explorer</code> relies on:</p> <ul> <li><code>node</code></li> <li><code>npm</code></li> </ul> <p>Install <code>node</code> and <code>npm</code>, on ubuntu:</p> <pre><code>sudo apt install npm\n</code></pre> <p>The packaged version ot <code>npm</code> is probably out of date (require <code>npm&gt;=5</code>), to update it:</p> <pre><code>npm install npm@latest -g\n</code></pre>"},{"location":"plant_imager/install/plant3dexplorer_setup/#install-romi-packages-their-dependencies","title":"Install ROMI packages &amp; their dependencies:","text":"<p>Activate your <code>plant_imager</code> environment!</p> <p>Clone the visualizer git repository :</p> <pre><code>git clone https://github.com/romi/plant-3d-explorer.git\ncd plant-3d-explorer\n</code></pre> <p>Install node packages and build the pages:</p> <pre><code>npm install\n</code></pre>"},{"location":"plant_imager/install/plant3dexplorer_setup/#use-the-plant-3d-explorer","title":"Use the <code>Plant 3d explorer</code>","text":""},{"location":"plant_imager/install/plant3dexplorer_setup/#with-the-official-romi-database","title":"With the official ROMI database","text":"<p>You can use the ROMI database to test the installation of the <code>Plant 3d explorer</code>:</p> <pre><code>export REACT_APP_API_URL='https://db.romi-project.eu'\nnpm start\n</code></pre>"},{"location":"plant_imager/install/plant3dexplorer_setup/#with-a-running-local-database","title":"With a running local database","text":"<p>If you have followed the installation instructions of the ROMI database (here), you can use it with the <code>Plant 3d explorer</code>:</p> <pre><code>export REACT_APP_API_URL='0.0.0.0'\nnpm start\n</code></pre> <p>Tip</p> <p>To permanently set this URL as the location of the DB, add it to your <code>~/.bashrc</code> file. <pre><code>echo 'export REACT_APP_API_URL=0.0.0.0' &gt;&gt; ~/.bashrc\n</code></pre></p>"},{"location":"plant_imager/install/plant_imager_setup/","title":"Install ROMI software for the plant imager","text":"<p>To follows this guide you should have a <code>conda</code> or a Python <code>venv</code>, see here.</p> <p>For the sake of clarity the environment will be called <code>plant_imager</code>.</p>"},{"location":"plant_imager/install/plant_imager_setup/#install-romi-packages-with-pip","title":"Install ROMI packages with <code>pip</code>:","text":"<p>Activate your <code>plant_imager</code> environment!</p> <pre><code>conda activate plant_imager\n</code></pre> <p>Note</p> <p>Since this is still under development, the packages are installed in \"editable mode\" with the <code>-e</code> option.</p>"},{"location":"plant_imager/install/plant_imager_setup/#install-plant-imager-sources","title":"Install <code>plant-imager</code> sources:","text":"<p>To pilot the hardware you have to install the <code>plant-imager</code>package:</p> <pre><code>python3 -m pip install -e git+https://github.com/romi/plant-imager.git@dev\n</code></pre>"},{"location":"plant_imager/install/plant_imager_setup/#install-plant-3d-vision-sources","title":"Install <code>plant-3d-vision</code> sources:","text":"<p>To start \"acquisition jobs\", you have to install the <code>plant-3d-vision</code> package:</p> <pre><code>python3 -m pip install -e git+https://github.com/romi/plant3dvision.git@dev\n</code></pre>"},{"location":"plant_imager/install/plant_imager_setup/#install-plantdb-sources","title":"Install <code>plantdb</code> sources:","text":"<p>Since we will need an active database to export the acquisitions, you have to install the <code>plantdb</code> package:</p> <pre><code>python3 -m pip install -e git+https://github.com/romi/plantdb.git@dev\n</code></pre>"},{"location":"plant_imager/install/plant_imager_setup/#example-database","title":"Example database","text":"<p>To quickly create an example DB you can use:</p> <pre><code>wget https://db.romi-project.eu/models/test_db.tar.gz\ntar -xf test_db.tar.gz\n</code></pre> <p>This will create a <code>integration_tests</code> folder with a ready to use test database.</p> <p>You should now be ready to perform \"plant acquisitions\" following the dedicated user guide.</p>"},{"location":"plant_imager/install/plant_imager_setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"plant_imager/install/plant_imager_setup/#serial-access-denied","title":"Serial access denied","text":"<p>Look here if you can not communicate with the scanner using usb.</p>"},{"location":"plant_imager/install/plant_reconstruction_setup/","title":"Install ROMI software for plants reconstruction","text":"<p>To follows this guide you should have an existing <code>conda</code> or a Python <code>venv</code>, see here.</p> <p>We prefer to use conda and will use examples command lines with it. Simply replace the <code>conda activate plant_imager</code> commands with <code>source plant_imager/bin/activate</code>.</p> <p>For the sake of clarity the environment will be called <code>plant_imager</code>.</p>"},{"location":"plant_imager/install/plant_reconstruction_setup/#requirements","title":"Requirements:","text":""},{"location":"plant_imager/install/plant_reconstruction_setup/#colmap","title":"Colmap","text":"<p>To save you the hassle of installing Colmap &amp; its dependencies, we wrote a mechanism allowing you to run the Colmap command straight into a docker container where it is already done!</p> <p>Note</p> <p>Choose between option A (recommended) OR B!</p>"},{"location":"plant_imager/install/plant_reconstruction_setup/#a-use-of-docker-image","title":"A - Use of docker image","text":"<p>You can use a pre-built docker image with Colmap &amp; its dependencies installed (named <code>geki/colmap</code>). This requires to install the docker-engine. To do so, follows the official instructions here: https://docs.docker.com/get-docker/</p> <p>It uses the Python docker SDK <code>docker</code> available on PyPi, to learn more read the official documentation.</p> <p>If you upgrade from an older install, you may have to install the Python docker SDK:</p> <pre><code>conda activate plant_imager\npython -m pip install docker\n</code></pre> <p>Important</p> <p>Make sure you can access the docker engine as a non-root user! On Linux:</p> <pre><code>1. Create the docker group: `$ sudo groupadd docker`\n2. Add your user to the docker group: `$ sudo usermod -aG docker $USER`\n3. Log out and log back in so that your group membership is re-evaluated.\n\nOfficial instructions [here](https://docs.docker.com/engine/install/linux-postinstall/)\n</code></pre>"},{"location":"plant_imager/install/plant_reconstruction_setup/#b-system-install","title":"B - System install","text":"<p>If you are a warrior or a computer expert, you can follow the procedure from the official documentation here. Make sure to use version 3.6.</p> <p>Note</p> <p>If you are using a conda environment, you can install <code>ceres-solver</code> dependency for Colmap from the conda-forge channel:</p> <pre><code>conda activate plant_imager\nconda install ceres-solver -c conda-forge\n</code></pre> <p>Important</p> <p>By default we use the docker mechanism, to enable the system install you need to export the environment variable <code>COLMAP_EXE='colmap'</code>.</p>"},{"location":"plant_imager/install/plant_reconstruction_setup/#install-romi-packages-with-pip","title":"Install ROMI packages with <code>pip</code>:","text":"<p>Activate your <code>plant_imager</code> environment!</p> <p>Note</p> <p>Since this is still under development, the packages are installed in \"editable mode\" with the <code>-e</code> option.</p> <p>Important</p> <p>You need an active ROMI database to import the images fileset to reconstruct. If it's not done yet, follow the installation instructions of a ROMI database (here).</p>"},{"location":"plant_imager/install/plant_reconstruction_setup/#install-plant3dvision-sources","title":"Install <code>plant3dvision</code> sources:","text":"<p>To start \"reconstruction jobs\", you have to install the <code>plant3dvision</code> package.</p> <p>Here we use the submodules but if you wish to edit other packages than <code>plant3dvision</code>, e.g. <code>plantdb</code>, install them from source!</p> <p><pre><code>conda activate plant_imager\ngit clone --branch dev https://github.com/romi/plant3dvision.git\ncd plant3dvision\ngit submodule init\ngit submodule update\npython3 -m pip install -e ./plantdb/ --no-cache-dir\npython3 -m pip install -e ./romitask/ --no-cache-dir\npython3 -m pip install -e ./romiseg/ --no-cache-dir\npython3 -m pip install -e ./romicgal/ --no-cache-dir\npython3 -m pip install -e ./dtw/ --no-cache-dir\npython3 -m pip install -r requirements.txt --no-cache-dir\npython3 -m pip install -e . --no-cache-dir\n</code></pre> You should now be ready to performs \"plant reconstructions\" following the dedicated user guide.</p>"},{"location":"plant_imager/install/plant_reconstruction_setup/#install-romicgal-sources","title":"Install <code>romicgal</code> sources","text":"<p>We use some algorithms from CGAL and propose a minimal python wrapper called <code>romicgal</code>. To install it:</p> <pre><code>conda activate plant_imager\npython3 -m pip install -e git+https://github.com/romi/romicgal.git@master\n</code></pre> <p>Note</p> <p>This takes some time since it has to download dependencies (<code>CGAL-0.5</code> &amp; <code>boost-1.72.0</code>) and compile them.</p>"},{"location":"plant_imager/install/plant_reconstruction_setup/#install-romiseg-sources","title":"Install <code>romiseg</code> sources","text":"<p>To install the additional Machine Learning based segmentation module:</p> <pre><code>conda activate plant_imager\npython3 -m pip install -e git+https://github.com/romi/romiseg@dev\n</code></pre> <p>Warning</p> <p>If not using CUDA 10.*, you have to install the matching <code>pytorch</code> distribution. For example, for CUDA 9.2, use: <pre><code>conda activate plant_imager \npip install torch==1.4.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html\n</code></pre> To upgrade to Cuda 11.1, type <pre><code>conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge\n</code></pre> See the PyTorch webpage for compatible Torch and Cuda combinations.    </p>"},{"location":"plant_imager/install/plantdb_setup/","title":"Create a ROMI database to host, receive &amp; serve plant scans","text":"<p>To follow this guide you should have a <code>conda</code> environment, see here. For the sake of clarity it will be called <code>plantscans_db</code>.</p> <p>Note</p> <p>Since this is still under development, the packages are installed in \"editable mode\" with the <code>-e</code> option.</p> <p>Note</p> <p>If you do not want the hassle of having to create environment &amp; install python libraries, there is a pre-built docker image, with usage instructions here.</p>"},{"location":"plant_imager/install/plantdb_setup/#install-plantdb-sources","title":"Install <code>plantdb</code> sources","text":"<p>Activate your <code>plantscans_db</code> environment!</p> <pre><code>conda activate plantscans_db\n</code></pre> <p>To create an active ROMI database, you have to install the <code>plantdb</code> package:</p> <pre><code>git clone https://github.com/romi/plantdb.git &amp;&amp; \\\ncd plantdb &amp;&amp; \\\ngit checkout dev &amp;&amp; \\\npython3.7 -m pip install setuptools setuptools-scm &amp;&amp; \\\npython3.7 -m pip install luigi pillow &amp;&amp; \\\npython3.7 -m pip install flask flask-restful flask-cors &amp;&amp; \\\npython3.7 -m pip install .\n</code></pre>"},{"location":"plant_imager/install/plantdb_setup/#initialize-a-romi-database","title":"Initialize a ROMI database","text":"<p>The <code>FSDB</code> class from the <code>plantdb</code> module is used to manage a local file system for data storage. A database is any folder which contains a file named <code>plantdb</code>.</p> <p>To create an empty database, just create a new folder, and an empty file named <code>plantdb</code> in it. For example:</p> <pre><code>mkdir /data/romi_db\ntouch /data/romi_db/plantdb\n</code></pre> <p>Then define its location in an environment variable <code>DB_LOCATION</code>:</p> <pre><code>export DB_LOCATION='/data/ROMI/DB'\n</code></pre> <p>Note</p> <p>To permanently set this directory as the location of the DB, add it to your <code>~/.bashrc</code> file. <pre><code>echo 'export DB_LOCATION=/data/ROMI/DB' &gt;&gt; ~/.bashrc\n</code></pre></p>"},{"location":"plant_imager/install/plantdb_setup/#serve-the-rest-api","title":"Serve the REST API","text":"<p>Then you can start the REST API with <code>romi_scanner_rest_api</code>:</p> <pre><code>romi_scanner_rest_api\n</code></pre> <p>You should see something like:</p> <pre><code>n scans = 2\n * Serving Flask app \"romi_scanner_rest_api\" (lazy loading)\n * Environment: production\n   WARNING: This is a development server. Do not use it in a production deployment.\n   Use a production WSGI server instead.\n * Debug mode: off\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n</code></pre> <p>To access the REST API, open your favorite browser and use URLs to access:</p> <ul> <li>the list of all scans: http://0.0.0.0:5000/scans</li> <li>the '2018-12-17_17-05-35' dataset: http://0.0.0.0:5000/scans/2018-12-17_17-05-35</li> </ul> <p>You should see JSON formatted text.</p> <p>Troubleshooting: When starting the REST API with <code>romi_scanner_rest_api</code>, if you get an error message about this executable not being found, it may be missing from the <code>$PATH</code> environment variable. Add it with:</p> <pre><code>export PATH=$PATH:\"/home/$USER/.local/bin\"\n</code></pre> <p>Note</p> <p>To permanently set this in your bash terminal, add it to your <code>~/.bashrc</code> file. <pre><code>echo 'export PATH=$PATH:/home/$USER/.local/bin' &gt;&gt; ~/.bashrc\n</code></pre></p>"},{"location":"plant_imager/install/virtual_plant_setup/","title":"Install ROMI software for virtual plants acquisition &amp; reconstruction","text":"<p>To follows this guide you should have a <code>conda</code> or a Python <code>venv</code>, see here.</p> <p>For the sake of clarity the environment will be called <code>virtual_plants</code>.</p>"},{"location":"plant_imager/install/virtual_plant_setup/#notice-for-using-the-virtual-scanner","title":"Notice for using the virtual scanner","text":"<p>If you want to use the virtual scanner, the modified python version bundled with <code>blender</code> and the environment python version have to match.</p> <p>To obtain the python version bundled with your distribution of blender, type:</p> <pre><code>blender -b --python-expr \"import sys; print(sys.version)\"\n</code></pre> <p>It will output something like:</p> <pre><code>Blender 2.82 (sub 7) (hash 5b416ffb848e built 2020-02-14 16:19:45)\nALSA lib pcm_dmix.c:1089:(snd_pcm_dmix_open) unable to open slave\n3.8.1 (default, Jan 22 2020, 06:38:00) \n[GCC 9.2.0]\n\nBlender quit\n</code></pre> <p>In this case, this means Blender bundle Python 3.8, and you should too.</p> <p>In the following, we will assume that you are using conda environments. If not, adapt with corresponding virtualenv commands.</p>"},{"location":"plant_imager/install/virtual_plant_setup/#install-romi-packages-with-pip","title":"Install ROMI packages with <code>pip</code>:","text":"<p>Activate your <code>virtual_plants</code> environment!</p> <p>Note</p> <p>Since this is still under development, the packages are installed in \"editable mode\" with the <code>-e</code> option.</p>"},{"location":"plant_imager/install/virtual_plant_setup/#install-openalealpy","title":"Install <code>openalea.lpy</code>","text":"<p>If you're using <code>python&gt;=3.7</code> and <code>conda</code>, just install <code>lpy</code> from conda:</p> <pre><code>conda install -c conda-forge -c fredboudon openalea.lpy\n</code></pre>"},{"location":"plant_imager/install/virtual_plant_setup/#install-romicgal-sources","title":"Install <code>romicgal</code> sources","text":"<p>To pilot the hardware you have to install the <code>plantimager</code>package:</p> <pre><code>python3 -m pip install -e git+https://github.com/romi/romicgal.git@dev\n</code></pre> <p>Note</p> <p>This takes some time since it has to download dependencies (<code>CGAL-0.5</code> &amp; <code>boost-1.72.0</code>) and compile them.</p>"},{"location":"plant_imager/install/virtual_plant_setup/#install-plant3dvision-sources","title":"Install <code>plant3dvision</code> sources","text":"<p>To start \"acquisition jobs\", you have to install the <code>plant3dvision</code> package:</p> <pre><code>python3 -m pip install -e git+https://github.com/romi/plant3dvision.git@dev\n</code></pre>"},{"location":"plant_imager/install/virtual_plant_setup/#install-plantdb-sources","title":"Install <code>plantdb</code> sources","text":"<p>Since we will need an active database to export the acquisitions, you have to install the <code>plantdb</code> package:</p> <pre><code>python3 -m pip install -e git+https://github.com/romi/plantdb.git@dev\n</code></pre>"},{"location":"plant_imager/install/virtual_plant_setup/#install-romiseg-sources","title":"Install <code>romiseg</code> sources","text":"<p>To install the additional segmentation module:</p> <pre><code>python3 -m pip install git+https://github.com/romi/romiseg@dev\n</code></pre> <p>Warning</p> <p>If not using CUDA 10.*, you have to install the matching <code>pytorch</code> distribution. For example, for CUDA 9.2, use: <pre><code>pip install torch==1.4.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html\n</code></pre></p>"},{"location":"plant_imager/install/virtual_plant_setup/#example-database","title":"Example database","text":"<p>To quickly create an example DB you can use:</p> <pre><code>wget https://db.romi-project.eu/models/test_db.tar.gz\ntar -xf test_db.tar.gz\n</code></pre> <p>This will create a <code>integration_tests</code> folder with a ready to use test database.</p> <p>You should now be ready to perform tasks on virtual plants such as \"creation\", \"acquisition\" &amp; \"reconstruction\" following the dedicated user guide.</p>"},{"location":"plant_imager/metadata/","title":"FAIR data for the ROMI plant scanner project","text":""},{"location":"plant_imager/metadata/#fair-data","title":"FAIR data","text":"<p>We aim at using FAIR data principles in the ROMI plant scanner project.</p> <p>Quoting the GoFAIR website:</p> <p>Quote</p> <p>In 2016, the \"FAIR Guiding Principles for scientific data management and stewardship\" were published in Scientific Data. The authors intended to provide guidelines to improve the Findability, Accessibility, Interoperability, and Reuse of digital assets. The principles emphasise machine-actionability (i.e., the capacity of computational systems to find, access, interoperate, and reuse data with none or minimal human intervention) because humans increasingly rely on computational support to deal with data as a result of the increase in volume, complexity, and creation speed of data.</p> <p>In our context, a biological dataset is a set of RAW images (eg: RGB images), used to reconstruct the plant 3D structure, associated with a set of metadata of different nature: biological, hardware &amp; software.</p>"},{"location":"plant_imager/metadata/#romi-plant-scanner-metadata","title":"ROMI plant scanner metadata","text":"<p>Metadata are stored hierarchically. We currently use the JSON format.</p> <p>There are many JSON files containing metadata in the <code>metadata</code> directory attached to a dataset.</p>"},{"location":"plant_imager/metadata/#general-metadata","title":"General metadata","text":"<p>The first you should consider is <code>metadata/metadata.json</code>. Its top-level entries are:</p> <ul> <li>\"scanner\", the hardware metadata (see here)</li> <li>\"object\", the biological metadata (see here)</li> <li>\"path\", the parameter values used for the task <code>Scan</code> (see here)</li> <li>\"computed\", the parameter values used for the task <code>Colmap</code> (see here)</li> <li>\"measures\", the parameter values used for the task <code>AnglesAndInternodes</code> (see here)</li> </ul> <p>Todo</p> <p>Remove top-level entries \"path\", \"computed\" &amp; \"measures\", they look like duplicates from their respective task metadata.</p>"},{"location":"plant_imager/metadata/#scanning-operation","title":"Scanning operation","text":"<p>Found under the <code>path</code> top level section, it contains:</p> <ul> <li>the trajectory of the camera under <code>path</code></li> </ul> <p>Example:</p> <pre><code>    \"path\": {\n\"args\": {\n\"filetype\": \"jpg\",\n\"num_points\": 72,\n\"radius\": 350,\n\"tilt\": 0.45,\n\"xc\": 400,\n\"yc\": 400,\n\"z\": 0\n},\n\"id\": \"circular_72\",\n\"type\": \"circular\"\n}\n</code></pre> <p>Todo</p> <p>Potential duplication from the <code>Scan</code> task metadata!</p>"},{"location":"plant_imager/metadata/#colmap-reconstruction","title":"Colmap reconstruction","text":"<p>Found under the <code>computed</code> top level section, it contains the <code>camera_model</code> used by Colmap.</p> <p>Example:</p> <pre><code>    \"computed\": {\n\"camera_model\": {\n\"height\": 1080,\n\"id\": 1,\n\"model\": \"OPENCV\",\n\"params\": [\n1102.2709767952233,\n1102.2709767952233,\n808.0,\n540.0,\n-0.015118876273724434,\n-0.015118876273724434,\n0.0,\n0.0\n],\n\"width\": 1616\n}\n}\n</code></pre> <p>Todo</p> <p>Potential duplication from the <code>Colmap</code> task metadata!</p>"},{"location":"plant_imager/metadata/#measures-of-angles-and-internodes","title":"Measures of angles and internodes","text":"<p>Found under the <code>measures</code> top level section:</p> <ul> <li>Measured angles are under <code>angles</code></li> <li>Measured internodes are under <code>internodes</code></li> </ul> <p>Example:</p> <pre><code>    \"measures\": {\n\"angles\": [\n2.6179938779914944,\n1.3089969389957472,\n...\n2.0943951023931953\n],\n\"internodes\": [\n41,\n32,\n...\n1\n]\n}\n</code></pre> <p>Todo</p> <p>Potential duplication from the <code>AnglesAndInternodes</code> task metadata!</p>"},{"location":"plant_imager/metadata/#tasks-metadata","title":"Tasks metadata","text":"<p>Then there are the JSON files attached to each task, e.g. <code>Colmap_True____feature_extrac_3bbfcb1413.json</code>, they contain the parameter used to run this task. Their content and meaning is explained in the task metadata section.</p>"},{"location":"plant_imager/metadata/#images-metadata","title":"Images metadata","text":"<p>The image JSON file <code>metadata/images.json</code> contains ??? and is produced by ???.</p> <p>Example:</p> <pre><code>{\n\"task_params\": {\n\"fileset_id\": \"images\",\n\"output_file_id\": \"out\",\n\"scan_id\": \"\"\n}\n}\n</code></pre> <p>Found under the <code>metadata/images</code> directory, these JSON files contains ??? and is produced by ???.</p> <p>Note</p> <p>Sub-section <code>camera_model</code> seems redundant with same section in reconstruction.</p> <p>Example:</p> <pre><code>{\n\"calibrated_pose\": [\n49.04537654162613,\n401.1470121046677,\n-0.10613970524327433\n],\n\"colmap_camera\": {\n\"camera_model\": {\n\"height\": 1080,\n\"id\": 1,\n\"model\": \"OPENCV\",\n\"params\": [\n1106.9593323985682,\n1106.9593323985682,\n808.0,\n540.0,\n-0.012379986602455324,\n-0.012379986602455324,\n0.0,\n0.0\n],\n\"width\": 1616\n},\n\"rotmat\": [\n[\n-0.07758281248083276,\n0.9961595266033345,\n0.040584538496628464\n],\n[\n-0.4230067224604736,\n-0.069751540308706,\n0.9034378979087665\n],\n[\n0.9027991027691664,\n0.05292372040950383,\n0.42679369706827314\n]\n],\n\"tvec\": [\n-397.2357284138349,\n49.50722946972662,\n-66.64522999892229\n]\n},\n\"pose\": [\n50.0,\n400.0,\n0,\n0.0,\n0.45\n]\n}\n</code></pre>"},{"location":"plant_imager/metadata/#visualization-metadata","title":"Visualization metadata","text":"<p>The visualization JSON file <code>metadata/Visualization.json</code> contains ??? and is produced by ???.</p> <p>Important</p> <p>Run the <code>Visualization</code> task to complete this section!</p> <p>Example:</p> <pre><code>{\n\"files\": {\n\"angles\": \"AnglesAndInternodes\",\n\"images\": [\n\"image_rgb-000\",\n\"image_rgb-001\",\n\"...\"\n\"image_rgb-071\"\n],\n\"mesh\": \"TriangleMesh\",\n\"point_cloud\": \"PointCloud\",\n\"poses\": \"images\",\n\"skeleton\": \"CurveSkeleton\",\n\"thumbnails\": [\n\"thumbnail_rgb-000\",\n\"thumbnail_rgb-001\",\n\"...\"\n\"thumbnail_rgb-071\"\n],\n\"zip\": \"scan\"\n}\n}\n</code></pre> <p>Found under the <code>metadata/Visualization/</code> directory, there are two category of JSON files:</p> <ul> <li><code>image_*.json</code> contains ??? and is produced by ???.</li> <li><code>thumbnail_*.json</code> contains ??? and is produced by ???.</li> </ul> <p>Example for <code>image_*.json</code>:</p> <pre><code>{\n\"image_id\": \"rgb-000\"\n}\n</code></pre> <p>Example for <code>thumbnail_*.json</code>:</p> <pre><code>{\n\"image_id\": \"rgb-000\"\n}\n</code></pre>"},{"location":"plant_imager/metadata/#other-metadata","title":"Other metadata","text":"<p>Todo</p> <p>Explain what are &amp; who produce: - the image JSON file <code>metadata/images.json</code> - the image JSON files found under <code>metadata/images/*.json</code> - the visualization JSON file <code>metadata/Visualization.json</code> - the visualization JSON files found under <code>metadata/Visualization/*.json</code></p>"},{"location":"plant_imager/metadata/biological_metadata/","title":"Biological Metadata","text":"<p>Biological metadata are informative of the biological object and its growth conditions.</p>"},{"location":"plant_imager/metadata/biological_metadata/#definitions","title":"Definitions","text":"<p>Here is a list of biological metadata and their definition:</p> <ul> <li>species: the species of the biological object analysed, eg: \"Arabidopsis thaliana\";</li> <li>seed stock: an identifier of the seed stock used, eg: \"Col-0\", \"186.AV.L1\", ...;</li> <li>plant id: an identifier for the plant, eg: \"GT1\";</li> <li>growth environment: , eg: \"Lyon - Indoor\";</li> <li>growth conditions: growth condition used, eg: \"LD\", \"SD\", \"LD+SD\";</li> <li>treatment: specific treatment applied, if any, eg: \"Auxin 1mM\";</li> <li>DAG: Days After Germination or age of the plant in days, eg: 40;</li> <li>sample: part of the plant used, if any, eg: \"main stem\";</li> <li>experiment id: an identifier for the experiment, eg: \"dry plant\";</li> <li>dataset id: the Omero dataset identifier for the biological dataset, eg: 12;</li> </ul>"},{"location":"plant_imager/metadata/biological_metadata/#configuration","title":"Configuration","text":"<p>Todo</p> <p>How is it defined in a TOML configuration file ?</p>"},{"location":"plant_imager/metadata/biological_metadata/#database-location","title":"Database location","text":"<p>Located in <code>metadata/metadata.json</code> and found under the <code>object</code> top level section, it contains biologically relevant information such as the studied species, its age and growth conditions. This information is not restricted in its format but should contain a minimal set of entries.</p> <p>Todo</p> <p>Defines the minimal set of entries! Use the MIAPPE standard?</p>"},{"location":"plant_imager/metadata/biological_metadata/#json-example","title":"JSON example","text":"<p>Example of a <code>metadata/metadata.json</code> file for biological metadata:</p> <pre><code>    \"object\": {\n\"age\": \"62d\",\n\"culture\": \"LD\",\n\"environment\": \"Lyon indoor\",\n\"experiment_id\": \"living plant\",\n\"object\": \"plant\",\n\"plant_id\": \"Col0_26_10_2018_B\",\n\"sample\": \"main stem\",\n\"species\": \"Arabidopsis thaliana\",\n\"stock\": \"186AV.L1\",\n\"treatment\": \"none\"\n}\n</code></pre>"},{"location":"plant_imager/metadata/hardware_metadata/","title":"Hardware metadata &amp; scan settings","text":"<p>Hardware metadata are informative of the hardware setup like the used camera, its firmwares or the workspace size.</p>"},{"location":"plant_imager/metadata/hardware_metadata/#definitions","title":"Definitions","text":"<p>Here is a list of hardware metadata and their definition:</p> <ul> <li>frame: scanner frame type and version, eg: \"30profile v1\";</li> <li>X_motor: type of motor used for the X axis, eg: \"X-Carve NEMA23\";</li> <li>Y_motor: type of motor used for the Y axis, eg: \"X-Carve NEMA23\";</li> <li>Z_motor: type of motor used for the Z axis, eg: \"X-Carve NEMA23\";</li> <li>pan_motor: , type of motor used for the camera pan axis, eg: \"Dynamixel\";</li> <li>tilt_motor: , type of motor used for the camera tilt axis, eg: \"gimbal\";</li> <li>sensor: type of sensor used during acquisition, eg: \"Sony alpha\";</li> <li>scan_os: , eg: \"\";</li> </ul> <p>The metadata dictionary made of <code>frame</code>, <code>X_motor</code>, <code>Y_motor</code>, <code>Z_motor</code>, <code>pan_motor</code> &amp; <code>tilt_motor</code> define the hardware_id used in the <code>README.md</code>.</p> <p>The <code>sensor</code> metadata could be more detailed, for example as a dictionary or a reference to a sensor_id database?</p>"},{"location":"plant_imager/metadata/hardware_metadata/#configuration","title":"Configuration","text":"<p>Todo</p> <p>How is it defined in a TOML configuration file ?</p>"},{"location":"plant_imager/metadata/hardware_metadata/#database-location","title":"Database location","text":"<p>Located in <code>metadata/metadata.json</code> and found under the <code>scanner</code> top level section, it contains information about the hardware and software used for the scan:</p> <ul> <li>the used camera with <code>camera_args</code>, <code>camera_firmware</code>,  <code>camera_hardware</code> &amp;  <code>camera_lens</code></li> <li>the model and version of the scanning station with <code>id</code></li> <li>list of hardware and software components and their versions with <code>cnc_args</code>, <code>cnc_firmware</code>, <code>cnc_hardware</code>, <code>frame</code>, <code>gimbal_args</code>, <code>gimbal_firmware</code>, <code>gimbal_hardware</code></li> <li>the used workspace with <code>workspace</code></li> </ul>"},{"location":"plant_imager/metadata/hardware_metadata/#json-example","title":"JSON example","text":"<p>Example of a <code>metadata/metadata.json</code> file for hardware metadata:</p> <pre><code>    \"scanner\": {\n\"camera_args\": {\n\"api_url\": \"http://192.168.122.1:8080\"\n},\n\"camera_firmware\": \"sony_wifi\",\n\"camera_hardware\": \"Sony Alpha 5100\",\n\"camera_lens\": \"16-35 stock\",\n\"cnc_args\": {\n\"homing\": true,\n\"port\": \"/dev/ttyUSB1\"\n},\n\"cnc_firmware\": \"grbl-v1.1\",\n\"cnc_hardware\": \"xcarve-v2\",\n\"frame\": \"alu 40mm\",\n\"gimbal_args\": {\n\"baud_rate\": 57600,\n\"dev\": \"/dev/ttyUSB0\",\n\"tilt0\": 3072\n},\n\"gimbal_firmware\": \"dynamixel-usb2dynamixel\",\n\"gimbal_hardware\": \"dynamixel\",\n\"id\": \"lyon_1\",\n\"workspace\": {\n\"x\": [\n200,\n600\n],\n\"y\": [\n200,\n600\n],\n\"z\": [\n-180,\n260\n]\n}\n}\n</code></pre> <p>Todo</p> <p>Gather all camera parameters under a <code>camera</code> section? Gather all cnc parameters under a <code>cnc</code> section? Gather all gimbal parameters under a <code>gimbal</code> section?</p>"},{"location":"plant_imager/metadata/software_metadata/","title":"Software metadata: versioning and history","text":"<p>They aim at keeping track of the algorithm versions, using their git tag, during any ROMI task. They enable to trace back results and compare them.</p>"},{"location":"plant_imager/metadata/tasks_metadata/","title":"Tasks metadata: parametrization","text":""},{"location":"plant_imager/metadata/tasks_metadata/#aim","title":"Aim","text":"<p>They aim at keeping track of the parameters used by algorithms during any ROMI task. They enable to trace back results and compare them.</p>"},{"location":"plant_imager/metadata/tasks_metadata/#database-location","title":"Database location","text":"<p>Located under the <code>metadata</code> directory of the plant scan dataset, these JSON files contain the parameters used to run the task and are produced by each task.</p> <p>Examples of task metadata JSON file names:</p> <pre><code>AnglesAndInternodes_1_0_2_0_0_1_9e87e344e6.json\nColmap_True____feature_extrac_3bbfcb1413.json\nMasks_True_5_out_9adb9db801.json\nTreeGraph_out__CurveSkeleton_5dca9a2821.json\nUndistorted_out_____fb3e3fa0ff.json\nVoxels_False___background___False_0ac9c133f7.json\nVisualization.json\n</code></pre> <p>The tasks ids are a concatenation of the task name, the first values of the first 3 parameters sorted by parameter name and a md5hash of the name/parameters as a cananocalised json (from luigi documentation of task_id_str)</p>"},{"location":"plant_imager/metadata/tasks_metadata/#anglesandinternodes-task","title":"AnglesAndInternodes task","text":""},{"location":"plant_imager/metadata/tasks_metadata/#configuration","title":"Configuration","text":"<p>To configure this task, we use the <code>[AnglesAndInternodes]</code> section in the TOML configuration file. For example:</p> <pre><code>[AnglesAndInternodes]\nupstream_task = \"TreeGraph\"\ncharacteristic_length = 1.0\nstem_axis_inverted = false\n</code></pre>"},{"location":"plant_imager/metadata/tasks_metadata/#database-location_1","title":"Database location","text":"<p>Found under <code>metadata/AnglesAndInternodes_*.json</code>.</p>"},{"location":"plant_imager/metadata/tasks_metadata/#json-example","title":"JSON example","text":"<p>Example of <code>metadata/AnglesAndInternodes_*.json</code> corresponding to the example TOML configuration file:</p> <pre><code>{\n\"task_params\": {\n\"characteristic_length\": 1.0,\n\"min_elongation_ratio\": 2.0,\n\"min_fruit_size\": 0.1,\n\"number_nn\": 50,\n\"output_file_id\": \"out\",\n\"scan_id\": \"\",\n\"stem_axis\": 2,\n\"stem_axis_inverted\": \"False\",\n\"upstream_task\": \"ClusteredMesh\"\n}\n}\n</code></pre>"},{"location":"plant_imager/metadata/tasks_metadata/#clusteredmesh-task","title":"ClusteredMesh task","text":""},{"location":"plant_imager/metadata/tasks_metadata/#configuration_1","title":"Configuration","text":"<p>To configure this task, we use the <code>[Colmap]</code> section in the TOML configuration file. For example:</p> <pre><code>[ClusteredMesh]\nupstream_task = \"SegmentedPointCloud\"\nmin_vol = 1.0\nmin_length = 10.0\n</code></pre>"},{"location":"plant_imager/metadata/tasks_metadata/#database-location_2","title":"Database location","text":"<p>Found under <code>metadata/ClusteredMesh_*.json</code>.</p>"},{"location":"plant_imager/metadata/tasks_metadata/#json-example_1","title":"JSON example","text":"<p>Example of <code>metadata/ClusteredMesh_*.json</code> corresponding to the example TOML configuration file:</p> <pre><code>{\n\"task_params\": {\n\"min_length\": 10.0,\n\"min_vol\": 1.0,\n\"output_file_id\": \"out\",\n\"scan_id\": \"\",\n\"upstream_task\": \"SegmentedPointCloud\"\n}\n}\n</code></pre>"},{"location":"plant_imager/metadata/tasks_metadata/#colmap-task","title":"Colmap task","text":""},{"location":"plant_imager/metadata/tasks_metadata/#configuration_2","title":"Configuration","text":"<p>To configure this task, we use the <code>[Colmap]</code> section in the TOML configuration file. For example:</p> <pre><code>[Colmap]\nmatcher = \"exhaustive\"\ncompute_dense = false\n\n[Colmap.cli_args.feature_extractor]\n\"--ImageReader.single_camera\" = \"1\"\n\"--SiftExtraction.use_gpu\" = \"1\"\n\n[Colmap.cli_args.exhaustive_matcher]\n\"--SiftMatching.use_gpu\" = \"1\"\n\n[Colmap.cli_args.model_aligner]\n\"--robust_alignment_max_error\" = \"10\"\n\n[Colmap.bounding_box] # default to None\nx = [150, 650]\ny = [150, 650]\nz = [-90, 300]\n</code></pre>"},{"location":"plant_imager/metadata/tasks_metadata/#database-location_3","title":"Database location","text":"<p>Found under <code>metadata/Colmap_*.json</code>.</p>"},{"location":"plant_imager/metadata/tasks_metadata/#json-example_2","title":"JSON example","text":"<p>Example of <code>metadata/Colmap_*.json</code> corresponding to the example TOML configuration file:</p> <pre><code>{\n\"bounding_box\": {\n\"x\": [\n282.3466418101626,\n590.7798175997629\n],\n\"y\": [\n386.77943280470265,\n589.1307313142569\n],\n\"z\": [\n22.820168903922998,\n259.7594718279537\n]\n},\n\"task_params\": {\n\"align_pcd\": \"True\",\n\"calibration_scan_id\": \"\",\n\"cli_args\": {\n\"exhaustive_matcher\": {\n\"--SiftMatching.use_gpu\": \"1\"\n},\n\"feature_extractor\": {\n\"--ImageReader.single_camera\": \"1\",\n\"--SiftExtraction.use_gpu\": \"1\"\n},\n\"model_aligner\": {\n\"--robust_alignment_max_error\": \"10\"\n}\n},\n\"compute_dense\": \"False\",\n\"matcher\": \"exhaustive\",\n\"output_file_id\": \"out\",\n\"scan_id\": \"\",\n\"upstream_task\": \"ImagesFilesetExists\"\n}\n}\n</code></pre>"},{"location":"plant_imager/metadata/tasks_metadata/#curveskeleton-task","title":"CurveSkeleton task","text":""},{"location":"plant_imager/metadata/tasks_metadata/#configuration_3","title":"Configuration","text":"<p>To configure this task, we use the <code>[CurveSkeleton]</code> section in the TOML configuration file. For example:</p> <pre><code>[CurveSkeleton]\nupstream_task = \"TriangleMesh\"\n</code></pre>"},{"location":"plant_imager/metadata/tasks_metadata/#database-location_4","title":"Database location","text":"<p>Found under <code>metadata/CurveSkeleton_*.json</code>.</p>"},{"location":"plant_imager/metadata/tasks_metadata/#json-example_3","title":"JSON example","text":"<p>Example of <code>metadata/CurveSkeleton_*.json</code> corresponding to the example TOML configuration file:</p> <pre><code>{\n\"task_params\": {\n\"output_file_id\": \"out\",\n\"scan_id\": \"\",\n\"upstream_task\": \"TriangleMesh\"\n}\n}\n</code></pre>"},{"location":"plant_imager/metadata/tasks_metadata/#masks-task","title":"Masks task","text":""},{"location":"plant_imager/metadata/tasks_metadata/#configuration_4","title":"Configuration","text":"<p>To configure this task, we use the <code>[Masks]</code> section in the TOML configuration file. Default parametrization based on linear masking. For example:</p> <pre><code>[Masks]\nupstream_task = \"Undistorted\"\ntype = \"linear\"\nparameters = \"[0,1,0]\"\ndilation = 5\nbinarize = true\nthreshold = 0.3\n</code></pre>"},{"location":"plant_imager/metadata/tasks_metadata/#database-location_5","title":"Database location","text":"<p>Found under <code>metadata/Masks_*.json</code>.</p>"},{"location":"plant_imager/metadata/tasks_metadata/#json-example_4","title":"JSON example","text":"<p>Example of <code>metadata/Masks_*.json</code> corresponding to the example TOML configuration file:</p> <pre><code>{\n\"task_params\": {\n\"binarize\": \"True\",\n\"dilation\": 5,\n\"output_file_id\": \"out\",\n\"parameters\": [\n0,\n1,\n0\n],\n\"query\": {},\n\"scan_id\": \"\",\n\"threshold\": 0.3,\n\"type\": \"linear\",\n\"upstream_task\": \"ImagesFilesetExists\"\n}\n}\n</code></pre>"},{"location":"plant_imager/metadata/tasks_metadata/#pointcloud-task","title":"PointCloud task","text":""},{"location":"plant_imager/metadata/tasks_metadata/#configuration_5","title":"Configuration","text":"<p>To configure this task, we use the <code>[PointCloud]</code> section in the TOML configuration file. For example:</p> <pre><code>[PointCloud]\nupstream_task = \"Voxels\"\nlevel_set_value = 1.0\nlog = false\nbackground_prior= -200\nmin_contrast = 10.0\nmin_score = 0.2\n</code></pre>"},{"location":"plant_imager/metadata/tasks_metadata/#database-location_6","title":"Database location","text":"<p>Found under <code>metadata/PointCloud_*.json</code>.</p>"},{"location":"plant_imager/metadata/tasks_metadata/#json-example_5","title":"JSON example","text":"<p>Example of <code>metadata/PointCloud_*.json</code>:</p> <pre><code>{\n\"task_params\": {\n\"background_prior\": 0.5,\n\"level_set_value\": 1,\n\"log\": \"False\",\n\"min_contrast\": 10.0,\n\"min_score\": 0.2,\n\"output_file_id\": \"out\",\n\"scan_id\": \"\",\n\"upstream_task\": \"Voxels\"\n}\n}\n</code></pre>"},{"location":"plant_imager/metadata/tasks_metadata/#segmentation2d-task","title":"Segmentation2D task","text":""},{"location":"plant_imager/metadata/tasks_metadata/#configuration_6","title":"Configuration","text":"<p>To configure this task, we use the <code>[Segmentation2D]</code> section in the TOML configuration file. For example:</p> <pre><code>[Segmentation2D]\nupstream_task = \"Undistorted\"\nmodel_fileset = \"ModelFileset\"\nquery = \"{\\\"channel\\\":\\\"rgb\\\"}\"\nmodel_id = \"Resnetdataset_gl_png_896_896_epoch50\"\nresize = true\nbinarize = true\ndilation = 1\nSx = 896\nSy = 896\nepochs = 1\nbatch = 1\nlearning_rate = 0.0001\nthreshold = 0.0035\n\n[ModelFileset]\nscan_id = \"models\"\n</code></pre>"},{"location":"plant_imager/metadata/tasks_metadata/#database-location_7","title":"Database location","text":"<p>Found under <code>metadata/Segmentation2D_*.json</code>.</p>"},{"location":"plant_imager/metadata/tasks_metadata/#json-example_6","title":"JSON example","text":"<p>Example of <code>metadata/Segmentation2D_*.json</code> corresponding to the example TOML configuration file:</p> <pre><code>{\n\"task_params\": {\n\"Sx\": 896,\n\"Sy\": 896,\n\"binarize\": \"True\",\n\"dilation\": 1,\n\"inverted_labels\": [\n\"background\"\n],\n\"labels\": [],\n\"model_fileset\": \"ModelFileset\",\n\"model_id\": \"Resnetdataset_gl_png_896_896_epoch50\",\n\"output_file_id\": \"out\",\n\"query\": {\n\"channel\": \"rgb\"\n},\n\"resize\": \"True\",\n\"scan_id\": \"\",\n\"threshold\": 0.0035,\n\"upstream_task\": \"Undistorted\"\n}\n}\n</code></pre>"},{"location":"plant_imager/metadata/tasks_metadata/#segmentedpointcloud-task","title":"SegmentedPointCloud task","text":""},{"location":"plant_imager/metadata/tasks_metadata/#configuration_7","title":"Configuration","text":"<p>To configure this task, we use the <code>[SegmentedPointCloud]</code> section in the TOML configuration file. For example:</p> <pre><code>[SegmentedPointCloud]\nupstream_segmentation = \"Segmentation2D\"\nupstream_task = \"PointCloud\"\nuse_colmap_poses = true\n</code></pre>"},{"location":"plant_imager/metadata/tasks_metadata/#database-location_8","title":"Database location","text":"<p>Found under <code>metadata/SegmentedPointCloud_*.json</code>.</p>"},{"location":"plant_imager/metadata/tasks_metadata/#json-example_7","title":"JSON example","text":"<p>Example of <code>metadata/SegmentedPointCloud_*.json</code> corresponding to the example TOML configuration file:</p> <pre><code>{\n\"task_params\": {\n\"output_file_id\": \"out\",\n\"scan_id\": \"\",\n\"upstream_segmentation\": \"Segmentation2D\",\n\"upstream_task\": \"PointCloud\",\n\"use_colmap_poses\": \"True\"\n}\n}\n</code></pre>"},{"location":"plant_imager/metadata/tasks_metadata/#treegraph-task","title":"TreeGraph task","text":""},{"location":"plant_imager/metadata/tasks_metadata/#configuration_8","title":"Configuration","text":"<p>To configure this task, we use the <code>[TreeGraph]</code> section in the TOML configuration file. For example:</p> <pre><code>[TreeGraph]\nupstream_task = \"CurveSkeleton\"\nz_axis = 2\nstem_axis_inverted = false\n</code></pre>"},{"location":"plant_imager/metadata/tasks_metadata/#database-location_9","title":"Database location","text":"<p>Found under <code>metadata/TreeGraph_*.json</code>.</p>"},{"location":"plant_imager/metadata/tasks_metadata/#json-example_8","title":"JSON example","text":"<p>Example of <code>metadata/TreeGraph_*.json</code> corresponding to the example TOML configuration file:</p> <pre><code>{\n\"task_params\": {\n\"output_file_id\": \"out\",\n\"scan_id\": \"\",\n\"upstream_task\": \"CurveSkeleton\",\n\"z_axis\": 2,\n\"z_orientation\": 1\n}\n}\n</code></pre>"},{"location":"plant_imager/metadata/tasks_metadata/#trianglemesh-task","title":"TriangleMesh task","text":""},{"location":"plant_imager/metadata/tasks_metadata/#configuration_9","title":"Configuration","text":"<p>To configure this task, we use the <code>[TriangleMesh]</code> section in the TOML configuration file. For example:</p> <pre><code>[TriangleMesh]\nupstream_task = \"PointCloud\"\n</code></pre>"},{"location":"plant_imager/metadata/tasks_metadata/#database-location_10","title":"Database location","text":"<p>Found under <code>metadata/TriangleMesh_*.json</code>.</p>"},{"location":"plant_imager/metadata/tasks_metadata/#json-example_9","title":"JSON example","text":"<p>Example of <code>metadata/TriangleMesh_*.json</code> corresponding to the example TOML configuration file:</p> <pre><code>{\n\"task_params\": {\n\"output_file_id\": \"out\",\n\"scan_id\": \"\",\n\"upstream_task\": \"PointCloud\"\n}\n}\n</code></pre>"},{"location":"plant_imager/metadata/tasks_metadata/#undistorted-task","title":"Undistorted task","text":""},{"location":"plant_imager/metadata/tasks_metadata/#configuration_10","title":"Configuration","text":"<p>To configure this task, we use the <code>[Undistorted]</code> section in the TOML configuration file. For example:</p> <pre><code>[Undistorted]\nupstream_task = \"ImagesFilesetExists\"\n</code></pre>"},{"location":"plant_imager/metadata/tasks_metadata/#database-location_11","title":"Database location","text":"<p>Found under <code>metadata/Undistorted_*.json</code>.</p>"},{"location":"plant_imager/metadata/tasks_metadata/#json-example_10","title":"JSON example","text":"<p>Example of <code>metadata/Undistorted_*.json</code> corresponding to the example TOML configuration file:</p> <pre><code>{\n\"task_params\": {\n\"output_file_id\": \"out\",\n\"query\": {},\n\"scan_id\": \"\",\n\"upstream_task\": \"ImagesFilesetExists\"\n}\n}\n</code></pre>"},{"location":"plant_imager/metadata/tasks_metadata/#visualization-task","title":"Visualization task","text":""},{"location":"plant_imager/metadata/tasks_metadata/#configuration_11","title":"Configuration","text":"<p>To configure this task, we use the <code>[Visualization]</code> section in the TOML configuration file. For example:</p> <pre><code>[Visualization]\nupstream_point_cloud = \"PointCloud\"\nupstream_mesh = \"TriangleMesh\"\nupstream_colmap = \"Colmap\"\nupstream_angles = \"AnglesAndInternodes\"\nupstream_skeleton = \"CurveSkeleton\"\nupstream_images = \"ImagesFilesetExists\"\nmax_image_size = 1500\nmax_point_cloud_size = 10000\nthumbnail_size = 150\n</code></pre>"},{"location":"plant_imager/metadata/tasks_metadata/#database-location_12","title":"Database location","text":"<p>Found under <code>metadata/Visualization.json</code>.</p>"},{"location":"plant_imager/metadata/tasks_metadata/#json-example_11","title":"JSON example","text":"<p>Example of <code>metadata/Visualization.json</code> corresponding to the example TOML configuration file:</p> <pre><code>{\n\"files\": {\n\"angles\": \"AnglesAndInternodes\",\n\"camera\": \"cameras\",\n\"images\": [\n\"image_00000_rgb\",\n\"image_00001_rgb\",\n\"image_00002_rgb\",\n\"image_00003_rgb\",\n\"image_00004_rgb\",\n\"image_00005_rgb\",\n\"image_00006_rgb\",\n\"image_00007_rgb\",\n\"image_00008_rgb\",\n\"image_00009_rgb\",\n\"image_00010_rgb\",\n\"image_00011_rgb\",\n\"image_00012_rgb\",\n\"image_00013_rgb\",\n\"image_00014_rgb\",\n\"image_00015_rgb\",\n\"image_00016_rgb\",\n\"image_00017_rgb\",\n\"image_00018_rgb\",\n\"image_00019_rgb\",\n\"image_00020_rgb\",\n\"image_00021_rgb\",\n\"image_00022_rgb\",\n\"image_00023_rgb\",\n\"image_00024_rgb\",\n\"image_00025_rgb\",\n\"image_00026_rgb\",\n\"image_00027_rgb\",\n\"image_00028_rgb\",\n\"image_00029_rgb\",\n\"image_00030_rgb\",\n\"image_00031_rgb\",\n\"image_00032_rgb\",\n\"image_00033_rgb\",\n\"image_00034_rgb\",\n\"image_00035_rgb\",\n\"image_00036_rgb\",\n\"image_00037_rgb\",\n\"image_00038_rgb\",\n\"image_00039_rgb\",\n\"image_00040_rgb\",\n\"image_00041_rgb\",\n\"image_00042_rgb\",\n\"image_00043_rgb\",\n\"image_00044_rgb\",\n\"image_00045_rgb\",\n\"image_00046_rgb\",\n\"image_00047_rgb\",\n\"image_00048_rgb\",\n\"image_00049_rgb\",\n\"image_00050_rgb\",\n\"image_00051_rgb\",\n\"image_00052_rgb\",\n\"image_00053_rgb\",\n\"image_00054_rgb\",\n\"image_00055_rgb\",\n\"image_00056_rgb\",\n\"image_00057_rgb\",\n\"image_00058_rgb\",\n\"image_00059_rgb\"\n],\n\"measures\": \"measures\",\n\"mesh\": \"TriangleMesh\",\n\"point_cloud\": \"PointCloud\",\n\"poses\": \"images\",\n\"skeleton\": \"CurveSkeleton\",\n\"thumbnails\": [\n\"thumbnail_00000_rgb\",\n\"thumbnail_00001_rgb\",\n\"thumbnail_00002_rgb\",\n\"thumbnail_00003_rgb\",\n\"thumbnail_00004_rgb\",\n\"thumbnail_00005_rgb\",\n\"thumbnail_00006_rgb\",\n\"thumbnail_00007_rgb\",\n\"thumbnail_00008_rgb\",\n\"thumbnail_00009_rgb\",\n\"thumbnail_00010_rgb\",\n\"thumbnail_00011_rgb\",\n\"thumbnail_00012_rgb\",\n\"thumbnail_00013_rgb\",\n\"thumbnail_00014_rgb\",\n\"thumbnail_00015_rgb\",\n\"thumbnail_00016_rgb\",\n\"thumbnail_00017_rgb\",\n\"thumbnail_00018_rgb\",\n\"thumbnail_00019_rgb\",\n\"thumbnail_00020_rgb\",\n\"thumbnail_00021_rgb\",\n\"thumbnail_00022_rgb\",\n\"thumbnail_00023_rgb\",\n\"thumbnail_00024_rgb\",\n\"thumbnail_00025_rgb\",\n\"thumbnail_00026_rgb\",\n\"thumbnail_00027_rgb\",\n\"thumbnail_00028_rgb\",\n\"thumbnail_00029_rgb\",\n\"thumbnail_00030_rgb\",\n\"thumbnail_00031_rgb\",\n\"thumbnail_00032_rgb\",\n\"thumbnail_00033_rgb\",\n\"thumbnail_00034_rgb\",\n\"thumbnail_00035_rgb\",\n\"thumbnail_00036_rgb\",\n\"thumbnail_00037_rgb\",\n\"thumbnail_00038_rgb\",\n\"thumbnail_00039_rgb\",\n\"thumbnail_00040_rgb\",\n\"thumbnail_00041_rgb\",\n\"thumbnail_00042_rgb\",\n\"thumbnail_00043_rgb\",\n\"thumbnail_00044_rgb\",\n\"thumbnail_00045_rgb\",\n\"thumbnail_00046_rgb\",\n\"thumbnail_00047_rgb\",\n\"thumbnail_00048_rgb\",\n\"thumbnail_00049_rgb\",\n\"thumbnail_00050_rgb\",\n\"thumbnail_00051_rgb\",\n\"thumbnail_00052_rgb\",\n\"thumbnail_00053_rgb\",\n\"thumbnail_00054_rgb\",\n\"thumbnail_00055_rgb\",\n\"thumbnail_00056_rgb\",\n\"thumbnail_00057_rgb\",\n\"thumbnail_00058_rgb\",\n\"thumbnail_00059_rgb\"\n],\n\"zip\": \"scan\"\n},\n\"task_params\": {\n\"max_image_size\": 1500,\n\"max_point_cloud_size\": 10000,\n\"output_file_id\": \"out\",\n\"scan_id\": \"\",\n\"thumbnail_size\": 150,\n\"upstream_angles\": \"AnglesAndInternodes\",\n\"upstream_colmap\": \"Colmap\",\n\"upstream_images\": \"Undistorted\",\n\"upstream_mesh\": \"TriangleMesh\",\n\"upstream_point_cloud\": \"PointCloud\",\n\"upstream_skeleton\": \"CurveSkeleton\"\n}\n}\n</code></pre>"},{"location":"plant_imager/metadata/tasks_metadata/#voxels-task","title":"Voxels task","text":""},{"location":"plant_imager/metadata/tasks_metadata/#configuration_12","title":"Configuration","text":"<p>To configure this task, we use the <code>[Voxels]</code> section in the TOML configuration file. For example:</p> <pre><code>[Voxels]\nupstream_mask = \"Masks\"\nupstream_colmap = \"Colmap\"\nuse_colmap_poses = true\nvoxel_size = 1.0\ntype = \"carving\"\nlog = false\ninvert = false\nlabels = \"[]\"\n</code></pre>"},{"location":"plant_imager/metadata/tasks_metadata/#database-location_13","title":"Database location","text":"<p>Found under <code>metadata/Voxels_*.json</code>.</p>"},{"location":"plant_imager/metadata/tasks_metadata/#json-example_12","title":"JSON example","text":"<p>Example of <code>metadata/Voxels_*.json</code> corresponding to the example TOML configuration file:</p> <pre><code>{\n\"task_params\": {\n\"invert\": \"False\",\n\"labels\": [\n\"background\"\n],\n\"log\": \"False\",\n\"output_file_id\": \"out\",\n\"scan_id\": \"\",\n\"type\": \"averaging\",\n\"upstream_colmap\": \"Colmap\",\n\"upstream_mask\": \"Segmentation2D\",\n\"use_colmap_poses\": \"True\",\n\"voxel_size\": 0.01\n}\n}\n</code></pre>"},{"location":"plant_imager/metadata/tasks_metadata/#scan-task","title":"Scan task","text":"<p>Todo</p> <p>For some reason the parameters are defined in the <code>metadata/metadata.json</code> file. Definition can be found here.</p>"},{"location":"plant_imager/specifications/configuration_files/","title":"Tasks configuration","text":""},{"location":"plant_imager/specifications/configuration_files/#introduction","title":"Introduction","text":"<p>Each task has some default configuration but to create a working pipeline, you need to set some parameters in a TOML configuration file.</p> <p>There are two types of parameters:</p> <ol> <li>those associated with luigi: upstream tasks, requirements &amp; outputs</li> <li>those tuning the used algorithms: variables &amp; parameters</li> </ol> <p>For examples look at the configurations examples provided in <code>plant3dvision/configs</code>.</p>"},{"location":"plant_imager/specifications/configuration_files/#io-tasks","title":"I/O tasks","text":"<p>These tasks refer to the inputs and outputs of each algorithmic tasks. They are often related to task defined in the <code>plantdb</code> library.</p>"},{"location":"plant_imager/specifications/configuration_files/#filesetexists","title":"FilesetExists","text":"<p>Check that the given <code>Fileset</code> id exists (in dataset).</p> <p>No luigi parameters (no upstream task).</p> <p>List of task variables:</p> <ul> <li><code>fileset_id</code>: the id (<code>str</code>) to check for existence.</li> </ul>"},{"location":"plant_imager/specifications/configuration_files/#imagesfilesetexistsfilesetexists","title":"ImagesFilesetExists(FilesetExists)","text":"<p>Check that the image file set exists.</p> <p>No luigi parameters (no upstream task).</p> <p>List of task variables:</p> <ul> <li><code>fileset_id</code>: the id (<code>str</code>) to check for existence <code>'images'</code> by default.</li> </ul>"},{"location":"plant_imager/specifications/configuration_files/#modelfilesetfilesetexists","title":"ModelFileset(FilesetExists)","text":"<p>Check that the trained ML models file set exists.</p> <p>No luigi parameters (no upstream task).</p> <p>List of task variables:</p> <ul> <li><code>fileset_id</code>: the id (<code>str</code>) to check for existence <code>'models'</code> by default.</li> </ul> <p>Example:</p> <pre><code>[ModelFileset]\nscan_id = \"models\"\n</code></pre> <p>Defines the location of the trained ML models in a dataset named <code>'models'</code>.</p>"},{"location":"plant_imager/specifications/configuration_files/#database-tasks","title":"Database tasks","text":""},{"location":"plant_imager/specifications/configuration_files/#clean","title":"Clean","text":"<p>Defined in <code>plantdb.task</code>, it is used to clean a scan dataset from the \"processing folders\".</p> <p>No luigi parameters (no upstream task).</p> <p>List of task variables:</p> <ul> <li><code>no_confirm</code>: boolean indicating whether a confirmation is required to clean the dataset, <code>False</code> by default, i.e. confirmation required.</li> </ul> <p>Example:</p> <pre><code>[Clean]\nno_confirm = true\n</code></pre> <p>Use this to clean without confirmation.</p>"},{"location":"plant_imager/specifications/configuration_files/#algorithmic-tasks","title":"Algorithmic tasks","text":""},{"location":"plant_imager/specifications/configuration_files/#colmap-task","title":"Colmap task","text":"<p>Defined in <code>plant3dvision.task.colmap</code>, it is used to match scan images and estimate camera poses. It can also be used to compute a sparse and/or dense point cloud.</p> <p>List of luigi task parameters:</p> <ul> <li><code>upstream_task</code>: task upstream of the <code>Colmap</code> task, default is <code>ImagesFilesetExists</code></li> </ul> <p>List of task variables:</p> <ul> <li><code>matcher</code>: images matching method, can be either \"exhaustive\" (default) or \"sequential\";</li> <li><code>compute_dense</code>: boolean indicating whether to run the dense Colmap to obtain a dense point cloud, <code>False</code> by default;</li> <li><code>cli_args</code>: dictionary of parameters for Colmap command line prompts;</li> <li><code>align_pcd</code>: boolean indicating whether to align point cloud on calibrated or metadata poses, <code>True</code> by default;</li> <li><code>calibration_scan_id</code> : ID of the calibration scan, requires ???.</li> </ul> <p>Example:</p> <pre><code>[Colmap]\nmatcher = \"exhaustive\"\ncompute_dense = false\ncalibration_scan_id = \"calib_scan_shortpath\"\n\n[Colmap.cli_args.feature_extractor]\n\"--ImageReader.single_camera\" = \"1\"\n\"--SiftExtraction.use_gpu\" = \"1\"\n\n[Colmap.cli_args.exhaustive_matcher]\n\"--SiftMatching.use_gpu\" = \"1\"\n\n[Colmap.cli_args.model_aligner]\n\"--robust_alignment_max_error\" = \"10\"\n</code></pre> <p>Todo</p> <p>Add a <code>compute_sparse</code>?</p>"},{"location":"plant_imager/specifications/configuration_files/#undistorted-task","title":"Undistorted task","text":"<p>Defined in <code>plant3dvision.task.proc2d</code>, it is used to create undistorted images from pre-computed intrinsic camera parameters.</p>"},{"location":"plant_imager/specifications/configuration_files/#masks-task","title":"Masks task","text":"<p>Defined in <code>plant3dvision.task.proc2d</code>, it is used to create binary mask of the plant location within each image.</p> <p>List of luigi task parameters:</p> <ul> <li><code>upstream_task</code>: task upstream of the <code>Masks</code> task, default is <code>Undistorted</code> but can be <code>None</code>.</li> </ul> <p>List of task variables:</p> <ul> <li><code>type</code>: method to use to compute masks, choices are: <code>'linear'</code>, <code>'excess_green'</code>, <code>'vesselness'</code>, <code>'invert'</code>;</li> <li><code>parameters</code>: list of scalar parameters, depends on type hence no default values;</li> <li><code>dilation</code>: integer defining the dilation factor to apply when using a binary mask, no default values;</li> <li><code>binarize</code>: boolean indicating whether to binarize the mask, default is <code>True</code>;</li> <li><code>threshold</code>: float used as threshold for binarization step, default is <code>0.0</code>;</li> </ul>"},{"location":"plant_imager/specifications/configuration_files/#voxels-task","title":"Voxels task","text":"<p>Defined in <code>plant3dvision.task.cl</code>, it is used to reconstruct the 3D structure of the plant from binary or segmented masks.</p> <p>List of luigi task parameters:</p> <ul> <li><code>upstream_task</code>: task upstream of the <code>Colmap</code> task, default is <code>ImagesFilesetExists</code></li> </ul> <p>List of task variables:</p> <ul> <li><code>type</code>: ;</li> </ul> <p>Example:</p> <pre><code>[Voxels]\nupstream_mask = \"Segmentation2D\"\nlabels = \"[\\\"background\\\"]\"\nvoxel_size = 0.3\ntype = \"averaging\"\ninvert = false\nuse_colmap_poses = true\nlog = false\n</code></pre> <p>Note</p> <p>Choose <code>'Segmentation2D'</code> as <code>` for ML pipeline or</code>'Masks'` for geometrical pipeline.</p>"},{"location":"plant_imager/specifications/configuration_files/#pointcloud-task","title":"PointCloud task","text":"<p>Example:</p> <pre><code>[PointCloud]\nlevel_set_value = 1\nbackground_prior= 0.5\nlog = false\n</code></pre>"},{"location":"plant_imager/specifications/configuration_files/#segmentedpointcloud-task","title":"SegmentedPointCloud task","text":"<p>Example:</p> <pre><code>[SegmentedPointCloud]\nupstream_segmentation = \"Segmentation2D\"\nupstream_task = \"PointCloud\"\nuse_colmap_poses = true\n</code></pre>"},{"location":"plant_imager/specifications/configuration_files/#segmentation2d-task","title":"Segmentation2D task","text":"<p>Example:</p> <pre><code>[Segmentation2D]\nupstream_task = \"Undistorted\"\nquery = \"{\\\"channel\\\":\\\"rgb\\\"}\"\nmodel_id = \"Resnetdataset_gl_png_896_896_epoch50\"\nresize = true\nbinarize = true\ndilation = 1\nSx = 896\nSy = 896\nepochs = 1\nbatch = 1\nlearning_rate = 0.0001\nthreshold = 0.0035\n</code></pre>"},{"location":"plant_imager/specifications/configuration_files/#visualization-task","title":"Visualization task","text":"<p>Example:</p> <pre><code>[Visualization]\nmax_image_size = 1500\nmax_point_cloud_size = 10000\nthumbnail_size = 150\npcd_source = \"vox2pcd\"\nmesh_source = \"delaunay\"\n</code></pre>"},{"location":"plant_imager/specifications/configuration_files/#clusteredmesh-task","title":"ClusteredMesh task","text":"<p>Example:</p> <pre><code>[ClusteredMesh]\nupstream_task = \"SegmentedPointCloud\"\n</code></pre>"},{"location":"plant_imager/specifications/configuration_files/#anglesandinternodes-task","title":"AnglesAndInternodes task","text":"<p>Example:</p> <pre><code>[AnglesAndInternodes]\nupstream_task = \"ClusteredMesh\"\n</code></pre>"},{"location":"plant_imager/specifications/configuration_files/#evaluation-tasks","title":"Evaluation tasks","text":""},{"location":"plant_imager/specifications/configuration_files/#voxelgroundtruth","title":"VoxelGroundTruth","text":"<p>Example:</p>"},{"location":"plant_imager/specifications/configuration_files/#pointcloudgroundtruth","title":"PointCloudGroundTruth","text":"<p>Example:</p> <pre><code>[PointCloudGroundTruth]\npcd_size = 10000\n</code></pre>"},{"location":"plant_imager/specifications/configuration_files/#clusteredmeshgroundtruth","title":"ClusteredMeshGroundTruth","text":"<p>Example:</p>"},{"location":"plant_imager/specifications/configuration_files/#pointcloudevaluation-task","title":"PointCloudEvaluation task","text":"<p>Example:</p> <pre><code>[PointCloudEvaluation]\nmax_distance = 0.2\n</code></pre>"},{"location":"plant_imager/specifications/configuration_files/#pointcloudsegmentationevaluation-task","title":"PointCloudSegmentationEvaluation task","text":""},{"location":"plant_imager/specifications/configuration_files/#segmentation2devaluation-task","title":"Segmentation2DEvaluation task","text":""},{"location":"plant_imager/specifications/data/","title":"Data","text":"<p>This page describes how to use the romi package <code>plantdb</code> accessible here.</p> <p>A shared example datasets is accessible here.</p>"},{"location":"plant_imager/specifications/data/#getting-started","title":"Getting started","text":""},{"location":"plant_imager/specifications/data/#installation","title":"Installation","text":"<p>Warning</p> <p>If you intend to contribute to the development of <code>plantdb</code> or want to be able to edit the code and test your changes, you should choose editable mode.</p>"},{"location":"plant_imager/specifications/data/#non-editable-mode","title":"Non-editable mode","text":"<p>Install from GitHub using <code>pip</code>:</p> <pre><code>pip install git+ssh://git@github.com/romi/plantdb.git#dev\n</code></pre> <p>Note</p> <p>This uses <code>ssh</code> and thus requires to be registered as part of the project and to deploy ssh keys.</p>"},{"location":"plant_imager/specifications/data/#editable-mode","title":"Editable mode","text":"<p>Clone from GitHub and install using <code>pip</code>:</p> <pre><code>git clone https://github.com/romi/plantdb.git\ncd plantdb\npip install -e .\n</code></pre>"},{"location":"plant_imager/specifications/data/#minimal-working-example","title":"Minimal working example","text":"<p>Let's assume you have a list of images of a given object and that you want to add them to a ROMI database as a \"scan\".</p>"},{"location":"plant_imager/specifications/data/#1-initialize-database","title":"1 - Initialize database","text":"<p>First create the directory for the database and add the <code>romidb</code> marker to it:</p> <pre><code>from os.path import join\nfrom tempfile import mkdtemp\nmydb = mkdtemp(prefix='romidb_')\nopen(join(mydb, 'romidb'), 'w').close()\n</code></pre> <p>Now you can initialize a ROMI <code>FSDB</code> database object:</p> <pre><code>from plantdb.fsdb import FSDB\ndb = FSDB(mydb)\ndb.connect() # Locks the database and allows access\n</code></pre>"},{"location":"plant_imager/specifications/data/#2-create-a-new-dataset","title":"2 - Create a new dataset","text":"<p>To create a new dataset, here named <code>myscan_001</code>, do:</p> <pre><code>scan = db.create_scan(\"myscan_001\")\n</code></pre> <p>To add scan metadata (eg. camera settings, biological metadata, hardware metadata...), do:</p> <pre><code>scan.set_metadata({\"scanner\": {\"harware\": 'test'}})\n</code></pre> <p>This will results in several changes in the local database:</p> <ol> <li>Add a <code>myscan_001</code> sub-directory in the database root directory;</li> <li>Add a <code>metadata</code> sub-directory in <code>myscan_001</code> and a <code>metadata.json</code> gathering the given scan metadata.</li> </ol>"},{"location":"plant_imager/specifications/data/#3-add-images-as-new-fileset","title":"3 - Add images as new fileset","text":"<ol> <li> <p>OPTIONAL - create a list of RGB images If you do not have a scan datasets available, either download a shared datasets here or generate a list of images as follows:     <pre><code>import numpy as np\n# Generate random noise images\nn_images = 99\nimgs = []\nfor i in range(n_images):\n    img = 256 * np.random.rand(256, 256, 3)\n    img = np.array(img, dtype=np.uint8)\n    imgs.append(img)\n</code></pre></p> </li> <li> <p>Create a new <code>fileset</code>:     <pre><code>fileset = scan.create_fileset(\"images\")\n</code></pre></p> </li> <li> <p>Add the images to the fileset:    Load the file list (or skip if you generated random images):     <pre><code>from os import listdir\nimgs = listdir(\"&lt;/path/to/my/scan/images&gt;\")\n</code></pre></p> </li> <li> <p>Then loop the images list and add them to the <code>fileset</code>, optionally attach some metadata to each image:     <pre><code>from plantdb import io\nfor i, img in enumerate(imgs):\n    file = fileset.create_file(\"%i\"%i)\n    io.write_image(file, img)\n    file.set_metadata(\"key\", \"%i\"%i)\n</code></pre></p> </li> </ol> <p>This will results in several changes in the local database:</p> <ol> <li>Reference the image by its file name by adding an entry in <code>files.json</code>;</li> <li>Write a <code>scan_img_1.jpeg</code> image in the <code>images</code> sub-directory of the scan <code>\"myscan\"</code>.</li> <li>Add an <code>images</code> sub-directory in the <code>metadata</code> sub-directory, and JSON files with the image <code>id</code> as name to store the image metadata.</li> </ol>"},{"location":"plant_imager/specifications/data/#4-access-image-files-in-a-fileset","title":"4 - Access image files in a fileset","text":"<p>To access the image files in a fileset (in a datasets, itself in an existing and accessible database), proceed as follows:</p> <pre><code>from plantdb.fsdb import FSDB\ndb = FSDB(mydb)\ndb.connect() # Locks the database and allows access\n\nscan = db.get_scan(\"myscan\")\nfileset = scan.get_fileset(\"images\")\nfor f in fileset.get_files():\n    im = io.read_image(f) # reads image data\n    print(f.get_metadata(\"key\")) # i\n\n\ndb.disconnect()\n</code></pre>"},{"location":"plant_imager/specifications/data/#examples","title":"Examples","text":"<pre><code>from plantdb.fsdb import FSDB\nfrom plantdb import io\nimport numpy as np\n\n# Generate random noise images\nn_images = 100\nimgs = []\nfor i in range(n_images):\n    img = 256*np.random.rand(256, 256, 3)\n    img = np.array(img, dtype=np.uint8)\n    imgs.append(img)\n\nfrom os import listdir\nfrom os.path import join\nfrom tempfile import mkdtemp\n# Create a temporary DB directory:\nmydb = mkdtemp(prefix='romidb_')\n# Create the `romidb` file in previously created temporary DB directory:\nopen(join(mydb, 'romidb'), 'w').close()\nlistdir(mydb)\n\n# Connect to the DB:\ndb = FSDB(mydb)\ndb.connect() # Locks the database and allows access\n\n# Add a scan datasets to the DB:\nscan = db.create_scan(\"myscan_001\")\nlistdir(mydb)\n# Add metadata to a scan datasets:\nscan.set_metadata({\"scanner\": {\"hardware\": 'test'}})\nlistdir(join(mydb, \"myscan_001\"))\nlistdir(join(mydb, \"myscan_001\", \"metadata\"))\n\nfileset = scan.create_fileset(\"images\")\nlistdir(join(mydb, \"myscan_001\"))\n\nfor i, img in enumerate(imgs):\n    file = fileset.create_file(\"%i\"%i)\n    io.write_image(file, img)\n    file.set_metadata(\"key\", \"%i\"%i) # Add some metadata\n\n# read files in the fileset:\nscan = db.get_scan(\"myscan\")\nfileset = scan.get_fileset(\"images\")\nfor f in fileset.get_files():\n    im = io.read_image(f) # reads image data\n    print(f.get_metadata(\"key\")) # i\n\ndb.disconnect()\n</code></pre>"},{"location":"plant_imager/specifications/data/#database-structure","title":"Database structure","text":""},{"location":"plant_imager/specifications/data/#overview","title":"Overview","text":"<p>Hereafter we give an overview of the database structure using the ROMI database terminology:</p> <pre><code>plantdb_root/\n\u251c\u2500\u2500 dataset_001/\n\u2502   \u251c\u2500\u2500 fileset_A/\n\u2502   \u2502   \u251c\u2500\u2500 file_A_001.ext\n\u2502   \u2502   \u251c\u2500\u2500 [...]\n\u2502   \u2502   \u2514\u2500\u2500 file_A_009.ext\n\u2502   \u251c\u2500\u2500 fileset_B/\n\u2502   \u2502   \u251c\u2500\u2500 file_B_001.ext\n\u2502   \u2502   \u251c\u2500\u2500 [...]\n\u2502   \u2502   \u2514\u2500\u2500 file_B_009.ext\n\u2502   \u251c\u2500\u2500 metadata\n\u2502   \u2502   \u251c\u2500\u2500 fileset_A.json\n\u2502   \u2502   \u251c\u2500\u2500 fileset_B.json\n\u2502   \u2502   \u2514\u2500\u2500 metadata.json\n\u2502   \u2514\u2500\u2500 files.json\n\u251c\u2500\u2500 dataset_002/\n\u2502   \u2514\u2500\u2500 [...]\n\u251c\u2500\u2500 [...]\n\u251c\u2500\u2500 (lock)\n\u2514\u2500\u2500 romidb\n</code></pre>"},{"location":"plant_imager/specifications/data/#database-root-directory","title":"Database root directory","text":"<p>A root database directory is defined, eg. <code>mydb/</code>. Inside this directory we need to define (add) the <code>romidb</code> marker, so it may be used by <code>FSDB</code> class. We may also find the <code>lock</code> file used to limit the access to the database to only one user.</p> <p>Note that the database initialization part is manual. To create them, in a terminal:</p> <pre><code>mkdir mydb\ntouch mydb/romidb\n</code></pre> <p>We just created the following tree structure:</p> <pre><code>mydb/\n\u2514\u2500\u2500 romidb\n</code></pre> <p>Once you have created this root directory and the <code>romidb</code> marker file, you can initialize a ROMI <code>FSDB</code> database object in Python:</p> <pre><code>from plantdb.fsdb import FSDB\ndb = FSDB(\"mydb\")\ndb.connect()\n</code></pre> <p>The method <code>FSDB.connect()</code> locks the database with a <code>lock</code> file at root directory and allows access. To disconnect and free the database do:</p> <pre><code>db.disconnect()\n</code></pre> <p>If for some reason the Python terminal unexpectedly terminate without a call to the <code>disconnect</code> method, you may have to remove the <code>lock</code> file manually. Check that no one else is using the database!</p> <p>Within this root database directory you will find other directories corresponding to datasets.</p>"},{"location":"plant_imager/specifications/data/#datasets-directories","title":"Datasets directories","text":"<p>At the next level, we find the datasets directory(s), eg. named <code>myscan_001</code>. Their names must be uniques, and you create them as follows: <pre><code>scan = db.create_scan(\"myscan_001\")\n</code></pre></p> <p>If you add scan metadata (eg. camera settings, biological metadata, hardware metadata...) with <code>scan.set_metadata()</code>, you get another directory <code>metadata</code> with a <code>metadata.json</code> file.</p> <pre><code>scan.set_metadata({\"scanner\": {\"hardware\": 'test'}})\n</code></pre> <p>We now have the following tree structure:</p> <pre><code>mydb/\n\u251c\u2500\u2500 myscan_001/\n\u2502   \u2514\u2500\u2500 metadata/\n\u2502       \u2514\u2500\u2500 metadata.json\n\u2514\u2500\u2500 romidb\n</code></pre> <p>And the file <code>metadata.json</code> should look like this:</p> <pre><code>{\n\"scanner\": {\n\"hardware\": \"test\"\n}\n}\n</code></pre>"},{"location":"plant_imager/specifications/data/#images-directories","title":"Images directories","text":"<p>Inside <code>myscan_001/</code>, we find the datasets or fileset in <code>plantdb</code> terminology. In the case of the \"plant scanner\", this is a list of RGB image files acquired by a camera moving around the plant. To store the datasets, we thus name the created fileset \"images\":</p> <pre><code>fileset = scan.create_fileset(\"images\")\n</code></pre> <p>This creates an <code>images</code> directory and a <code>files.json</code> at the dataset root directory. We now have the following tree structure:</p> <pre><code>mydb/\n\u251c\u2500\u2500 myscan_001/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 metadata/\n\u2502   \u2502   \u2514\u2500\u2500 metadata.json\n\u2502   \u2514\u2500\u2500 files.json\n\u2514\u2500\u2500 romidb\n</code></pre> <p>The JSON should look like this:</p> <pre><code>{\n\"filesets\": [\n{\n\"files\": [],\n\"id\": \"images\"\n}\n]\n}\n</code></pre> <p>We then create random RGB images to add to the dataset:</p> <pre><code>import numpy as np\n\n# Generate random noise images\nn_images = 100\nimgs = []\nfor i in range(n_images):\n    img = 256*np.random.rand(256, 256, 3)\n    img = np.array(img, dtype=np.uint8)\n    imgs.append(img)\n</code></pre> <p>And we add them with their metadata to the database:</p> <pre><code>for i, img in enumerate(imgs):\n    fname = f\"img_{str(i).zfill(2)}.png\"\n    file = fileset.create_file(fname)\n    io.write_image(file, img)\n    file.set_metadata(\"key\", fname)\n    file.set_metadata(\"id\", i)\n</code></pre> <p>Inside this <code>images/</code> directory will reside the images added to the database. At the same time you added images with REF_TO_TUTO, you created an entry in a JSON file referencing the files. If you added metadata along with the files (eg. camera poses, jpeg metadata...) it should be referenced in <code>metadata/images/</code> eg. <code>metadata/images/&lt;scan_img_01&gt;.json</code>.</p> <pre><code>mydb/\n\u251c\u2500\u2500 myscan_001/\n\u2502   \u251c\u2500\u2500 files.json\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 scan_img_01.jpg\n\u2502   \u2502   \u251c\u2500\u2500 scan_img_02.jpg\n\u2502   \u2502   \u251c\u2500\u2500 [...]\n\u2502   \u2502   \u2514\u2500\u2500 scan_img_99.jpg\n\u2502   \u251c\u2500\u2500 metadata/\n\u2502   \u2502   \u251c\u2500\u2500 images\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 scan_img_01.json\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 scan_img_02.json\n\u2502   \u2502   \u251c\u2500\u2500 [...]\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 scan_img_99.json\n\u2502   \u2502   \u2514\u2500\u2500 metadata.json\n\u2514\u2500\u2500 romidb\n</code></pre>"},{"location":"plant_imager/specifications/data/#example","title":"Example","text":"<pre><code>mydb/\n\u251c\u2500\u2500 myscan_001/\n\u2502   \u251c\u2500\u2500 AnglesAndInternodes_1_0_2_0_0_1_dd8d67653a\n\u2502   \u2502   \u2514\u2500\u2500 AnglesAndInternodes.json\n\u2502   \u251c\u2500\u2500 Colmap_True____feature_extrac_3bbfcb1413\n\u2502   \u2502   \u251c\u2500\u2500 cameras.json\n\u2502   \u2502   \u251c\u2500\u2500 images.json\n\u2502   \u2502   \u251c\u2500\u2500 points3d.json\n\u2502   \u2502   \u2514\u2500\u2500 sparse.ply\n\u2502   \u251c\u2500\u2500 CurveSkeleton_out__TriangleMesh_6a92751c20\n\u2502   \u2502   \u2514\u2500\u2500 CurveSkeleton.json\n\u2502   \u251c\u2500\u2500 files.json\n\u2502   \u251c\u2500\u2500 images\n\u2502   \u2502   \u251c\u2500\u2500 pict20190201_110110_0.jpg\n\u2502   \u2502   \u251c\u2500\u2500 [...]\n\u2502   \u2502   \u2514\u2500\u2500 pict20190201_111209_0.jpg\n\u2502   \u251c\u2500\u2500 Masks_True_5_out_9adb9db801\n\u2502   \u2502   \u251c\u2500\u2500 pict20190201_110110_0.jpg\n\u2502   \u2502   \u251c\u2500\u2500 [...]\n\u2502   \u2502   \u2514\u2500\u2500 pict20190201_111209_0.jpg\n\u2502   \u251c\u2500\u2500 measures.csv\n\u2502   \u251c\u2500\u2500 metadata\n\u2502   \u2502   \u251c\u2500\u2500 AnglesAndInternodes_1_0_2_0_0_1_dd8d67653a.json\n\u2502   \u2502   \u251c\u2500\u2500 Colmap_True____feature_extrac_3bbfcb1413.json\n\u2502   \u2502   \u251c\u2500\u2500 CurveSkeleton_out__TriangleMesh_6a92751c20.json\n\u2502   \u2502   \u251c\u2500\u2500 images\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 pict20190201_110110_0.json\n\u2502   \u2502   \u251c\u2500\u2500 [...]\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 pict20190201_111209_0.json\n\u2502   \u2502   \u251c\u2500\u2500 images.json\n\u2502   \u2502   \u251c\u2500\u2500 Masks_True_5_out_9adb9db801\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 pict20190201_110110_0.json\n\u2502   \u2502   \u251c\u2500\u2500 [...]\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 pict20190201_111209_0.json\n\u2502   \u2502   \u251c\u2500\u2500 Masks_True_5_out_e90d1804eb.json\n\u2502   \u2502   \u251c\u2500\u2500 metadata.json\n\u2502   \u2502   \u251c\u2500\u2500 PointCloud_1_0_1_0_False_9ab5a15d9b\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 PointCloud.json\n\u2502   \u2502   \u251c\u2500\u2500 PointCloud_1_0_1_0_False_9ab5a15d9b.json\n\u2502   \u2502   \u251c\u2500\u2500 PointCloud__200_0_1_0_False_4ce2e46446.json\n\u2502   \u2502   \u251c\u2500\u2500 TreeGraph_out__CurveSkeleton_5dca9a2821.json\n\u2502   \u2502   \u251c\u2500\u2500 TriangleMesh_out__PointCloud_80dc94ac81.json\n\u2502   \u2502   \u251c\u2500\u2500 Undistorted_out_____fb3e3fa0ff\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 pict20190201_110110_0.json\n\u2502   \u2502   \u251c\u2500\u2500 [...]\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 pict20190201_111209_0.json\n\u2502   \u2502   \u251c\u2500\u2500 Undistorted_out_____fb3e3fa0ff.json\n\u2502   \u2502   \u251c\u2500\u2500 Voxels_False____False_567dc7f48b\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 Voxels.json\n\u2502   \u2502   \u251c\u2500\u2500 Voxels_False____False_567dc7f48b.json\n\u2502   \u2502   \u251c\u2500\u2500 Voxels_False____True_af037e876e.json\n\u2502   \u2502   \u2514\u2500\u2500 Voxels_False____True_cd9a5ff06b.json\n\u2502   \u251c\u2500\u2500 pipeline.toml\n\u2502   \u251c\u2500\u2500 PointCloud_1_0_1_0_False_9ab5a15d9b\n\u2502   \u2502   \u2514\u2500\u2500 PointCloud.ply\n\u2502   \u251c\u2500\u2500 TreeGraph_out__CurveSkeleton_5dca9a2821\n\u2502   \u2502   \u2514\u2500\u2500 TreeGraph.p\n\u2502   \u251c\u2500\u2500 TriangleMesh_out__PointCloud_80dc94ac81\n\u2502   \u2502   \u2514\u2500\u2500 TriangleMesh.ply\n\u2502   \u2514\u2500\u2500 Voxels_False____False_567dc7f48b\n\u2502       \u2514\u2500\u2500 Voxels.npz\n\u251c\u2500\u2500 colmap_log.txt\n\u251c\u2500\u2500 lock\n\u2514\u2500\u2500 romidb\n</code></pre>"},{"location":"plant_imager/specifications/hardware/","title":"Hardware setup and instructions","text":""},{"location":"plant_imager/specifications/hardware/#network-overview","title":"Network overview","text":"<p>The general network design of the ROMI Plant Imager is as the following:</p> <p></p> <p>The raspberry pi controls the movements of the camera thanks to the CNC (for the x,y,z coordinates) and the Gimbal (pan and tilt). Both of them are connected to the pi by USB cables. For the camera, several configurations exists. It is possible to retrieve the photos either by Wi-Fi (might lead to a lower resolution) or directly via a micro USB.</p>"},{"location":"plant_imager/specifications/hardware/#hardware-configuration-files","title":"Hardware configuration files","text":"<p>To gather configuration information on the hardware during an acquisition with the plant imager we use <code>toml</code> files.</p> <p>For example, saving the following lines in a <code>config.toml</code>:</p> <pre><code>[ScanPath]\nclass_name = \"Circle\" # Circle, Line, Cylinder\n\n[ScanPath.kwargs]\ncenter_x = 375\ncenter_y = 375\nz = 80\ntilt = 0\nradius = 300\nn_points = 60\n\n[Scan.scanner.cnc]\nmodule = \"plantimager.grbl\"\n\n[Scan.scanner.cnc.kwargs]\nhoming = true\nport = \"/dev/ttyACM0\"\n\n[Scan.scanner.gimbal]\nmodule = \"plantimager.blgimbal\"\n\n[Scan.scanner.gimbal.kwargs]\nport = \"/dev/ttyACM1\"\nhas_tilt = false\nzero_pan = 0\ninvert_rotation = true\n\n[Scan.scanner.camera]\nmodule = \"plantimager.sony\"\n\n[Scan.scanner.camera.kwargs]\ndevice_ip = \"192.168.122.1\"\napi_port = \"10000\"\npostview = true\nuse_flashair = false\nrotation = 270\n\n[Scan.metadata.object]\nspecies = \"chenopodium album\"\nseed_stock = \"Col-0\"\nplant_id = \"3dt_chenoA\"\ngrowth_environment = \"Lyon-indoor\"\ngrowth_conditions = \"SD+LD\"\ntreatment = \"None\"\nDAG = 40\nsample = \"main_stem\"\nexperiment_id = \"3dt_2021-01\"\ndataset_id = \"3dt\"\n\n[Scan.metadata.hardware]\nframe = \"30profile v1\"\nX_motor = \"X-Carve NEMA23\"\nY_motor = \"X-Carve NEMA23\"\nZ_motor = \"X-Carve NEMA23\"\npan_motor = \"iPower Motor GM4108H-120T Brushless Gimbal Motor\"\ntilt_motor = \"None\"\nsensor = \"RX0\"\n\n[Scan.metadata.workspace]\nx = [200, 600, ]\ny = [200, 600, ]\nz = [-100, 300, ]\n</code></pre> <p>Some arguments in this example have default values and for others (commented \"mandatory\" in the following description) it has to be specified in the configuration file.</p> <p>Here, a more detailed explanation with a full default parameters list:</p> <ul> <li>The acquisition path:</li> </ul> <pre><code>[ScanPath] # mandatory\nclass_name = \"Circle\"\n</code></pre> <p><code>class_name</code> defines the type of path the robotic arm will follow. In this case it will be a circle, the other possibilities are commented next to the variable in the example above.</p> <pre><code>[ScanPath.kwargs] # mandatory\ncenter_x = 375\ncenter_y = 375\nz = 80\ntilt = 0\nradius = 300\nn_points = 60\n</code></pre> <p>The kwargs related to the path are in this section. The arm will perform a circle of 300 around the point (375, 375) with a fixed z (80) and a tilt of 0\u00b0. The angle between each pose will be 5\u00b0 because the <code>n_points</code> is 60 on a 360\u00b0 circle. The <code>center_x</code>, <code>center_y</code>, <code>z</code> and <code>radius</code> parameters are expressed in mm and are related to the axis of the CNC. To have an idea of valid values for those it's possible to get the limits of the CNC axis with steps described in the cnc calibration description.</p> <ul> <li>Needed parameters for connection between hardware components (CNC, Gimbal and camera) and software:</li> </ul> <pre><code>[Scan.scanner.cnc] # mandatory\nmodule = \"plantimager.grbl\"\n</code></pre> <p>Here for example for the CNC you will have to inform about the python <code>module</code> required to connect to the hardware. It will depend on the type of the device.</p> <pre><code>[Scan.scanner.cnc.kwargs]\nhoming = true\nport = \"/dev/ttyUSB0\"\nbaud_rate = 115200\nx_lims = None\ny_lims = None\nz_lims = None\nsafe_start = True\ninvert_x = true\ninvert_y = true\ninvert_z = true\n</code></pre> <p>The arguments all have default values here, but you might need to change the <code>port</code> (check with <code>dmesg -w</code>).</p> <pre><code>[Scan.scanner.gimbal] # mandatory\nmodule = \"plantimager.blgimbal\"\n\n[Scan.scanner.gimbal.kwargs]\nport = \"/dev/ttyUSB0\"\nhas_tilt = True\nsteps_per_turn = 360\nzero_pan = 0\nzero_tilt = 0\ninvert_rotation = False\n</code></pre> <p>Similarly, for the Gimbal, again with default arguments that could be changed depending on your setup</p> <pre><code>[Scan.scanner.camera] # mandatory\nmodule = \"plantimager.sony\" # or plantimager.gp2\n\n[Scan.scanner.camera.kwargs]\ndevice_ip = \"192.168.122.1\" # mandatory\napi_port = \"10000\" # mandatory\ntimeout: time_s = 10\npostview = false\nuse_adb = false\nuse_flashair = false\nflashair_host = None\ncamera_params = None\nrotation = 0\n</code></pre> <p>Finally, the camera (in this case the SONY RX0 communicating via Wi-Fi) with more specific arguments that will depend on the type of sensor used. A more precise documentation on several cameras and their associated parameters can be found here</p> <ul> <li> <p>Object metadata:   In principle, you can put any information that appear important as part of an experiment but to have a guideline of relevant parameters in the context of phenotyping you might want to check the biological metadata documentation</p> </li> <li> <p>Hardware metadata:   Again here, some guidelines for this section can be found in the hardware metadata   description.</p> </li> </ul> <pre><code>[Scan.metadata.workspace] # mandatory\nx = [200, 600, ]\ny = [200, 600, ]\nz = [-100, 300, ]\n</code></pre> <p>Concerning the workspace, it is not properly required for the scan to perform but if a reconstruction is to be made it will be needed. As for the path, appropriate coordinates can be collected from information contained in the cnc calibration.</p> <p>To load the config file in python:</p> <pre><code>&gt;&gt; &gt; import toml\n&gt;&gt; &gt; conf = toml.load(open('config.toml'))\n&gt;&gt; &gt; print(conf)\n{'Scan': {'scanner': {'camera_firmware': 'sony_wifi', 'cnc_firmware': 'grbl-v1.1', 'gimbal_firmware': 'blgimbal'}}}\n&gt;&gt; &gt; print(conf[\"Scan\"][\"scanner\"][\"camera_firmware\"])\n\"sony_wifi\"\n</code></pre>"},{"location":"plant_imager/specifications/hardware/#pizero-camera-rovercam","title":"PiZero camera <code>rovercam</code>","text":"<p>WORK IN PROGRESS!!!!!</p>"},{"location":"plant_imager/specifications/hardware/#configuring-the-access-point-host-software-hostapd","title":"Configuring the access point host software (hostapd)","text":"<p>Source: Raspberry Foundation website.</p>"},{"location":"plant_imager/specifications/hardware/#1-general-setup","title":"1. General setup","text":"<p>Switch over to <code>systemd-networkd</code>:</p> <pre><code># remove classic networking\nsudo apt --autoremove purge ifupdown dhcpcd5 isc-dhcp-client isc-dhcp-common\nrm -r /etc/network /etc/dhcp\n\n# enable systemd-networkd\nsystemctl enable systemd-networkd.service\n\n# setup systemd-resolved\nsystemctl enable systemd-resolved.service\napt --autoremove purge avahi-daemon\napt install libnss-resolve\nln -sf /run/systemd/resolve/stub-resolv.conf /etc/resolv.conf\n</code></pre>"},{"location":"plant_imager/specifications/hardware/#2-configure-wpa_supplicant-as-access-point","title":"2. Configure <code>wpa_supplicant</code> as access point","text":"<p>To configure <code>wpa_supplicant</code> as access point create this file with your settings for <code>country=</code>, <code>ssid=</code>, <code>psk=</code> and maybe <code>frequency=</code>. You can just copy and paste this in one block to your command line beginning with cat and including both EOF (delimiter EOF will not get part of the file):</p> <pre><code>cat &gt; /etc/wpa_supplicant/wpa_supplicant-wlan0.conf &lt;&lt;EOF\ncountry=DE\nctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev\nupdate_config=1\n\nnetwork={\n    ssid=\"RPiNet\"\n    mode=2\n    frequency=2437\n    #key_mgmt=NONE   # uncomment this for an open hotspot\n    # delete next 3 lines if key_mgmt=NONE\n    key_mgmt=WPA-PSK\n    proto=RSN WPA\n    psk=\"password\"\n}\nEOF\n</code></pre> <pre><code>chmod 600 /etc/wpa_supplicant/wpa_supplicant-wlan0.conf\nsystemctl disable wpa_supplicant.service\nsystemctl enable wpa_supplicant@wlan0.service\n</code></pre>"},{"location":"plant_imager/specifications/hardware/#setting-up-a-stand-alone-access-point","title":"Setting up a stand alone access point","text":"<p>Example for this setup:</p> <p><pre><code>                 wifi\nmobile-phone &lt;~.~.~.~.~&gt; (wlan0)RPi(eth0)\n\\             /\n           (dhcp)   192.168.4.1\n</code></pre> Do \"General setup\" then create the following file to configure <code>wlan0</code>. We only have the access point. There is no ethernet device configured.</p> <p><pre><code>cat &gt; /etc/systemd/network/08-wlan0.network &lt;&lt;EOF\n[Match]\nName=wlan0\n[Network]\nAddress=192.168.4.1/24\nMulticastDNS=yes\nDHCPServer=yes\nEOF\n</code></pre> If you want this then reboot. That's it. Otherwise, go on, no need to reboot this time.</p>"},{"location":"plant_imager/specifications/hardware/#troubleshooting","title":"Troubleshooting","text":""},{"location":"plant_imager/specifications/hardware/#serial-access-denied","title":"Serial access denied","text":"<p>Look here if you can not communicate with the scanner using usb.</p>"},{"location":"plant_imager/specifications/luigi/","title":"Luigi in the ROMI Plant Scanner project","text":"<p>Hereafter we explain how we used the <code>luigi</code> Python package in the ROMI Plant Scanner project. If you are not familiar with <code>luigi</code>, let's just say that it is useful to create long-running batch processes where you want to chain many tasks with dependencies and requirements.</p> <p>In the context of the ROMI Plant Scanner project, we faced the challenge of creating complex pipelines, especially for the 3D reconstruction &amp; analysis of a plant structure after its acquisition in the form of a series of RGB images. Several complex and fairly distinct algorithm are required to achieve our goals, and we thus decided to break down this chain of tasks to achieve greater modularity and robustness.</p> <p>Using <code>luigi</code> led us to abstract several concepts like <code>Task</code>, <code>Target</code> &amp; <code>Parameter</code>:</p> <ul> <li>a task is limited to a single algorithmic operation with input(s), output(s) &amp; parameter(s);</li> <li>a target is a (set of) file(s) that can be the input required by a task or (one of) its output(s);</li> <li>a parameter is a value controlling the algorithm;</li> </ul>"},{"location":"plant_imager/specifications/luigi/#parameters-configuration","title":"Parameters &amp; configuration","text":"<p>As we run <code>luigi</code> using the command-line tool, and the constructed workflow can be made of many tasks each with several parameters, we use TOML configuration files to define them. In addition to the TOML configuration file, the <code>romi_run_task</code> script requires the definition of two values:</p> <ul> <li>the name of the ROMI task to run;</li> <li>the name of the <code>Scan</code> dataset on which to run the ROMI task;</li> </ul> <p>As we will see later, each \"computational task\" has an upstream task. Using these tasks dependencies luigi will create the required workflow. Hence you do not have to know or defines the required workflow to run a task, just call it and luigi will do the rest for you!</p>"},{"location":"plant_imager/specifications/luigi/#target-subclass","title":"Target subclass","text":"<p>In order to check for a task requirement(s) or handle its output(s) <code>luigi</code> implement the <code>Target</code> class. As we use our own Python database implementation <code>FSDB</code> from <code>plantdb</code>, and it implements the concept of a set of files as a <code>Fileset</code> class, we subclassed the <code>luigi.Target</code> class as <code>TargetFileset</code>. It is thus used to get/create files from the <code>FSDB</code> database by our <code>luigi.Task</code> subclasses.</p> <p>For example, the raw RGB images set obtained after a <code>Scan</code> task is the <code>'images'</code> <code>Fileset</code>.</p>"},{"location":"plant_imager/specifications/luigi/#task-subclasses","title":"Task subclasses","text":"<p>This is where the computation is done with the <code>run()</code> method and targets are controlled with the <code>requires()</code> &amp; <code>output()</code> methods.</p> <p>Note</p> <p>ROMI tasks do not use a <code>Scan</code> dataset identifier as it is assumed that they only work on one <code>Scan</code> at a time.</p>"},{"location":"plant_imager/specifications/pipelines/","title":"Pipelines","text":"<p>Warning</p> <p>This is a work in progress... the original author has no idea what he is doing!</p>"},{"location":"plant_imager/specifications/pipelines/#legend","title":"Legend","text":"<p>Let's start with a description of the used symbols: </p> <ul> <li>Note shaped boxes are <code>RomiConfig</code>, they are TOML files that contains parameters for each task.</li> <li>Round shaped boxes are <code>RomiTasks</code> with their name on the first level, then the module names (<code>--module</code> option in <code>romi_run_task</code>) and a quick description of the tasks at hand.</li> <li>Folder shaped boxes are <code>RomiTarget</code>, they indicate files input/output and the file extension is given between parenthesis.</li> </ul>"},{"location":"plant_imager/specifications/pipelines/#acquisitions","title":"Acquisitions","text":""},{"location":"plant_imager/specifications/pipelines/#acquisition-of-real-plant-datasets","title":"Acquisition of real plant datasets","text":""},{"location":"plant_imager/specifications/pipelines/#acquisition-of-virtual-plant-datasets","title":"Acquisition of virtual plant datasets","text":""},{"location":"plant_imager/specifications/pipelines/#plant-reconstruction-from-rgb-images","title":"Plant Reconstruction from RGB images","text":""},{"location":"plant_imager/specifications/pipelines/#3d-plant-phenotyping","title":"3D Plant Phenotyping","text":""},{"location":"plant_imager/specifications/pipelines/#geometric-approach","title":"Geometric approach","text":""},{"location":"plant_imager/specifications/pipelines/#machine-learning-approach","title":"Machine Learning approach","text":""},{"location":"plant_imager/specifications/pipelines/#evaluation","title":"Evaluation","text":""},{"location":"plant_imager/specifications/pipelines/#mask-task-evaluation","title":"Mask task evaluation","text":""},{"location":"plant_imager/specifications/pipelines/#voxel-task-evaluation","title":"Voxel task evaluation","text":""},{"location":"plant_imager/specifications/pipelines/#pointcloud-task-evaluation","title":"PointCloud task evaluation","text":""},{"location":"plant_imager/specifications/virtual-plant-imager/","title":"Instructions and main specifications for the virtual plant imager","text":"<p>Warning</p> <p>clearly separate Lpy and the VPI</p>"},{"location":"plant_imager/specifications/virtual-plant-imager/#general-description","title":"General description","text":"<p>The <code>Virtual Plant Imager</code> functions as a digital twin of the real romi robot <code>Plant imager</code>: it takes RGB images of one to several virtual 3D plant models and generate all data and metadata necessary to proceed downstream with a training of a neural network or with an analysis pipeline of the <code>plant-3d-vision</code> tool suite.</p> <ul> <li>As input, it takes a 3D model (<code>.obj</code>) file</li> <li>As output, it provides one to several datasets in a romi database format, ready for downstream use (training or analysis)</li> </ul> <pre><code>db/\n\u251c\u2500\u2500 virtual_imageset/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 metadata/\n\u2502   \u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u2514\u2500\u2500 images.json\n\u2502   \u251c\u2500\u2500 files.json\n\u2502   \u2514\u2500\u2500 scan.json\n\u2514\u2500\u2500 romidb\n</code></pre> <p>The <code>Virtual Plant Imager</code> relies on Blender v2.93 to generate at set of (2D) RGB images from a plant 3D model. An HTTP server acts as an interface to drive Blender generation scripts.</p> <p>Note</p> <p>The <code>Virtual Plant Imager</code> is closely integrated with the plant generator L-Py. For information related to the generation of virtual 3D model of plants, you will be redirected to other LPy documentation.</p>"},{"location":"plant_imager/specifications/virtual-plant-imager/#input-data-for-romi_run_task-virtualscan","title":"Input data (for romi_run_task VirtualScan)","text":"<p>As for all ROMI tools, the <code>Virtual Plant Imager</code> requires a proper database to store, access and generate new data. Let's call <code>virtual_db</code> this database. In particular, it contains data for the virtual plant generation and/or imaging grouped in a so-called <code>vscan_data</code> folder:</p> <p>Legend:</p> <ul> <li>(*) the name is fixed and cannot be changed</li> <li>(!) the folder/file must exist (no tag means that the folder is not required for the program to run)</li> </ul> <pre><code>virtual_db\n\u251c\u2500\u2500 vscan_data/ (!*) \n\u2502   \u251c\u2500\u2500 hdri/ (*)\n\u2502   \u2502   \u251c\u2500\u2500 hdri_file1.hdr\n\u2502   \u2502   \u251c\u2500\u2500 hdri_file2.hdr\n\u2502   \u2502   \u2514\u2500\u2500 etc...\n\u2502   \u251c\u2500\u2500 lpy/ (*)\n\u2502   \u2502   \u2514\u2500\u2500 my_plant_species_model.lpy\n\u2502   \u251c\u2500\u2500 obj/ (*)\n\u2502   \u2502   \u251c\u2500\u2500 VirtualPlant.obj\n\u2502   \u2502   \u2514\u2500\u2500 VirtualPlant_mtl\n\u2502   \u251c\u2500\u2500 metadata/ (!*)\n\u2502   \u2502   \u251c\u2500\u2500 hdri.json\n\u2502   \u2502   \u251c\u2500\u2500 lpy.json\n\u2502   \u2502   \u251c\u2500\u2500 obj.json\n\u2502   \u2502   \u251c\u2500\u2500 palette.json\n\u2502   \u2502   \u2514\u2500\u2500 scenes.json\n\u2502   \u251c\u2500\u2500 palette/ (*)\n\u2502   \u2502   \u2514\u2500\u2500 my_plant_species_model.png\n\u2502   \u2514\u2500\u2500 scenes/ (*)\n\u251c\u2500\u2500 files.json\n\u2514\u2500\u2500 romidb (!*) # a (empty) marker file for recognition by the plantdb module\n</code></pre> <p>The following sections detail the content of each file and subfolder.</p>"},{"location":"plant_imager/specifications/virtual-plant-imager/#hdri","title":"hdri","text":"<p>If no HDRI files are provided, a uniform black background will be applied. Even if they are provided, HDRI backgrounds can be deactivated in the <code>.toml</code> file (related romi_run task: <code>VirtualScan</code>) and the default black background will be applied.</p> <p>New background HDRI files can be downloaded from hdri haven and store in the <code>hdri</code> folder. Limited resolution is enough for downstream applications (we recommend 2K resolution, 6.2 MB total download).</p> <p>Note</p> <p>If there are several hdri files -&gt; which one is used ?</p>"},{"location":"plant_imager/specifications/virtual-plant-imager/#lpy","title":"lpy","text":"<p><code>.lpy</code> files contain a parametric model of a plant that Lpy will use to generate a 3D model as an <code>.obj</code> file. With the above data_example, we provide the file <code>arabidopsis_notex.lpy</code>, a LPy model for the laboratory model plant Arabidopsis thaliana (copyright C. Godin, Inria - RDP Mosaic).</p> <p>You can replace this file with any other LPy model file. The correct name of the file must then be specified in the configuration .toml file (see below)</p> <pre><code>[VirtualPlant]\nlpy_file_id = \"arabidopsis_notex\" #base name of the .lpy to be used by Lpy\n</code></pre> <p>If you do not want to use Lpy to generate your virtual plant, you can also directly import your custom 3D plant (generated elsewhere), which must be at the <code>.obj</code> format. In this case, lpy subfolder is dispensable, store data in the <code>obj</code> subfolder (see next)</p>"},{"location":"plant_imager/specifications/virtual-plant-imager/#obj","title":"obj","text":"<p>Alternatively to Lpy-generated, custom 3D plant model can be provided. Data must consist in an <code>.obj</code> and an <code>.mtl</code> file.</p> <p>In the <code>.obj</code>, each semantic type of label desired to segment the plant must correspond to a distinct mesh. Each of these meshes must have a single material whose name is the name of the label.</p> <p>(note QUESTION: and then, what are the groundtruth ? how are they provided ?)</p>"},{"location":"plant_imager/specifications/virtual-plant-imager/#metadata","title":"metadata","text":"<p>lpy.json <pre><code>{\n\"task_params\": {\n\"output_file_id\": \"out\",\n\"scan_id\": \"vscan_data\"\n}\n}\n</code></pre> other files (hdri/palette/scenes.json): <pre><code>{\n\"task_parameters\": {},\n\"task_params\": {\n\"output_file_id\": \"out\",\n\"scan_id\": \"vscan_data\"\n}\n}\n</code></pre></p>"},{"location":"plant_imager/specifications/virtual-plant-imager/#palette","title":"palette","text":"<p>a <code>.png</code> file containing textures that will LPy will apply on the virtual plant.</p>"},{"location":"plant_imager/specifications/virtual-plant-imager/#scenes","title":"scenes","text":""},{"location":"plant_imager/specifications/virtual-plant-imager/#filesjson","title":"files.json","text":"<p>fileset descriptor with \"id\" definitions.</p> <pre><code>{\n\"filesets\": [\n{\n\"files\": [\n{\n\"file\": \"felsenlabyrinth_2k.hdr\",\n\"id\": \"felsenlabyrinth_2k\"\n},\n{\n\"file\": \"forest_slope_2k.hdr\",\n\"id\": \"forest_slope_2k\"\n}\n],\n\"id\": \"hdri\"\n},\n{\n\"files\": [\n{\n\"file\": \"arabidopsis_notex.lpy\",\n\"id\": \"arabidopsis_notex\"\n}\n],\n\"id\": \"lpy\"\n},\n{\n\"etc...\"\n}\n]    }\n</code></pre>"},{"location":"plant_imager/specifications/virtual-plant-imager/#running-the-virtual-plant-imager","title":"Running the virtual plant imager","text":""},{"location":"plant_imager/specifications/virtual-plant-imager/#basic-command-lines","title":"Basic command lines","text":"<p>Corresponding task with romi_run_task:  <code>VirtualScan</code> <code>shell romi_run_task VirtualScan \\   --config vpi_single_dataset.toml  \\   path/to/db/generated_dataset</code> the content of the configuration file <code>vpi_single_dataset.toml</code> will be detailed un the next section</p>"},{"location":"plant_imager/specifications/virtual-plant-imager/#configuration-file-for-the-task-virtualscan","title":"Configuration file for the Task VirtualScan","text":"<p>see also the page on specifications&gt;tasks&gt;VPI (does not exist yet, to be created)</p>"},{"location":"plant_imager/specifications/tasks/","title":"Task definitions","text":"<p>As we have seen previously, we are using the luigi paradigm and have defined a series of tasks to create flexible and modular pipelines.</p> <p>This section is a more general overview, For more details see the reference documentation (TODO)!</p>"},{"location":"plant_imager/specifications/tasks/#base-task-class","title":"Base task class","text":"<p><code>RomiTask</code> is the base abstract class for the ROMI Plant Scanner project and subclass <code>luigi.Task</code>.</p> <p>It implements the following methods:</p> <ul> <li><code>requires()</code>, by default an upstream task is required;</li> <li><code>output()</code>, by default the output of a task is a <code>Fileset</code> with the task's name as an identifier;</li> <li><code>input_file()</code>, helper method to get the output of the upstream task;</li> <li><code>output_file()</code>, helper method to create the output of the current task;</li> </ul>"},{"location":"plant_imager/specifications/tasks/acquisition_tasks/","title":"Acquisition-related tasks","text":""},{"location":"plant_imager/specifications/tasks/acquisition_tasks/#scan-task","title":"Scan task","text":"<p>This task class is used to acquire a set of RGB images from a real plant to study using the Plant imager hardware. It produces a <code>Fileset</code> named <code>'images'</code> (designated as raw scan) that may go with a metadata dictionary if provided as parameter.</p>"},{"location":"plant_imager/specifications/tasks/acquisition_tasks/#virtualscan-task","title":"VirtualScan task","text":"<p>This task class is used to acquire a set of RGB images from a mesh computer model (OBJ file) representing a plant using blender. It produces a <code>Fileset</code> named <code>'images'</code> that may be accompanied by a metadata dictionary if provided as parameter.</p>"},{"location":"plant_imager/specifications/tasks/acquisition_tasks/#virtualplant-task","title":"VirtualPlant task","text":"<p>This task class is used to generate a mesh computer model (OBJ file) representing a plant using a programmable plant model generator called LPY. It produces ??? (TODO)</p>"},{"location":"plant_imager/specifications/tasks/acquisition_tasks/#calibrationscan-task","title":"CalibrationScan task","text":"<p>This task class is used to ??? (TODO)</p>"},{"location":"plant_imager/specifications/tasks/evaluation_tasks/","title":"Evaluation-related tasks","text":"<p>These tasks require the definition of a ground truth.</p>"},{"location":"plant_imager/specifications/tasks/evaluation_tasks/#evaluationtask","title":"EvaluationTask","text":"<p>This is the base (abstract) evaluation task.</p>"},{"location":"plant_imager/specifications/tasks/evaluation_tasks/#pointcloudsegmentationevaluation","title":"PointCloudSegmentationEvaluation","text":""},{"location":"plant_imager/specifications/tasks/evaluation_tasks/#pointcloudevaluation","title":"PointCloudEvaluation","text":""},{"location":"plant_imager/specifications/tasks/evaluation_tasks/#segmentation2devaluation","title":"Segmentation2DEvaluation","text":""},{"location":"plant_imager/specifications/tasks/evaluation_tasks/#voxelsevaluation","title":"VoxelsEvaluation","text":""},{"location":"plant_imager/specifications/tasks/evaluation_tasks/#cylinderradiusestimation","title":"CylinderRadiusEstimation","text":"<p>Thanks to ground truth derived from virtual plants data, most of the reconstruction tasks can be properly evaluated. However, the acquisition made by the plant imager can not be assessed with virtual ground truth and needs some from real world with an adapted comparison method. The 2 main outputs of the plant imager are a set of images and the corresponding positions from the robotic arm. Those data are used later in the reconstruction pipeline, therefore their precision can be evaluated by the precision of the reconstructed object. The evaluation of this acquisition component will be described in this section.</p>"},{"location":"plant_imager/specifications/tasks/evaluation_tasks/#objective","title":"Objective","text":"<p>The goal here is to be able to evaluate the precision of an acquisition by comparing the measurement of a specific trait in an actual object with the same feature in its reconstruction. One basic shape that can often be encountered in plants is the cylinder (stem are often cylindrical, and sometimes so are fruits).  It has been therefore chosen as model from which to extract main characteristics (as the height or the radius). In this evaluation we focused on the radius estimation. The several steps are:  </p> <ul> <li>Make an acquisition of a cylindrical object  </li> <li>Generate a point cloud from the collected images  </li> <li>Estimate the radius of the reconstructed object and compare it with the \"ground truth\" radius  </li> </ul>"},{"location":"plant_imager/specifications/tasks/evaluation_tasks/#prerequisite","title":"Prerequisite","text":"<ul> <li>install romi <code>plant-imager</code> (from source or using a docker image ) &amp; read install procedure</li> <li>install romi <code>plant-3d-vision</code> (from source or using a docker image) &amp; read install procedure</li> <li>Create and activate isolated python environment (see the procedure here )</li> </ul>"},{"location":"plant_imager/specifications/tasks/evaluation_tasks/#step-by-step-tutorial","title":"Step-by-step tutorial","text":""},{"location":"plant_imager/specifications/tasks/evaluation_tasks/#1-image-a-cylindrical-object","title":"1. Image a cylindrical object","text":"<p>In order to have a precise measurement of the object radius, it can be a good idea to use manufactured items with a calibrated size. In our case we chose to evaluate our plant imager with a can.</p> <p>Note</p> <p>Be careful to be able to detect your object from the background in the future reconstruction process.  Different segmentation methods are described here and as we use the \"Binary segmentation\" algorithm the can has been painted in a very tasteful green. </p> <p>The procedure to take images is described in the plant imager tutorial but here are the basic steps:  </p> <ul> <li>DB creation <pre><code>mkdir path/to/db\ntouch path/to/db/romidb\n</code></pre></li> <li>Run an acquisition with the <code>Scan</code> task, and a <code>hardware.toml</code> configuration file in the newly created DB <pre><code>romi_run_task Scan /path/to/db/imageset_id/ --config plant-imager/config/hardware.toml\n</code></pre></li> <li>The <code>imageset_id</code> fileset is now filled : <pre><code>db/\n\u251c\u2500\u2500 imageset_id/\n\u2502   \u251c\u2500\u2500 files.json\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 metadata/\n\u2502   \u2502   \u2514\u2500\u2500 images/\n\u2502   \u2502   \u2514\u2500\u2500 images.json\n\u2502   \u2514\u2500\u2500 scan.toml\n\u2514\u2500\u2500 romidb\n</code></pre></li> </ul> <p>As for the <code>AnglesAndInternodes</code> measurement, in order for the <code>CylinderRadiusEstimation</code> to retrieve the ground truth radius value, it must be added manually in a <code>imageset_id/measures.json</code> file: <pre><code>{\n\"radius\":7.95\n}\n</code></pre></p>"},{"location":"plant_imager/specifications/tasks/evaluation_tasks/#2-point-cloud-reconstruction","title":"2. Point cloud reconstruction","text":"<p>The next step is to compute a point cloud from the acquired data. As before, the full explanation of the operations concerning the reconstruction pipeline can be found in this tutorial but mainly are: <pre><code>romi_run_task PointCloud /path/to/db/imageset_id/ --config plant-3d-vision/config/pipeline.toml\n</code></pre></p> <p>Resulting an equivalent of this tree structure (depending on the used configuration file): <pre><code>db/\n\u251c\u2500\u2500 imageset_id/\n\u2502   \u251c\u2500\u2500 Colmap__/\n\u2502   \u251c\u2500\u2500 files.json\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 Masks__/\n\u2502   \u251c\u2500\u2500 measures.json\n\u2502   \u251c\u2500\u2500 metadata/\n\u2502   \u2502   \u2514\u2500\u2500 images/\n\u2502   \u2502   \u2514\u2500\u2500 images.json\n\u2502   \u251c\u2500\u2500 pipeline.toml\n\u2502   \u251c\u2500\u2500 PointCloud__/\n\u2502   \u251c\u2500\u2500 scan.toml\n\u2502   \u251c\u2500\u2500 Undistorted__/\n\u2502   \u2514\u2500\u2500 Voxels__/\n\u2514\u2500\u2500 romidb\n</code></pre></p> <p>Note</p> <p>Be sure to obtain a proper reconstructed cylinder (as maybe a section of the object) by checking if you are satisfied with the <code>PointCloud_created_fileset/PointCloud.ply</code> point cloud in you favorite 3d software.  If not, you can try to modify parameters in the <code>pipeline.toml</code> configuration file as: - the <code>bounding box</code> in the <code>Colmap</code> section - the <code>threshold</code> parameter (or equivalent) of the <code>Masks</code> task - the <code>voxel_size</code> linked to the <code>Voxels</code> task</p>"},{"location":"plant_imager/specifications/tasks/evaluation_tasks/#3-cylinder-radius-evaluation","title":"3. Cylinder Radius Evaluation","text":"<p>It is now possible to extract the radius from the point cloud using the <code>CylinderRadiusEstimation</code>. Parameters in the <code>pipeline.toml</code> associated to the task must be defined accordingly: <pre><code>[CylinderRadiusEstimation]\nupstream_task = \"PointCloud\"\n</code></pre></p> <p>Note</p> <p>By default the <code>upstream_task</code> parameter of <code>CylinderRadiusEstimation</code> is <code>CylinderRadiusGroundTruth</code></p> <p>With the following command line: <pre><code>romi_run_task CylinderRadiusEstimation /path/to/db/imageset_id/ --config plant-3d-vision/config/pipeline.toml --module plant3dvision.tasks.evaluation\n</code></pre></p> <p>The <code>CylinderRadiusEstimation.json</code> can be found in the <code>CylinderRadiusEstimation__</code> fileset with the output results: <pre><code>{\n\"calculated_radius\": 8.265318865620745,\n\"gt_radius\": 7.95,\n\"err (%)\": 3.97\n}\n</code></pre></p>"},{"location":"plant_imager/specifications/tasks/evaluation_tasks/#evaluate-the-cylinder-radius-estimation-task","title":"Evaluate the cylinder radius estimation task","text":"<p>To assess the accuracy of the cylinder radius estimate from a point-cloud, we developed a <code>CylinderRadiusGroundTruth</code> task. It produces a ground truth point-cloud with a cylinder shape. By default, random dimensions (height and radius) are assumed.</p> <p>To generate the cylinder point-cloud, assuming you did set a <code>ROMI_DB</code> environment variable to your local database: 0. Set a <code>ROMI_DB</code> environment variable pointing to your local database:     <pre><code>export ROMI_DB=$USER\"/romi_db/\"\n</code></pre> 1. Create a scan folder, e.g. <code>virtual_cylinder</code>:     <pre><code>mkdir $ROMI_DB/virtual_cylinder\n</code></pre> 2. Then, it can be run the same way as any other task:     <pre><code>romi_run_task CylinderRadiusGroundTruth $ROMI_DB/virtual_cylinder/ --module plant3dvision.tasks.evaluation\n</code></pre></p> <p>You may want to specify a <code>radius</code> and/or <code>height</code> in the toml config: <pre><code>[CylinderRadiusGroundTruth]\nradius=66.6\nheight=13.0\n</code></pre></p> <p>To evaluate of the <code>CylinderRadiusEstimation</code> task thanks to the virtual cylinder generated by <code>CylinderRadiusGroundTruth</code>: 0. Set a <code>ROMI_DB</code> environment variable pointing to your local database:     <pre><code>export ROMI_DB=$USER\"/romi_db/\"\n</code></pre> 1. Then, run the <code>CylinderRadiusEstimation</code> task with:     <pre><code>romi_run_task CylinderRadiusEstimation $ROMI_DB/virtual_cylinder/ --config plant-3d-vision/config/virtual_cylinder.toml  --module plant3dvision.tasks.evaluation\n</code></pre></p>"},{"location":"plant_imager/specifications/tasks/file_tasks/","title":"File-related tasks","text":""},{"location":"plant_imager/specifications/tasks/file_tasks/#filesetexists","title":"FilesetExists","text":"<p>This task takes a <code>Fileset</code> identifier as a parameter and makes sure it is found in the <code>Scan</code> instance it is working on. No upstream task definition is required and it returns the <code>Fileset</code>.</p>"},{"location":"plant_imager/specifications/tasks/file_tasks/#imagesfilesetexists","title":"ImagesFilesetExists","text":"<p>This is a specific case of the <code>FilesetExists</code> class for <code>'images'</code> <code>Fileset</code>, i.e. the set of RGB images obtained after the <code>Scan</code> task.</p>"},{"location":"plant_imager/specifications/tasks/file_tasks/#modelfileset","title":"ModelFileset","text":"<p>This is a specific case of the <code>FilesetExists</code> class for <code>'models'</code> <code>Fileset</code>, i.e. the training file obtained from machine learning.</p>"},{"location":"plant_imager/specifications/tasks/file_tasks/#filebyfiletask","title":"FileByFileTask","text":"<p>This is an abstract class used to apply a <code>RomiTask</code> on each file of a <code>Fileset</code>.</p>"},{"location":"plant_imager/specifications/tasks/file_tasks/#clean-task","title":"Clean task","text":"<p>This task class is used to clean a <code>Scan</code> dataset by removing all <code>Fileset</code>s except the <code>'images'</code> <code>Fileset</code>, i.e. the set of RGB images obtained after the <code>Scan</code> task.</p>"},{"location":"plant_imager/specifications/tasks/ground_truth_tasks/","title":"Defining ground truth","text":"<p>To performs evaluation tasks, first you have to defines a ground truth to compare to. This is the aim of the following task classes.</p>"},{"location":"plant_imager/specifications/tasks/ground_truth_tasks/#voxelgroundtruth","title":"VoxelGroundTruth","text":""},{"location":"plant_imager/specifications/tasks/ground_truth_tasks/#pointcloudgroundtruth","title":"PointCloudGroundTruth","text":""},{"location":"plant_imager/specifications/tasks/ground_truth_tasks/#clusteredmeshgroundtruth","title":"ClusteredMeshGroundTruth","text":""},{"location":"plant_imager/specifications/tasks/reconstruction_tasks/","title":"Reconstruction related tasks","text":""},{"location":"plant_imager/specifications/tasks/reconstruction_tasks/#undistorted","title":"Undistorted","text":"<p>This task class is used to \"undistort\" images obtained by a camera that may not have a perfect lens. It produces a <code>Fileset</code> of rgb images saved under the task id.</p>"},{"location":"plant_imager/specifications/tasks/reconstruction_tasks/#colmap","title":"Colmap","text":"<p>This task class is used to estimate camera poses from a set of RGB images. By default, it is downstream ImagesFilesetExists (raw scan) An alternative upstream task choice could be the Undistorted. It produces ??? (TODO)</p> <p>Clarification required!</p> <ul> <li>don't we use the positions (x, y, z, pan) from the CNC &amp; gimbal ?</li> <li>why do we compute the sparse reconstruction (point-cloud) ?</li> </ul>"},{"location":"plant_imager/specifications/tasks/reconstruction_tasks/#masks","title":"Masks","text":"<p>This task class is used to create a binary mask of each (real or virtual) plant RGB image. By default, it is downstream the Undistorted task. An alternative upstream task choice could be the Undistorted.</p> <p>The following methods are available to compute masks:</p> <ul> <li>linear</li> <li>excess_green</li> <li>vesselness</li> <li>invert</li> </ul> <p>Clarification required!</p> <ul> <li>document mask algorithms!</li> </ul> <p>It produces a <code>Fileset</code> of binary images saved under the task id.</p>"},{"location":"plant_imager/specifications/tasks/reconstruction_tasks/#voxels","title":"Voxels","text":"<p>This task class is used to compute a volume (ref?) from back-projection of the binary (Masks task) or labelled (Segmentation2D task) masks. By default, it is downstream the Masks &amp; Colmap tasks.</p> <p>The following methods are available to compute back-projection:</p> <ul> <li>carving</li> <li>averaging</li> </ul> <p>Clarification required!</p> <ul> <li>what is a volume ?! the difference with \"point-cloud\" is not too clear... * document back-projection algorithms!</li> </ul> <p>It produces a 3D array saved as NPZ (compressed numpy array). This array may be binary if downstream of the Masks task, or a labelled array if downstream the Segmentation2D task.</p>"},{"location":"plant_imager/specifications/tasks/reconstruction_tasks/#pointcloud","title":"PointCloud","text":"<p>This task class is used to transform the binary volumetric data, from the Voxels tasks into an <code>open3d</code> 3D point-cloud. By default, it is downstream the Voxels task.</p> <p>It uses an Exact Euclidean distance transform method (ref?).</p> <p>It produces a PLY file.</p>"},{"location":"plant_imager/specifications/tasks/reconstruction_tasks/#trianglemesh","title":"TriangleMesh","text":"<p>This task class is used to transform a 3D point-cloud into an <code>open3d</code> 3D triangulated mesh. By default, it is downstream the PointCloud task.</p> <p>It uses the <code>poisson_mesh</code> method from the CGAL library described here.</p> <p>It produces a PLY file.</p>"},{"location":"plant_imager/specifications/tasks/reconstruction_tasks/#curveskeleton","title":"CurveSkeleton","text":"<p>This task class is used to compute a skeleton (ref?) from a 3D triangulated mesh. By default, it is downstream the PointCloud task.</p> <p>It uses the <code>skeletonize_mesh</code> method from the CGAL library described here.</p> <p>It produces a PLY file.</p>"},{"location":"plant_imager/specifications/tasks/reconstruction_tasks/#treegraph","title":"TreeGraph","text":"<p>This task class is used to generate a tree graph structure (ref?) from a skeleton. By default, it is downstream the CurveSkeleton task.</p> <p>It uses <code>networkx</code> Python package to compute a minimum spanning tree with ??? (TODO)</p> <p>It produces a JSON file ??? (TODO)</p>"},{"location":"plant_imager/specifications/tasks/reconstruction_tasks/#anglesandinternodes","title":"AnglesAndInternodes","text":"<p>This task class is used to compute angles and internodes between successive organs along the main stem. By default, it is downstream the TreeGraph task.</p> <p>It produces a JSON file ??? (TODO)</p>"},{"location":"plant_imager/specifications/tasks/reconstruction_tasks/#segmentation2d","title":"Segmentation2D","text":"<p>This task class is used to</p>"},{"location":"plant_imager/specifications/tasks/reconstruction_tasks/#segmentedpointcloud","title":"SegmentedPointCloud","text":"<p>This task class is used to transform the multiclass volumetric data, from the Segmentation2D tasks into an <code>open3d</code> labelled 3D point-cloud. By default, it is downstream the Segmentation2D task.</p> <p>It produces a PLY file.</p>"},{"location":"plant_imager/specifications/tasks/reconstruction_tasks/#clusteredmesh","title":"ClusteredMesh","text":"<p>This task class is used to transform a labelled 3D point-cloud into an <code>open3d</code> 3D triangulated mesh. By default, it is downstream the SegmentedPointCloud task.</p> <p>It produces a PLY file.</p>"},{"location":"plant_imager/tutorials/","title":"Tutorials","text":""},{"location":"plant_imager/tutorials/#basics","title":"Basics","text":"<ul> <li>CLI basics: Usage of the <code>romi_run_task</code> generic command</li> </ul>"},{"location":"plant_imager/tutorials/#plant-imager","title":"Plant Imager","text":"<ul> <li>Plant Imager: Make an acquisition with the Plant Imager</li> <li>Intrinsic calibration: Estimate the camera intrinsic parameters</li> <li>Extrinsic calibration: Calibrate the Plant Imager</li> </ul>"},{"location":"plant_imager/tutorials/#virtual-plant-imager","title":"Virtual Plant Imager","text":"<ul> <li>Virtual world: Evaluate an analysis in a virtual world (both virtual plant imager and plant imager) </li> <li>Virtual plant images: Generate, analyze and visualize virtual plant images using docker containers</li> <li>Training data for ML: Generate large \"virtual\" training data for machine-learning</li> </ul>"},{"location":"plant_imager/tutorials/#reconstruction-and-analysis","title":"Reconstruction and analysis","text":"<ul> <li>Reconstruct 3D data: Reconstruct 3D data from RGB images</li> <li>Explore your data: View your reconstructions and analysis with the <code>plant-3d-explorer</code></li> <li>SM-DTW with simulated data: Evaluate phyllotaxis measurements with <code>SM-DTW</code> from simulated data</li> </ul>"},{"location":"plant_imager/tutorials/basics/","title":"How to use the <code>romi_run_task</code> generic command ?","text":"<p>We here assume you followed the \"installation instructions\" available here.</p>"},{"location":"plant_imager/tutorials/basics/#getting-started","title":"Getting started","text":"<p>There are some requirements to use the different algorithms in the pipeline. Most of them are installed automatically from the requirements file when using pip. The most important part is Colmap (v3.6).</p> <p>The two requirements that are not shipped with <code>pip</code> are:</p> <ul> <li>Colmap (v3.6 or v3.7) for the structure from motion algorithms</li> <li>Blender (&gt;= 2.93) to be able to use the virtual plant imager</li> </ul> <p>Preferably, create a virtual environment for python 3.7 or python 3.8 using <code>virtualenv</code> or a conda environment specific to the 3D Scanner.</p> <p>Warning</p> <p>If using python 3.8, Open3D binaries are not yet available on <code>pip</code>, therefore you have to build Open3D from sources!</p>"},{"location":"plant_imager/tutorials/basics/#basic-usage","title":"Basic usage","text":"<p>Every task is launched through the <code>romi_run_task</code> command provided in the <code>romitask</code> library. It is a wrapper for <code>luigi</code>, with preloaded tasks from the <code>romitask</code>, <code>plantimager</code> &amp; <code>plant3dvision</code> modules.</p> <p>The general usage is as follows:</p> <pre><code>romi_run_task [-h]\n[--config CONFIG]\n[--luigicmd LUIGICMD]\n[--module MODULE]\n[--log-level {DEBUG,INFO,WARNING,ERROR,CRITICAL}]\n[local-scheduler]\ntask\n              [dataset_path]\n</code></pre> <ul> <li><code>CONFIG</code> is either a file or a folder. If a file, it must be JSON or TOML and contains the configuration of the task to run. If a folder, it will read all  configuration files in JSON or TOML format from the folder.</li> <li><code>LUIGICMD</code> is an optional parameter specifying an alternative command for <code>luigi</code>.</li> <li><code>MODULE</code> is an optional parameter for running task from external modules (see TODO).</li> <li><code>LOG_LEVEL</code> is the level of logging. Defaults to <code>INFO</code>, but can be set to <code>DEBUG</code> to increase verbosity.</li> <li><code>task</code> is the name of the class to run, see here for a list of task or have a look at the output of <code>romi_run_task -h</code> for a more up-to-date list</li> <li><code>dataset_path</code> is the location of the target <code>Scan</code> on which to process the task. It is of the form <code>$DB_LOCATION/SCAN_ID</code>, where <code>$DB_LOCATION</code> is a path containing the <code>plantdb</code> marker.</li> </ul>"},{"location":"plant_imager/tutorials/basics/#defining-the-dataset-path","title":"Defining the dataset path","text":"<p>To defines the <code>dataset_path</code> you can specify a dataset name like <code>my_scan_007</code>.</p> <p>You may also use Unix pattern matching with \"<code>*</code>\" and \"<code>?</code>\" to select a list of scan dataset. For example, in a DB with the following scan dataset: <code>test_1</code> <code>my_scan_test</code>, <code>my_scan_002</code>, <code>my_scan_007</code>, <code>my_scan_011</code></p> <ul> <li><code>$DB_LOCATION/*</code> will match ALL scan dataset</li> <li><code>$DB_LOCATION/my_scan_*</code> will match <code>my_scan_test</code>, <code>my_scan_002</code>, <code>my_scan_007</code> &amp; <code>my_scan_011</code></li> <li><code>$DB_LOCATION/my_scan_???</code> will match <code>my_scan_002</code>, <code>my_scan_007</code> &amp; <code>my_scan_011</code></li> <li><code>$DB_LOCATION/my_scan_00?</code> will match <code>my_scan_002</code> &amp; <code>my_scan_007</code></li> <li><code>$DB_LOCATION/my_scan_00*</code> will match <code>my_scan_002</code> &amp; <code>my_scan_007</code></li> </ul>"},{"location":"plant_imager/tutorials/basics/#configuration-files","title":"Configuration files","text":"<p>The configuration is in the form of a dictionary, in which each key is the ID of a given task.</p> <p>In TOML format, it reads as follows:</p> <pre><code>[FirstTask]\nparameter1 = value1\nparameter2 = value2\n\n[SecondTask]\nparameter1 = value1\nparameter2 = value2\n</code></pre> <p>An example TOML configuration file for the geometric reconstruction pipeline of a point-cloud is:</p> <pre><code>[Colmap]\nupstream_task = \"ImagesFilesetExists\"\nmatcher = \"exhaustive\"\ncompute_dense = false\nalign_pcd = true\nuse_gpu = true\nsingle_camera = true\nrobust_alignment_max_error = 10\n\n[Undistorted]\nupstream_task = \"ImagesFilesetExists\"\n\n[Masks]\nupstream_task = \"Undistorted\"\nquery = \"{\\\"channel\\\":\\\"rgb\\\"}\"\ntype = \"linear\"\nparameters = \"[0, 1, 0]\"\ndilation = 3\nbinarize = true\nthreshold = 0.2\n\n[Voxels]\nupstream_mask = \"Masks\"\nupstream_colmap = \"Colmap\"\nvoxel_size = 0.5\ntype = \"carving\"\n\n[PointCloud]\nupstream_task = \"Voxels\"\nlevel_set_value = 0.0\n</code></pre> <p>To run the full reconstruction pipeline use this configuration file with <code>romi_run_task</code>:</p> <pre><code>romi_run_task --config scanner.json AnglesAndInternodes /path/to/db/scan_id/ --local-scheduler\n</code></pre> <p>This will process all tasks up to the <code>AnglesAndInternodes</code> task. Every task produces a <code>Fileset</code>, a subdirectory in the scan directory whose name starts the same as the task name. The characters following are a hash of the configuration of the task, so that the outputs of the same task with different parameters can coexist in the same scan. Any change in the parameters will make the needed task to be recomputed with subsequent calls of <code>romi_run_task</code>. Already computed tasks will be left untouched.</p> <p>To recompute a task, just delete the corresponding folder in the scan directory and rerun <code>romi_run_task</code>.</p>"},{"location":"plant_imager/tutorials/basics/#default-task-reference","title":"Default task reference","text":"<pre><code>MODULES = {\n    # Scanning module:\n    \"Scan\": \"plantimager.tasks.scan\",\n    \"ScannerToCenter\": \"plantimager.tasks.scan\",\n    \"VirtualPlant\": \"plantimager.tasks.lpy\",\n    \"VirtualScan\": \"plantimager.tasks.scan\",\n    \"CalibrationScan\": \"plantimager.tasks.scan\",\n    \"IntrinsicCalibrationScan\": \"plantimager.tasks.scan\",\n    # Calibration module:\n    \"CreateCharucoBoard\" : \"plant3dvision.tasks.calibration\",\n    \"DetectCharuco\" : \"plant3dvision.tasks.calibration\",\n    \"ExtrinsicCalibration\" : \"plant3dvision.tasks.calibration\",\n    \"IntrinsicCalibration\" : \"plant3dvision.tasks.calibration\",\n    # Geometric reconstruction module:\n    \"Colmap\": \"plant3dvision.tasks.colmap\",\n    \"Undistorted\": \"plant3dvision.tasks.proc2d\",\n    \"Masks\": \"plant3dvision.tasks.proc2d\",\n    \"Voxels\": \"plant3dvision.tasks.cl\",\n    \"PointCloud\": \"plant3dvision.tasks.proc3d\",\n    \"TriangleMesh\": \"plant3dvision.tasks.proc3d\",\n    \"CurveSkeleton\": \"plant3dvision.tasks.proc3d\",\n    # Machine learning reconstruction module:\n    \"Segmentation2D\": \"plant3dvision.tasks.proc2d\",\n    \"SegmentedPointCloud\": \"plant3dvision.tasks.proc3d\",\n    \"ClusteredMesh\": \"plant3dvision.tasks.proc3d\",\n    \"OrganSegmentation\": \"plant3dvision.tasks.proc3d\",\n    # Quantification module:\n    \"TreeGraph\": \"plant3dvision.tasks.arabidopsis\",\n    \"AnglesAndInternodes\": \"plant3dvision.tasks.arabidopsis\",\n    # Evaluation module:\n    \"VoxelsGroundTruth\": \"plant3dvision.tasks.evaluation\",\n    \"VoxelsEvaluation\": \"plant3dvision.tasks.evaluation\",\n    \"PointCloudGroundTruth\": \"plant3dvision.tasks.evaluation\",\n    \"PointCloudEvaluation\": \"plant3dvision.tasks.evaluation\",\n    \"ClusteredMeshGroundTruth\": \"plant3dvision.tasks.evaluation\",\n    \"SegmentedPointCloudEvaluation\": \"plant3dvision.tasks.evaluation\",\n    \"Segmentation2DEvaluation\": \"plant3dvision.tasks.evaluation\",\n    \"AnglesAndInternodesEvaluation\": \"plant3dvision.tasks.evaluation\",\n    \"CylinderRadiusGroundTruth\": \"plant3dvision.tasks.evaluation\",\n    \"CylinderRadiusEstimation\": \"plant3dvision.tasks.evaluation\",\n    # Visualization module:\n    \"Visualization\": \"plant3dvision.tasks.visualization\",\n    # Database module:\n    \"Clean\": \"romitask.task\"\n}\n</code></pre> <p>Warning</p> <p>This is for reference only, please update the changes in the code. This will be later replaced by a reference doc generated from the code!</p>"},{"location":"plant_imager/tutorials/extrinsic_calibration/","title":"Calibrate the Plant imager","text":""},{"location":"plant_imager/tutorials/extrinsic_calibration/#objective","title":"Objective","text":"<p>Calibration is giving the right scale to your images and is thus crucial to perform measures from phenotyping imaging.</p> <p>Scale (pixel size) is a priori unknown in a picture. In addition, some aspects of an hardware setup can create artefacts affecting scaling during 3D reconstruction. Hence, we developed a procedure of extrinsic calibration to scale 3D reconstructions to real world unit and correct possible artefacts induced by the configuration of our plant-imager robot.</p> <p>In this tutorial, you will learn how to calibrate an image acquisition for downstream analysis and how to re-use a previously made calibration for an analysis, provided that the set-up is the same.</p> <p>Note</p> <p>Intrinsic calibration corrects possible defects induced by the lens of the camera. These type of defects are not addressed by this procedure.</p>"},{"location":"plant_imager/tutorials/extrinsic_calibration/#prerequisite","title":"Prerequisite","text":"<ul> <li> <p>Make sure that you installed all the ROMI software to run image acquisitions with the ROMI <code>Plant Imager</code> (explanation here)</p> </li> <li> <p>install also <code>plant-3d-vision</code> to perform the calibration.</p> </li> </ul>    We highly recommend the use of docker containers to run ROMI software, if you wish to use the docker images we provide, have a look here.  <ul> <li> <p>Before reading this tutorial, you should first be able to run a basic acquisition without calibration, as explained in this tutorial.</p> </li> <li> <p>set up a database or quickly generate a simple database with the following commands:</p> </li> </ul> <p><pre><code>mkdir path/to/db\ntouch path/to/db/romidb\n</code></pre> You have now your file-based romi database plantdb</p>"},{"location":"plant_imager/tutorials/extrinsic_calibration/#principles-of-the-extrinsic-calibration-performed-here","title":"Principles of the extrinsic calibration performed here","text":"<p>The motor positions moving the camera along the acquisition path give a first indication of the scale of the picture, but this motor information is as accurate as the encoder allows them to be.</p> <p>To have the closest scaling from reality, we use Colmap (a structure-from-motion algorithm) at the very beginning of the 3d reconstructions (see here): this technique allows to refine the positions of the camera given by the robot motors. However, those computed positions are determined up to a scaling and roto-translation of the world and as a result, present a problem for measuring real world unit quantities. In addition, each camera pose is aligned with the corresponding CNC arm position. This can lead to a bias in scaling induced by the offset between the camera optical center and the CNC arm as represented in the following picture:   </p> <p>It is particularly true when doing circular path (which is often the case with the phenotyping station).  Indeed, because of that offset, the distance between 2 camera poses is bigger than it should be and as a result, the reconstructed the object is bigger than it is in real life (with a relative error of 2d / D).  To correct that, a procedure has been developed to perform an extrinsic calibration and apply the results for further image acquisitions using the same hardware settings.</p>"},{"location":"plant_imager/tutorials/extrinsic_calibration/#step-by-step-tutorial","title":"Step-by-step tutorial","text":""},{"location":"plant_imager/tutorials/extrinsic_calibration/#1-calibration-acquisition","title":"1. Calibration acquisition","text":"<p>Because the bias is mainly induced by making a circular path of image acquisition, one way to avoid it is to do a calibration acquisition with first a path constituted of two lines (two orthogonal lines in our case) followed by the path that will be used by other acquisitions.  </p> <p></p> <p>To do so, run the task 'CalibrationScan' the same way as for a regular acquisition with the <code>romi_run_task</code> command, including your regular configuration file adapted to your plant imager (hereafter: <code>hardware.toml</code>).</p> <p>In the command, define a folder inside your romi database (called above <code>plantdb</code>) that will store the data of this calibration acquisition:</p> <pre><code>romi_run_task --config config/hardware.toml \\ #command and config\nCalibrationScan \\ # romi task\n/path/to/plantdb/calibration_scan_id/ #data destination of this calibration scan \n</code></pre> <p>Note</p> <p>Run this command either in a docker container of the plant-imager or, if you install plant-imager in your system, do not forget to activate the appropriate virtual environment to make <code>romi_run_task</code> command accessible.</p> <p>Colmap performance increases when several \"recognizable\" objects are present in the scene, so that the program easily defines matching reference points between overlapping images. We advise to put such objects in the scene for the calibration acquisition (they could be removed later)</p>"},{"location":"plant_imager/tutorials/extrinsic_calibration/#2-compute-circular-poses-from-path-lines-with-colmap","title":"2. Compute circular poses from path lines with Colmap","text":"<p>Thanks to the linear path added to the circular one, Colmap can now retrieve accurate poses with a proper scaling. Colmap can be easily run with romi software <code>plant-3d-vision</code>.</p> <p>For such a run, a proper configuration file (.toml) is required. A default one is provided with <code>plant-3d-vision</code>, and accessible from your local git-cloned repository or in the repository included inside the docker container.</p> <pre><code>romi_run_task --config path/toconfig/geom_pipe_full.toml \\ #command and path to config file\nColmap \\ #the task Colmap\n/path/to/plantdb/calibration_scan_id/ #data destination folder\n</code></pre> <p>Note</p> <p>Run this command either in a docker container of <code>plant-3d-vision</code> or, if you install plant-imager in your system, do not forget to activate the appropriate virtual environment to make <code>romi_run_task</code> command accessible.</p>"},{"location":"plant_imager/tutorials/extrinsic_calibration/#3-reuse-the-poses-extracted-from-the-calibration-scan","title":"3. (re)Use the poses extracted from the calibration scan","text":"<p>Now, the calibrated poses can be used to properly scale 3d reconstruction each time an analysis is performed (full process detailed here) on other image dataset.</p> <p>To do so, just add in the Colmap section of the configuration .toml file for reconstruction:</p> <pre><code>[Colmap]\ncalibration_scan_id = \"calibration_scan_id\" #name of the folder containing calibration data\n</code></pre> <p>Important: </p> <pre><code>*   Calibration_scan_id and the other dataset to be analyzed must be in the same romi database.\n*   To be valid, calibration data can only be used if the camera position has not changed at all (tilt, etc...) in the robotic arm. For instance, a new calibration acquisition should be performed each time the camera is removed and replaced back on the arm.\n</code></pre>"},{"location":"plant_imager/tutorials/hardware_scan/","title":"Make an acquisition with the Plant Imager","text":""},{"location":"plant_imager/tutorials/hardware_scan/#objective","title":"Objective","text":"<p>This tutorial will guide through the steps of acquiring images of a plant using the <code>plant imager</code> robot  In order to collect data in the process of plant phenotyping, the plant imager robot takes RGB images of an object following a particular path with precise camera poses.</p>"},{"location":"plant_imager/tutorials/hardware_scan/#prerequisite","title":"Prerequisite","text":"<p>To run an acquisition, you should previously have:</p> <ul> <li>built the robot following the guidelines here</li> <li>installed the necessary ROMI software here</li> </ul>"},{"location":"plant_imager/tutorials/hardware_scan/#step-by-step-tutorial","title":"Step-by-step tutorial","text":""},{"location":"plant_imager/tutorials/hardware_scan/#1-check-that-you-are-well-interfaced-with-the-plant-imager","title":"1. Check that you are well interfaced with the plant imager","text":"<ul> <li>make sure you are in the conda environment or that you run properly the docker for the <code>plantimager</code> repository</li> <li>interface the machine running the ROMI software with the plant imager:<ol> <li>check that your device is correctly connected to the Gimbal and CNC both by USB</li> <li>turn on camera and connect it to the device via Wi-Fi</li> </ol> </li> <li>set up a DB or quickly generate a simple database with the following commands:</li> </ul> <pre><code>mkdir path/to/db\ntouch path/to/db/romidb\n</code></pre> <p>You have now your file based database plantdb</p>"},{"location":"plant_imager/tutorials/hardware_scan/#2-get-the-right-configuration","title":"2. Get the right configuration","text":"<p><code>Scan</code> is the basic task for running an acquisition with the robot. To run this task properly with <code>romi_run_task</code>, a configuration file is needed. A default one for the plant imager can be found under <code>plantimager/config/hardware.toml</code>. It regroups specifications on:</p> <ul> <li>the acquisition path (ScanPath)</li> <li>needed parameters for connection between hardware components (CNC, Gimbal and camera) and software (Scan.scanner)</li> <li>object metadata (in Scan.metadata.object)</li> <li>hardware metadata (in Scan.metadata.hardware)</li> </ul> <p>An important parameter is the number of images acquisition you want to perform, defined by <code>n_points</code>. If you pick a number of acquisition in the following range of values, it will result in an integer rotation angle: 1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 15, 18, 20, 24, 30, 36, 40, 45, 60, 72, 90, 120, 180, 360.</p> <p>The truly recommended values are in bold.</p> <pre><code>[ScanPath] # Example, circular path with 60 points:\nclass_name = \"Circle\"\n\n[ScanPath.kwargs]\ncenter_x = 375\ncenter_y = 375\nz = 80\ntilt = 0\nradius = 300\nn_points = 60\n\n[Scan.scanner.cnc] # module and kwargs linked to the CNC\nmodule = \"plantimager.grbl\"\n\n[Scan.scanner.cnc.kwargs]\nhoming = true\nport = \"/dev/ttyACM0\"\n\n[Scan.scanner.gimbal] # module and kwargs linked to the gimbal\nmodule = \"plantimager.blgimbal\"\n\n[Scan.scanner.gimbal.kwargs]\nport = \"/dev/ttyACM1\"\nhas_tilt = false\nzero_pan = 0\ninvert_rotation = true\n\n[Scan.scanner.camera] # camera related parameters\nmodule = \"plantimager.sony\"\n\n[Scan.scanner.camera.kwargs]\ndevice_ip = \"192.168.122.1\"\napi_port = \"10000\"\npostview = true\nuse_flashair = false\nrotation = 270\n\n[Scan.metadata.object] # object related metadata\nspecies = \"chenopodium album\"\nseed_stock = \"Col-0\"\nplant_id = \"3dt_chenoA\"\ngrowth_environment = \"Lyon-indoor\"\ngrowth_conditions = \"SD+LD\"\ntreatment = \"None\"\nDAG = 40\nsample = \"main_stem\"\nexperiment_id = \"3dt_26-01-2021\"\ndataset_id = \"3dt\"\n\n[Scan.metadata.hardware] # hardware related metadata\nframe = \"30profile v1\"\nX_motor = \"X-Carve NEMA23\"\nY_motor = \"X-Carve NEMA23\"\nZ_motor = \"X-Carve NEMA23\"\npan_motor = \"iPower Motor GM4108H-120T Brushless Gimbal Motor\"\ntilt_motor = \"None\"\nsensor = \"RX0\"\n\n[Scan.metadata.workspace] # A volume containing the target imaged object\nx = [ 200, 600,]\ny = [ 200, 600,]\nz = [ -100, 300,]\n</code></pre> <p>Warning</p> <p>This is a default configuration file. You will most probably need to create one to fit your hardware setup. Check the configuration documentation for the hardware and the imaged object</p>"},{"location":"plant_imager/tutorials/hardware_scan/#3-run-an-acquisition-with-the-scan-task","title":"3. Run an acquisition with the <code>Scan</code> task","text":"<p>Assuming you have an active database, you can now run the <code>Scan</code> task using <code>romi_run_task</code>:</p> <pre><code>romi_run_task --config config/hardware.toml Scan /path/to/db/imageset_id/\n</code></pre> <p>where:</p> <ul> <li><code>/path/to/db</code> must be an existing FSDB database</li> <li>there is no <code>/path/to/db/imageset_id</code> already existing in the database.</li> </ul> <p>This will create the corresponding folder and fill it with images from the imageset.</p> <p>Warning</p> <p>After a rather short time following running the command, you should hear the robot start and when the acquisition is finished, a <code>This progress looks :)</code> should appear. If it's not the case, try to look at the Troubleshooting section at the end of this tutorial</p>"},{"location":"plant_imager/tutorials/hardware_scan/#4-obtain-an-image-set","title":"4. Obtain an image set","text":"<p>Once the acquisition is done, the database is updated, and we now have the following tree structure:</p> <pre><code>db/\n\u251c\u2500\u2500 imageset_id/\n\u2502   \u251c\u2500\u2500 files.json\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 metadata/\n\u2502   \u2502   \u2514\u2500\u2500 images/\n\u2502   \u2502   \u2514\u2500\u2500 images.json\n\u2502   \u2514\u2500\u2500 scan.toml\n\u2514\u2500\u2500 romidb\n</code></pre> <p>with:</p> <ul> <li><code>images</code> containing a list of RGB images acquired by the camera moving around the plant</li> <li><code>metadata/images</code> a folder filled with json files recording the poses (camera coordinates) for each taken image</li> <li><code>metadata/images.json</code> containing parameters of the acquisition that will be used later in reconstruction (type of format for the images, info on the object and the workspace)</li> <li><code>files.json</code> detailing the files contained in the imageset_id</li> <li><code>scan.json</code>, a copy of the acquisition config file</li> </ul> <p>You can now reconstruct your plant in 3d !</p>"},{"location":"plant_imager/tutorials/hardware_scan/#troubleshooting","title":"Troubleshooting","text":""},{"location":"plant_imager/tutorials/hardware_scan/#serial-access-denied","title":"Serial access denied","text":"<ul> <li>The CNC and Gimbal might be connected to different ports than the ones specified in the configuration file. Please check with the <code>dmesg -w</code> command.</li> <li>Look here if you can not communicate with the scanner using usb.</li> <li>Make sure the device used to run the acquisition is indeed connected to the camera (Wi-Fi)</li> <li>Message to Gimbal still transiting :</li> </ul> <pre><code>Traceback (most recent call last):\n  File \"/home/romi/miniconda3/envs/scan_0.8/lib/python3.8/site-packages/serial/serialposix.py\", line 265, in open\n    self.fd = os.open(self.portstr, os.O_RDWR | os.O_NOCTTY | os.O_NONBLOCK)\nOSError: [Errno 16] Device or resource busy: '/dev/ttyACM0'\n</code></pre> <p>Try disconnect and reconnect the USB link and rerun an acquisition</p>"},{"location":"plant_imager/tutorials/intrinsic_calibration/","title":"Calibrate the camera","text":"<p>Some cameras introduce significant distortion to images. The two major kinds of distortion are radial distortion and tangential distortion.</p> <p>With radial distortion, straight lines appear curved while with tangential distortion some objects of the image may appear closer than they are in reality.</p> <p>Illustration of an image captured with radial distortion as shown by the straight red lines added on top the picture afterward.     Source: OpenCV Python tutorial on camera calibration.</p>"},{"location":"plant_imager/tutorials/intrinsic_calibration/#objective","title":"Objective","text":"<p>Correcting these \"aberrations\" prior to image processing can be a good idea to improve quality and accuracy of the reconstructed 3D scenes by structure from motion algorithms.</p> <p>In this tutorial, you will learn how to estimate the intrinsic camera parameters to calibrate image acquisition for downstream analysis and how to re-use it for a reconstruction pipeline, provided that the set-up is the same (same camera, optics, image size...).</p>"},{"location":"plant_imager/tutorials/intrinsic_calibration/#prerequisite","title":"Prerequisite","text":"<ul> <li> <p>Install the <code>plant-imager</code> ROMI library required to perform image acquisitions together with the Plant Imager hardware.</p> </li> <li> <p>Install the <code>plant-3d-vision</code> ROMI library required to perform intrinsic calibration.</p> </li> <li> <p>Set up a ROMI <code>plantdb</code> local database or quickly create it (under <code>/data/ROMI/DB</code>) with the following commands:     <pre><code>export DB_LOCATION=/data/ROMI/DB\nmkdir $DB_LOCATION\ntouch $DB_LOCATION/romidb\n</code></pre></p> </li> </ul>    We highly recommend the use of docker containers to run ROMI software, if you wish to use the docker images we provide, have a look here."},{"location":"plant_imager/tutorials/intrinsic_calibration/#step-by-step-tutorial","title":"Step-by-step tutorial","text":""},{"location":"plant_imager/tutorials/intrinsic_calibration/#1-make-a-charuco-board-and-print-it","title":"1. Make a ChArUco board and print it","text":"<p>A ChArUco board is the combination of a chess board and of ArUco markers.</p> An example of a 14x10 ChArUco board with 20mm chess square and 15mm 4x4 ArUco markers. <p>The previous figure shows the default board that we will use in this tutorial.</p> <p>To create it, you have to run the <code>create_charuco_board</code> CLI as follows: <pre><code>create_charuco_board plant-3d-vision/config/intrinsic_calibration.toml\n</code></pre> This will create a file named <code>charuco_board.png</code> in the current working directory.</p> <p>We strongly advise to use the same TOML configuration file with <code>create_charuco_board</code> &amp; <code>romi_run_task</code> commands to avoid inadvertently changing parameter values. Also, you will later need it for the estimation of the intrinsic camera parameters.</p> <p>An example of <code>intrinsic_calibration.toml</code> configuration file is: <pre><code>[CreateCharucoBoard]\nn_squares_x = \"14\"  # Number of chessboard squares in X direction.\nn_squares_y = \"10\"  # Number of chessboard squares in Y direction.\nsquare_length = \"2.\"  # Length of square side, in cm\nmarker_length = \"1.5\"  # Length of marker side, in cm\naruco_pattern = \"DICT_4X4_1000\"  # 'DICT_4X4_50', 'DICT_4X4_100', 'DICT_4X4_250', 'DICT_4X4_1000'\n\n[DetectCharuco]\nupstream_task = \"ImagesFilesetExists\"\nboard_fileset = \"CreateCharucoBoard\"\nmin_n_corners = \"20\"  # Minimum number of detected corners to export them\n\n[IntrinsicCalibration]\nupstream_task = \"DetectCharuco\"\nboard_fileset = \"CreateCharucoBoard\"\n</code></pre></p> <p>You may now print the ChArUco board image. Pay attention to use a software (like GIMP) that allows you to set the actual size of the image you want to print. With the previous configuration it should be:</p> <ul> <li>width : <code>n_squares_x * square_length = 14 * 2. = 28cm</code></li> <li>height : <code>n_squares_y * square_length = 10 * 2. = 20cm</code></li> </ul> <p>Finally, tape it flat onto something solid in order to avoid deformation of the printed pattern!</p>"},{"location":"plant_imager/tutorials/intrinsic_calibration/#2-scan-the-charuco-board","title":"2. Scan the ChArUco board","text":"<p>To scan your newly printed ChArUco board, use the <code>IntrinsicCalibrationScan</code> task from <code>plant_imager</code>: <pre><code>romi_run_task IntrinsicCalibrationScan $DB_LOCATION/intrinsic_calib_1 --config plant-3d-vision/config/scan.toml\n</code></pre></p> <p>The camera should move to the center front of the plant imager where you will hold your pattern and take <code>20</code> pictures (according to the previous configuration). Try to take pictures of the board in different positions.</p> 00000_rgb.jpg 00003_rgb.jpg 00005_rgb.jpg 00014_rgb.jpg <p>As illustrated by the previous image examples, it is not required to have the whole board in the picture, the ArUco markers will be used to detect the occluded sections!</p> <p>An example for the <code>scan.toml</code> configuration file is: <pre><code>[IntrinsicCalibrationScan]\nn_poses = 20  # Number of acquisition of the printed ChArUco board\noffset = 5\n\n[CalibrationScan]\nn_points_line = 11\noffset = 5\n\n[ScanPath]\nclass_name = \"Circle\"\n\n[ScanPath.kwargs]\ncenter_x = 375\ncenter_y = 375\nz = 90\ntilt = 0\nradius = 300\nn_points = 36\n\n[Scan.scanner.camera]\nmodule = \"plantimager.sony\"  # RX-0 camera\n\n[Scan.scanner.gimbal]\nmodule = \"plantimager.blgimbal\"  # plant imager hardware v2\n\n[Scan.scanner.cnc]\nmodule = \"plantimager.grbl\"  # plant imager hardware v2\n\n[Scan.metadata.object]\nspecies = \"none\"\nseed_stock = \"none\"\nplant_id = \"test\"\ngrowth_environment = \"none\"\ngrowth_conditions = \"None\"\ntreatment = \"none\"\nDAG = 0\nsample = \"test_sample\"\nexperiment_id = \"None\"\ndataset_id = \"test\"\n\n[Scan.metadata.hardware]\nframe = \"30profile v2\"\nX_motor = \"X-Carve NEMA23\"\nY_motor = \"X-Carve NEMA23\"\nZ_motor = \"X-Carve NEMA23\"\npan_motor = \"iPower Motor GM4108H-120T Brushless Gimbal Motor\"\ntilt_motor = \"None\"\nsensor = \"Sony RX-0\"\n\n[Scan.metadata.workspace]\nx = [ 100, 500,]\ny = [ 100, 500,]\nz = [ -300, 100,]\n\n[Scan.scanner.camera.kwargs]\ndevice_ip = \"192.168.122.1\"\napi_port = \"10000\"\npostview = true\nuse_flashair = false\nrotation = 270\n\n[Scan.scanner.gimbal.kwargs]\nport = \"/dev/ttyACM1\"\nhas_tilt = false\nzero_pan = 0\ninvert_rotation = true\n\n[Scan.scanner.cnc.kwargs]\nport = \"/dev/ttyACM0\"\nbaud_rate = 115200\nhoming = true\n</code></pre></p>"},{"location":"plant_imager/tutorials/intrinsic_calibration/#3-performs-the-camera-parameters-estimation","title":"3. Performs the camera parameters estimation","text":"<p>You may now estimate the camera parameters, for a given camera model with: <pre><code>romi_run_task IntrinsicCalibration $DB_LOCATION/intrinsic_calib_1 --config plant-3d-vision/config/intrinsic_calibration.toml\n</code></pre> This should generate a <code>camera_model.json</code> inside the <code>$DB_LOCATION/intrinsic_calib_1/camera_model</code> folder.</p> <p>An example of a <code>camera_model.json</code> file is: <pre><code>{\n\"model\": \"OPENCV\",\n\"RMS_error\": 0.3484289537533634,\n\"camera_matrix\": [\n[\n1201.7588127324675,\n0.0,\n702.5429671940506\n],\n[\n0.0,\n1199.117692017527,\n536.7266695161917\n],\n[\n0.0,\n0.0,\n1.0\n]\n],\n\"distortion\": [\n0.021462456820485233,\n-0.04707700665017203,\n-0.00014475851274869323,\n-0.0011459776173976073,\n0.0\n],\n\"height\": 1440,\n\"width\": 1080\n}\n</code></pre></p> <p>Important</p> <p>Do not hesitate to make several independent attempts at camera calibration, like 3 to 5, and choose the one with the lowest overall RMS error. Obviously, independent here means that you should perform multiple scans of the board and camera parameters estimation.</p>"},{"location":"plant_imager/tutorials/multi_scan/","title":"Performs multiples scans of the same object while changing an acquisition parameter","text":"<p>When performing tests and evaluations, you would like to acquire the same object multiple times with while changing one parameter to assess its importance of find the optimal one. Normally this would require multiple calls to the <code>romi_run_task</code> CLI with a specific configuration file and a specific dataset name.</p> <p>In our experience, this is prone to errors!</p> <p>So we developed a CLI for this case named <code>multi_scan</code>. Hereafter we will provide a usage example.</p>"},{"location":"plant_imager/tutorials/multi_scan/#cli-overview","title":"CLI overview","text":"<pre><code>usage: multi_scan [-h] [--type {int,str,float}] [--task TASK] [--config CONFIG] [--module MODULE]\n                  [--log-level {DEBUG,INFO,WARNING,ERROR,CRITICAL}]\n                  database_path dataset_fmt param value [value ...]\n\nPerforms multiple calls to a ROMI task changing the value of a single parameter.\n\npositional arguments:\n  database_path         Path to the database (directory) to populate with the acquisitions.\n  dataset_fmt           Generic name to give to the datasets, should contain a `{}` to indicate the\n                        parameter position.\n  param                 Name of the parameter to change from the loaded `config`.\n  value                 Value(s) of the parameter to use.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --type {int,str,float}\n                        To specify the type of the parameter value(s), defaults to 'int'.\n\nROMI arguments:\n  --task TASK           Name of the task to perform, `Scan` by default.\n  --config CONFIG       Pipeline configuration file or directory (JSON or TOML). If a file, read the\n                        configuration from it. If a directory, read &amp; concatenate all configuration\n                        files in it\n  --module MODULE       Library and module of the task. Use it if not available or different than\n                        defined in `romitask.modules.MODULES`.\n  --log-level {DEBUG,INFO,WARNING,ERROR,CRITICAL}\n                        Level of message logging, defaults to 'INFO'.\n</code></pre>"},{"location":"plant_imager/tutorials/multi_scan/#test-the-effect-of-the-number-of-images-on-reconstruction","title":"Test the effect of the number of images on reconstruction","text":"<p>To test the effect of the number of images on the quality of the reconstructed 3D point-cloud, you want to acquire the same object multiple times while changing the number of views.</p> <p>You can do that simply with: <pre><code>multi_scan '/data/romi_db/test_nb_views/' 'my_plant_A_{}' ScanPath.kwargs.n_points 10 20 30 36 40 60 72 --config '/data/configs/scan.toml'\n</code></pre></p> <p>This assumes:</p> <ul> <li><code>/data/romi_db/test_nb_views/</code> to be the location of your database</li> <li><code>my_plant_A_{}</code> to be the name of the dataset to create, the <code>{}</code> indicate where to add the number of views as character</li> <li><code>ScanPath.kwargs.n_points</code> is the <code>section.parameter</code> to modify</li> <li><code>10 20 30 36 40 60 72</code> to be the list of number of view to acquire</li> <li><code>/data/configs/scan.toml</code> to be the default scanning config to use</li> </ul> <p>This will call the <code>romi_run_task</code> command with the appropriate arguments.</p> <p>This should generate the following file structure: <pre><code>/data/romi_db/test_nb_views/\n\u251c\u2500\u2500 my_plant_A_10/\n\u2502         \u251c\u2500\u2500 files.json\n\u2502         \u251c\u2500\u2500 images/\n\u2502         \u251c\u2500\u2500 metadata/\n\u2502         \u2514\u2500\u2500 scan.toml\n\u251c\u2500\u2500 my_plant_A_20/\n\u2502         \u251c\u2500\u2500 files.json\n\u2502         \u251c\u2500\u2500 images/\n\u2502         \u251c\u2500\u2500 metadata/\n\u2502         \u2514\u2500\u2500 scan.toml\n\u251c\u2500\u2500 my_plant_A_30/\n\u2502         \u251c\u2500\u2500 files.json\n\u2502         \u251c\u2500\u2500 images/\n\u2502         \u251c\u2500\u2500 metadata/\n\u2502         \u2514\u2500\u2500 scan.toml\n\u251c\u2500\u2500 my_plant_A_36/\n\u2502         \u251c\u2500\u2500 files.json\n\u2502         \u251c\u2500\u2500 images/\n\u2502         \u251c\u2500\u2500 metadata/\n\u2502         \u2514\u2500\u2500 scan.toml\n\u251c\u2500\u2500 my_plant_A_40/\n\u2502         \u251c\u2500\u2500 files.json\n\u2502         \u251c\u2500\u2500 images/\n\u2502         \u251c\u2500\u2500 metadata/\n\u2502         \u2514\u2500\u2500 scan.toml\n\u251c\u2500\u2500 my_plant_A_60/\n\u2502         \u251c\u2500\u2500 files.json\n\u2502         \u251c\u2500\u2500 images/\n\u2502         \u251c\u2500\u2500 metadata/\n\u2502         \u2514\u2500\u2500 scan.toml\n\u251c\u2500\u2500 my_plant_A_72/\n\u2502         \u251c\u2500\u2500 files.json\n\u2502         \u251c\u2500\u2500 images/\n\u2502         \u251c\u2500\u2500 metadata/\n\u2502         \u2514\u2500\u2500 scan.toml\n\u2514\u2500\u2500 romidb\n</code></pre></p>"},{"location":"plant_imager/tutorials/plant-3d-explorer/","title":"View your results with plant-3d-explorer","text":"<p>How to see directly the results of your plant phenotyping with the <code>plant-3d-explorer</code> ?</p>"},{"location":"plant_imager/tutorials/plant-3d-explorer/#objective","title":"Objective","text":"<p>Throughout the whole process of plant phenotyping, viewing data is often needed. This tutorial explains how to use the romi <code>plant-3d-explorer</code>, a web-server tool, to explore, display and interact with most of the diverse data generated during a typical plant phenotyping experiment from 2D images (2D images, 3D objects like meshes or point cloud, quality evaluations, trait measurements). After this tutorial, you should be able to:</p> <ul> <li>connect the plant-3d-explorer to a database containing the phenotyping data of one to several plants ;</li> <li>explore the database content with the <code>plant-3d-explorer</code> menu page ;</li> <li>For each plant, display, overlay and inspect in 3d every data generated during analysis</li> </ul>"},{"location":"plant_imager/tutorials/plant-3d-explorer/#prerequisite","title":"Prerequisite","text":"<ul> <li>install romi <code>plant-3d-explorer</code> (from source or using a docker image ) &amp; read install procedure</li> <li>install romi <code>plantdb</code> (from source or using a docker image) &amp; read install procedure</li> <li>install romi <code>plant-3d-vision</code> (from source or using a docker image) &amp; read install procedure</li> <li>Create and activate isolated python environment (see the procedure here )</li> </ul> <p>Note for docker users </p> <p>You can avoid installs by using docker only. Read first the docker procedures ( 'docker for plant-3d-vision' and 'docker-compose to run both database and 3d explorer with docker containers' ). In the following tutorial (steps 1, 2 and 3), follow the docker logo to adapt the procedure.</p>"},{"location":"plant_imager/tutorials/plant-3d-explorer/#linked-documentation","title":"Linked documentation","text":"<ul> <li>Manual of the romi plant-3d-explorer</li> </ul>"},{"location":"plant_imager/tutorials/plant-3d-explorer/#step-by-step-tutorial","title":"Step-by-step tutorial","text":"<p>Principle: the <code>plant-3d-explorer</code> is a web client that displays in your favorite web browser data exposed by a server (here, romi <code>plantdb</code>) on a particular url. The process consists in pointing the server to your folder of interest, starting the server and starting the client that points to the served url.</p> <p>Warning</p> <p>the <code>plant-3d-explorer</code> has only been developed and tested on Chrome.</p>"},{"location":"plant_imager/tutorials/plant-3d-explorer/#1-preparing-your-database-for-display-by-the-plant-3d-explorer","title":"1. Preparing your database for display by the <code>plant-3d-explorer</code>","text":"<p>Starting point: your database is made of one or several datasets, which all correspond to a single plant phenotyping experiment: each dataset contains at least 2D images (raw acquisitions) and metadata, and possibly several other data generated by subsequent 3D reconstruction, segmentation and analysis.</p> <p>Note</p> <p>Your database must follow the rules of romi databases: please make sure that you comply to requirements. You can also download an example database here.</p> <p>example: let's consider a database called <code>my_experiment</code> containing 3 datasets (named plant1, plant2, plant3) generated by phenotyping three plants.</p> <ul> <li>open a terminal and go to your local database directory</li> <li>if romi commands (like <code>romi_run_task</code>) are not accessible from your terminal, activate the appropriate python environment (e.g. using venv or conda) required   for romi commands (or read this procedure)</li> <li>process all datasets for display by the <code>plant-3d-explorer</code> by running the following code</li> </ul> <pre><code>dataset_list=('plant1','plant2','plant3')\n\nfor ds in \"${dataset_list[@]}\"\ndo romi_run_task Visualization path_to/my_experiment/\"$ds\"/ --config ~/config/ml_pipe_real.toml\ndone\n</code></pre> <p>Note</p> <p>For more information about using romi_run_task command, the Visualization task and the config file, please read XXXXX.</p> <p>Note for docker users </p> <ul> <li>Start a docker container by mounting your database as a volume (details)</li> <li>In the container, run the same Visualization Task has above</li> </ul> <p>check result: a new folder called <code>Visualization</code> should have been created in each dataset of your database</p> <p>Note for docker users </p> <p>Skip step 2 &amp; 3 and follow instead instructions given by'docker-compose to run both database and 3d explorer with docker containers'</p>"},{"location":"plant_imager/tutorials/plant-3d-explorer/#2-connect-your-database-to-a-local-server","title":"2. Connect your database to a local server","text":"<ul> <li>Continue in the same shell terminal (if you open a new terminal, do not forget to activate appropriate python environment)</li> <li>set the DB location using the <code>DB_LOCATION</code> environment variable</li> <li>Type the following commands to launch the server:</li> </ul> <pre><code>export DB_LOCATION=/path/to/your/db\nromi_scanner_rest_api #command that starts the server\n</code></pre> <p>check result: the terminal prints various information given by the server (e.g. number of datasets in the database). Do not stop this terminal as this will shut down the server.</p>"},{"location":"plant_imager/tutorials/plant-3d-explorer/#3-connect-the-plant-3d-explorer-to-the-server","title":"3. Connect the <code>plant-3d-explorer</code> to the server","text":"<ul> <li>Open a new terminal</li> <li>go to your local cloned directory of <code>plant-3d-explorer/</code></li> <li>start the frontend visualization server by entering:</li> </ul> <pre><code>npm start\n</code></pre> <p>You should now be able to access the <code>plant-3d-explorer</code> on http://localhost:3000. Depending on you system preferences, your default web browser may automatically open a window displaying the server content. If not, open your web browser and enter http://localhost:3000 in the url bar.</p> <p>Note</p> <p>You need to add a file <code>.env.local</code> at project's root to set the API URL: <code>REACT_APP_API_URL='{`API URL}'</code>. Without this, the app will use: http://localhost:5000, which is the default for <code>romi_scanner_rest_api</code>.</p>"},{"location":"plant_imager/tutorials/plant-3d-explorer/#4-explore-your-database-content-via-the-menu-page","title":"4. Explore your database content via the menu page","text":"<p>Note</p> <p>More description about the function of the menu page: read here</p> <p>The starting page of the <code>plant-3d-explorer</code> lists the datasets of the connected database as a table and looks like this:</p> <p> </p> <ul> <li>The top search bar allows you to find particular datasets based on keywords.</li> <li> <p>Data filters: in the header row, click on an icon to activate the filter (datasets that do not contain the data will be filtered out)</p> <ul> <li> <p> 3d objects generated from the plant 2D images (icons respectively stand for: mesh, point cloud, segmented point cloud, skeleton and organs)</p> </li> <li> <p> or phyllotaxis data (manual or computed phyllotaxis measurements)</p> </li> </ul> </li> <li> <p>Open a dataset with the green 'Open' button at the far right of a   row </p> </li> </ul>"},{"location":"plant_imager/tutorials/plant-3d-explorer/#5-view-a-single-dataset-and-all-related-data","title":"5. View a single dataset and all related data","text":"<p>Note</p> <p>This is only a brief description to allow a quick start. More description here</p> <p>By default, the <code>plant-3d-explorer</code> displays in the main panel the skeleton and the organs (if available) and phyllotaxis data (as graphs) in the right panel.</p> <p> </p> <p>Mouse-over most elements provides a brief description.</p>"},{"location":"plant_imager/tutorials/plant-3d-explorer/#select-the-3d-layers-to-display","title":"Select the 3D layers to display","text":"<p>In the top left corner of the main panel, icons allows you to quickly (un)select 3D layers (if available):</p> <p></p> <ul> <li>White icons are active, dark grey are available but not active, light grey are not available for this dataset</li> <li>From left to right, icons represents respectively the mesh, the point-cloud, the segmented point cloud, the skeleton and the organs.</li> </ul> <p>In the center of the middle panel are icons for general viewing options: </p> <ul> <li>Activate the camera icon displays the camera poses (only works if overlay with 2 images is deactivated)</li> <li>Click the round arrow to reset the view</li> </ul>"},{"location":"plant_imager/tutorials/plant-3d-explorer/#moving-the-view-in-the-free-3d-without-2d-overlay","title":"Moving the view in the \"free\" 3D (without 2D overlay)","text":"<p>Easy movements are accessible with a mouse:</p> <ul> <li>scroll to zoom in/out</li> <li>left click rotate</li> <li>right click translate</li> </ul>"},{"location":"plant_imager/tutorials/plant-3d-explorer/#activate-overlay-with-2d-images","title":"Activate overlay with 2D images","text":"<p>click on any image of the bottom carousel to activate the display of 2 images in the main panel. On Mouse-over, a single picture is enlarged and proposes to open it in the main panel.</p> <p></p> <p>Overlay with active 3D layers is automatic. In the carousel, the box around the active displayed 2D image is now permanent.</p> <p>To close the 2D overlay, just click the close button of the boxed picture in the carousel.</p> <p></p>"},{"location":"plant_imager/tutorials/plant-3d-explorer/#moving-the-view-with-2d-overlay","title":"Moving the view with 2D overlay","text":"<p>Note that movement control with the mouse slightly changes compared to the \"free\" view without 2D overlay. Notably, the free rotation mode is not possible anymore, since it is constrained by the real movements made by the camera when it took the pictures.</p> <ul> <li>Slide right/left the active box picture in the carousel to reproduce the camera movement</li> <li>scroll to zoom in/out</li> <li>left-click to translate</li> </ul>"},{"location":"plant_imager/tutorials/plant-3d-explorer/#the-phyllotaxis-measure-plots","title":"The phyllotaxis measure plots","text":"<p>Plots represent the successive measures of divergence angles (left, in degrees) and internode length (right, in mm) between consecutive pairs of organs (here fruits) along the stem, from the base to the inflorescence tip. Both plots can be closed by clicking the cross at the far right of the plot's title. Closed plot panels can be re-opened by clicking a green \"+\" sign appearing at the right-hand corner when at least one plot is closed. In the plots, a blue curve correspond to \"automated\" measure computed through an analysis pipeline (such as pipelines developed in romi plant-3d-vision). If available in the dataset, a red curve indicates a ground-truth \"manual\" measure.</p> <p>Mouse-over any of the two plots highlights an interval that correspond to a measure between two consecutive organs (fruits) segmented by the analysis. The interval appears synchronously on both plots if opened. This interval and the organs are numbered by their order from the base of the stem, these numbers appear on the (vertical) X-axis of the plots. The exact value of the selected interval is displayed on top of the plot (\"automated\" and \"manual\" values in blue and red respectively, if available).</p> <p></p> <p>As shown above, when the 'organ' 3d-layer is active in the main panel, mouse-over the plot synchronously select the corresponding pair of organs in the current view (all other organ layers just disappear). Clicking the interval in the plot maintains the selection of the organ pair active despite further mouse movements. To deactivate the selection, click the green cross at the end of the shaded selection rectangle on either of the plots. Organ colors are also synchronized between main panel and plot panels. In the free 3D mode only, clicking a fruit layer display a bubble telling the organ number, colored as the corresponding fruit layer. Bubbles stay on screen if the 2D overlay is activated (as in the picture above).</p>"},{"location":"plant_imager/tutorials/plant-3d-explorer/#go-back-to-main-page","title":"Go back to main page","text":"<p>In the top left corner of the page, click \"all scans\":</p> <p></p>"},{"location":"plant_imager/tutorials/reconstruct_scan/","title":"Reconstruct 3D data from images","text":"<p>Plant reconstruction and analysis pipeline</p>"},{"location":"plant_imager/tutorials/reconstruct_scan/#getting-started","title":"Getting started","text":"<p>To follows this guide you should have:</p> <ul> <li>installed the necessary ROMI software here or followed the instructions for the docker   image here</li> <li>access to a database with a \"plant acquisition\" to reconstruct (or use the provided examples)</li> </ul>"},{"location":"plant_imager/tutorials/reconstruct_scan/#reconstruction-pipeline","title":"Reconstruction pipeline","text":""},{"location":"plant_imager/tutorials/reconstruct_scan/#cleaning-a-dataset","title":"Cleaning a dataset","text":"<p>If you made a mess, had a failure or just want to start fresh with your dataset, no need to save a copy on the side, you can use the <code>Clean</code> task:</p> <pre><code>romi_run_task Clean integration_tests/2019-02-01_10-56-33 \\\n--config plant3dvision/config/original_pipe_0.toml --local-scheduler\n</code></pre> <p>Here the config may use the <code>[Clean]</code> section where you can define the <code>force</code> option:</p> <pre><code>[Clean]\nforce=true\n</code></pre> <p>If <code>true</code> the <code>Clean</code> task will run silently, else in interactive mode.</p>"},{"location":"plant_imager/tutorials/reconstruct_scan/#geometric-pipeline","title":"Geometric pipeline","text":""},{"location":"plant_imager/tutorials/reconstruct_scan/#real-scan-dataset","title":"Real scan dataset","text":"<p>The full geometric pipeline, ie. all the way to angles and internodes measurement, can be called on real dataset with:</p> <pre><code>romi_run_task AnglesAndInternodes integration_tests/2019-02-01_10-56-33 \\\n--config plant3dvision/config/original_pipe_0.toml --local-scheduler\n</code></pre> <p>Note</p> <p>This example uses a real scan dataset from the test database.</p>"},{"location":"plant_imager/tutorials/reconstruct_scan/#virtual-plant-dataset","title":"Virtual plant dataset","text":"<p>The full geometric pipeline, ie. all the way to angles and internodes measurement, can be called on a virtual dataset with:</p> <pre><code>romi_run_task AnglesAndInternodes integration_tests/arabidopsis_26 \\\n--config plant3dvision/config/original_pipe_0.toml --local-scheduler\n</code></pre> <p>Note</p> <p>This example uses a virtual scan dataset from the test database.</p> <p>Warning</p> <p>If you get something like this during the <code>Voxel</code> tasks:</p> <p><pre><code>Choose platform:\n[0] &lt;pyopencl.Platform 'NVIDIA CUDA' at 0x55d904d5af50&gt;\nChoice [0]:\n</code></pre> that mean you need to specify the environment variable <code>PYOPENCL_CTX='0'</code></p>"},{"location":"plant_imager/tutorials/reconstruct_scan/#machine-learning-pipeline","title":"Machine Learning pipeline","text":"<p>Warning</p> <p>This requires the installation of the <code>romiseg</code> libraries (see here for install instructions) and a trained PyTorch model!</p> <p>Note</p> <p>A trained model, to place under <code>&lt;dataset&gt;/models/models</code>, is accessible here: https://media.romi-project.eu/data/Resnetdataset_gl_png_896_896_epoch50.pt</p>"},{"location":"plant_imager/tutorials/reconstruct_scan/#real-scan-dataset_1","title":"Real scan dataset","text":"<p>The full geometric pipeline, ie. all the way to angles and internodes measurement, can be called on real dataset with:</p> <pre><code>romi_run_task PointCloud integration_tests/2019-02-01_10-56-33 --config plant3dvision/config/ml_pipe_vplants_3.toml --local-scheduler\n</code></pre> <p>Note</p> <p>This example uses a real scan dataset from the test database.</p>"},{"location":"plant_imager/tutorials/reconstruct_scan/#virtual-plant-dataset_1","title":"Virtual plant dataset","text":"<p>The full geometric pipeline, ie. all the way to angles and internodes measurement, can be called on a virtual dataset with:</p> <pre><code>romi_run_task PointCloud integration_tests/arabidopsis_26 \\\n--config plant3dvision/config/ml_pipe_vplants_3.toml --local-scheduler\n</code></pre> <p>Note</p> <p>This example uses a virtual scan dataset from the test database.</p>"},{"location":"plant_imager/tutorials/robustness_evaluation/","title":"Evaluation of task robustness","text":"<p>We have a CLI tool to evaluate the robustness of the Reconstruction &amp; Quantification (R&amp;Q) tasks. Indeed, some tasks like <code>Colmap</code> are known to be stochastic, notably the feature detection and matching that use non-deterministic algorithms.</p> <p>Hereafter we give a design overview, some examples about task evaluation and the API.</p>"},{"location":"plant_imager/tutorials/robustness_evaluation/#design-overview","title":"Design overview","text":"<p>To evaluate to robustness of a task, we sought at a very simplistic and empirical approach: repeat the task and analyze the variability of the results.</p> <p>To do so we follow these steps:</p> <ol> <li>copy the selected scan dataset in a temporary database (and clean it from previous R&amp;Q if necessary)</li> <li>run the R&amp;Q pipeline up to the upstream task of the task to evaluate, if any upstream task exist</li> <li>replicate (copy) this result to an evaluation database as many times as requested (by the <code>-n</code> option, defaults to <code>30</code>)</li> <li>run the task to evaluate on each replicated scan dataset</li> <li>compare the directories and files of the task to evaluate pair by pair</li> <li>use the comparison metrics for the task to evaluate, as defined in <code>robustness_evaluation.json</code> </li> </ol> <p>Please note that:</p> <ul> <li>at step 3, a replicate id is appended to the replicated scan dataset name</li> <li>directory comparisons are done at the scale of the files generated by the selected task.</li> <li>we use metrics to get a quantitative comparison on the output of the task.</li> <li>it is possible to create fully independent repetitions by running the whole R&amp;Q pipeline on each scan dataset using the <code>--full-pipe</code> option (or the shorter <code>-f</code>).</li> <li>in order to use the ML-based R&amp;Q pipeline, you will have to:</li> <li>create an output directory</li> <li>use the <code>--models</code> argument to copy the CNN trained models</li> </ul>"},{"location":"plant_imager/tutorials/robustness_evaluation/#evaluate-the-robustness-of-the-structure-from-motion-algorithms","title":"Evaluate the robustness of the Structure from Motion algorithms","text":"<p>To evaluate the robustness of the Structure from Motion algorithms, one option is to independently reproduce the same estimation of the camera intrinsic and extrinsic parameters on several datasets.</p> <p>Camera extrinsic can be compared to their Euclidean distance to the expected CNC poses and the median poses obtained from Colmap. Camera intrinsics can be compared for a given camera model using simple statistics like the deviation from the mean value of each parameter.</p>"},{"location":"plant_imager/tutorials/robustness_evaluation/#command-line","title":"Command line","text":"<p>To evaluate the robustness of the Structure from Motion algorithms defined in the <code>Colmap</code> task proceed as follows: <pre><code>robustness_evaluation Colmap /Data/ROMI/eval_dataset_* \\\n--config config/geom_pipe_real.toml --clean --suffix exhaustive_matcher -n 50\n</code></pre></p>"},{"location":"plant_imager/tutorials/robustness_evaluation/#explanations","title":"Explanations","text":"<p>Let's break it down:</p> <ul> <li>we use a list of scan dataset matching the UNIX glob expression <code>eval_dataset_*</code> accessible under <code>/Data/ROMI/</code></li> <li>we use the <code>geom_pipe_real.toml</code> from the <code>config</code> directory of the <code>plant-3d-vision</code> Python module</li> <li>we request a <code>Clean</code> task to be performed prior to the robustness evaluation with the <code>--clean</code> option</li> <li>we append the <code>exhaustive_matcher</code> as a suffix to the evaluation database name with <code>--suffix exhaustive_matcher</code></li> <li>we request 50 repetitions of the <code>Colmap</code> task with <code>-n 50</code></li> </ul> <p>Note</p> <p>You may specify list of scan dataset, to use instead of a UNIX glob expression. It should be provided as a space separated list of directory names, e.g. <code>robustness_evaluation Colmap /Data/ROMI/eval_dataset_2 /Data/ROMI/eval_dataset_3</code>.</p>"},{"location":"plant_imager/tutorials/robustness_evaluation/#what-to-expect","title":"What to expect","text":"<p>The evaluation database, that is the database containing the replicated &amp; processed scan dataset, will be located under <code>/Data/ROMI</code> and should be named something like <code>YYYY.MM.DD_HH.MM_Eval_Colmap_exhaustive_matcher</code>.</p> <p>Within that directory you should also find the outputs of the evaluation methods.</p>"},{"location":"plant_imager/tutorials/robustness_evaluation/#evaluate-the-robustness-of-the-geometry-based-reconstruction-pipeline","title":"Evaluate the robustness of the geometry-based reconstruction pipeline","text":"<p>To evaluate the robustness of geometry-based reconstruction pipeline, one option is to independently reproduce the same workflow multiple times to see if we get a consistent point cloud. The distance between point clouds can be evaluated using a <code>chamfer distance</code>.</p>"},{"location":"plant_imager/tutorials/robustness_evaluation/#command-line_1","title":"Command line","text":"<p>To evaluate the robustness of the geometry-based reconstruction pipeline, defined by using the <code>geom_pipe_real.toml</code> configuration up to the <code>PointCloud</code> task, proceed as follows: <pre><code>robustness_evaluation PointCloud /Data/ROMI/eval_dataset/ \\\n--config config/geom_pipe_real.toml --full-pipe --clean --suffix independent_reconstruction -n 50\n</code></pre></p>"},{"location":"plant_imager/tutorials/robustness_evaluation/#explanations_1","title":"Explanations","text":"<p>Let's break it down:</p> <ul> <li>we use a scan dataset named <code>eval_dataset</code> accessible under <code>/Data/ROMI/</code></li> <li>we use the <code>geom_pipe_real.toml</code> from the <code>config</code> directory of the <code>plant-3d-vision</code> Python module</li> <li>we run the geometry-based reconstruction pipeline independently on each replicate</li> <li>we request a <code>Clean</code> task to be performed prior to the robustness evaluation with the <code>--clean</code> option</li> <li>we append the <code>independent_reconstruction</code> as a suffix to the evaluation database name with <code>--suffix independent_reconstruction</code></li> <li>we request 50 repetitions of the <code>Colmap</code> task with <code>-n 50</code></li> </ul>"},{"location":"plant_imager/tutorials/robustness_evaluation/#what-to-expect_1","title":"What to expect","text":"<p>The evaluation database, that is the database containing the replicated &amp; processed scan dataset, will be located under <code>/Data/ROMI</code> and should be named something like <code>YYYY.MM.DD_HH.MM_Eval_PointCloud_independent_reconstruction</code>.</p> <p>Within that directory you should also find the outputs of the evaluation methods.</p>"},{"location":"plant_imager/tutorials/robustness_evaluation/#reference-api","title":"Reference API","text":"<p>Please note that this might not be 100% accurate as this is a copy/paste from the terminal and the API might evolve.</p> <p>To be absolutely sure of the API, use the <code>robustness_evaluation -h</code> command.</p> <pre><code>usage: robustness_evaluation [-h] [--config CONFIG] [-n N_REPLICATES] [-c] [-f] [-np]\n                             [--suffix SUFFIX] [--eval-db EVAL_DB] [--date-fmt DATE_FMT] [--no-date]\n                             [--models MODELS]\n                             [--log-level {CRITICAL,FATAL,ERROR,WARN,WARNING,INFO,DEBUG,NOTSET}]\n                             task [dataset_path ...]\n\npositional arguments:\n  task                  Task to evaluate, should be in: AnglesAndInternodes, ClusteredMesh, Colmap,\n                        CurveSkeleton, ExtrinsicCalibration, IntrinsicCalibration, Masks,\n                        OrganSegmentation, PointCloud, Segmentation2D, Segmentation2d,\n                        SegmentedPointCloud, TreeGraph, TriangleMesh, Undistorted, Voxels\n  dataset_path          Path to scan dataset to use for task robustness evaluation.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --config CONFIG       Path to the pipeline TOML configuration file.\n\nEvaluation options:\n  -n N_REPLICATES, --n_replicates N_REPLICATES\n                        Number of replicates to create for task robustness evaluation. Defaults to `30`.\n  -c, --clean           Run a Clean task on scan dataset prior to duplication.\n  -f, --full-pipe       Use this to run the whole pipeline independently for each replicate. Else the task to evaluate is run on clones of the results from the upstream task, if any.\n  -np, --no-pipeline    Do not run the pipeline, only compare tasks outputs. Use with `--eval-db` to rerun this code on an existing test evaluation database!\n\nDatabase options:\n  --suffix SUFFIX       Suffix to append to the evaluation database directory to create.\n  --eval-db EVAL_DB     Existing evaluation database location to use.Use with `-np` to rerun this code on an existing test evaluation database!\n  --date-fmt DATE_FMT   Datetime format to use as prefix for the name of the evaluation database directory to create. Defaults to `\"%Y.%m.%d_%H.%M\"`.\n  --no-date             Do not add the datetime as prefix to the name of the evaluation database directory to create.\n\nOther options:\n  --models MODELS       Models database location to use with ML pipeline.\n  --log-level {CRITICAL,FATAL,ERROR,WARN,WARNING,INFO,DEBUG,NOTSET}\n                        Set message logging level. Defaults to `INFO`.\n</code></pre>"},{"location":"plant_imager/tutorials/tutorial_sm-dtw_simulatephyllotaxis/","title":"Evaluate a phyllotaxis measurement (case1) Use sm-dtw with simulated data","text":"<p>How to use the 'sm-dtw' assessment tool with simulated synthetic phyllotaxis data</p>"},{"location":"plant_imager/tutorials/tutorial_sm-dtw_simulatephyllotaxis/#related-materials","title":"Related Materials:","text":"<ul> <li> <p>Download and view the full tutorial video. See also below how to navigate rapidely into the content of this video.</p> </li> <li> <p>related readme procedure: you can also help by reading the readme of the project.</p> </li> </ul>"},{"location":"plant_imager/tutorials/tutorial_sm-dtw_simulatephyllotaxis/#objective","title":"Objective","text":"<ul> <li>generate synthetic data to use the sm-dtw program</li> <li>use sm-dtw</li> <li>evaluate the prediction made by sm-dtw</li> </ul>"},{"location":"plant_imager/tutorials/tutorial_sm-dtw_simulatephyllotaxis/#prerequisite","title":"Prerequisite","text":"<p>you only need to install docker. No other third-party software (R, python, jupyter, conda) is required, as all the programs are run from inside the docker container</p>"},{"location":"plant_imager/tutorials/tutorial_sm-dtw_simulatephyllotaxis/#step-by-step-tutorial","title":"Step-by-step tutorial","text":""},{"location":"plant_imager/tutorials/tutorial_sm-dtw_simulatephyllotaxis/#1-download-the-docker-image","title":"1. Download the docker image","text":"<p>It should be downloaded automatically when trying to run the docker for the first time</p> <pre><code>docker run -p 8888:8888 -it roboticsmicrofarms/sm-dtw_demo:latest bash\n</code></pre>"},{"location":"plant_imager/tutorials/tutorial_sm-dtw_simulatephyllotaxis/#2-using-jupyter-notebook-inside-a-docker","title":"2. Using jupyter notebook inside a docker:","text":""},{"location":"plant_imager/tutorials/tutorial_sm-dtw_simulatephyllotaxis/#running-the-jupyter-notebook","title":"Running the Jupyter notebook:","text":"<ol> <li>In your local machine, create a working directory to store the results generated by the notebook. It will be mounted inside the container, where its name will be \"docker_sandbox\". The name in you local machine can be identical or different, but keep the name inside the docker to avoid further changes in the notebooks. Sop open a terminal and type: </li> </ol> <pre><code>mkdir docker_sandbox # this folder can be created anywhere in your local machine\ncd docker_sandbox # Move into this working directory\n</code></pre> <ol> <li>Start the container:</li> </ol> <p><pre><code>docker run -v $(pwd):/myapp/docker_sandbox -p 8888:8888 -it roboticsmicrofarms/sm-dtw_demo:latest bash\n</code></pre> Note: you can start this command from anywhere in your local machine by correctly mapping the mounted volume in the string before the semi-column (<code>-v path/in/your/machine:/myapp/docker_sandbox</code>) </p> <ol> <li>Inside the container, serve the notebooks by entering the following command after the running container's prompt (which should be something like: <code>(dtw) root@aa7632c1fc29:/myapp#</code>):</li> </ol> <pre><code>jupyter notebook Phyllotaxis-sim-eval/notebooks/docker_run --ip 0.0.0.0 --no-browser --allow-root\n</code></pre> <ol> <li>Outside the docker, start your browser</li> <li>enter the url with token:\"http://127.0.0.1:8888/?token=.....\" or directly Ctrl+click on it</li> <li>select the correct kernel (R, dtw or bash, as indicated in the first cell of the notebook)</li> <li>run the notebook !</li> </ol>"},{"location":"plant_imager/tutorials/tutorial_sm-dtw_simulatephyllotaxis/#outline-of-the-tutorial-video","title":"Outline of the tutorial video","text":"<p>Here is the main content of the video and indicative time to help you directly jump into the chapters you are interested in !</p> Chapter Time start Content description Introduction 00min 00s Romi Project, object of this video Content Outline 00min 30s details the content of the video What is sm-dtw ? 1min 05s general description of sm-dtw program Why/when do I need 'sm-dtw' 2min 40s Presents the precise example of our use for phyllotaxis measurement in Romi and propose other possible user cases Where to find online written documentation 8min15 provides a link to the 'readme' page of the github repo of the project Docker &amp; Jupyter notebooks 8min 47 Download and start docker + start Jupyter Notebooks Prepare a working folder to store your data on your local machine Step1 (1st Jupyter notebook) 12min 12s Explains how to simulate synthetic data made of a sequence of reference phyllotaxis data (angles and internodes) a test sequence containing errors Step2 (2nd Jupyter notebook) 28min 15s Explains how to realignthe two phyllotaxis sequences generated in step1 using sm-dtw Step3 (3rd Jupyter notebook) 39min 11s Explains how to check that the alignment prediction made by sm-dtw correspond to the errors generated in step1 Quit and close Jupyter notebooks and docker container 51min 20s proper procedure to close and quit jupyter notebooks and the docker container."},{"location":"plant_imager/tutorials/virtual_plant_generate_analyze_visualize/","title":"Evaluate a 3D reconstruction and automated measures with a virtual plant as ground truth","text":"<p>Generate, analyze and visualize virtual plant images using public docker containers.</p>"},{"location":"plant_imager/tutorials/virtual_plant_generate_analyze_visualize/#objective","title":"Objective","text":"<p>Instead of installing <code>virtual-plant-imager</code>, <code>plant-3d-vision</code> and <code>plant-3d-explorer</code> on your computer you can pull their corresponding docker images and use directly the pre-installed software. After reading this tutorial, you should be able to generate, analyze and visualize virtual plant images using only public docker images.</p>"},{"location":"plant_imager/tutorials/virtual_plant_generate_analyze_visualize/#prerequisite","title":"Prerequisite","text":"<p>You must have <code>docker</code> and <code>docker-compose</code> installed on your system and ideally your user account has sudo privileges.</p>"},{"location":"plant_imager/tutorials/virtual_plant_generate_analyze_visualize/#1-prepare-the-database","title":"1. Prepare the database","text":"<p>First, you have to give the right access to your database path</p> <pre><code>sudo chmod -R 777 /path/of/your/db\n</code></pre> <p>Todo</p> <p>Fix these instructions as they are WRONG! We now use Unix group rights to access the database path!</p>"},{"location":"plant_imager/tutorials/virtual_plant_generate_analyze_visualize/#2-generate-virtual-dataset-with-plant-imager","title":"2. Generate virtual dataset with plant-imager","text":"<p>Pull the public docker image of <code>virtual-plant-imager</code></p> <pre><code>docker pull roboticsmicrofarms/virtual-plant-imager:latest\n</code></pre> <p>Run the docker image with the database mounted volume</p> <pre><code>docker run --rm -it --gpus all -v /path/to/your_db:/myapp/db roboticsmicrofarms/virtual-plant-imager:latest /bin/bash\n</code></pre> <p>Inside docker container, run <code>VirtualScan</code> task to generate a dataset named <code>virtual_ds_example</code></p> <pre><code>romi_run_task --config plant-imager/config/vscan_lpy_blender.toml VirtualScan /myapp/db/virtual_ds_example\n</code></pre> <p>Wait for the generation time, after complete, make sure that <code>virtual_ds_example</code> has been generated correctly.</p> <p>Exit docker container</p> <pre><code>exit\n</code></pre>"},{"location":"plant_imager/tutorials/virtual_plant_generate_analyze_visualize/#3-angles-and-internodes-analysis-and-visualization-generation-of-the-dataset","title":"3. Angles and Internodes analysis and visualization generation of the dataset","text":"<p>Pull the public docker image of <code>plant-3d-vision</code></p> <pre><code>docker pull roboticsmicrofarms/plant-3d-vision:latest\n</code></pre> <p>Run the docker image with the database mounted volume</p> <pre><code>docker run --rm -it --gpus all -v /path/to/your_db:/myapp/db roboticsmicrofarms/plant-3d-vision:latest /bin/bash\n</code></pre> <p>If not already activated, activate the right virtual environment</p> <pre><code>source /venv/bin/activate\n</code></pre> <p>Inside docker container, run <code>AnglesAndInternodes</code> task</p> <pre><code>romi_run_task --config config/geom_pipe_virtual.toml AnglesAndInternodes /myapp/db/virtual_ds_example/\n</code></pre> <p>Make sure that the folder <code>AnglesAndInternodes</code> has been generated</p> <p>Run the <code>Visualization</code> task</p> <pre><code>romi_run_task --config config/geom_pipe_virtual.toml Visualization /myapp/db/virtual_ds_example/\n</code></pre> <p>Make sure the <code>Visualization</code> folder has been generated</p> <p>Exit the docker container</p> <pre><code>exit\n</code></pre>"},{"location":"plant_imager/tutorials/virtual_plant_generate_analyze_visualize/#4-visualize-the-virtual-dataset-on-plant-3d-explorer","title":"4. Visualize the virtual dataset on plant-3d-explorer","text":"<p>Pull the public docker image of <code>plantdb</code></p> <pre><code>docker pull roboticsmicrofarms/plantdb\n</code></pre> <p>Pull the public docker image of <code>plant-3d-explorer</code></p> <pre><code>docker pull roboticsmicrofarms/plant-3d-explorer\n</code></pre> <p>Set <code>ROMI_DB</code> environment variable to point to your database</p> <pre><code>export ROMI_DB=/path/to/your/db\n</code></pre> <p>Create a file named <code>docker-compose.yml</code> so that it will contain the following:</p> <pre><code>version: '3'\nservices:\nplantdb:\nimage: \"roboticsmicrofarms/plantdb\"\nvolumes:\n- ${ROMI_DB}:/myapp/db\nports:\n- \"5000:5000\"\nhealthcheck:\ntest: \"exit 0\"\nplant-3d-explorer:\nimage: \"roboticsmicrofarms/plant-3d-explorer\"\ndepends_on:\n- plantdb\nenvironment:\nREACT_APP_API_URL: http://localhost:5000\nports:\n- \"3000:3000\"\n</code></pre> <p>Run the docker compose in the directory that contains <code>docker-compose.yml</code></p> <pre><code>docker-compose up -d\n</code></pre> <p>After a while, if everything is okay, you can visualize the database (containing <code>virtual_ds_example</code>) in your internet browser</p> <p>at the addres http://localhost:3000/</p> <p>Make sure that <code>virtual_ds_example</code> can be visualized correctly.</p> <p>Stop docker compose</p> <pre><code>docker-compose stop\n</code></pre>"},{"location":"plant_imager/tutorials/virtual_plant_imager_large_dataset/","title":"Generate large \"virtual\" training data for machine-learning","text":"<p>How to use the Virtual Plant Imager to generate a large dataset of virtual plant for machine learning purposes</p>"},{"location":"plant_imager/tutorials/virtual_plant_imager_large_dataset/#objective","title":"Objective","text":"<p>Working with virtual plants instead of real ones makes data acquisition inexpensive and has the advantage to parametrize the type of data. By design, ground truth data can be easily extracted from virtual datasets for evaluation purposes and building machine learning models. The Virtual Plant Imager is designed two address these two issues. After reading this tutorial, you should be able to generate a single virtual plant dataset in order to evaluate the robustness of plant-3d-vision.</p>"},{"location":"plant_imager/tutorials/virtual_plant_imager_large_dataset/#prerequisite","title":"Prerequisite","text":"<p>If it is not already done, you must be able to build and run the docker image by following the instructions.</p>"},{"location":"plant_imager/tutorials/virtual_plant_imager_large_dataset/#step-by-step-tutorial","title":"Step-by-step tutorial","text":"<p>Principle: Technically, the <code>Virtual Plant Imager</code> relies on Blender v2.93 to generate the images of 3d model of the plants. The 3d model can be provided as an input or can be also generated by lpy based on biological rules. An HTTP server acts as an interface to drive Blender generation scripts.</p>"},{"location":"plant_imager/tutorials/virtual_plant_imager_large_dataset/#1-preparing-your-scan-data","title":"1. Preparing your scan data","text":"<p>First, you have to create a working database on your host machine, let's say <code>home/host/path/database_example</code>. You can find an example of this database here.</p> <p>You can obtain sample data for the scanner here, and put it in the data folder.</p> <pre><code>wget https://db.romi-project.eu/models/arabidopsis_data.zip\nunzip arabidopsis_data.zip -d data\n</code></pre> <p>To use custom data, it must consist in <code>.obj</code> file, in which each type of organ corresponds to a distinct mesh. This mesh must have a single material whose name is the name of the organ. The data dir must contain the <code>obj</code> and <code>mtl</code> files.</p> <p>Additionally, background HDRI files can be downloaded from hdri haven. Download <code>.hdr</code> files and put them in the <code>hdri</code> folder.</p>"},{"location":"plant_imager/tutorials/virtual_plant_imager_large_dataset/#2-generating-a-large-dataset-for-machine-learning-purposes","title":"2. Generating a large dataset for machine learning purposes","text":"<p>After preparing your working database directory. You have to run the docker container with the database mounted.</p> <pre><code>cd plant-imager/docker\n./run.sh -db /home/host/path/database_example  # This will map to `db` directory located in the the docker's user home\n</code></pre> <p>To generate a large dataset, you have to run the script <code>generate_dataset.py</code> by passing the config file and the output folder.</p> <pre><code>(lpyEnv) user@5c9e389f223d  python generate_dataset.py plant-imager/config/vscan_lpy_blender.toml db/learning_set\n</code></pre> <p>After a while, and if the generation succeeded the <code>learning_set</code> folder will be populated by virtual plants.</p>"},{"location":"plant_imager/tutorials/virtual_plant_imager_single_dataset/","title":"Evaluate an analysis in a virtual world (both virtual plant and imager)","text":""},{"location":"plant_imager/tutorials/virtual_plant_imager_single_dataset/#objective","title":"Objective","text":"<p>Quantitative evaluation of a 3D reconstruction and/or automated measure from a phenotyping experiment is critical, from both developer and end-user perspectives. However, obtaining ground truth reference is often tedious (e.g. manual measurements, it must be anticipated (synchronous measures with image acquisition), and some type of data are just inacessible with available technologies (e.g. having a reference point cloud).</p> <p>Virtual plants makes data acquisition inexpensive and allows to parametrize the type of data. By design, ground truth data can be easily extracted from these virtual datasets. We thus designed the <code>Virtual Plant Imager</code> to take images of any 3D object, as a digital twin of our real <code>plant imager</code>.</p> <p>After reading this tutorial, you should be able to generate a single virtual plant dataset (including several ground truth reference) in order to evaluate the phenotyping results generated through an analysis pipeline made with our plant-3d-vision tool suite.</p>"},{"location":"plant_imager/tutorials/virtual_plant_imager_single_dataset/#prerequisite","title":"Prerequisite","text":"We highly recommend the use of docker containers to run ROMI software, if you wish to use the docker images we provide, have a look here.  <p>If it is not already done, you must be able to build and run the docker images of: </p> <ul> <li>the <code>(Virtual) Plant Imager</code> by following these instructions. This is required to generate the virtual data (initial plant 3D model, ground truth and RGB images).</li> <li>the <code>plant-3d-vision</code> by following these instructions. This is required to reconstruct a 3D model from the virtual 2D images, as if they were images of real plants. This docker will also allow you to evaluate this reconstruction using the available virtual ground truth data.</li> </ul>"},{"location":"plant_imager/tutorials/virtual_plant_imager_single_dataset/#step-by-step-tutorial","title":"Step-by-step tutorial","text":"<p>Principle:  You want to evaluate the results generated by an analysis pipeline made with our <code>plant-3d-vision</code> tool suite. Let's say that this pipeline is defined by a typical configuration file, <code>test_pipe.toml</code>. </p> <p>The idea is to generate images of a virtual 3D plant and provide these picture as input to the tested analysis pipeline. Technically, the <code>Virtual Plant Imager</code> relies on Blender v2.93 to generate at set of (2D) RGB images from the plant 3d model, mimicking what a real camera would do on a real plant. Any virtual camera pose can be generated (ie. distance, angle), but virtual poses similar to the real robot (<code>plant imager</code>) are preferred. An HTTP server acts as an interface to drive Blender generation scripts.</p> <p>The virtual plant 3D model (with some of its ground truth references) can be imported and given as an input. However, we provide an integrated procedure to generate a virtual 3D plant directly \"on the fly\" with Lpy, using a Lpy model and customizable parameters. Some ground truth references will also be automatically generated.</p> <p>Once this virtual plant has been virtually imaged, there are all data and metadata required to run an analysis with the tested pipeline. The results of this analysis will be compared to the virtual ground truth. Four type of evaluations are currently implemented :</p> <ul> <li>evaluation of a 2D segmentation</li> <li>evaluation of a 3D segmentation of the point cloud</li> <li>comparison of point cloud similarity</li> <li>evaluation of phyllotaxis measures (angles and internodes)</li> </ul>"},{"location":"plant_imager/tutorials/virtual_plant_imager_single_dataset/#1-prepare-data-into-a-proper-database","title":"1. Prepare data into a proper database","text":"<p>First, create a working directory on your host machine, let's say <code>home/host/path/my_virtual_db</code>. You can find an example of such a directory here.</p> <p>This working directory is a proper \"romi\" database which contains additional data for the virtual plant generation and/or imaging grouped in a so-called <code>`vscan_data</code> folder:</p> <p>Legend: </p> <ul> <li>(*) the name is fixed and cannot be changed</li> <li>(!) the folder/file must exist (no tag means that the folder is not required for the program to run)</li> </ul> <pre><code>my_virtual_db/\n\u2502   romidb (!*) # a (empty) marker file for recognition by the plantdb module\n\u2514\u2500\u2500\u2500vscan_data/ (!*) \n\u2502   \u2514\u2500\u2500\u2500hdri/ (*)\n\u2502       \u2502   hdri_file1.hdr\n\u2502       \u2502   hdri_file2.hdr\n\u2502       \u2502   etc...\n\u2502   \u2514\u2500\u2500\u2500lpy/ (*)\n\u2502       \u2502   my_plant_species_model.lpy\n\u2502   \u2514\u2500\u2500\u2500obj/ (*)\n\u2502       \u2502   VirtualPlant.obj\n\u2502       \u2502   VirtualPlant_mtl\n\u2502   \u2514\u2500\u2500\u2500metadata/(!*)\n\u2502       \u2502   hdri.json\n\u2502       \u2502   lpy.json\n\u2502       \u2502   obj.json\n\u2502       \u2502   palette.json\n\u2502       \u2502   scenes.json\n\u2502   \u2514\u2500\u2500\u2500palette/ (*)\n\u2502       \u2502   my_plant_species_model.png\n\u2502   \u2514\u2500\u2500\u2500scenes/ (*)\n\u2502   files.json\n</code></pre>"},{"location":"plant_imager/tutorials/virtual_plant_imager_single_dataset/#11-quick-ready-to-use-example","title":"1.1 quick ready-to-use example","text":"<p>Recommended if you are not familiar with the virtual plant imager.  You can directly obtain a functional working directory from the repository of the plant-imager you cloned in your host machine</p> <p>So if your working directory is named <code>my_virtual_db</code>, execute in a terminal: <pre><code>cd plant-imager # enter the cloned repository in your host machine\ncp -r database_example home/host/path/my_virtual_db\n</code></pre> To skip details and directly run the <code>virtual plant imager</code>, go now to section [2.](#2-generate-virtual-images-from-a-lpy-virtual-plant-in-a-virtual-scene)</p>"},{"location":"plant_imager/tutorials/virtual_plant_imager_single_dataset/#12-customize-data-of-the-virtual-plant-andor-of-the-virtual-images","title":"1.2 Customize data of the virtual plant and/or of the virtual images","text":"<p>Warning</p> <p>For advanced users. If you modify data, you most likely need to modify the configuration .toml file downstream. </p> <p>You can modify and enrich the virtual dataset in several manner (modifying the LPy model and parameters, importing your own model and avoiding Lpy-generation, change background scenes, etc...). For all these options, please refer to the specifications of the <code>virtual plant imager</code>.</p>"},{"location":"plant_imager/tutorials/virtual_plant_imager_single_dataset/#2-generate-virtual-images-from-a-lpy-virtual-plant-in-a-virtual-scene","title":"2. Generate virtual images from a (Lpy) virtual plant in a virtual scene","text":"<p>Start the docker container of the <code>plant-imager</code> with your database mounted:</p> <p><pre><code>cd plant-imager/docker\n./run.sh -db /home/host/path/my_virtual_db  # This will map your working databse to the `db` directory located in the docker's user home\n</code></pre> Then, in this docker container, generate the virtual dataset by running the following command:</p> <pre><code>(lpyEnv) user@5c9e389f223d  romi_run_task --config plant-imager/config/vscan_lpy_blender.toml VirtualScan db/my_virtual_plant # Run VirtualScan by specifying the output folder 'my_virtual_plant'\n</code></pre> <p>The computation can take a few minutes, depending on your system capacities. if it works, the terminal should display something like that: <pre><code>===== Luigi Execution Summary =====\n\nScheduled 4 tasks of which:\n* 2 complete ones were encountered:\n    - 1 LpyFileset(scan_id=vscan_data)\n- 1 PaletteFileset(scan_id=vscan_data)\n* 2 ran successfully:\n    - 1 VirtualPlant(...)\n- 1 VirtualScan(...)\n\nThis progress looks :) because there were no failed tasks or missing dependencies\n</code></pre> Results: in your database, a new folder (here called my_virtual_plant) should have been created, that contain data and metadata related to the virtual image acquisition of this virtual plant !</p> <pre><code>my_virtual_db\n\u2502   romidb\n\u2514\u2500\u2500\u2500vscan_data/\n\u2514\u2500\u2500\u2500my_virtual_plant/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 metadata/\n\u2502   \u2502   \u2514\u2500\u2500 images/\n\u2502   \u2502   \u2514\u2500\u2500 images.json\n\u2502   \u2514\u2500\u2500 files.json\n\u2502   \u2514\u2500\u2500 scan.toml\n</code></pre> <p>With the default parameters provided with this example (Lpy model and configuration file), there is only one generated plant, which has the following main characteristics</p> <ul> <li>It is a model of an Arabidopsis thaliana plant</li> <li>It has only a main stem and no lateral branches (simplified architecture)</li> <li>It is a mature plant, that has grown an elongated inflorescence stem bearing several mature fruit (called a 'silique', the typical pod of the Brassicaceae family) and still has some flowers at the very tip.</li> </ul> <p>In the next two sections, we point to simple paramaters of the configuration file used for this task to modify either the virtual plant or  the virtual imaging.</p>"},{"location":"plant_imager/tutorials/virtual_plant_imager_single_dataset/#21-optional-how-to-modify-the-virtual-plant-with-lpy-parameters","title":"2.1 (optional) how to modify the virtual plant with LPy parameters","text":"<p>Note</p> <p>Detailed description can be found here</p> <p>Below are the main lpy parameters that can be customized to change how the virtual plant looks like (age, size, branching, etc...).</p> <pre><code>[VirtualPlant.lpy_globals]\nBRANCHON = false\nMEAN_NB_DAYS = 70\nSTDEV_NB_DAYS = 5\nBETA = 51\nINTERNODE_LENGTH = 1.3\nSTEM_DIAMETER = 0.09\n</code></pre>"},{"location":"plant_imager/tutorials/virtual_plant_imager_single_dataset/#22-optional-how-to-modify-the-virtual-imaging-performed-by-the-virtual-imager","title":"2.2 (optional) how to modify the virtual imaging performed by the virtual imager","text":"<p>Note</p> <p>Detailed description can be found here</p> <p>Below are the main lpy parameters that can be customized to change how the virtual images are taken (path, background scenes, resolution, etc...)</p> <p>virtual camera path [ScanPath] class_name = \"Circle\" [ScanPath.kwargs] center_x = -2 center_y = 3 z = 65 tilt = 8 radius = 75 n_points = 18</p>"},{"location":"plant_imager/tutorials/virtual_plant_imager_single_dataset/#3-running-a-reconstruction-pipeline-on-the-virtual-dataset","title":"3. Running a reconstruction pipeline on the virtual dataset","text":"<p>Once you have a virtual dataset of images that all look like a real one, you can analyze it like a real one with romi pipelines from our <code>plant-3d-vision</code> tool suite !</p> <p>Remember that the pipeline you want to evaluate is defined by the following configuration file: <code>test_pipe.toml</code>.</p> <p>To adapt to the virtual imaging and focus the evaluation to the downstream image analysis and 3D reconstruction, you can adapt the configuration file to include ground truth from virtual imaging to use ground truth poses. Create a new configuration file for the evaluation and modify it as follows:</p> <p><pre><code>cp test_pipe.toml test_pipe_veval.toml #copy and rename the configuration file of the pipeline you want to test \n</code></pre> In the newly created <code>test_pipe_veval.toml</code>, deactivate use of colmap poses for the volume carving algorithm ([Voxel] Task of the pipeline). <pre><code>[Voxels]\nuse_colmap_poses = false\n[Masks]\nupstream_task = Scan\n</code></pre></p> <p>Then the analysis pipeline can be run as usual except that <code>colmap</code> will not be run :</p> <pre><code>romi_run_task \\ #romi pipeline scheduler \n--config path/to/test_pipe_veval.toml \\ # configuration of the pipeline to evaluate\nAnglesAndInternodes \\ #Last task to execute\n/path/to/my_virtual_plant #folder inside the database on which the analysis will be run\n</code></pre> <p>This run should process all dependencies and generates notably a segmented point cloud and measures of the phyllotaxis (angles and internodes) ! </p> <p>Note</p> <p>any available romi Tasks for image analysis can be runned here. Please refer to the list of Tasks implemented in our romi software suite.</p> <p>Note</p> <p>The command line can be executed in docker container or in a terminal if you have activated the correct virtual environments and proceeded to local installation of the software.</p> <p>Please refer to this tutorial if you encounter problems to run pipeline from our <code>plant-3d-vision</code> tool suite.</p> <p>After execution, the terminal should display luigi execution summary, as in this example: <pre><code>===== Luigi Execution Summary =====\n\nScheduled 8 tasks of which:\n* 2 complete ones were encountered:\n    - 1 ImagesFilesetExists(scan_id=, fileset_id=images)\n- 1 ModelFileset(scan_id=models)\n* 6 ran successfully:\n    - 1 AnglesAndInternodes(...)\n- 1 OrganSegmentation(scan_id=, upstream_task=SegmentedPointCloud, eps=2.0, min_points=5)\n- 1 PointCloud(...)\n- 1 Segmentation2D(...)\n- 1 SegmentedPointCloud(scan_id=, upstream_task=PointCloud, upstream_segmentation=Segmentation2D, use_colmap_poses=False)\n...\n\nThis progress looks :) because there were no failed tasks or missing dependencies\n\n===== Luigi Execution Summary =====\n</code></pre> Results: new subfolders and metadata resulting from the analysis should have been created in the folder of the analyzed plant dataset (called <code>my_virtual_plant</code> in this example).</p> <p>The particular data generated depends on the pipeline you called. We provide an example here with a pipeline involving machine-learning based segmentation of 2D images and proceeding up to phyllotaxis measures (angles &amp; internodes.)</p> <p>legend:    * (.) indicates data that were already present before the run of the pipeline (but the data content may have been modified)   * folder names generated by the analysis generally start with the corresponding Task name end with a hashcode to keep track of task execution by the Luigi scheduler (e.g. _1_0_2_0_0_1_5f7aad388e). Such code is replaced by '_hashcode' suffix in the example below</p> <pre><code>my_virtual_db\n\u2502   romidb (.)\n\u2514\u2500\u2500\u2500vscan_data/ (.)\n\u2514\u2500\u2500\u2500my_virtual_plant/ (.)\n\u2502   \u251c\u2500\u2500 images/ (.)\n\u2502   \u251c\u2500\u2500 metadata/ (.)\n\u2502   \u251c\u2500\u2500 AnglesAndInternodes_hashcode/\n\u2502   \u251c\u2500\u2500 OrganSegmentation_hashcode/\n\u2502   \u251c\u2500\u2500 PointCloud_hashcode/\n\u2502   \u251c\u2500\u2500 Segmentation2D_hashcode/\n\u2502   \u251c\u2500\u2500 PointCloudGroundTruth_100000__VirtualPlantObj_hashcode/\n\u2502   \u251c\u2500\u2500 SegmentedPointCloud__Segmentation2D_PointCloud_3a1e8e0010/\n\u2502   \u251c\u2500\u2500 VirtualPlant/\n\u2502   \u251c\u2500\u2500 VirtualPlant_arabidopsis_note___BRANCHON___fal___angles____inte_hashcode/\n\u2502   \u251c\u2500\u2500 Voxels_False___background_____False_hashcode/\n\u2502   \u2514\u2500\u2500 files.json (.)\n\u2502   \u2514\u2500\u2500 scan.toml (.)\n\u2502   \u2514\u2500\u2500 pipeline.toml\n</code></pre>"},{"location":"plant_imager/tutorials/virtual_plant_imager_single_dataset/#4-evaluate-the-quality-of-the-construction-by-comparing-to-the-virtual-ground-truth-data","title":"4. Evaluate the quality of the construction by comparing to the virtual ground truth data","text":"<p>(work in progress)</p> <p>Once the analysis results are generated, you can now compare this results to the expected ground truth reference of the virtual plant.</p> <p>Several Evaluation Tasks have been developed by romi: check the list to know which results are evaluating each of them.</p> <p>In the below example, we would like to evaluate the point-cloud reconstruction, so we run:</p> <pre><code>romi_run_task \\ #romi pipeline scheduler \n--config path/to/test_pipe_veval.toml \\ # configuration of the pipeline to evaluate \nPointCloudEvaluation \\ #evaluation task of to run\n/path/to/my_virtual_plant #analyzed data folder of the database that you want to evaluate\n</code></pre> <p>Note</p> <p>Please refer to this [tutorial] if you encounter problems to run pipeline from our <code>plant-3d-vision</code> tool suite.</p>"},{"location":"plant_imager/tutorials/virtual_plant_imager_single_dataset/#5-view-and-scrutinize-in-3d-all-data-generated-images-reconstruction-and-evaluation","title":"5. View and scrutinize in 3D all data generated  (images, reconstruction and evaluation)","text":"<p>(work in progress) Use of the plant-3d-explorer</p>"},{"location":"training/","title":"Training &amp; workshops","text":"<p>We here regroup some information about the training sessions, workshops and talks we made around the world for this project.</p>"},{"location":"training/#training","title":"Training","text":"<p>As part of the ROMI project, we here propose a list of training courses related to the usage of our technologies:</p> <ul> <li> Learn how to create virtual plants with L-Py.</li> <li> You may also find video training session on our general website.</li> </ul>"},{"location":"training/#workshop","title":"Workshop","text":"<ul> <li> IPPN Angers 2023: material used and presented to the 2023 IPPN workshop in Angers, France.</li> </ul>"},{"location":"training/ippn_2023/","title":"Workshop IPPN Anger 2023","text":""},{"location":"training/ippn_2023/#talk","title":"Talk","text":"<p>By Fabrice Besnard.</p> <p>You can find the slides used during the presentation made by Fabrice Besnard on the ROMI Google Drive here.</p>"},{"location":"training/ippn_2023/#demonstration","title":"Demonstration","text":"<p>By Jonathan Legrand &amp; Fabrice Besnard.</p> <p>You can find the slides used during the demonstration made by Jonathan Legrand on the ROMI Google Drive here</p>"},{"location":"training/ippn_2023/#getting-started","title":"Getting started","text":""},{"location":"training/ippn_2023/#install-docker-engine","title":"Install Docker Engine","text":"<p>If it's not done already, install the Docker Engine on your machine.</p> <p>Follow the official instructions here: Install Docker Engine.</p>"},{"location":"training/ippn_2023/#pull-the-docker-image","title":"Pull the Docker image","text":"<p>We made a specific Docker image for this workshop shipping an example dataset and notebooks.</p> <p>To pull this image, from our DockerHub: <pre><code>docker pull roboticsmicrofarms/plant-3d-vision:ippn\n</code></pre></p>"},{"location":"training/ippn_2023/#start-a-container","title":"Start a container","text":"<p>To start a container based on this <code>roboticsmicrofarms/plant-3d-vision:ippn</code> image: <pre><code>docker run --rm --gpus all -p 8888:8888 roboticsmicrofarms/plant-3d-vision:ippn\n</code></pre></p> <p>This should start a container and jupyter notebook.</p> <p>Note</p> <ul> <li><code>--rm</code> automatically remove the container when it exits</li> <li><code>-gpus all</code> add all GPU devices to the container</li> <li><code>-p 8888:8888</code> bing your port 8888 to the container port 8888</li> </ul> <p>To stop the container, press ctrl + c twice to exit the jupyter notebook, this will also stop (and destroy) the container.</p>"},{"location":"training/ippn_2023/#connect-to-the-notebook","title":"Connect to the notebook","text":"<p>Following the previous <code>docker run</code> command, a series of messages should appear in the terminal. They are emitted by jupyter notebook, and they should give you something like:</p> <pre><code>    To access the notebook, open this file in a browser:\n        file:///home/myuser/.local/share/jupyter/runtime/nbserver-1-open.html\n    Or copy and paste one of these URLs:\n        http://fdc0ad0ca87d:8888/?token=0598c4791146a98cf9b0c9344c32a7df15e703af79415ea7\n     or http://127.0.0.1:8888/?token=0598c4791146a98cf9b0c9344c32a7df15e703af79415ea7\n</code></pre> <p>You may click or copy/paste the URL starting with <code>127.0.0.1:8888</code> in your favorite browser. This should take you to the landing page of jupyter notebook.</p> <p>You should see something like this: </p>"},{"location":"training/ippn_2023/#run-a-notebook","title":"Run a notebook","text":"<p>To start playing around with our notebooks, move to the <code>notebooks</code> folder, then select the notebook you want to use.</p>"},{"location":"training/ippn_2023/#access-a-running-container","title":"Access a running container","text":"<p>Once your container is up and running, you may open a second terminal and connect to the running container. This can be useful, notably to perform other tasks than running the notebook. </p> <p>Your first need to know the ID or NAME of your container. You may access this information by calling <code>docker container list</code>.</p> <p>For example: <pre><code>CONTAINER ID   IMAGE                                     COMMAND                  CREATED          STATUS          PORTS                                       NAMES\n969493021b00   roboticsmicrofarms/plant-3d-vision:ippn   \"/bin/bash -c 'jupyt\u2026\"   30 seconds ago   Up 29 seconds   0.0.0.0:8888-&gt;8888/tcp, :::8888-&gt;8888/tcp   exciting_shamir\n</code></pre></p> <p>To access this container you can then call: <pre><code>docker exec -it 969493021b00 bash\n</code></pre> You should now be logged as <code>myuser</code> inside the <code>969493021b00</code> container, and your terminal should look like this: <pre><code>myuser@969493021b00:~$ </code></pre> To exit the running container, press ctrl + d to log out and destroy it.</p>"},{"location":"training/ippn_2023/#launch-an-interactive-container","title":"Launch an interactive container","text":"<p>If you do not wish to use the jupyter notebook or want to start the container and gain access to the terminal, simply do: <pre><code>docker run -it --rm --gpus all -p 8888:8888 roboticsmicrofarms/plant-3d-vision:ippn bash\n</code></pre></p> <p>To exit the running container, press ctrl + d to log out and destroy it.</p>"},{"location":"training/ippn_2023/#perform-a-reconstruction-job","title":"Perform a reconstruction job","text":"<p>Once you followed the access a running container or launch an interactive container instruction, you can launch a reconstruction job on a dataset.</p> <p>For example, to obtain a point cloud using the geometric based reconstruction pipeline for the Ler_20220803_2_1 dataset, simply run: <pre><code># Start by cleaning the dataset:\nromi_run_task Clean $DB_LOCATION/Ler_20220803_2_1 --config plant-3d-vision/config/geom_pipe_real.toml\n# Reconstruct the point cloud:\nromi_run_task PointCloud $DB_LOCATION/Ler_20220803_2_1 --config plant-3d-vision/config/geom_pipe_real.toml\n</code></pre></p>"},{"location":"training/ippn_2023/#access-a-host-directory-from-the-container","title":"Access a host directory from the container","text":"<p>As we did not bind a local directory to the container, everything you do in that container will stay inside it (hence the name...). This also mean that upon exit (and destruction with the <code>--rm</code> option), all of your work and modification will be gone!</p> <p>To prevent this, you may use the <code>-v</code> option to bind mount a local folder as a volume in the container. For example, to mount another database from your host, registered under <code>$DB_LOCATION</code>, use: <code>-v $DB_LOCATION:/myapp/db</code>.</p> <p>Note</p> <p>This will 'overwrite' the demonstration database (for this container only).</p>"},{"location":"training/ippn_2023/#fyi","title":"FYI","text":"<p>Here are a few facts about this Docker image:</p> <ul> <li>The <code>plant-3d-vision</code> sources can be found under <code>$HOME/plant-3d-vision</code>.</li> <li>The database can be found under <code>/myapp/db</code>, this path is saved to the <code>DB_LOCATION</code> environment variable.</li> <li>The shared database has been acquired using our Plant Imager v2 with a Sony RX-0 camera (more   info here).</li> <li>If you perform a reconstruction using the task <code>Colmap</code> it will use another Docker image   named <code>roboticsmicrofarms/colmap</code>.</li> </ul>"},{"location":"training/lpy/","title":"Create virtual plants with L-Py","text":"<p>To create virtual plants you can use the L-Py library that implement L-systems in Python with an integrated visual development environment to facilitate the creation of plant models.</p> <p>L-systems are a mathematical framework for modeling growth of plants. You can learn more about the L-systems on the official documentation.</p>"},{"location":"training/lpy/#getting-started","title":"Getting started","text":"<p>The official L-Py documentation can be found here: https://lpy.readthedocs.io/en/latest/index.html</p> <p>Install with conda:</p> <pre><code>conda create -n lpy openalea.lpy -c fredboudon -c conda-forge\n</code></pre> <p>Then, you need to activate the <code>lpy</code> environment</p> <pre><code>conda activate lpy\n</code></pre> <p>Finally, you can start L-Py with:</p> <pre><code>lpy\n</code></pre>"},{"location":"training/lpy/#training-courses","title":"Training courses","text":""},{"location":"training/lpy/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>a first contact with python language.</li> <li>laptop or computer with Linux, MacOS (or possibly windows).</li> </ul>"},{"location":"training/lpy/#helpcard","title":"Helpcard","text":"<p>https://lpy.readthedocs.io/en/latest/user/helpcard.html</p>"}]}
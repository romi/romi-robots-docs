{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"about/","text":"About the ROMI project Link Project funding Link This project has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement No 773875. Research teams Link IAAC develops an aerial robot that can be used by farmers. Iaac also performs real-world tests in the experimental gardens at the Valldaura Self-Sufficient Labs and imagine end-user scenarios. They help deliver the robotics platform to new markets, managing the communication and user communities. Sony CSL is responsible for the development of the LettuceThink robot. They also contribute to the development of the computer vision and machine learning algorithms, in particular, on the 3D plant scanning and the coupling between the formal plant models and the convolutional neural networks. The Virtual Plants team brings its strong expertise in the area of 3D plant architecture reconstruction and modelling. Notably, the team develops computer pipelines to reconstruct plant architecture from 3D data, to assess their reconstruction, and to segment the architecture in its constituent organs. The Adaptive Systems Group expertise lies in models for closed-loop learning and prediction of sensorimotor data, as well as behaviour recognition and generation. The tasks planned will focus on the learning and adaptive techniques for the interaction between robots and plants. The RDP team has a deep understanding of the development and evolution of plant reproductive systems. RDP leads the advanced sensing and analysis of crops, and brings its expertise on the developmental dynamics and modelling of plant architecture. Chatelain P\u00e9pini\u00e8res runs a commercial market farm near Paris. They perform field studies to test the efficiency of the weeding robot and the usefulness of the crop monitoring applications in real-world situations. FEI provides assistance and training for projects partly funded by the European Commission, as coordinator or as partner. FEI intervenes close to them in the administrative, financial coordination and management of their projects. Official Website Link This is the documentation website of the ROMI project, to access the public project presentation , follow this link: https://romi-project.eu/ GitHub sources Link For now these sources are private. Presentation videos Link Preliminary videos to learn more about the project tools!","title":"About"},{"location":"about/#about-the-romi-project","text":"","title":"About the ROMI project"},{"location":"about/#project-funding","text":"This project has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement No 773875.","title":"Project funding"},{"location":"about/#research-teams","text":"IAAC develops an aerial robot that can be used by farmers. Iaac also performs real-world tests in the experimental gardens at the Valldaura Self-Sufficient Labs and imagine end-user scenarios. They help deliver the robotics platform to new markets, managing the communication and user communities. Sony CSL is responsible for the development of the LettuceThink robot. They also contribute to the development of the computer vision and machine learning algorithms, in particular, on the 3D plant scanning and the coupling between the formal plant models and the convolutional neural networks. The Virtual Plants team brings its strong expertise in the area of 3D plant architecture reconstruction and modelling. Notably, the team develops computer pipelines to reconstruct plant architecture from 3D data, to assess their reconstruction, and to segment the architecture in its constituent organs. The Adaptive Systems Group expertise lies in models for closed-loop learning and prediction of sensorimotor data, as well as behaviour recognition and generation. The tasks planned will focus on the learning and adaptive techniques for the interaction between robots and plants. The RDP team has a deep understanding of the development and evolution of plant reproductive systems. RDP leads the advanced sensing and analysis of crops, and brings its expertise on the developmental dynamics and modelling of plant architecture. Chatelain P\u00e9pini\u00e8res runs a commercial market farm near Paris. They perform field studies to test the efficiency of the weeding robot and the usefulness of the crop monitoring applications in real-world situations. FEI provides assistance and training for projects partly funded by the European Commission, as coordinator or as partner. FEI intervenes close to them in the administrative, financial coordination and management of their projects.","title":"Research teams"},{"location":"about/#official-website","text":"This is the documentation website of the ROMI project, to access the public project presentation , follow this link: https://romi-project.eu/","title":"Official Website"},{"location":"about/#github-sources","text":"For now these sources are private.","title":"GitHub sources"},{"location":"about/#presentation-videos","text":"Preliminary videos to learn more about the project tools!","title":"Presentation videos"},{"location":"data/","text":"","title":"Data"},{"location":"documentation/","text":"Robotics for micro-farms Link ROMI is a four-year Europe-funded research project committed to promote a sustainable, local, and human-scale agriculture. The goal is to develop an open-source, affordable, multipurpose platform adapted to support organic and poly-culture market-garden farms. Plant Phenotyping Crop Monitoring Rover Complete module list Link Type Source code link Hardware Plant Imager Cable Bot Rover Storage PlantDB Viewers Plant 3D Explorer Farmers Dashboard Algorithms Virtual Plant Imager Plant 3D Vision romiseg DTW Third-party & wrapping romicgal","title":"Robotics for micro-farms"},{"location":"documentation/#robotics-for-micro-farms","text":"ROMI is a four-year Europe-funded research project committed to promote a sustainable, local, and human-scale agriculture. The goal is to develop an open-source, affordable, multipurpose platform adapted to support organic and poly-culture market-garden farms. Plant Phenotyping Crop Monitoring Rover","title":"Robotics for micro-farms"},{"location":"documentation/#complete-module-list","text":"Type Source code link Hardware Plant Imager Cable Bot Rover Storage PlantDB Viewers Plant 3D Explorer Farmers Dashboard Algorithms Virtual Plant Imager Plant 3D Vision romiseg DTW Third-party & wrapping romicgal","title":"Complete module list"},{"location":"glossary/","text":"Glossary Link We hereafter defines the semantic, names and abbreviations to use in the projects documentations and communications. ROMI Software : the whole set of software developed by ROMI; ROMI Hardware : the three types of robots developed by ROMI, namely the \"cable bot\", the \"rover\" and the \"plant imager\"; Database related Link database : the database itself; scan : a set of images, and the pipelines results; fileset : a set of files ( e.g. a set of RGB images of a plant); file : a file ( e.g. an RGB image of a plant); plant metadata : set of FAIR metadata attached to the plant ( e.g. species, age, growth conditions...); acquisition metadata : set of metadata attached to the acquisition procedure & hardware configuration ( e.g. version of the CNC controller, camera settings, ...); Danger \"scans\" could be renamed \"dataset\" or !","title":"Glossary"},{"location":"glossary/#glossary","text":"We hereafter defines the semantic, names and abbreviations to use in the projects documentations and communications. ROMI Software : the whole set of software developed by ROMI; ROMI Hardware : the three types of robots developed by ROMI, namely the \"cable bot\", the \"rover\" and the \"plant imager\";","title":"Glossary"},{"location":"glossary/#database-related","text":"database : the database itself; scan : a set of images, and the pipelines results; fileset : a set of files ( e.g. a set of RGB images of a plant); file : a file ( e.g. an RGB image of a plant); plant metadata : set of FAIR metadata attached to the plant ( e.g. species, age, growth conditions...); acquisition metadata : set of metadata attached to the acquisition procedure & hardware configuration ( e.g. version of the CNC controller, camera settings, ...); Danger \"scans\" could be renamed \"dataset\" or !","title":"Database related"},{"location":"research/","text":"Research & communications Link Journal papers Link INRIA: Chaudhury A., Godin C. Skeletonization of Plant Point Cloud Data Using Stochastic Optimization Framework . Front Plant Sci. 2020;11:773. Published 2020 Jun 16. doi:10.3389/fpls.2020.00773 UBER, Sony : Schillaci G., Pico Villalpando A., Hafner V. V., Hanappe P., Colliaux D. and Wintz, T. Intrinsic motivation and episodic memories for robot exploration of high-dimensional sensory spaces . Adaptive Behavior , June 2020, doi:10.1177/1059712320922916. ( arXiv:2001.01982 ) UBER : Hafner, V. V., Loviken, P., Pico Villalpando, A., Schillaci, G. (2020). Prerequisites for an Artificial Self . Frontiers in Neurorobotics . Vol. 14, p.5. doi:10.3389/fnbot.2020.00005 . ISSN 1662-5218. Book chapters Link INRIA: A. Chaudhury and C. Godin, Geometry Reconstruction of Plants , in Intelligent Image Analysis for Plant Phenotyping , CRC Press/Taylor and Francis, 2020 (to appear). doi: doi:10.1201/9781315177304 Participation in conferences Link UBER: Pico, A., Schillaci, G., Hafner, V.V. and Lara, B. (2019). Ego-Noise Predictions for Echolocation in Wheeled Robots , Alife 2019 - The 2019 Conference on Artificial Life . pp. 567-573. MIT Press. doi:10.1162/isal_a_00222 Participation in workshops Link INRIA: Chaudhury A., Boudon, F., and Godin C. 3D Plant Phenotyping: All You Need is Labelled Point Cloud Data . Workshop on Computer Vision Problems for Plant Phenotyping, 2020. IAAC, Sony : Sollazzo A., Colliaux D., Garivani S., Minchin J., Garlanda L. and Hanappe, P. Automated vegetable growth analysis from outdoor images acquired with a cablebot . Workshop on Computer Vision Problems for Plant Phenotyping, 2020. (PDF) INRIA: Florian Ingels, contributed talk at the annual meeting of the French Statistical Society Journ\u00e9es de Statistique 2019 (Romain Aza\u00efs and Florian Ingels), 2019/06/03-07. Sony: Wintz T., Colliaux D., Hanappe P. Automated extraction of phyllotactic traits from Arabidopsis thaliana . Workshop on Computer Vision Problems in Plant Phenotyping (CVPPP), 2018 (PDF, last visited 30/10/2020). Sony: Colliaux D., Lahlou A., Wintz T., Boucher E., Garlanda L., Mac\u00e9 A., Hanappe P. Robots for data collection in biology and agriculture . Poster presentation, International Crop Modelling Symposium (iCROPM2020), 3-5 February 2020, Montpellier, France. Sony: Colliaux D., Lahlou A., Wintz T., Boucher E., Garlanda L., Mac\u00e9 A., Hanappe P. Developing low-cost robots for micro-farms: the benefits of computer vision . Poster at the Plant People Planet Symposium, London, 4\u20135 September 2019 (Abstracts book)","title":"Research"},{"location":"research/#research-communications","text":"","title":"Research &amp; communications"},{"location":"research/#journal-papers","text":"INRIA: Chaudhury A., Godin C. Skeletonization of Plant Point Cloud Data Using Stochastic Optimization Framework . Front Plant Sci. 2020;11:773. Published 2020 Jun 16. doi:10.3389/fpls.2020.00773 UBER, Sony : Schillaci G., Pico Villalpando A., Hafner V. V., Hanappe P., Colliaux D. and Wintz, T. Intrinsic motivation and episodic memories for robot exploration of high-dimensional sensory spaces . Adaptive Behavior , June 2020, doi:10.1177/1059712320922916. ( arXiv:2001.01982 ) UBER : Hafner, V. V., Loviken, P., Pico Villalpando, A., Schillaci, G. (2020). Prerequisites for an Artificial Self . Frontiers in Neurorobotics . Vol. 14, p.5. doi:10.3389/fnbot.2020.00005 . ISSN 1662-5218.","title":"Journal papers"},{"location":"research/#book-chapters","text":"INRIA: A. Chaudhury and C. Godin, Geometry Reconstruction of Plants , in Intelligent Image Analysis for Plant Phenotyping , CRC Press/Taylor and Francis, 2020 (to appear). doi: doi:10.1201/9781315177304","title":"Book chapters"},{"location":"research/#participation-in-conferences","text":"UBER: Pico, A., Schillaci, G., Hafner, V.V. and Lara, B. (2019). Ego-Noise Predictions for Echolocation in Wheeled Robots , Alife 2019 - The 2019 Conference on Artificial Life . pp. 567-573. MIT Press. doi:10.1162/isal_a_00222","title":"Participation in conferences"},{"location":"research/#participation-in-workshops","text":"INRIA: Chaudhury A., Boudon, F., and Godin C. 3D Plant Phenotyping: All You Need is Labelled Point Cloud Data . Workshop on Computer Vision Problems for Plant Phenotyping, 2020. IAAC, Sony : Sollazzo A., Colliaux D., Garivani S., Minchin J., Garlanda L. and Hanappe, P. Automated vegetable growth analysis from outdoor images acquired with a cablebot . Workshop on Computer Vision Problems for Plant Phenotyping, 2020. (PDF) INRIA: Florian Ingels, contributed talk at the annual meeting of the French Statistical Society Journ\u00e9es de Statistique 2019 (Romain Aza\u00efs and Florian Ingels), 2019/06/03-07. Sony: Wintz T., Colliaux D., Hanappe P. Automated extraction of phyllotactic traits from Arabidopsis thaliana . Workshop on Computer Vision Problems in Plant Phenotyping (CVPPP), 2018 (PDF, last visited 30/10/2020). Sony: Colliaux D., Lahlou A., Wintz T., Boucher E., Garlanda L., Mac\u00e9 A., Hanappe P. Robots for data collection in biology and agriculture . Poster presentation, International Crop Modelling Symposium (iCROPM2020), 3-5 February 2020, Montpellier, France. Sony: Colliaux D., Lahlou A., Wintz T., Boucher E., Garlanda L., Mac\u00e9 A., Hanappe P. Developing low-cost robots for micro-farms: the benefits of computer vision . Poster at the Plant People Planet Symposium, London, 4\u20135 September 2019 (Abstracts book)","title":"Participation in workshops"},{"location":"Farmers%20Dashboard/","text":"Because of the legal restrictions on the use of drones and because of the rapid evolution of the drone market, the ROMI project has decided to direct its effort to a hardware solution that complements the existing tools (commercial drones, the Rover): the Cablebot. The Cablebot is adapted for use in greenhouses and polytunnels. These installations take up more than 10% of microfarms and are ill-suited for the use of drones. This reorientation increases the impact of ROMI since we can handle a wider variety of contexts than planned. The use of drones is still an option. Existing drones can still be used in combination with the Farmer\u2019s Dashboard. To maximize the reuse of components, the Cablebot consists of the following modules: A mobile carrier that can moves along the cable. A camera module that easily clips on/off the mobile carrier. A fixed battery charging station . For convenience, when we use the term cablebot, we sometimes refer to the mobile carrier mentioned above, and sometimes to the three modules used as a whole. In general, the meaning should be clear from the context. The design of the camera module is shared between the Cablebot and the Plant Scanner.","title":"Introduction"},{"location":"Farmers%20Dashboard/app/","text":"","title":"Farmers Dashboard App"},{"location":"Farmers%20Dashboard/bot/","text":"The mobile carrier (cablebot) Link The mobile carrier is an autonomous motion platform capable of travelling suspended on a single tensioned nylon cable. Its design allows it to move it from one cable to another as well to adapt to different cable tensions dynamically ; those features are unique among other systems Structure Link The whole design uses a CNC folded aluminium-plastic sandwich panel designed to provide adequate outdoor resistance while being light-weight. The moving parts and other fixtures use custom 3D printed plastic (PLA/ABS). All the cabling and electronics integrate into the internal structure to minimize damage caused by environmental factors. The tension mechanism design allows it to move it from one cable to another as well to adapt to different cable tensions dynamically, and it uses a self-lubricated polymer to avoid maintenance. All the parts can be made on a Fab Lab or other rapid prototyping facility with no customized tooling required. Motion system Link The platform is battery powered (12V LiPo 2Ah) and uses a brushless servo motor (DFROBOT FIT0441) combined with an optical cable tracker (ADNS-9800) to precisely move using a close loop system. It also contains built-in end stops switches as well as an inertial measurement unit to ensure it can safely operate autonomously. Control Link The entire motion system is controlled by a low-cost and low-power microcontroller (Microchip ATmega328) that interfaces with the camera module. The much powerful computer in the camera module runs the main logics and communication subsystem based on the software and hardware stack used in the Rover, ensuring modularity and scalability. As both the camera module and the Rover run the Raspberry PI ARM based Linux architecture our software stack is portable across each one of the robots. Those ensuring the Rover and the carrier use the same remote management interfaces. Following that approach the carrier can be managed using the same standard RC remote controller for on-site maintenance operations. Interface and management Link The primary communication is achieved via Wi-Fi to interact with the farmer phone or laptop, the local farm server or to a remote instance through the internet. Wi-Fi ensures enough bandwidth is available to perform the image uploads as well as over-the-air software updates, and it eliminates the need for customized gateways. When a remote connection is required, and the farm does not have one, a Wi-Fi to 4G (or 5G) gateway is located in the recharging station at the cable end.","title":"Introduction"},{"location":"Farmers%20Dashboard/bot/#the-mobile-carrier-cablebot","text":"The mobile carrier is an autonomous motion platform capable of travelling suspended on a single tensioned nylon cable. Its design allows it to move it from one cable to another as well to adapt to different cable tensions dynamically ; those features are unique among other systems","title":"The mobile carrier (cablebot)"},{"location":"Farmers%20Dashboard/bot/#structure","text":"The whole design uses a CNC folded aluminium-plastic sandwich panel designed to provide adequate outdoor resistance while being light-weight. The moving parts and other fixtures use custom 3D printed plastic (PLA/ABS). All the cabling and electronics integrate into the internal structure to minimize damage caused by environmental factors. The tension mechanism design allows it to move it from one cable to another as well to adapt to different cable tensions dynamically, and it uses a self-lubricated polymer to avoid maintenance. All the parts can be made on a Fab Lab or other rapid prototyping facility with no customized tooling required.","title":"Structure"},{"location":"Farmers%20Dashboard/bot/#motion-system","text":"The platform is battery powered (12V LiPo 2Ah) and uses a brushless servo motor (DFROBOT FIT0441) combined with an optical cable tracker (ADNS-9800) to precisely move using a close loop system. It also contains built-in end stops switches as well as an inertial measurement unit to ensure it can safely operate autonomously.","title":"Motion system"},{"location":"Farmers%20Dashboard/bot/#control","text":"The entire motion system is controlled by a low-cost and low-power microcontroller (Microchip ATmega328) that interfaces with the camera module. The much powerful computer in the camera module runs the main logics and communication subsystem based on the software and hardware stack used in the Rover, ensuring modularity and scalability. As both the camera module and the Rover run the Raspberry PI ARM based Linux architecture our software stack is portable across each one of the robots. Those ensuring the Rover and the carrier use the same remote management interfaces. Following that approach the carrier can be managed using the same standard RC remote controller for on-site maintenance operations.","title":"Control"},{"location":"Farmers%20Dashboard/bot/#interface-and-management","text":"The primary communication is achieved via Wi-Fi to interact with the farmer phone or laptop, the local farm server or to a remote instance through the internet. Wi-Fi ensures enough bandwidth is available to perform the image uploads as well as over-the-air software updates, and it eliminates the need for customized gateways. When a remote connection is required, and the farm does not have one, a Wi-Fi to 4G (or 5G) gateway is located in the recharging station at the cable end.","title":"Interface and management"},{"location":"Farmers%20Dashboard/camera/","text":"Both the Cablebot and the Scanner require a reliable camera module, although the usage in both cases is slightly different: For the Scanner , the camera is positioned at a given angle. The movement from angle position to the next is relativement infrequent and slow. For the Cablebot , the camera must adjust in real-time for swinging movements of the system. Controller board Link We decided to use a brushless motor, as is the custom in camera mount systems. We designed a controller board that exploits the functions offered by the TI DRV8313 chip. The DRV8313 requires as an input three Pulse Width Modulation signals (PWM) that encode the phase of each of the three voltages applied to the solenoids of the brushless motor. The control software is integrated into the code for the Romi Rover: https://github.com/romi/romi-rover-build-and-test/tree/ci_dev/romi-rover/gimbal_bldc The design files can be found at https://github.com/romi/bldc_featherwing Microcontroller Link The PWMs signals should be of a high frequency, as to avoid any ripples in the signal, and should be closely synchronised. We therefore opted for a Cortex M0 microcontroller instead of the more common AVR microcontrollers found on the Arduino Uno, for example. Concretely, we are using the Adafruit Feather M0 Basic, but the code should run on any Arduino -compatible SAMD21 microcontroller board. Motor and encoder Link To estimate the angular position of the motor we use a HAL-based encoder. We are using standard components that are sold for the drone market. In particular, we are using the iPower Motor GM4108H-120T Gimbal Motor with AS5048A Encoder and slip ring from iFlight-rc.com. Camera Link We use the recent Raspberry Pi High Quality Camera Module. The module is connected to the Raspberry Pi Zero W single-board computer. The camera module has a CS mount that allows us to change the lens. Wiring Link We had recurring problems with the cabling of the cameras in our previous solutions. The micro USB connectors were not reliable enough and often lost contact. The transmission of the power and the communication over a long USB cable often failed (power drop, broken serial link). We therefore choose to bring a 12V cable to the camera, connect it over a sturdy plug, and include a DC-DC converter inside the camera. Also, we use the WiFi functionality offered by the Raspberry Pi Zero W to send commands and download the images. Housing Link A housing was designed that can be made using a 3D printer. Control software: A firmware for the microcontroller board was written. The code has been integrated into the Romi Rover code repository so that it can be shared among all devices (Cablebot, Scanner, and Rover). The control software reuses the network functionality provided by the Rcom library developed in WP2 and the camera functions are accessible using standard web technology (HTTP and WebSockets). Info Camera module tests on cablebot carrier Info The camera module is is still on testing.","title":"Camera Module"},{"location":"Farmers%20Dashboard/camera/#controller-board","text":"We decided to use a brushless motor, as is the custom in camera mount systems. We designed a controller board that exploits the functions offered by the TI DRV8313 chip. The DRV8313 requires as an input three Pulse Width Modulation signals (PWM) that encode the phase of each of the three voltages applied to the solenoids of the brushless motor. The control software is integrated into the code for the Romi Rover: https://github.com/romi/romi-rover-build-and-test/tree/ci_dev/romi-rover/gimbal_bldc The design files can be found at https://github.com/romi/bldc_featherwing","title":"Controller board"},{"location":"Farmers%20Dashboard/camera/#microcontroller","text":"The PWMs signals should be of a high frequency, as to avoid any ripples in the signal, and should be closely synchronised. We therefore opted for a Cortex M0 microcontroller instead of the more common AVR microcontrollers found on the Arduino Uno, for example. Concretely, we are using the Adafruit Feather M0 Basic, but the code should run on any Arduino -compatible SAMD21 microcontroller board.","title":"Microcontroller"},{"location":"Farmers%20Dashboard/camera/#motor-and-encoder","text":"To estimate the angular position of the motor we use a HAL-based encoder. We are using standard components that are sold for the drone market. In particular, we are using the iPower Motor GM4108H-120T Gimbal Motor with AS5048A Encoder and slip ring from iFlight-rc.com.","title":"Motor and encoder"},{"location":"Farmers%20Dashboard/camera/#camera","text":"We use the recent Raspberry Pi High Quality Camera Module. The module is connected to the Raspberry Pi Zero W single-board computer. The camera module has a CS mount that allows us to change the lens.","title":"Camera"},{"location":"Farmers%20Dashboard/camera/#wiring","text":"We had recurring problems with the cabling of the cameras in our previous solutions. The micro USB connectors were not reliable enough and often lost contact. The transmission of the power and the communication over a long USB cable often failed (power drop, broken serial link). We therefore choose to bring a 12V cable to the camera, connect it over a sturdy plug, and include a DC-DC converter inside the camera. Also, we use the WiFi functionality offered by the Raspberry Pi Zero W to send commands and download the images.","title":"Wiring"},{"location":"Farmers%20Dashboard/camera/#housing","text":"A housing was designed that can be made using a 3D printer. Control software: A firmware for the microcontroller board was written. The code has been integrated into the Romi Rover code repository so that it can be shared among all devices (Cablebot, Scanner, and Rover). The control software reuses the network functionality provided by the Rcom library developed in WP2 and the camera functions are accessible using standard web technology (HTTP and WebSockets). Info Camera module tests on cablebot carrier Info The camera module is is still on testing.","title":"Housing"},{"location":"Farmers%20Dashboard/electronics/","text":"The navigation control is managed by any Arduino compatible board. This board will receive direct instructions via RC control or commands through the Serial port sent by the Raspberry pi in the Camera Module. General Mobile Carrier electronics schematic For now an Arduino Nano (Atmega328) is used as the main microcontroller, the code has been kept easily portable to any Cortex M0 board like the Feather M0 basic used in the Camera module. INPUTS Notes Endstops 2 interrupts Laser position encoder SPI Motor encoder 1 interrupt Battery voltage from voltage divider RF speed channel 1 interrupts per channel Control commands USB or UART Serial port via level shifter in case of 5v IMU I2C Temperature / Humidity sensor (Sensirion SHT35) I2C bus (on the works) Charger connected signal 1 interrupt User button 1 interrupt Outputs Notes Motor control PWM User led Addressable RGB Navigation info Serial Port Brushless DC Motor with Encoder Link The carrier use this brushless motor with an integrated encoder. The Torque in this motor is small but no external control electronics are needed, it is being tested to see if the torque is enough to handle the carrier weight on a big range of cable tensions. Operating Voltage 12V Motor Rated Speed 7100-7300rpm Torque 2.4kg*cm Speed 159 rpm Reduction ratio 45:1 Signal cycle pulse number 45*6 (Each cycle outputs 6 pulse) Control mode PWM speed control, Direction control, Feedback pulse output Weight 65g with kitchen scale A higher torque motor is going to be tested This motor has shown a lack of torque on hight tension cables, research to find a suitable higher torque replacement is on the works Remote control Link As a way to control the carrier when no camera module is present or when the user needs to do simple and direct control for maintenance tasks a standard PWM remote control control is used. Exponential smoothing is used to remove noise on the RC signal. Mouse laser motion sensor Link As a way to get real closed loop navigation the ADNS-9800 Laser Motion Sensor is being used to track the movement over the cable. This sensor is still to be integrated in the latest prototype, tests are being made to warranty that the cable is always visible to the sensor independently of the tension level. Optimal position of the laser sensor. IMU Link An Inertial measurement unit is used to give feedback to the microcontroller about balance and vibrations, acceleration and speed algorithms to take advantage of this data are still under development. The ISM330 Adafruit QWIIC breakout board is being used through I2C bus. Compatible Adafruit library Arduino library Endstops Link To detect collisions OMRON D3V-013-1C23 miniature switches are used as end stops on both sides of the cablebot. A 3d printed cover protects the electronic parts and trigers the switch when an obstacle is found. Cabling is routed through the structure to avoid any damage on the lines. The cabling is routed trough machined channels between the aluminum sandwich sheets keeping it protected and organized.","title":"Electronics"},{"location":"Farmers%20Dashboard/electronics/#brushless-dc-motor-with-encoder","text":"The carrier use this brushless motor with an integrated encoder. The Torque in this motor is small but no external control electronics are needed, it is being tested to see if the torque is enough to handle the carrier weight on a big range of cable tensions. Operating Voltage 12V Motor Rated Speed 7100-7300rpm Torque 2.4kg*cm Speed 159 rpm Reduction ratio 45:1 Signal cycle pulse number 45*6 (Each cycle outputs 6 pulse) Control mode PWM speed control, Direction control, Feedback pulse output Weight 65g with kitchen scale A higher torque motor is going to be tested This motor has shown a lack of torque on hight tension cables, research to find a suitable higher torque replacement is on the works","title":"Brushless DC Motor with Encoder"},{"location":"Farmers%20Dashboard/electronics/#remote-control","text":"As a way to control the carrier when no camera module is present or when the user needs to do simple and direct control for maintenance tasks a standard PWM remote control control is used. Exponential smoothing is used to remove noise on the RC signal.","title":"Remote control"},{"location":"Farmers%20Dashboard/electronics/#mouse-laser-motion-sensor","text":"As a way to get real closed loop navigation the ADNS-9800 Laser Motion Sensor is being used to track the movement over the cable. This sensor is still to be integrated in the latest prototype, tests are being made to warranty that the cable is always visible to the sensor independently of the tension level. Optimal position of the laser sensor.","title":"Mouse laser motion sensor"},{"location":"Farmers%20Dashboard/electronics/#imu","text":"An Inertial measurement unit is used to give feedback to the microcontroller about balance and vibrations, acceleration and speed algorithms to take advantage of this data are still under development. The ISM330 Adafruit QWIIC breakout board is being used through I2C bus. Compatible Adafruit library Arduino library","title":"IMU"},{"location":"Farmers%20Dashboard/electronics/#endstops","text":"To detect collisions OMRON D3V-013-1C23 miniature switches are used as end stops on both sides of the cablebot. A 3d printed cover protects the electronic parts and trigers the switch when an obstacle is found. Cabling is routed through the structure to avoid any damage on the lines. The cabling is routed trough machined channels between the aluminum sandwich sheets keeping it protected and organized.","title":"Endstops"},{"location":"Farmers%20Dashboard/legacy/","text":"Wirebot Link Main Board and Eletronics Link Raspberry Pi - packages, Dependencies and Configurations Link Download the image Robotics Ubuntu+ROS Raspberry Pi Image (3B+ Support) that comes with Ubuntu 16.04 (LXDE), and ROS Kinetic. Copy the image to the SD card. Instructions here. Resizes the file system to fill the SD card before booting following this instructions. Acces to the raspi-config utility: $computer :~ $sudo raspi-config Choose \"Expand root partition to fill SD card\" option: The Ubiquityrobotics images come up as a Wifi acces point. The SSID is ubiquityrobotXXXX where XXXX is part of the MAC address. Connect to the wifi hostopost and use folowing wifi password: robotseverywhere Once connected, it is possible to log into the Pi with ssh ubuntu@10.42.0.1 with the following password of: ubuntu Desable the default robots and node runing on the pi. $ ubuntu@ubiquityrobot.local: $sudo systemctl disable magni-base Raspberry Pi - Setting up the WIREDBOT to the Network Link Open a new terminal window, and log in to the robot with ssh: ATENTION : The HOSTNAME for firts time is \u201cubiquityrobot.local\u201d. $ computer:~ $ssh ubuntu@ubiquityrobot.local ATENTION : The password for firts time is \u201cubuntu\u201d. Change the hostname using pifi. Type the following command: $ ubuntu@ubiquityrobot.local:~ $sudo pifi set-hostname wiredbot Reboot the Pi. $ ubuntu@ubiquityrobot.local:~ $sudo reboot Log in to the robot with the new hostname \"wiredbot\": $ computer:~ $ssh ubuntu@wiredbot.local Use pifi to list the nearby networks: $ ubuntu@wiredbot:~ $pifi list seen ATENTION : Search for the network where the robots are connected. Swich to to the desire network by using the following command. $ ubuntu@NEWHOSTNAME:~ $sudo pifi add localNetwork password ATTENTION : The keyword \"localNetwork\" on this documentation refert to the network the robot need to be connected. The keyword \"pass\" on this documentation refer to the password of the network. Reboot the Pi. $ ubuntu@wiredbot:~ $sudo reboot Test the connectivity with the Pi. Open a new terminal window on a external on a diferent computer: $ computer:~ $ping wirebot.local TIP : Press control-c to stop the pinging ADVERTENCE : If something goes wrong, the PI will come back up as access point mode. Search on the network for the name ubiquityrobot, reboot and start over. Log into the PI by using: $ computer:~ $ssh ubuntu@wirebot.local the output will be: The authenticity of host \u201810.0.0.113 ( 10 .0.0.113 ) \u2019 can\u2019t be established. ECDSA key fingerprint is SHA256:sDDeGZzL8FPY3kMmvhwjPC9wH+mGsAxJL/dNXpoYnsc. Are you sure you want to continue connecting ( yes/no ) ? continue by wrinting: $ computer:~ $yes the password is still. ubuntu Update and updagrade de Pi. $ ubuntu@wiredbot:~ $sudo apt-get update $ ubuntu@wiredbot:~ $sudo apt-get upgrade ROS - Setting up the ROS NODES and Arduino Firmware. Link Make sure you have installed the resent updates and updagrades. $ ubuntu@wiredbot:~ $sudo apt-get update $ ubuntu@wiredbot:~ $sudo apt-get upgrade Point to the workspace folder for ros packages Clone the repository on the Pi, the romi/grlbl_serial into the /src folder of your catkin workspace and rebuild your workspace: $ ubuntu@wiredbot:~ $cd ~/catkin_ws/src/ $ ubuntu@wiredbot:~ $git clone git@github.com:romi/grlbl_serial.git $ ubuntu@wiredbot:~ $catkin_make Clone the repository on the Pi, the romi/i2c_pca9685_driver into the /src folder of your catkin workspace and rebuild your workspace: $ ubuntu@wiredbot:~ $cd ~/catkin_ws/src/ $ ubuntu@wiredbot:~ $git clone git@github.com:romi/i2c_pca9685_driver.git $ ubuntu@wiredbot:~ $catkin_make Wiring diagram. Link Schematics: List Part Item Description Quantity 0 Raspberry pi model 3b+ 1 1 Raspberry Pi Camera Module v2 1 2 16-Channel 12-bit - I2C interface - PCA9685 1 3 Arduino UNO 1 4 Arduino CNC Shield V3 1 5 A4988 Stepper Motor Driver 4 6 Nema 23 Unipolar 1.8deg 1 7 Survey3W Camera - Orange+Cyan+NIR (OCN, NDVI) 1 8 Survey3W HDMI PWM Trigger Cable 1 9 Survey3 Advanced GPS Receiver 1 10 12V Power Supply 1 11 Wires and general hardware - Hardware Setup. Link 1.Drawings * Assebly drawing - Top view * Assebly drawing - Botton View List Part Item Description Quantity 0 Aluminium Profile 20\u00d720 T-Slot 5 4 1 Idler Pulley Plate 6 2 Join Plat T 4 3 Corner connector 90 degree (V-Slot) 2 4 Gantry Plate V-Slot 20-80 2 5 3M Drop in Tee Nuts \u2013 Insert nuts 50 6 3M Allen Low Profile Screws 50 7 M8 Allen Screw - 45mm Long 6 8 Motor Mount Plate NEMA 23 1 9 Nylon Pulley And Wheel - 40 mm Diameter - 8 mm Bearing 6 10 Nema 23 stepper motor 1 11 P65 Weatherproof Enclosure/electrical enclosure box 2 12 5mm Shock Cord - Marine Grade Polyester Coated Rubber Rope - Running ROS node - Path Planning Link ROS Nodes Overview. ROS Master - Run ROS Nodes over the raspberry PI. Log into the raspberry PI by using: $ computer:~ $ssh ubuntu@wirebot.local (OPTIONAL) Edit the path planning according to the dimensions of the field to scan and the desired length and amount of waypoints. $ ubuntu@ubiquityrobot.local:~ $sudo nano ~/catkin_ws/src/grlbl_serial/src/path_planning_action_client.py * Edit the path_planning_action_client.py by changing the variable movement_goal.xyz_position that is under the function def path_planning_client() . Here is an example of a Path planning that takes pictures of every 500mm in a distance of 10mts: movement_goal.xyz_position = [\"{'x':0, 'y':0, 'z':500, 'delay':20}\", \"{'x':0, 'y':0, 'z':1000, 'delay':20}\", \"{'x':0, 'y':0, 'z':1500, 'delay':20}\", \"{'x':0, 'y':0, 'z':2000, 'delay':20}\", \"{'x':0, 'y':0, 'z':2500, 'delay':20}\", \"{'x':0, 'y':0, 'z':3000, 'delay':20}\", \"{'x':0, 'y':0, 'z':3500, 'delay':20}\", \"{'x':0, 'y':0, 'z':4000, 'delay':20}\", \"{'x':0, 'y':0, 'z':4500, 'delay':20}\", \"{'x':0, 'y':0, 'z':5000, 'delay':20}\", \"{'x':0, 'y':0, 'z':5500, 'delay':20}\", \"{'x':0, 'y':0, 'z':6000, 'delay':20}\", \"{'x':0, 'y':0, 'z':6500, 'delay':20}\", \"{'x':0, 'y':0, 'z':7000, 'delay':20}\", \"{'x':0, 'y':0, 'z':7500, 'delay':20}\", \"{'x':0, 'y':0, 'z':8000, 'delay':20}\", \"{'x':0, 'y':0, 'z':8500, 'delay':20}\", \"{'x':0, 'y':0, 'z':9000, 'delay':20}\", \"{'x':0, 'y':0, 'z':9500, 'delay':20}\"] Start up the nodes and the ROS master by launching the path_planning_action_server_node node under the raspberry PI: $ ubuntu@ubiquityrobot.local:~ $roslaunch grlbl_serial path_planning_action_server_node.launch (ADVERTENCE) If the ROS package is not under the autocomplete method of the terminal. The problem will be solve by sourcing the devel/setup.bash. $ ubuntu@ubiquityrobot.local:~ $source ~/catkin_ws/src/devel/setup.bash 3. ROS Slave - Run ROS Nodes over the Remote Computer. Start up the nodes by launching the path_planning_action_client_node node under the remote computer: $ ubuntu@ubiquityrobot.local:~ $roslaunch grlbl_serial path_planning_action_client_node.launch (OPTIONAL) This node as well can by launch over the raspberry PI. This can be done by lauching the node over a new terminal. By launching the previous ROS node on the WIREDBOT. The starting process of collecting photos from the Mapir camera and the Raspi Cam will be launch automatically according to the path planning instructions save on the path_planning_action_client.py file. Saving the data from the WIREDBOT. (UNDER-DEVELOPMENT) Link Kepp running or re start the node and the ROS master by launching the path_planning_action_server_node node under the raspberry PI: $ ubuntu@ubiquityrobot.local:~ $roslaunch grlbl_serial path_planning_action_server_node.launch Publish a 1500us to the /i2c_pca9685_driver wiredbot_PWMValues/wiredbot_PWMValues mapir_control_pwm: $ ubuntu@ubiquityrobot.local:~ $rostopic pub -1 /i2c_pca9685_driver wiredbot_PWMValues/wiredbot_PWMValues int16 mapir_control_pwm 1500 int16 motor_A_pwm int16 motor_B_pwm Once ros is publishing the message mapir_control_pwm 1500us under the topic \\i2c_pca9685_driver\\wiredbot_PWMValues. The camera is ready to mount. On the raspberry PI. Mount the camera by using the following commands. $ ubuntu@ubiquityrobot.local:~ $mkdir /mapir $ ubuntu@ubiquityrobot.local:~ $mkdir sudo mount -t vfat /dev/sdb2 /mapir $ ubuntu@ubiquityrobot.local:~ $cd /mapir/DCMI/Photos * Image Gallery - Valldaura: Link Lettuce Think and Wirebot: Wirebot on the field: Wirebot on the field: WIREBOT 3D Scans: Link","title":"Legacy"},{"location":"Farmers%20Dashboard/legacy/#wirebot","text":"","title":"Wirebot"},{"location":"Farmers%20Dashboard/legacy/#main-board-and-eletronics","text":"","title":"Main Board and Eletronics"},{"location":"Farmers%20Dashboard/legacy/#raspberry-pi-packages-dependencies-and-configurations","text":"Download the image Robotics Ubuntu+ROS Raspberry Pi Image (3B+ Support) that comes with Ubuntu 16.04 (LXDE), and ROS Kinetic. Copy the image to the SD card. Instructions here. Resizes the file system to fill the SD card before booting following this instructions. Acces to the raspi-config utility: $computer :~ $sudo raspi-config Choose \"Expand root partition to fill SD card\" option: The Ubiquityrobotics images come up as a Wifi acces point. The SSID is ubiquityrobotXXXX where XXXX is part of the MAC address. Connect to the wifi hostopost and use folowing wifi password: robotseverywhere Once connected, it is possible to log into the Pi with ssh ubuntu@10.42.0.1 with the following password of: ubuntu Desable the default robots and node runing on the pi. $ ubuntu@ubiquityrobot.local: $sudo systemctl disable magni-base","title":"Raspberry Pi - packages, Dependencies and Configurations"},{"location":"Farmers%20Dashboard/legacy/#raspberry-pi-setting-up-the-wiredbot-to-the-network","text":"Open a new terminal window, and log in to the robot with ssh: ATENTION : The HOSTNAME for firts time is \u201cubiquityrobot.local\u201d. $ computer:~ $ssh ubuntu@ubiquityrobot.local ATENTION : The password for firts time is \u201cubuntu\u201d. Change the hostname using pifi. Type the following command: $ ubuntu@ubiquityrobot.local:~ $sudo pifi set-hostname wiredbot Reboot the Pi. $ ubuntu@ubiquityrobot.local:~ $sudo reboot Log in to the robot with the new hostname \"wiredbot\": $ computer:~ $ssh ubuntu@wiredbot.local Use pifi to list the nearby networks: $ ubuntu@wiredbot:~ $pifi list seen ATENTION : Search for the network where the robots are connected. Swich to to the desire network by using the following command. $ ubuntu@NEWHOSTNAME:~ $sudo pifi add localNetwork password ATTENTION : The keyword \"localNetwork\" on this documentation refert to the network the robot need to be connected. The keyword \"pass\" on this documentation refer to the password of the network. Reboot the Pi. $ ubuntu@wiredbot:~ $sudo reboot Test the connectivity with the Pi. Open a new terminal window on a external on a diferent computer: $ computer:~ $ping wirebot.local TIP : Press control-c to stop the pinging ADVERTENCE : If something goes wrong, the PI will come back up as access point mode. Search on the network for the name ubiquityrobot, reboot and start over. Log into the PI by using: $ computer:~ $ssh ubuntu@wirebot.local the output will be: The authenticity of host \u201810.0.0.113 ( 10 .0.0.113 ) \u2019 can\u2019t be established. ECDSA key fingerprint is SHA256:sDDeGZzL8FPY3kMmvhwjPC9wH+mGsAxJL/dNXpoYnsc. Are you sure you want to continue connecting ( yes/no ) ? continue by wrinting: $ computer:~ $yes the password is still. ubuntu Update and updagrade de Pi. $ ubuntu@wiredbot:~ $sudo apt-get update $ ubuntu@wiredbot:~ $sudo apt-get upgrade","title":"Raspberry Pi - Setting up the WIREDBOT to the Network"},{"location":"Farmers%20Dashboard/legacy/#ros-setting-up-the-ros-nodes-and-arduino-firmware","text":"Make sure you have installed the resent updates and updagrades. $ ubuntu@wiredbot:~ $sudo apt-get update $ ubuntu@wiredbot:~ $sudo apt-get upgrade Point to the workspace folder for ros packages Clone the repository on the Pi, the romi/grlbl_serial into the /src folder of your catkin workspace and rebuild your workspace: $ ubuntu@wiredbot:~ $cd ~/catkin_ws/src/ $ ubuntu@wiredbot:~ $git clone git@github.com:romi/grlbl_serial.git $ ubuntu@wiredbot:~ $catkin_make Clone the repository on the Pi, the romi/i2c_pca9685_driver into the /src folder of your catkin workspace and rebuild your workspace: $ ubuntu@wiredbot:~ $cd ~/catkin_ws/src/ $ ubuntu@wiredbot:~ $git clone git@github.com:romi/i2c_pca9685_driver.git $ ubuntu@wiredbot:~ $catkin_make","title":"ROS - Setting up the ROS NODES and Arduino Firmware."},{"location":"Farmers%20Dashboard/legacy/#wiring-diagram","text":"Schematics: List Part Item Description Quantity 0 Raspberry pi model 3b+ 1 1 Raspberry Pi Camera Module v2 1 2 16-Channel 12-bit - I2C interface - PCA9685 1 3 Arduino UNO 1 4 Arduino CNC Shield V3 1 5 A4988 Stepper Motor Driver 4 6 Nema 23 Unipolar 1.8deg 1 7 Survey3W Camera - Orange+Cyan+NIR (OCN, NDVI) 1 8 Survey3W HDMI PWM Trigger Cable 1 9 Survey3 Advanced GPS Receiver 1 10 12V Power Supply 1 11 Wires and general hardware -","title":"Wiring diagram."},{"location":"Farmers%20Dashboard/legacy/#hardware-setup","text":"1.Drawings * Assebly drawing - Top view * Assebly drawing - Botton View List Part Item Description Quantity 0 Aluminium Profile 20\u00d720 T-Slot 5 4 1 Idler Pulley Plate 6 2 Join Plat T 4 3 Corner connector 90 degree (V-Slot) 2 4 Gantry Plate V-Slot 20-80 2 5 3M Drop in Tee Nuts \u2013 Insert nuts 50 6 3M Allen Low Profile Screws 50 7 M8 Allen Screw - 45mm Long 6 8 Motor Mount Plate NEMA 23 1 9 Nylon Pulley And Wheel - 40 mm Diameter - 8 mm Bearing 6 10 Nema 23 stepper motor 1 11 P65 Weatherproof Enclosure/electrical enclosure box 2 12 5mm Shock Cord - Marine Grade Polyester Coated Rubber Rope -","title":"Hardware Setup."},{"location":"Farmers%20Dashboard/legacy/#running-ros-node-path-planning","text":"ROS Nodes Overview. ROS Master - Run ROS Nodes over the raspberry PI. Log into the raspberry PI by using: $ computer:~ $ssh ubuntu@wirebot.local (OPTIONAL) Edit the path planning according to the dimensions of the field to scan and the desired length and amount of waypoints. $ ubuntu@ubiquityrobot.local:~ $sudo nano ~/catkin_ws/src/grlbl_serial/src/path_planning_action_client.py * Edit the path_planning_action_client.py by changing the variable movement_goal.xyz_position that is under the function def path_planning_client() . Here is an example of a Path planning that takes pictures of every 500mm in a distance of 10mts: movement_goal.xyz_position = [\"{'x':0, 'y':0, 'z':500, 'delay':20}\", \"{'x':0, 'y':0, 'z':1000, 'delay':20}\", \"{'x':0, 'y':0, 'z':1500, 'delay':20}\", \"{'x':0, 'y':0, 'z':2000, 'delay':20}\", \"{'x':0, 'y':0, 'z':2500, 'delay':20}\", \"{'x':0, 'y':0, 'z':3000, 'delay':20}\", \"{'x':0, 'y':0, 'z':3500, 'delay':20}\", \"{'x':0, 'y':0, 'z':4000, 'delay':20}\", \"{'x':0, 'y':0, 'z':4500, 'delay':20}\", \"{'x':0, 'y':0, 'z':5000, 'delay':20}\", \"{'x':0, 'y':0, 'z':5500, 'delay':20}\", \"{'x':0, 'y':0, 'z':6000, 'delay':20}\", \"{'x':0, 'y':0, 'z':6500, 'delay':20}\", \"{'x':0, 'y':0, 'z':7000, 'delay':20}\", \"{'x':0, 'y':0, 'z':7500, 'delay':20}\", \"{'x':0, 'y':0, 'z':8000, 'delay':20}\", \"{'x':0, 'y':0, 'z':8500, 'delay':20}\", \"{'x':0, 'y':0, 'z':9000, 'delay':20}\", \"{'x':0, 'y':0, 'z':9500, 'delay':20}\"] Start up the nodes and the ROS master by launching the path_planning_action_server_node node under the raspberry PI: $ ubuntu@ubiquityrobot.local:~ $roslaunch grlbl_serial path_planning_action_server_node.launch (ADVERTENCE) If the ROS package is not under the autocomplete method of the terminal. The problem will be solve by sourcing the devel/setup.bash. $ ubuntu@ubiquityrobot.local:~ $source ~/catkin_ws/src/devel/setup.bash 3. ROS Slave - Run ROS Nodes over the Remote Computer. Start up the nodes by launching the path_planning_action_client_node node under the remote computer: $ ubuntu@ubiquityrobot.local:~ $roslaunch grlbl_serial path_planning_action_client_node.launch (OPTIONAL) This node as well can by launch over the raspberry PI. This can be done by lauching the node over a new terminal. By launching the previous ROS node on the WIREDBOT. The starting process of collecting photos from the Mapir camera and the Raspi Cam will be launch automatically according to the path planning instructions save on the path_planning_action_client.py file.","title":"Running ROS node - Path Planning"},{"location":"Farmers%20Dashboard/legacy/#saving-the-data-from-the-wiredbot-under-development","text":"Kepp running or re start the node and the ROS master by launching the path_planning_action_server_node node under the raspberry PI: $ ubuntu@ubiquityrobot.local:~ $roslaunch grlbl_serial path_planning_action_server_node.launch Publish a 1500us to the /i2c_pca9685_driver wiredbot_PWMValues/wiredbot_PWMValues mapir_control_pwm: $ ubuntu@ubiquityrobot.local:~ $rostopic pub -1 /i2c_pca9685_driver wiredbot_PWMValues/wiredbot_PWMValues int16 mapir_control_pwm 1500 int16 motor_A_pwm int16 motor_B_pwm Once ros is publishing the message mapir_control_pwm 1500us under the topic \\i2c_pca9685_driver\\wiredbot_PWMValues. The camera is ready to mount. On the raspberry PI. Mount the camera by using the following commands. $ ubuntu@ubiquityrobot.local:~ $mkdir /mapir $ ubuntu@ubiquityrobot.local:~ $mkdir sudo mount -t vfat /dev/sdb2 /mapir $ ubuntu@ubiquityrobot.local:~ $cd /mapir/DCMI/Photos *","title":"Saving the data from the WIREDBOT. (UNDER-DEVELOPMENT)"},{"location":"Farmers%20Dashboard/legacy/#image-gallery-valldaura","text":"Lettuce Think and Wirebot: Wirebot on the field: Wirebot on the field:","title":"Image Gallery - Valldaura:"},{"location":"Farmers%20Dashboard/legacy/#wirebot-3d-scans","text":"","title":"WIREBOT 3D Scans:"},{"location":"Farmers%20Dashboard/pipeline/","text":"Generating Orthomosaic Image Link For the scope of this application, a rigorous system of collecting photographs of the fields was set, representing a first set of data to process; also to test different strategies of automatic navigation paths, by sending the same waypoints to the drones and to cable-bot. This set of instructions are basically following the rules of sweeping the entire area to then transform the data into an orthomosaic image which covers the whole crop. It\u2019s important that the images have around 50% to 70% vertical overlap and around 30% overlap on sides. Each two pairs of images which have overlaps together are processed to find the same key points available between both (see fig 2) and following this pairing the images are geometrically corrected (orthorectified) such that the scale is uniform in all of them. OpenDroneMap software/API was used to generate the orthomosaic image, the final outcome of this process is an aerial image of the whole crop which has a uniform scale. (see fig 3) Frame Alignment Generated orthomosaic images could contain some extra parts such as extra borders, as well as random orientation. To be able to compare and analyze the maps generated from each scan overtime, we need to orient and crop them with similar boundaries and orientation. For this we include two processes of rotating the image if needed to straighten it, and as well, cropping the unnecessary parts in the borders to optimize the computation parts related to segmentation and future steps. The process of straightening the image starts by applying principal component analysis (PCA) to extract the axis line of the image, this axis line is then used to straighten the image. (see fig 4) After aligning and cropping the orthomosaic images, they are stored in our database for the image segmentation task. Plant Detection Link The collected orthomosaic images are generated with the pipeline explained above. In this section, we describe how the lettuces are detected and masked using Mask-RCNN, which is an instance segmentation algorithm. The generated output images show individual masks for each lettuce that is detected. The Mask-RCNN method has proven to be accurate (see the discussion of the segmentation methods in Section 1.3.6 - T6.3) . This method is able to separate lettuces from different kinds of plants or weeds, as well works well with different light and shadow conditions. The orthomosaic images generated in step 4 are high quality large scale images. Training and Detection algorithms on these large scale images are computationally heavy therefore to apply the detection on orthomosaic images, first we divide the images to a smaller grid. The division and cell size of this grid is dependent on the crop scenario and can be defined manually by the user. The detection algorithm is applied on each cell (see Fig. 3.7). The results are merged back together to generate the overall detected image (see Fig. 3.8). In addition to the masked images, a json file containing the position and area (in the scale of image) of each detected plant is stored as well. In the next stages the masked images and the json files are used to catalog individual plants and monitor them through different scans. Tracking Individual Plants Link Once we have segmented the scan image and obtained the list of individual plants, we have to identify each plant and track each individual throughout all the scans. We tested two methods to track individual plants. In the first method, we detect each plant in each image and extract the center point of each lettuce, the centers points from each scan are then compared together with Iterative Closest Point (ICP) algorithm to track the same lettuce in different scans. In the second method, we register all maps to a common frame using feature matching and detected plants at initial growth stage. The area around each plant is then considered in all images for measuring each plant size as approximated by the projected leaf area. Iterative closest point algorithm on the centers of lettuces This process uses the center points of lettuces in consecutive scans for the ICP method to find the same lettuces over different scans. However this method has two major issues. Undetected lettuces in some images create unmatched points Changes in appearance of lettuces move their center. Registration Through Feature Matching In this process we use the position of the detected lettuces as well as the orthomosaic image generated in step 4, to track individual plants over different scans. One of the main challenges in registering and tracking individual plants in this process is to have the same frame and coordinate system for all the images, This challenge is due to two main reasons. First, the orthomosaic images have different resolutions, as well the main frame for orthomosaic images is not exactly the same for all the images. To create the same coordinate system for all the images, we use the first scan as our reference coordinate system and find the transformations between each two consecutive images using SURF feature matching methods available in OpenCV library which is based on the RANSAC algorithm. (Fig. 3.10) For a series of images, the transformations between each two consecutive images gets combined with all the previous transformations to calculate the transformation to our reference image (first scan). Then all the images as well as the coordinates of plants from the json file are transformed to match the same coordinate system. (see Fig. 3.11) Creating a Catalog of Individual Plants Link Registration of images as well as plant\u2019s coordinates on the same coordinate system, allows for tracking of the same plant within different scans. This happens through clustering the plants\u2019 coordinates that are within a certain distance from each other (see Fig. 3.12). Next step is to Index the detected plant with the same ID over different scans. As well as creating a catalog of individual plants. Leaf Area Monitoring and plant growth curves Link The process of registering different scans over a common reference model results in having all the coordinates and detection images in the same scale. By knowing the scale of the reference image we can scale all the images to real size world coordinates. This is crucial for extracting the leaf area from 2D images. In order to extract the leaf area we calculate the amount of detected pixels (black) over the total amount of pixels in the image which is then multiplied by the scale factor of the image. (see Fig. 3.14) The individual leaf area extracted in each scan, is stored together with the plant\u2019s index in a database. Creation of weed maps Link The results of the semantic segmentation can be used to map the weeds as well as the crops (Fig. 3.16). The resulting weed map provides an indicator on the areas where the pressure of the weeds is highest. These maps can be used in combination with the weeding Rover to prioritise the weeding activities.","title":"Crops Insights Pipeline"},{"location":"Farmers%20Dashboard/pipeline/#generating-orthomosaic-image","text":"For the scope of this application, a rigorous system of collecting photographs of the fields was set, representing a first set of data to process; also to test different strategies of automatic navigation paths, by sending the same waypoints to the drones and to cable-bot. This set of instructions are basically following the rules of sweeping the entire area to then transform the data into an orthomosaic image which covers the whole crop. It\u2019s important that the images have around 50% to 70% vertical overlap and around 30% overlap on sides. Each two pairs of images which have overlaps together are processed to find the same key points available between both (see fig 2) and following this pairing the images are geometrically corrected (orthorectified) such that the scale is uniform in all of them. OpenDroneMap software/API was used to generate the orthomosaic image, the final outcome of this process is an aerial image of the whole crop which has a uniform scale. (see fig 3) Frame Alignment Generated orthomosaic images could contain some extra parts such as extra borders, as well as random orientation. To be able to compare and analyze the maps generated from each scan overtime, we need to orient and crop them with similar boundaries and orientation. For this we include two processes of rotating the image if needed to straighten it, and as well, cropping the unnecessary parts in the borders to optimize the computation parts related to segmentation and future steps. The process of straightening the image starts by applying principal component analysis (PCA) to extract the axis line of the image, this axis line is then used to straighten the image. (see fig 4) After aligning and cropping the orthomosaic images, they are stored in our database for the image segmentation task.","title":"Generating Orthomosaic Image"},{"location":"Farmers%20Dashboard/pipeline/#plant-detection","text":"The collected orthomosaic images are generated with the pipeline explained above. In this section, we describe how the lettuces are detected and masked using Mask-RCNN, which is an instance segmentation algorithm. The generated output images show individual masks for each lettuce that is detected. The Mask-RCNN method has proven to be accurate (see the discussion of the segmentation methods in Section 1.3.6 - T6.3) . This method is able to separate lettuces from different kinds of plants or weeds, as well works well with different light and shadow conditions. The orthomosaic images generated in step 4 are high quality large scale images. Training and Detection algorithms on these large scale images are computationally heavy therefore to apply the detection on orthomosaic images, first we divide the images to a smaller grid. The division and cell size of this grid is dependent on the crop scenario and can be defined manually by the user. The detection algorithm is applied on each cell (see Fig. 3.7). The results are merged back together to generate the overall detected image (see Fig. 3.8). In addition to the masked images, a json file containing the position and area (in the scale of image) of each detected plant is stored as well. In the next stages the masked images and the json files are used to catalog individual plants and monitor them through different scans.","title":"Plant Detection"},{"location":"Farmers%20Dashboard/pipeline/#tracking-individual-plants","text":"Once we have segmented the scan image and obtained the list of individual plants, we have to identify each plant and track each individual throughout all the scans. We tested two methods to track individual plants. In the first method, we detect each plant in each image and extract the center point of each lettuce, the centers points from each scan are then compared together with Iterative Closest Point (ICP) algorithm to track the same lettuce in different scans. In the second method, we register all maps to a common frame using feature matching and detected plants at initial growth stage. The area around each plant is then considered in all images for measuring each plant size as approximated by the projected leaf area. Iterative closest point algorithm on the centers of lettuces This process uses the center points of lettuces in consecutive scans for the ICP method to find the same lettuces over different scans. However this method has two major issues. Undetected lettuces in some images create unmatched points Changes in appearance of lettuces move their center. Registration Through Feature Matching In this process we use the position of the detected lettuces as well as the orthomosaic image generated in step 4, to track individual plants over different scans. One of the main challenges in registering and tracking individual plants in this process is to have the same frame and coordinate system for all the images, This challenge is due to two main reasons. First, the orthomosaic images have different resolutions, as well the main frame for orthomosaic images is not exactly the same for all the images. To create the same coordinate system for all the images, we use the first scan as our reference coordinate system and find the transformations between each two consecutive images using SURF feature matching methods available in OpenCV library which is based on the RANSAC algorithm. (Fig. 3.10) For a series of images, the transformations between each two consecutive images gets combined with all the previous transformations to calculate the transformation to our reference image (first scan). Then all the images as well as the coordinates of plants from the json file are transformed to match the same coordinate system. (see Fig. 3.11)","title":"Tracking Individual Plants"},{"location":"Farmers%20Dashboard/pipeline/#creating-a-catalog-of-individual-plants","text":"Registration of images as well as plant\u2019s coordinates on the same coordinate system, allows for tracking of the same plant within different scans. This happens through clustering the plants\u2019 coordinates that are within a certain distance from each other (see Fig. 3.12). Next step is to Index the detected plant with the same ID over different scans. As well as creating a catalog of individual plants.","title":"Creating a Catalog of Individual Plants"},{"location":"Farmers%20Dashboard/pipeline/#leaf-area-monitoring-and-plant-growth-curves","text":"The process of registering different scans over a common reference model results in having all the coordinates and detection images in the same scale. By knowing the scale of the reference image we can scale all the images to real size world coordinates. This is crucial for extracting the leaf area from 2D images. In order to extract the leaf area we calculate the amount of detected pixels (black) over the total amount of pixels in the image which is then multiplied by the scale factor of the image. (see Fig. 3.14) The individual leaf area extracted in each scan, is stored together with the plant\u2019s index in a database.","title":"Leaf Area Monitoring and plant growth curves"},{"location":"Farmers%20Dashboard/pipeline/#creation-of-weed-maps","text":"The results of the semantic segmentation can be used to map the weeds as well as the crops (Fig. 3.16). The resulting weed map provides an indicator on the areas where the pressure of the weeds is highest. These maps can be used in combination with the weeding Rover to prioritise the weeding activities.","title":"Creation of weed maps"},{"location":"Farmers%20Dashboard/power/","text":"Power Link The power circuit has a main switch that avoids battery discharging during transportation or storing. Battery voltage is feeded directly in to the motor and measured trough a voltage divider, a DC/DC voltage converter provides low voltage to the logic electronics subsystem. Both voltages are exposed to the camera module allowing high voltage motors and avoiding the need of duplicated converters. Both battery terminals are directly exposed to the charger station allowing the carge process during rest periods. DC/DC switching regulator Link To provide low voltage (5v) the OKI-78SR-5/1.5-W36E-C DC/DC switching regulator is used. It can provide 1.5A @ 5v and can stand inputs as high as 36v input. With this regulator we feed all the electronics contained in the carrier unit and if needed provide power to the camera module.","title":"Power"},{"location":"Farmers%20Dashboard/power/#power","text":"The power circuit has a main switch that avoids battery discharging during transportation or storing. Battery voltage is feeded directly in to the motor and measured trough a voltage divider, a DC/DC voltage converter provides low voltage to the logic electronics subsystem. Both voltages are exposed to the camera module allowing high voltage motors and avoiding the need of duplicated converters. Both battery terminals are directly exposed to the charger station allowing the carge process during rest periods.","title":"Power"},{"location":"Farmers%20Dashboard/power/#dcdc-switching-regulator","text":"To provide low voltage (5v) the OKI-78SR-5/1.5-W36E-C DC/DC switching regulator is used. It can provide 1.5A @ 5v and can stand inputs as high as 36v input. With this regulator we feed all the electronics contained in the carrier unit and if needed provide power to the camera module.","title":"DC/DC switching regulator"},{"location":"Farmers%20Dashboard/station/","text":"The charging Station mechanically connects with the mobile carrier during the autonomous charging routine and recharges the carrier internal battery as well as provides shelter when it's not in use. Alternatively, the carrier can be quickly detached and charged indoors. The carrier has three pogo pins that, via an interrupt signal, allows it to know when it is on home position and start the charging process. The Charging Station requires a connection to the mains power.","title":"Charging Station (In development)"},{"location":"Farmers%20Dashboard/structure/","text":"The main structure of the carrier is built with CNC machined aluminum-plastic sandwich panel and 3d printed PLA parts. Side holders Link The miniature switches used as end stops are held by a 3d printed piece that integrates a plastic hinge as trigger element. Integrated in the same housing are the 3d printed lateral pulleys held by a 608zz bearing.","title":"Structure"},{"location":"Farmers%20Dashboard/structure/#side-holders","text":"The miniature switches used as end stops are held by a 3d printed piece that integrates a plastic hinge as trigger element. Integrated in the same housing are the 3d printed lateral pulleys held by a 608zz bearing.","title":"Side holders"},{"location":"Farmers%20Dashboard/tension/","text":"Dinamyc Tension Link The Farmers Dashboard carrier has a system the allows dinamyc adaptation to different cable tensions, allowing a light motor torque to move even on high tension cables neede to cover long distances and avoiding slips on low tension cables. Three pieces of machined HDPE in sandwich form are used to allow smooth movement of the central part of the robot maintaining correct alignment. To allow a bigger range of tensions pre-compresion level of the springs can be adjusted by turning the three screws under the base piece. By turning the front screws the level of friction between the HDPE slides can also be regulated. Still testing Test for finding the right combination of spring strength and movement range for common cable setups on poly tunnels is still on.","title":"Dinamyc Tension"},{"location":"Farmers%20Dashboard/tension/#dinamyc-tension","text":"The Farmers Dashboard carrier has a system the allows dinamyc adaptation to different cable tensions, allowing a light motor torque to move even on high tension cables neede to cover long distances and avoiding slips on low tension cables. Three pieces of machined HDPE in sandwich form are used to allow smooth movement of the central part of the robot maintaining correct alignment. To allow a bigger range of tensions pre-compresion level of the springs can be adjusted by turning the three screws under the base piece. By turning the front screws the level of friction between the HDPE slides can also be regulated. Still testing Test for finding the right combination of spring strength and movement range for common cable setups on poly tunnels is still on.","title":"Dinamyc Tension"},{"location":"Rover/","text":"In this document you will find information on how to use and build the ROMI Rover... User Manual Hardware Documentation Software Installation Developer Documentation Node APIs","title":"Index"},{"location":"Rover/api-nodes/","text":"API of the nodes Link The control software of the Romi Rover consists of number of nodes that communicate among each other. A number of topics are defined. These topics are a bit like an API. Besides the topics, the nodes also use different types of communication patterns. In most cases, the interaction with a node is similar to a remote procedure call. The client sends a JSON request over a messagelink (a websocket) to the node. The node replies with a JSON response message. In some cases, a simple HTTP request and response is used instead of a messagelink. In other cases, the node only braodcasts out JSON formatted events over a websocket. The types of communications are listed below. Type Description service Uses the HTML request-response pattern controller Uses RPC over WebSocket streamer Transmits a continuous data flow over HTML messagehub Communicates over a WebSocket datahub Broadcasts messages over UDP Different nodes can implement the same topic. For example, the nodes video4linux , picamera , fake_camera all implement the camera topic. However, only one of these nodes should be active at one time. A nodes can also implement several topics. For example, the motor controller implements both the motorcontroller and encoders topics. This document does not go into the details of messagelinks or how to do an HTTP request. Here we simply document all the available topics, the type of communication, and their messages they expect. In rcom, a communication end-point is identified by the combination of topic and type. So a streamer with the topic camera is distinct from the service with the same topic. The following table lists all the topics and the types that have been defined by Romi Rover. In the sections below you will find more details on each. API Description Type camera Provides RGB images service and streamer camera_recorder Records a sequence of images to disk controller cnc Controls a XYZ motion device controller configuration Exports the configuration file service control_panel Controls the display and power relays controller encoders Broadcasts the encoder values of the wheels datahub fsdb Broadcasts events about newly created files messagehub gimbal Controls the camera mount controller motorcontroller Controls the wheel motors controller navigation Controls the displacement of the rover controller pose Broadcasts the position and orientation of the rover datahub proxy A web proxy to all the nodes and a web server for static pages service & messagehub script_engine Executes scripts service tool_carrier Handles the mechanical weeding tool carrier controller camera Link The existing implementations are: video4linux , realsense , picamera , and fake_camera . The camera combines two communication end-points interfaces. It has a service that provides single JPEG images. It also has a streamer interface that broadcasts a continuous stream of JPEG images encoded as a multipart response (the corresponding mimetype is \"multipart/x-mixed-replace\"). The two handled URIs are: /camera.jpg : Use this URI to retrieve the latest RGB image from the camera service. /stream.html : This URI to get a continuous, video-over-html stream from the camera streamer. camera_recorder Link The camera_recorder is a controller that connects to a camera. Upon request, it will start recording the images of the camera to disk. It accepts to commands: start and stop. start Link Start recording the images to disk. Example: {\"command\": \"start\"} stop Link Stop the recording. Example: {\"command\": \"stop\"} configuration Link The configuration service allows nodes to obtain the settings of the rover. A simple HTML request with the name of the settings will return its associated value as a JSON-formatted object. Suppose the configuration file contains the following: { \"menu\" : { \"starters\" : [ \"velout\u00e9 de champignons\" , \"tomato & mozarella\" ], \"mains\" : [ \"eggplant lasagna\" , \"meatloaf with mashed potatoes\" ], \"deserts\" : [ \"chocolate mousse\" , \"panna cotta\" , \"cr\u00e8me brul\u00e9e\" ], } } Then an HTTP request to the URI /menu/starters will return [\"velout\u00e9 de champignons\", \"tomato & mozarella\"] in the body of the response. Implementation note: You can use the function client_get in the rcom API to obtain the value directly as a json_object_t : json_object_t list = client_get ( \"configuration\" , \"menu/starters\" ); control_panel Link The control_panel is the controller that interfaces with the physical control panel of the rover. It currently handles to commands: shutdown and display. shutdown Link Goes through the following steps: Asks the control panel to cut the power circuit (cuts the motors but not the controllers), then initiates the shutdown of the on-board computer, and requests the control panel to cut the power of the logical circuit (with a 5 seconds delay). Example: {\"command\": \"shutdown\"} display Link Asks the control panel to display a message. The length of the message is currently limited to 16 characters. The message may not be displayed immediately, or may be skipped, if too many messages are sent. Example: {\"command\": \"display\", \"message\": \"No network\"} cnc Link Existing implementations: grbl , oquam , and fake_cnc . The CNC has a JSON-over-WebSocket controller interface that accepts the commands documented below. moveto Link Move to a specified absolute position in meters. Example: {\"command\": \"moveto\", \"x\": 0.4, \"y\": 0.4} {\"command\": \"moveto\", \"z\": -0.1} homing Link Move to the home position, if the CNC has limit switches. Example: {\"command\": \"homing\"} travel Link The travel command serves to make the CNC travel a given path, defined by a list of points in absolute coordinates in meter. Example: {\"command\": \"travel\", \"path\": [[0,0,0], [0.5,0,0], [0.5,0.5,0], [0,0.5,0], [0,0,0]]} spindle Link Starts or stops the spindle. A speed can be specified with a value between 0 (stopped) and 1 (maximum speed). Example: {\"command\": \"spindle\", \"speed\": 1} encoders Link fsdb Link gimbal Link motor_controller Link navigation Link pose Link proxy Link script_engine Link tool_carrier Link","title":"Api nodes"},{"location":"Rover/api-nodes/#api-of-the-nodes","text":"The control software of the Romi Rover consists of number of nodes that communicate among each other. A number of topics are defined. These topics are a bit like an API. Besides the topics, the nodes also use different types of communication patterns. In most cases, the interaction with a node is similar to a remote procedure call. The client sends a JSON request over a messagelink (a websocket) to the node. The node replies with a JSON response message. In some cases, a simple HTTP request and response is used instead of a messagelink. In other cases, the node only braodcasts out JSON formatted events over a websocket. The types of communications are listed below. Type Description service Uses the HTML request-response pattern controller Uses RPC over WebSocket streamer Transmits a continuous data flow over HTML messagehub Communicates over a WebSocket datahub Broadcasts messages over UDP Different nodes can implement the same topic. For example, the nodes video4linux , picamera , fake_camera all implement the camera topic. However, only one of these nodes should be active at one time. A nodes can also implement several topics. For example, the motor controller implements both the motorcontroller and encoders topics. This document does not go into the details of messagelinks or how to do an HTTP request. Here we simply document all the available topics, the type of communication, and their messages they expect. In rcom, a communication end-point is identified by the combination of topic and type. So a streamer with the topic camera is distinct from the service with the same topic. The following table lists all the topics and the types that have been defined by Romi Rover. In the sections below you will find more details on each. API Description Type camera Provides RGB images service and streamer camera_recorder Records a sequence of images to disk controller cnc Controls a XYZ motion device controller configuration Exports the configuration file service control_panel Controls the display and power relays controller encoders Broadcasts the encoder values of the wheels datahub fsdb Broadcasts events about newly created files messagehub gimbal Controls the camera mount controller motorcontroller Controls the wheel motors controller navigation Controls the displacement of the rover controller pose Broadcasts the position and orientation of the rover datahub proxy A web proxy to all the nodes and a web server for static pages service & messagehub script_engine Executes scripts service tool_carrier Handles the mechanical weeding tool carrier controller","title":"API of the nodes"},{"location":"Rover/api-nodes/#camera","text":"The existing implementations are: video4linux , realsense , picamera , and fake_camera . The camera combines two communication end-points interfaces. It has a service that provides single JPEG images. It also has a streamer interface that broadcasts a continuous stream of JPEG images encoded as a multipart response (the corresponding mimetype is \"multipart/x-mixed-replace\"). The two handled URIs are: /camera.jpg : Use this URI to retrieve the latest RGB image from the camera service. /stream.html : This URI to get a continuous, video-over-html stream from the camera streamer.","title":"camera"},{"location":"Rover/api-nodes/#camera_recorder","text":"The camera_recorder is a controller that connects to a camera. Upon request, it will start recording the images of the camera to disk. It accepts to commands: start and stop.","title":"camera_recorder"},{"location":"Rover/api-nodes/#start","text":"Start recording the images to disk. Example: {\"command\": \"start\"}","title":"start"},{"location":"Rover/api-nodes/#stop","text":"Stop the recording. Example: {\"command\": \"stop\"}","title":"stop"},{"location":"Rover/api-nodes/#configuration","text":"The configuration service allows nodes to obtain the settings of the rover. A simple HTML request with the name of the settings will return its associated value as a JSON-formatted object. Suppose the configuration file contains the following: { \"menu\" : { \"starters\" : [ \"velout\u00e9 de champignons\" , \"tomato & mozarella\" ], \"mains\" : [ \"eggplant lasagna\" , \"meatloaf with mashed potatoes\" ], \"deserts\" : [ \"chocolate mousse\" , \"panna cotta\" , \"cr\u00e8me brul\u00e9e\" ], } } Then an HTTP request to the URI /menu/starters will return [\"velout\u00e9 de champignons\", \"tomato & mozarella\"] in the body of the response. Implementation note: You can use the function client_get in the rcom API to obtain the value directly as a json_object_t : json_object_t list = client_get ( \"configuration\" , \"menu/starters\" );","title":"configuration"},{"location":"Rover/api-nodes/#control_panel","text":"The control_panel is the controller that interfaces with the physical control panel of the rover. It currently handles to commands: shutdown and display.","title":"control_panel"},{"location":"Rover/api-nodes/#shutdown","text":"Goes through the following steps: Asks the control panel to cut the power circuit (cuts the motors but not the controllers), then initiates the shutdown of the on-board computer, and requests the control panel to cut the power of the logical circuit (with a 5 seconds delay). Example: {\"command\": \"shutdown\"}","title":"shutdown"},{"location":"Rover/api-nodes/#display","text":"Asks the control panel to display a message. The length of the message is currently limited to 16 characters. The message may not be displayed immediately, or may be skipped, if too many messages are sent. Example: {\"command\": \"display\", \"message\": \"No network\"}","title":"display"},{"location":"Rover/api-nodes/#cnc","text":"Existing implementations: grbl , oquam , and fake_cnc . The CNC has a JSON-over-WebSocket controller interface that accepts the commands documented below.","title":"cnc"},{"location":"Rover/api-nodes/#moveto","text":"Move to a specified absolute position in meters. Example: {\"command\": \"moveto\", \"x\": 0.4, \"y\": 0.4} {\"command\": \"moveto\", \"z\": -0.1}","title":"moveto"},{"location":"Rover/api-nodes/#homing","text":"Move to the home position, if the CNC has limit switches. Example: {\"command\": \"homing\"}","title":"homing"},{"location":"Rover/api-nodes/#travel","text":"The travel command serves to make the CNC travel a given path, defined by a list of points in absolute coordinates in meter. Example: {\"command\": \"travel\", \"path\": [[0,0,0], [0.5,0,0], [0.5,0.5,0], [0,0.5,0], [0,0,0]]}","title":"travel"},{"location":"Rover/api-nodes/#spindle","text":"Starts or stops the spindle. A speed can be specified with a value between 0 (stopped) and 1 (maximum speed). Example: {\"command\": \"spindle\", \"speed\": 1}","title":"spindle"},{"location":"Rover/api-nodes/#encoders","text":"","title":"encoders"},{"location":"Rover/api-nodes/#fsdb","text":"","title":"fsdb"},{"location":"Rover/api-nodes/#gimbal","text":"","title":"gimbal"},{"location":"Rover/api-nodes/#motor_controller","text":"","title":"motor_controller"},{"location":"Rover/api-nodes/#navigation","text":"","title":"navigation"},{"location":"Rover/api-nodes/#pose","text":"","title":"pose"},{"location":"Rover/api-nodes/#proxy","text":"","title":"proxy"},{"location":"Rover/api-nodes/#script_engine","text":"","title":"script_engine"},{"location":"Rover/api-nodes/#tool_carrier","text":"","title":"tool_carrier"},{"location":"Rover/configuration/","text":"The rover uses the following input files and directories: Configuration file: The main configuration file. This file should be passed on the command line to rclaunch . HTML directory: The location of the files for the web interface. The path should be set in the configuration file in the webproxy section. Script file: The list of operations and associated command sequences. The path should be set in the configuration file in the script_engine section. The configuration and script files are discussed in detail below. Configuration file Link The rover control software is starting using the rclaunch command. This command takes the path to the main configuration as an argument. Example configutation files can be found in the romi-rover/config directory. The launch section Link The launch section is required in all configuration files. A stripped-down version of this section looks like below: { \"launch\" : { \"general\" : { \"sessions-dir\" : \"/tmp/session\" , \"user\" : \"romi\" }, \"nodes\" : [{ \"path\" : \"rclogger\" }, { \"path\" : \"configuration\" }, { //... }] } } The launch section has two subsections, general and nodes . The first section groups the following settings: Name Value Required Description sessions-dir String Optional Where to store the session data (log files, database, dumps) user String Optional The username under which the nodes will be executed The nodes section is an array that lists all the nodes that should be launched. The nodes are launched in the order given, but no guarantee is given that a node in the front of the list will be ready for communication before a node later in the list. Each node object accepts the following fields: Name Value Required Description path String Required The path (or name) of the executable args String Optional Additional command-line arguments to be passed to the executable disabled Boolean Optional Whether to skip the node or not host String Optional To start the node on a remote machine user String Optional The user account to start the node on a remote machine The PATH environment variable is used to find the executable. The node settings Link Each node can have its own section in the configuration file. The name of the section must be the name of the node. For example, the webproxy node will look for the key webproxy at the root of the configuration file: { \"proxy\" : { \"html\" : \"romi-rover/html\" }, \"launch\" : { //... } } Script file Link The script file consists of a JSON array of script objects: [ { \"name\" : \"script1\" , \"display_name\" : \"The First Script\" , \"script\" : [ //... ] }, //... ] The script objects have the following fields: Name Value Required Description name String Required The internal name of the script display-name String Required The name to be shown to end-users script Array Required The list of actions to be taken The script itself is an array that lists all the actions to be taken, one after the other. For example: [ { \"name\" : \"script1\" , \"display_name\" : \"The First Script\" , \"script\" : [ { \"action\" : \"start_recording\" }, { \"action\" : \"move\" , \"distance\" : 8 , \"speed\" : 400 }, { \"action\" : \"stop_recording\" }, { \"action\" : \"move\" , \"distance\" : -8 , \"speed\" : -400 } ] }, //... ] Each action object has an action field. The list of available actions are: Name Description start_recording Start recording the images of the top camera stop_recording Stop recording the images of the top camera move Move a given distance (at a given speed) moveat Move at a given speed hoe Start weeding homing Go to the home position (for rovers with a limit switch only) shutdown Turn off the robot The move action takes the following parameters: Name Value Required Description distance Number (in meters) Required The distance to be travelled speed Number (-1000 < speed < 1000) Optional The speed The moveat action takes a speed value, as in the move command above. The hoe action takes the following parameters: Name Value Required Description method String Required See below Two methods are currenly available, quincunx and boustrophedon . Boustrophedon clean the complete surface underneath the rover. Quincunx tries to detect the quincunx pattern in which the vegetables are planted and clean between them. When the method is quincunx , the following additional parameters are can be passed: Name Value Default Description distance-plants number 0.3m The distance between plant on a row distance-rows number 0.25m The distance between rows radius-zones number 0.1m The radius of the plants diameter-tool number 0.05m The diameter of the weeding tool quincunx-threshold number 1.0 A threshold used in the detection of the plants. A lower value (> 0) means be more tolerant in the detection, a higher value means be more selective.","title":"Configuration"},{"location":"Rover/configuration/#configuration-file","text":"The rover control software is starting using the rclaunch command. This command takes the path to the main configuration as an argument. Example configutation files can be found in the romi-rover/config directory.","title":"Configuration file"},{"location":"Rover/configuration/#the-launch-section","text":"The launch section is required in all configuration files. A stripped-down version of this section looks like below: { \"launch\" : { \"general\" : { \"sessions-dir\" : \"/tmp/session\" , \"user\" : \"romi\" }, \"nodes\" : [{ \"path\" : \"rclogger\" }, { \"path\" : \"configuration\" }, { //... }] } } The launch section has two subsections, general and nodes . The first section groups the following settings: Name Value Required Description sessions-dir String Optional Where to store the session data (log files, database, dumps) user String Optional The username under which the nodes will be executed The nodes section is an array that lists all the nodes that should be launched. The nodes are launched in the order given, but no guarantee is given that a node in the front of the list will be ready for communication before a node later in the list. Each node object accepts the following fields: Name Value Required Description path String Required The path (or name) of the executable args String Optional Additional command-line arguments to be passed to the executable disabled Boolean Optional Whether to skip the node or not host String Optional To start the node on a remote machine user String Optional The user account to start the node on a remote machine The PATH environment variable is used to find the executable.","title":"The launch section"},{"location":"Rover/configuration/#the-node-settings","text":"Each node can have its own section in the configuration file. The name of the section must be the name of the node. For example, the webproxy node will look for the key webproxy at the root of the configuration file: { \"proxy\" : { \"html\" : \"romi-rover/html\" }, \"launch\" : { //... } }","title":"The node settings"},{"location":"Rover/configuration/#script-file","text":"The script file consists of a JSON array of script objects: [ { \"name\" : \"script1\" , \"display_name\" : \"The First Script\" , \"script\" : [ //... ] }, //... ] The script objects have the following fields: Name Value Required Description name String Required The internal name of the script display-name String Required The name to be shown to end-users script Array Required The list of actions to be taken The script itself is an array that lists all the actions to be taken, one after the other. For example: [ { \"name\" : \"script1\" , \"display_name\" : \"The First Script\" , \"script\" : [ { \"action\" : \"start_recording\" }, { \"action\" : \"move\" , \"distance\" : 8 , \"speed\" : 400 }, { \"action\" : \"stop_recording\" }, { \"action\" : \"move\" , \"distance\" : -8 , \"speed\" : -400 } ] }, //... ] Each action object has an action field. The list of available actions are: Name Description start_recording Start recording the images of the top camera stop_recording Stop recording the images of the top camera move Move a given distance (at a given speed) moveat Move at a given speed hoe Start weeding homing Go to the home position (for rovers with a limit switch only) shutdown Turn off the robot The move action takes the following parameters: Name Value Required Description distance Number (in meters) Required The distance to be travelled speed Number (-1000 < speed < 1000) Optional The speed The moveat action takes a speed value, as in the move command above. The hoe action takes the following parameters: Name Value Required Description method String Required See below Two methods are currenly available, quincunx and boustrophedon . Boustrophedon clean the complete surface underneath the rover. Quincunx tries to detect the quincunx pattern in which the vegetables are planted and clean between them. When the method is quincunx , the following additional parameters are can be passed: Name Value Default Description distance-plants number 0.3m The distance between plant on a row distance-rows number 0.25m The distance between rows radius-zones number 0.1m The radius of the plants diameter-tool number 0.05m The diameter of the weeding tool quincunx-threshold number 1.0 A threshold used in the detection of the plants. A lower value (> 0) means be more tolerant in the detection, a higher value means be more selective.","title":"Script file"},{"location":"Rover/developer/","text":"Developer Documentation Link To download and compile the code, plase refer to the latest doc at the romi-rover-build-and-test code repository .","title":"Developer"},{"location":"Rover/developer/#developer-documentation","text":"To download and compile the code, plase refer to the latest doc at the romi-rover-build-and-test code repository .","title":"Developer Documentation"},{"location":"Rover/hardware/","text":"Hardware Documentation Link This document describes the hardware, both the mechanical parts and the electronics.. The main structure Link The figure below gives an overview of the main components. The mechanical components Link The frame Link The wheels Link The boxes Link The CNC Link We use currently use the X-Carve. Please follow X-Carve's documentation at . The Z-axis Link The cover Link The electronics Link The basic architecture of the control modules Link NOTE : The current rovers don't implements the schema above, yet: The CNC has no encoders. The rover in Valldaur doesn't have a control panel. The control panel Link NOTE : The control panel is still being developed at the time of writing. The control panel is used to start and stop the rover and to display status information on the character display. Component Specifications Example Controller Arduino Uno or equivalent Proto shield The shield allows you to solder the wires securely Adafruit Sparkfun Amazon Relay (2x) TODO Sparkfun Push button with LEDS (2x) One green and one red Adafruit LCD Character Display Comptable with XXX Farnell The power circuit Link NOTE : This is the new power circuit that will be used with the control panel. It's currently not implemented in either rover. There are three separate power circuits: Always-on circuit : This circuits powers the control panel. Logic circuit : This circuits powers the embedded PC and other control circuits. Power circuit : This circuit drives all the motors. This is the circuit that is cut when the security switch (the big red button) is pressed. The control panel actuates two relays (Relay 1 & 2) according to the two start-up phases (the PC and the logic circuits start up first, then the motors are powered up). The third relay is designed to handle strong currents. It has a protection against sparks and back-currents. Most of the logic runs on 5V. (TODO: Add a Meanwell power converter. Q: One converter for the the control panel + one of the logic circuit? Or one single converter?) The figure does not show the power line for the weeding hoe. The hoe is turned on/off using a fourth relay that is connected to the gShield board of the CNC. It using the CNC's spindle on/off functionality. You can find more information on this in the section on the CNC below. Component Specifications Example Relay Non-Latching, protection against sparks and back-current (TODO: be more precise) RS Online Security switch Farnell: button and housing The navigation module Link The navigation uses a differential wheel drive, with two motorized wheels in the back and two swivel caster in the front. This makes the control fairly straight-forward and the components are easy to source. The navigation module can also receive input from a remote control to steer the rover. The main components are shown below: Components Link Component Specifications Example Motors Brushed DC motors , 24 V, minimum 200 W We are using wheel chair motors for now. We bought a set at Superdroid Robots . Encoders Incremental encoders US Digital E2 (Available from Superdroid Robotics ). Controller Arduino Uno or equivalent Proto shield The shield allows you to solder the wires securely Adafruit Sparkfun Amazon Motor driver Preferably one board that can drive two motors. Two drivers, one for each motor, is possible, too. Power input: 24V, Maximum output current: > 15 A per motor, Control signals: two PWM signals (similar to RC input) for the left and right motor. The driver implements a standard H-bridge to control to power supplied to the motors (both forward and backward rotation). Sabertooth 2x60A Sabertooth 2x32A RoboClaw 2x60A RC controller and receiver A standard RC receiver that outputs a PWM signal. Powered by 5V. We've succesfully used the remote controllers for Spektrum , this one for example or similar Wheels You will need those, too. We are using the wheel provided by Superdroid Robotics for now Wiring diagram Link The tool positioning Link A CNC is adapted for use in the rover. We replaced the spindle that is normally used to carve wooden pieces, with a rotating weeding hoe. We are using to larger, 1000 mm sized version of the X-Carve . The newer X-Carve uses a custom design board for the control. However, we prefer using the older solution that combines an Arduino Uno with a gShield because it is smaller and more generic. For the time being, we use the grbl language to send commands from the embedded PC to the CNC controller. Therefore, any solution that accepts grbl commands should be drop-in solution for the Uno+gShield combo. Component Specifications Example CNC Minimum work area: 0.7 m x 0.7m X-Carve Optional: Controller board Must run grbl interpreter Arduino Uno Optional: Stepper drivers (3 steppers) The driver must use the STEP/DIR control signals gShield [Arduino CNC Shield] eBay and RepRap You can still have a look at XCarve's older documentation on how to wire the controller boards: http://x-carve-instructions.inventables.com/xcarve2015/step10/ http://x-carve-instructions.inventables.com/xcarve2015/step14/ Notable, the following two diagrams are of interest: The yellow wire marked \"spinle\" in the image above is used to turn the weeding hoe on or off, as shown in the figure below (see also the figure in the section on the power circuit).","title":"Hardware"},{"location":"Rover/hardware/#hardware-documentation","text":"This document describes the hardware, both the mechanical parts and the electronics..","title":"Hardware Documentation"},{"location":"Rover/hardware/#the-main-structure","text":"The figure below gives an overview of the main components.","title":"The main structure"},{"location":"Rover/hardware/#the-mechanical-components","text":"","title":"The mechanical components"},{"location":"Rover/hardware/#the-frame","text":"","title":"The frame"},{"location":"Rover/hardware/#the-wheels","text":"","title":"The wheels"},{"location":"Rover/hardware/#the-boxes","text":"","title":"The boxes"},{"location":"Rover/hardware/#the-cnc","text":"We use currently use the X-Carve. Please follow X-Carve's documentation at .","title":"The CNC"},{"location":"Rover/hardware/#the-z-axis","text":"","title":"The Z-axis"},{"location":"Rover/hardware/#the-cover","text":"","title":"The cover"},{"location":"Rover/hardware/#the-electronics","text":"","title":"The electronics"},{"location":"Rover/hardware/#the-basic-architecture-of-the-control-modules","text":"NOTE : The current rovers don't implements the schema above, yet: The CNC has no encoders. The rover in Valldaur doesn't have a control panel.","title":"The basic architecture of the control modules"},{"location":"Rover/hardware/#the-control-panel","text":"NOTE : The control panel is still being developed at the time of writing. The control panel is used to start and stop the rover and to display status information on the character display. Component Specifications Example Controller Arduino Uno or equivalent Proto shield The shield allows you to solder the wires securely Adafruit Sparkfun Amazon Relay (2x) TODO Sparkfun Push button with LEDS (2x) One green and one red Adafruit LCD Character Display Comptable with XXX Farnell","title":"The control panel"},{"location":"Rover/hardware/#the-power-circuit","text":"NOTE : This is the new power circuit that will be used with the control panel. It's currently not implemented in either rover. There are three separate power circuits: Always-on circuit : This circuits powers the control panel. Logic circuit : This circuits powers the embedded PC and other control circuits. Power circuit : This circuit drives all the motors. This is the circuit that is cut when the security switch (the big red button) is pressed. The control panel actuates two relays (Relay 1 & 2) according to the two start-up phases (the PC and the logic circuits start up first, then the motors are powered up). The third relay is designed to handle strong currents. It has a protection against sparks and back-currents. Most of the logic runs on 5V. (TODO: Add a Meanwell power converter. Q: One converter for the the control panel + one of the logic circuit? Or one single converter?) The figure does not show the power line for the weeding hoe. The hoe is turned on/off using a fourth relay that is connected to the gShield board of the CNC. It using the CNC's spindle on/off functionality. You can find more information on this in the section on the CNC below. Component Specifications Example Relay Non-Latching, protection against sparks and back-current (TODO: be more precise) RS Online Security switch Farnell: button and housing","title":"The power circuit"},{"location":"Rover/hardware/#the-navigation-module","text":"The navigation uses a differential wheel drive, with two motorized wheels in the back and two swivel caster in the front. This makes the control fairly straight-forward and the components are easy to source. The navigation module can also receive input from a remote control to steer the rover. The main components are shown below:","title":"The navigation module"},{"location":"Rover/hardware/#components","text":"Component Specifications Example Motors Brushed DC motors , 24 V, minimum 200 W We are using wheel chair motors for now. We bought a set at Superdroid Robots . Encoders Incremental encoders US Digital E2 (Available from Superdroid Robotics ). Controller Arduino Uno or equivalent Proto shield The shield allows you to solder the wires securely Adafruit Sparkfun Amazon Motor driver Preferably one board that can drive two motors. Two drivers, one for each motor, is possible, too. Power input: 24V, Maximum output current: > 15 A per motor, Control signals: two PWM signals (similar to RC input) for the left and right motor. The driver implements a standard H-bridge to control to power supplied to the motors (both forward and backward rotation). Sabertooth 2x60A Sabertooth 2x32A RoboClaw 2x60A RC controller and receiver A standard RC receiver that outputs a PWM signal. Powered by 5V. We've succesfully used the remote controllers for Spektrum , this one for example or similar Wheels You will need those, too. We are using the wheel provided by Superdroid Robotics for now","title":"Components"},{"location":"Rover/hardware/#wiring-diagram","text":"","title":"Wiring diagram"},{"location":"Rover/hardware/#the-tool-positioning","text":"A CNC is adapted for use in the rover. We replaced the spindle that is normally used to carve wooden pieces, with a rotating weeding hoe. We are using to larger, 1000 mm sized version of the X-Carve . The newer X-Carve uses a custom design board for the control. However, we prefer using the older solution that combines an Arduino Uno with a gShield because it is smaller and more generic. For the time being, we use the grbl language to send commands from the embedded PC to the CNC controller. Therefore, any solution that accepts grbl commands should be drop-in solution for the Uno+gShield combo. Component Specifications Example CNC Minimum work area: 0.7 m x 0.7m X-Carve Optional: Controller board Must run grbl interpreter Arduino Uno Optional: Stepper drivers (3 steppers) The driver must use the STEP/DIR control signals gShield [Arduino CNC Shield] eBay and RepRap You can still have a look at XCarve's older documentation on how to wire the controller boards: http://x-carve-instructions.inventables.com/xcarve2015/step10/ http://x-carve-instructions.inventables.com/xcarve2015/step14/ Notable, the following two diagrams are of interest: The yellow wire marked \"spinle\" in the image above is used to turn the weeding hoe on or off, as shown in the figure below (see also the figure in the section on the power circuit).","title":"The tool positioning"},{"location":"Rover/manual/","text":">>>>> gd2md-html alert: ERRORs: 0; WARNINGs: 1; ALERTS: 28. See top comment block for details on ERRORs and WARNINGs. In the converted Markdown or HTML, search for inline alerts that start with >>>>> gd2md-html alert: for specific instances that need correction. Links to alert messages: alert1 alert2 alert3 alert4 alert5 alert6 alert7 alert8 alert9 alert10 alert11 alert12 alert13 alert14 alert15 alert16 alert17 alert18 alert19 alert20 alert21 alert22 alert23 alert24 alert25 alert26 alert27 alert28 >>>>> PLEASE check and correct alert issues and delete this message and the inline alerts. >>>>> gd2md-html alert: inline image link here (to images/image1.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> Romi Rover User Manual Link >>>>> gd2md-html alert: inline image link here (to images/image2.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> Draft - September 2020 \u00a9 Sony Computer Science Laboratories - CC BY-SA 4.0 License ROMI has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 773875. \\ This manual describes the usage of a fully assembled and ready to use rover. The information on how to build your own rover can be found in a separate document. Visit the ROMI web site at https://docs.romi-project.eu for more information. Short description Link The ROMI Rover is a farming tool that assists vegetable farmers in maintaining the vegetable beds free from weeds. It does this by regularly hoeing the surface of the soil and thus preventing small weeds from taking roots. It can do this task mostly autonomously and requires only minor changes to the organization of the farm. It is designed for vegetable beds between 70 cm and 120 cm wide (not including the passage ways) and currently handles two types of crops, lettuce and carrots. The lettuce can be planted out in any layout, most likely in a quincunx pattern. In this configuration the rover uses a precision rotary hoe to clean the soil both between the rows and the plants. For the carrots, the rover uses classical mechanical tools, such as stirrup hoe, to regularly clean the soil in between the rows. In this configuration, the carrots should be sown in line. A weekly passage of the robot should be sufficient to keep the population of weeds under control. In addition to the weeding, the rover provides the following useful functions. It can draw quincunx patterns or straight lines in empty vegetable beds to speed up seeding and planting out. The embedded camera can be used to collect images of the vegetable beds. It can also be used as a motorized tray. The ROMI Rover is targeted at farms that grow lettuce and carrots on a relatively small surface, between 200 m\u00b2 and 5 ha (Utilized Agricultural Area). Link Table of Content Link User Manual Short description Table of Content Technical specifications Functional specifications and requirements Examples for different farm sizes Operation instructions **Overview of the components **V3 Overview of the rover\u2019s usage Setting up the vegetable beds Laying out the rails Maximizing the use of rails (TODO) Setting up the Wi-Fi access point Charging the rover Protection cover Disassembling and reassembling the rover (V3) Adjusting the width of the rover (V3) Adjusting the height of the rover (V3) Attendance (TODO: regulations?) Storage Emergency button Engaging/disengaging the motor lock levers (freewheeling mode and drive mode) The control panel Control panel state message Start-up procedure Shut-down procedure **Switching from drive to freewheeling mode ** **Switching from freewheeling to drive mode ** First time configuration (**V2)** The configuration page of the rover (**V2)** Changing the password of the interface (**V2)** Changing the rover\u2019s name and address (**V2)** Using a Wi-Fi access point / Enabling Internet access (**V2)** Verifying the Wi-Fi connection (**V3)** Connecting a phone, tablet, or computer to the rover Moving the rover to the field Positioning the rover on a vegetable bed Remote control mode and software control mode Emergency control recovery with the remote control Using the remote control **Calibrating the CNC-**to-Camera mapping (V2) Change the weeding tool head Controlling the rover through the control panel (V2) Controlling the rover through the web interface Using the tool carrier Remote maintenance (V3) Remote maintenance (V3) The USB memory stick (**V2)** Manually editing the configuration Editing the rover buttons and actions Farmer\u2019s Dashboard (V3) Automatically uploading the images to the Farmer\u2019s Dashboard (**V3)** Manually uploading the images to the Farmer\u2019s Dashboard (**V3)** Supported web browsers Installing a new SD card Legend V2 The feature will be implemented in the second prototype (deadline: December 2020) V2 The feature refers to the second prototype and will be different in the third prototype. V3 The feature will be implemented in the third prototype (deadline: May 2021). V3 The feature refers to the third prototype and exists in a different form in the second prototype. Link Technical specifications Link Size V3 (Width x Length x Height) Min.: 1.45 m x 1.60 m x 1.46 m Max.: 1.70 m x 1.60 m x 1.46 m Weight V2 80 kg (estimate) Battery life V3 8 h (TODO: that\u2019s the target) Charging time TODO Weeding speed Precision weeding: 600 m\u00b2/day V3 (235 m\u00b2/day V2 ) Classical weeding: 7200 m\u00b2/day V3 (6400 m\u00b2/day V2 ) Width vegetable beds V3 Min.: 0.70 m Max.: 1.2 m Handled crops Precision weeding: Lettuce Classical weeding: Carrots Turning space at end of bed 2 m (TODO: verify) Functional specifications and requirements Link The following configuration is required for the use of the ROMI rover. Profile Price Features/Function Bed of vegetables Width of the bed: min 0.7 m max 1.2 m V3 Handled crops Lettuce : The lettuce can be planted out in any layout, most likely in a quincunx pattern Carrots : The carrots should be sown in line. ROMI rover with a remote control, a battery charger, and a protective cover >>>>> gd2md-html alert: inline image link here (to images/image3.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> V3 >>>>> gd2md-html alert: inline image link here (to images/image4.jpg). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> >>>>> gd2md-html alert: inline image link here (to images/image5.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> 5000 \u20ac (estimate) Prevents weed development Draw patterns for seeding and planting out Takes image scans of the beds Tools carrier Photo 1000 \u20ac (estimate) Used with the rover to carry the mechanical weeding tools (optional but needed in most configurations) Mechanical weeding tools (from Terrateck or other manufacturers) >>>>> gd2md-html alert: inline image link here (to images/image6.jpg). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> >>>>> gd2md-html alert: inline image link here (to images/image7.jpg). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> >>>>> gd2md-html alert: inline image link here (to images/image8.jpg). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> >>>>> gd2md-html alert: inline image link here (to images/image9.jpg). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> 400-1000\u20ac (estimate) The weeding tools that will be fixed on the tool carrier. Sold by other manufacturers, for example, by [Terrateck](https://www.terrateck.com) ([catalogue](https://www.terrateck.com/en/16-houes-pousse-pousse)) **TOTAL cost of an equipped rover** 6400 \u2013 7000 \u20ac **Guides** Photo Stainless steel tubes (45x1.5): 3 \u20ac/m Tuyau irrigation PE HD 50mm : 4.4 \u20ac/m The guides consist of either stainless metal tubes (preferred) or wooden boards. They facilitate the navigation of the rover along the bed so that it can operate accurately and reliably. **A mobile phone** >>>>> gd2md-html alert: inline image link here (to images/image10.jpg). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> ![alt_text](images/image10.jpg \"image_tooltip\") Existing phone: 0\u20ac Dedicated phone: 200 \u20ac * Used to control the rover * To browse the archived images of the online service **WiFi** >>>>> gd2md-html alert: inline image link here (to images/image11.jpg). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> ![alt_text](images/image11.jpg \"image_tooltip\") Rover as hotspot: 0 \u20ac/month Existing WiFi: 0 \u20ac/month GSM WiFi router: 120\u20ac (router) + 10 \u20ac/month (SIM card) WiFi connectivity in the field * To connect the mobile phone to the rover (required) * To archives the images taken by the rover (optional) * To provide remote assistance (optional) **Romi Online Service V3** >>>>> gd2md-html alert: inline image link here (to images/image12.jpg). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> ![alt_text](images/image12.jpg \"image_tooltip\") 1 \u20ac/month Monthly fee for the use of the Romi remote server (optional): * To archive and browse the images taken by the rover * To receive remote assistance **Training** 70 \u20ac/day Two days of training for the rover (optional) **Preprogramming according to the configuration of the farm** 50 \u20ac Assistance to fine-tune the rover\u2019s configuration to the farm (optional) **Maintenance costs** 5 \u20ac/month/rover Maintenance contract with the ROMI association (or a local distributor) (optional) Examples for different farm sizes Link For an urban farm of 5 000 m2 (lettuces or carrots) 83 beds of 50 meters (beds of 80 cm wide, pathway of 40 cm between beds, 3320 m\u00b2 cultivated) 4150 meters of guides (12 450 \u20ac\u2026) 1 rover with tool carrier and mechanical weeding tools (7000 \u20ac) Investment costs: 7000 to 7510 + 12450 \u20ac + 16 \u20ac/months (optional: SIM card, remote maintenance, server archiving) For a vegetable farm of 2 ha 124 beds of 100 meters (beds of 1.1 m wide, pathway of 0.50 m between beds, 13640 m\u00b2 cultivated) 12 400 meters of guides (37.2 k\u20ac) Lettuces (180 000 plants, 54 k\u20ac-129 k\u20ac revenu): 4 rovers Investment costs: 20.5 k\u20ac + 37.2 k\u20ac + 10 to 31 \u20ac/months (wifi and maintenance) Alternative solution: Plastic mulching film:** **12 x 226,50 \u20ac = 2712 \u20ac/year [ REF ] Carrots (1.24M plants, 60-130 tonnes, 31k\u20ac-130k\u20ac revenue) 1 rover Investment costs: 7000 to 7510 \u20ac + 49.6 k\u20ac + 10 to 16 \u20ac/months (wifi and maintenance) Sur carottes, l\u2019ensemble des op\u00e9rations de d\u00e9sherbage varie de 120 \u00e0 plus de 900 heures/ha. [ http://itab.asso.fr/downloads/fiches-lpc/lpc-carotte.pdf ] Prix [ https://rnm.franceagrimer.fr/prix?CAROTTE&12MOIS ] For a vegetable farm of 5 ha XXX beds XXX meters of guides XXX rovers Investment costs: XXX \u20ac + \u20ac/months (wifi and maintenance) It has to be pointed out that in many countries, there are public financial supports for farmers which want to invest in robotics. Operation instructions Link **Overview of the components **V3 Link >>>>> gd2md-html alert: inline image link here (to images/image13.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> Figure 1: The main components of the rover. TODO: Motor lock lever, Carrier tool, Carrier tool clamp, Control panel, Protection cover, Top camera, Recharging plug, Motor sockets (power & data), CNC socket (power & data), Tool carrier socket (power & data), Components inside the electronics box. Overview of the rover\u2019s usage Link The basic usage of the rover is to position it on a vegetable bed and let the machine clean the top-soil with a rotating precision hoe. The rover must be taken to the field using the remote control or by simply pushing it. The robot expects the vegetables to be grown in \u201cbeds\u201d of 0.7 to 1.2 m wide V3 . The robot is designed for smaller market farms of less than 5 ha but the size of the farm depends on the number of rovers that you will use, and the amount of crop you want to cover. To assist the rover in navigating along a bed, it is necessary to install rails (tubes or wooden boards) along the bed. Without the rails, the risk exists that the rover deviates from its course and drives into the vegetable bed. Once the rover is positioned along the rails in the beginning of a bed, it hoes the surface of the soil so that small weeds cannot take roots. It can perform this action all along the length of the bed. Note that the rover cannot remove mature weeds that have already established themselves. It is therefore necessary to start with a vegetable bed that has been cleaned from all weeds. This can be done with various classical techniques to prepare the vegetable beds. Once the beds are clean, the rover can be used to keep them clean. Two weeding methods are available. First, a precision weeding method in which the top-soil is turned over in between the plants and the rows. Second, a classical weeding method in which standard weeding tools are pulled between the rows of vegetables. For the precision weeding method, the rover uses a camera to detect the plants that are underneath the rover. It then moves the precision weeding tool over the surface whilst passing closely around the detected vegetables. Although the rover is autonomous for weeding a single bed, it is important to stay in proximity of the rover. You must also manually perform the U-turn at the end of the bed and reposition the rover on the rails of the next bed. Setting up the vegetable beds Link The use of the rover requires relatively flat beds. The precision weeding works best if the surface of the culture beds is flat. Ideally, the alleys between the beds should be flat, too, to facilitate the navigation of the rover. The precision weeding tool can mechanically adjust in height for small deviations in the soil level but there is less risk that the tool will detach from the soil or that it will dig into the soil when care has been taken to level the surface. There is no precise measure of how flat the beds should be but small holes in the ground should be avoided. The presence of stones should also be avoided. Small stones (approx. 1 cm) should not perturb the rover very much. NOTE: The width of the vegetable beds should be constant so that the width of the rover remains the same for all the beds. Laying out the rails Link The rails guide the wheels and allow the rover to navigate straight wards. It greatly increases the reliability of the navigation and the precision of the rover. We recommend the use of stainless-steel tubes with a diameter of approximatively 5 cm and thickness of 1.5 mm. There are two options to lay out the tubes. A rail on both sides of the bed : The most intuitive organisation is to place a tube on each side of the bed. The tubes should be positioned on the edge of the bed, one on each side, along the length of the bed. A bed of 10 m long will require 20 m of tubes. Care should be taken to assure that the tubes are parallel. There is no recommended method for fixing the rails in the soil so they remain in place. A simple solution is to drill a hole in the end of the tube and fix the tube in the soil using reinforcing bars (steel wire used to reinforce concrete). >>>>> gd2md-html alert: inline image link here (to images/image14.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> >>>>> gd2md-html alert: inline image link here (to images/image15.jpg). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> Figure 2: Left: The rover with two tube rails on each side of the bed. Right: Two stainless steel tubes fixed to the ground with a \u201cstaple\u201d of reinforcement steel wire Two rails every two beds : In the second organisation, two rails are positioned every two beds. The rails are laid in parallel in the same alley in between two beds. The spacing between the two tubes must be such that they fit the width of the front and rear wheel. In effect, the tubes squeeze the two wheels of one lateral side of the rover. The two wheels on the other side of the rover run freely. The tubes should be centred in the middle of the alley such that one pair can be used for two beds, on either side of the rails. Two beds of 10 m long (20 m total) will require 20 m of tubes, or twice less than when using rails on both sides of the bed. >>>>> gd2md-html alert: inline image link here (to images/image16.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> Figure 3: The rover with a stainless tube on each side of the wheel. >>>>> gd2md-html alert: inline image link here (to images/image17.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> Figure 4: A comparison of the two options for laying out the rails. Left: two rails be bed. Right: two rails for every two beds. An alternative option is to use wooden boards instead of steel tubes. This solution is less optimal because the wheels tend to get stuck when they slide against the angular edge of the board. The boards can be stuck partially in the ground with about 5 cm sticking out to guide the wheels. The use of plastic tubes, for example PVC or polyethylene irrigation tubes, can also be considered. The width of the rover must be adjusted so that the wheels roll along the outside of the rails (see \u201c Adjusting the width of the rover\u201d ). There must be no space between the wheels and the tubes. Maximizing the use of rails (TODO) Link To reduce the number of rails to be purchased, it is possible to buy \u2155 th of the total amount and displace the rails every day. (TODO: How long does it take to place the rails?) Setting up the Wi-Fi access point Link The use of a Wi-Fi access point is optional but strongly recommended. The rover must be connected to a Wi-Fi access point with Internet access for the following functionalities: To automatically upload the images taken by the rover to the Farmer\u2019s Dashboard web application. For remote maintenance. Both features are optional and can be left out when the rover is used for weeding only. However, if you decide to use an access point, it is important that the Wi-Fi signal is strong enough in all the zones where the rover will be used. If not, it may be impossible to connect to the rover\u2019s web interface with a phone or tablet to the rover to send instructions to the rover. It will still be possible to send instructions to the rover using the control panel (see \u201c Controlling the rover through the control panel \u201d). The set-up of the Wi-Fi network is not part of the Romi Rover package. In case of doubt, you should seek advice from a professional about the best solution for your premises. However, below, we briefly discuss several options. Using an existing Wi-Fi router : If the zone where you wish to use the rover is adjacent to existing infrastructure (home, barn) and you have the possibility to install an Internet connection at the premises (ADSL modem over a phone line or any other solution), the Wi-Fi capabilities of the modem can be used to offer Internet access to the rover. Expand the reach of an existing Wi-Fi network : An existing Wi-Fi can be extended to increase its reach using Wi-Fi range extenders. They pick up and retransmit an existing Wi-Fi signal. Most extenders require a standard power supply although some can be powered using an USB battery. Using an Ethernet cable of up to 100 meters long, it is possible to position a secondary access point. Some of the Wi-Fi access points can be powered directly over the Ethernet cable (PoE, Power over Ethernet) removing the need for a power socket. It is also possible to send the network signal over existing electricity cables using a technology called power-line communication (PLC). Finally, there exist also long-range wireless outdoor WiFi extenders that transmit the network between two antennas designed for transmission over distances from a 100 meters to over a kilometer. Install a GSM Wi-Fi router : If there is a good mobile phone signal strength in the field, a GSM Wi-Fi router is a viable option. A GSM Wi-Fi router connects to the Internet over mobile data link (GPRS, EDGE, 3G, HSDPA, 4G, \u2026) and provides access to other devices over Wi-Fi. Separate routers with good antennas can be purchased at reasonable prices but generally require a power plug. Smaller, USB-powered routers are available also and can be plugged directly into a USB port inside the rover. A mobile phone configured as a hotspot is an alternative solution (although with a smaller range than a dedicated router with good antennas). The downside of this option is that it requires a SIM card and a subscription with a mobile network operator. Using a USB GSM modem : In contrast to the solution above, a USB GSM modem is not a stand-alone router but, when plugged in, the Raspberry Pi will see the modem as an additional network interface. The rover remains the hotspot for the Wi-Fi network and will route any Internet traffic through the GSM data connection. This solution may require additional changes to the network configuration of the Raspberry Pi. Charging the rover Link The rover uses two 12 V Lithium batteries (the internal working voltage is 24 V). Use the supplied Victron Energy Blue Smart IP67 24V 5A Charger to reload the batteries. Plug 230 V side of the charger in a regular power plug. The 24 V side must be plugged into the POWER CHARGER plug on the battery box. The charger has LED indicators to show the status of the charging cycle. It is also possible to follow the status using a mobile phone using a Bluetooth connection. Check the official manual provided by Victron Energy charger for details. Protection cover Link The rover comes with a PVC protection cover. The cover must always be placed on the rover when the precision weeding is used. If the precision weeding is not used, it can be removed if there is no risk of the CNC becoming wet. The CNC, on its own, is not waterproof. If the CNC is removed, it is possible to use the rover without cover in light rain conditions (TODO: IP level?...) Disassembling and reassembling the rover (V3) Link The rover can be disassembled into its main components. This is useful for transport. CAUTION: This should be done by two people. CAUTION: Unplugging the power cables when the rover is on may cause sparks and may damage the rover\u2019s control circuits. Make sure the rover is powered off. Take off the protective cover. Unplug all the cables on both boxes. Untighten the screws of the arcs and of the top bar and remove them. Remove the pins that fix the CNC to the main frame and remove the CNC. Untighten the U-brackets of the wheel modules and remove them. Following the steps above, the main components are now separated: two wheel modules, the main frame, the CNC, the top bars, the arcs, and the protective cover. Adjusting the width of the rover (V3) Link The wheel-base of the rover can be adjusted to fit the width of the rails and beds. Loosen the four U-brackets (see Figure 1) that fix the wheel modules to the main frame and slide the wheel modules to the desired position. Assure that the position of the modules is symmetric relative to the main frame. After a change to the width of the wheel-base, the CNC must be calibrated (see \u201c Calibrating the CNC \u201d) Adjusting the height of the rover (V3) Link TODO Attendance (TODO: regulations?) Link IMPORTANT: The rover must be used only in the presence of an operator (TODO). The operator must be within a distance of XXX meters of the rover and must be able to reach the rover quickly in case of an emergency (TODO). The operator must carry the remote control with her at all times when the rover is On in order to be able to recover the navigation control of the rover in all circumstances (TODO: add emergency button on the remote control). The rover should not be used in proximity to people who have not been instructed to use the rover (TODO). IMPORTANT: The rover must be used only during the day in good light conditions. Storage Link The rover should be kept in a covered and not too humid space when not in use. Emergency button Link The emergency button on the back of the rover can be used to cut the power to the motors and CNC at any time. To cut the power, push the red button. To power up the motors, the button must be reactivated. This can be done by pulling the button out again. CAUTION: Before reactivating the button, make sure that the CNC and wheel motors are not moving (TODO: how?\u2026) Engaging/disengaging the motor lock levers (freewheeling mode and drive mode) Link The two wheel motors each have a lock lever that allows them to switch between freewheeling mode or motor drive mode. When the lock lever is in the horizontal position the wheels are freewheeling. In freewheeling mode, the robot can be moved simply by pushing it. Turn the lever 90\u00b0 into the vertical position to switch the drive mode. In the drive mode, the wheels are powered by the motors and to move the rover you must use the remote control or the command interface. CAUTION: Only switch to the drive mode when the rover is \u201coff\u201d to assure that the motors are powered off. (TODO: is there no simpler way to power off the motors without shutting down the rover?) >>>>> gd2md-html alert: inline image link here (to images/image18.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> >>>>> gd2md-html alert: inline image link here (to images/image19.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> Figure 5: Lock lever horizontal : Freewheeling mode Lock lever vertical: Motor drive mode The control panel Link The control panel provides a means to turn the rover on or off, and to view status messages. >>>>> gd2md-html alert: inline image link here (to images/image20.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> Figure 6: The control panel It has a display and five buttons, including the On/Off button. Please skip to the section \u201c Controlling the rover through the control panel \u201d for more information on how to use the control panel. Control panel state message Link The display of the control panel is divided in two lines. The upper line shows current status of the rover: State display (1 st line) Description ... The control panel has been powered up and is initializing. Off The on-board computer is powered down and the motors are off. Starting up The on-board computer is starting up. The power to the motors is cut. On The rover is ready for use. The on-board computer is running and the motors can be powered up. Shutting down The power to the motors has been cut and the on-board computer is shutting down. Error The rover is in an error state. Start-up procedure Link Before starting up, the rover should be in the following state: Verify that the emergency button is deactivated (pushed in). Verify that the rover is off (the control panel display shows \u201c** Off \u201d). If not \u2026see \u201cXXX\u201d Verify that the lock levers on the motor are disengaged to put the motors into freewheeling mode. The start-up can now proceed: Engage the lock levers on the motor to put the motors into drive mode. Activate the security button by pulling it out. Turn the rover on by holding the on/off button pressed for 5 seconds (see Fig. 1). At that point, the rover begins the start-up sequence and the display says \u201c** Starting up \u201d. When the start-up is completed, the display will show \u201c On \u201d. The motors of the wheels and the CNC are now powered up. The start-up is now finished. You can either do the following: If it is the first usage of the rover, you should go to the section \u201c First time configuration \u201d. You can use the remote control to move the rover (see \u201c Remote control mode and software control mode \u201d), or Use the control interface to send commands to the rover (see \u201c Controlling the rover through the control panel \u201d). Shut-down procedure Link To turn off the rover, press the On/Off button for 5 seconds. The display will briefly display the message \u201c** Shutting down \u201d, followed by the message \u201c Off \u201d. **Switching from drive to freewheeling mode ** Link When the rover is in drive mode (motors powered on, the lock levers on the motor engaged/vertical), it is possible to go to freewheeling mode as follows: Turn off the power of the motors by pressing the red emergency button. Turn the motor lock levers in the horizontal position (disengaged). Once these steps are completed, you can move the rover by pushing it. **Switching from freewheeling to drive mode ** Link To switch from freewheeling to drive mode, the rover\u2019s state should be \u201dOn\u201d. Then do the following operations: Assure that the motors are not moving by sending a STOP command (TODO). Turn the motor lock levers in the vertical position (engaged). Pull the red security button to power the motors. CAUTION: Make sure that the speed and direction controllers of the remote control are in the neutral position. First time configuration (**V2 )** Link To configure the robot, you need a mobile phone, tablet, or computer with Wi-Fi capabilities, a recent web browser (see \u201c Supported web browsers \u201d) and a screen of minimum 320x240 (TEST) pixels. Connect your device to the \u201cRomi\u201d wireless network, using the password \u201crover\u201d. Once you are connected, open a web browser and navigate to the page https://romirover.local . The configuration page of the rover (**V2 )** Link >>>>> gd2md-html alert: inline image link here (to images/image21.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> Figure 2: The configuration page. This page will be shown on the first connection to the rover\u2019s web address, or when you select the \u201cConfiguration\u201d button in the main page on subsequent access to the rover\u2019s interface. The configuration page lets you: Change the password of interface Change the name of the rover Change the WiFi settings Change the settings for the Farmer\u2019s Dashboard. Changing the password of the interface (**V2 )** Link The first thing you should do is change the default password to something more secure but still easy to remember. Changing the rover\u2019s name and address (**V2 )** Link The name of the rover can remain unchanged unless you have several rovers. In that latter case, you should give each a distinct name in order to access the web interface of each. In the configuration page you can enter the following two strings: Short name: A short string that satisfies the following constraints: minimum 5 characters and maximum 32 characters long, only letters (a-z, A_Z), digits (0-9), and the underscore character (_) are allowed. The name should start with a letter. Name: A free-form name of maximum 64 characters. When the name of the rover is changed, the address of the web interface will change to https://NEWNAME.localhost . The value of NEWNAME must be replaced with the short name you have given to the rover. This change will be active after the rover has been turned off and then turned on again. Using a Wi-Fi access point / Enabling Internet access (**V2 )** Link To change the Wi-Fi configuration, open the web page https://romirover.local (TODO: does Raspian use Rendez-Vous/Avahi by default?) (or its new location, see \u201c Changing the rover\u2019s name and address \u201d). In the configuration page, check \u201cUse access point\u201d and enter the name of the Wi-Fi network (the SSID) and the password. In case the rover fails to connect to the access point, the configuration will revert to the default configuration that initializes the rover as a local access point. (TODO: verify/configure with Raspian, similar to Ubuntu) Verifying the Wi-Fi connection (**V3 )** Link If the Wi-Fi fails to connect, the control panel will display \u201cNo network\u201d. In that case, please verify the network name and password as in the previous section. This status message is not a problem and can be ignored if this happens occasionally, for example, when the rover is far away from the access point. As soon as the rover will be in proximity of the access point, the connection will be re-established. If the message continues to appear when the rover is in proximity of the access point and after the rover has been turned off and on again, then you should verify the Wi-Fi configuration. Connecting a phone, tablet, or computer to the rover Link Adapt the Wi-Fi settings of the device such that it connects to the same Wi-Fi network as the rover. The interface of the rover is accessible through a web browser. On the mobile device, open up your preferred web browser (see \u201c Supported web browsers \u201d) and in the address field enter the following URL: https://ROVER_SHORT_NAME.local . By default, the ROVER_SHORT_NAME is \u201cromirover\u201d. If you changed the name of the rover, you must use the new short name instead (see \u201c Changing the rover\u2019s name and address \u201d). To facilitate the access to the interface, you can add the address to your bookmarks. Moving the rover to the field Link The rover can be moved to the field either by simply pushing it with the motors in the freewheeling mode or by using the remote control to steer the rover. CAUTION: When you use the remote controller, you must stay close to the rover (less than 3m away?). TODO: regulations? Positioning the rover on a vegetable bed Link Manually push the rover onto the beginning of the vegetable bed. Make sure that the front wheels (caster wheels) and the rear wheels slide nicely along the rails of the bed. Remote control mode and software control mode Link The navigation of the rover can be controlled either by the remote control or by the rover\u2019s software. TODO: Must add an indicator to know what mode the rover is in. TODO: How do we switch from one to another mode? Emergency control recovery with the remote control Link When the rover is in software control mode it is possible at any time to switch back to remote control mode by pushing the speed or direction controller to the maximum position. The rover will stop and remain immobilized for 3 seconds before listening to the commands of the remote control again. Using the remote control Link The rover comes with the Spektrum STX3 remote controller (RC). >>>>> gd2md-html alert: inline image link here (to images/image22.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> Figure 7: The remote control. The RC\u2019s steering wheel lets you define the direction in which the rover moves. The trigger lets you set the speed, both backward and forward (TODO: is this true with this controller?). Please check the Spektrum STX3 official user manual for detailed information on its use. The RC is powered by 4 AA batteries. **Calibrating the CNC-**to-Camera mapping (V2) Link The weeding algorithm must be able to map the position of a pixel in the image to a XY coordinate of the CNC. The following steps Replace weeding tool with TODO Change the weeding tool head Link Pin Controlling the rover through the control panel (V2) Link You can send commands to the rover using the control panel as follows. >>>>> gd2md-html alert: inline image link here (to images/image23.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> MAIN SCREEN The rover must be in the On state. Then press the MENU button. >>>>> gd2md-html alert: inline image link here (to images/image24.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> MENU SCREEN The name of the first task will appear on the bottom line of the display. Use the UP and DOWN buttons to navigate in the list of possible tasks. To cancel and return to the main screen, press MENU. >>>>> gd2md-html alert: inline image link here (to images/image25.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> To start the task, press the SELECT button. To return to the menu screen, press MENU. >>>>> gd2md-html alert: inline image link here (to images/image26.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> CONFIRM SCREEN Press the SEL button a second time to confirm the action, or press MENU to cancel the start of the action and return to the menu screen. >>>>> gd2md-html alert: inline image link here (to images/image27.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> PROGRESS SCREEN If the action is confirmed, the display will show the progress status. When the task is finished, the display will return to the main screen. Controlling the rover through the web interface Link The web interface is a convenient way to send commands to the rover. The interface consists of large buttons to facilitate its use on a mobile phone in the field. The buttons and associated actions are programmable (see \u201c Editing the rover scripts \u201d). By default, a couple of generic buttons are provided (see Fig. 3). When you press a button, a confirmation will be asked to avoid launching an action inadvertently. Upon confirmation, a progress screen will be shown with information on the advancement of the action. >>>>> gd2md-html alert: inline image link here (to images/image28.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> Figure 8: The web interface. Left: The main screen. Centre: The confirmation screen. Right: The progress screen. The list of buttons and associated actions can be programmed. This is useful to adapt them to your needs. See \u201c Editing the rover buttons and actions\u201d for more information. Using the tool carrier Link The tool carrier can pull classical weeding tools along the soil. It is best adapted for cultivars that are grown in dense lines, such as carrots. The weeding tools are not part of the Romi Rover package and must be purchased separately. The tool carrier must be attached to the main frame of the rover using the clamps that are welded on the carrier. Attach the power cable to the frame using the available clamps and plug it into the TOOL CARRIER socket on the control box of the rover. The tools must be fixed to one of the horizontal bars of the tool carrier using the dedicated clamps. Remote maintenance (V3) Link The rover automatically connects to the remote service provided by Dataplicity. Remote maintenance (V3) Link The rover automatically connects to the remote service provided by Dataplicity. The USB memory stick (**V2 )** Link The on-board computer stores all editable and generated data on a USB memory stick. When the rover is turned off, it is possible to remove the stick in the electronics box of the rover and connect it to another computer. This should normally not be necessary but can be convenient to make a backup of the recorded data or to edit the configuration files manually. The default organisation looks as follows. bin/ config/ +- config.json +- scripts.json +- wifi.json (TODO) database/ lib/ sessions/ +- 2020-08-15_08-47-51/ +- logs/ +- dumps/ +- 2020-08-16_08-53-03/ +- logs/ +- dumps/ The config directory contains several configuration files including: Filename Description config.json The main configuration file of the rover. It is not advised to change this file. scripts.json This file defines the list of buttons and their associated actions that are shown on the user interface (web interface and control panel). wifi.json The wifi set-up of the rover. (TODO) The database directory contains the images and analyses generated either by the weeding tasks or by the camera recorder. It is not advised to make any changes in this directory. The sessions directory contains the log and dump files that are used for the rover\u2019s maintenance. It is not advised to make any changes in this directory. Manually editing the configuration Link In some cases, it may be necessary or more convenient to edit the rover\u2019s configuration file directly instead of using the web interface (see \u201c The configuration page of the rover \u201d). TODO: continue Editing the rover buttons and actions Link Turn off the rover and open the box with the electronic components to recover the USB stick (see \u201cThe USB memory stick\u201d ). Open the file config/scripts.json on the memory stick using a plain text editor (On Windows, use Notepad, for example). The file uses the JSON format to describe the list of scripts and the associated sequences of actions. The general structure is as follows: [ { \"name\": \"move-forward\", \"display_name\": \"Forward\", \"script\": [ { \"action\": \"move\", \"distance\": 0.60 } ] }, { \"name\": \"move-backward\", \"display_name\": \"Backward\", \"script\": [ { \"action\": \"move\", \"distance\": -0.60 } ] }, { \"name\": \"scan\", \"display_name\": \"Scan\", \"script\": [ { \"action\": \"start_recording\" }, { \"action\": \"move\", \"distance\": 3.6 }, { \"action\": \"stop_recording\" } ] } ] The file contains a list of scripts. Each script has a name that is used to identify the script, a display_name that is shown in the user interface, and a script field that consists of a list of actions. The list of available actions and their parameters is out of the scope of this manual. Please refer to the online documentation at https://docs.romi-project.eu/Rover/configuration/ for details. If you make modifications to the file, it is very important that the new content is a valid JSON file. If not, the rover will fail to load the file and no buttons will be shown in the user interface. Farmer\u2019s Dashboard (V3) Link The Farmer\u2019s Dashboard is an online website to view and analyse images scans of the crops. The images collected by the rover can also be uploaded to the Farmer\u2019s Dashboard. The web site offers a means to browse the archive of images. The Farmer\u2019s Dashboard is a complementary service offered by the Romi Organisation. Since the software for the Farmer\u2019s Dashboard is Free and Open Source Software, it is possible to install it on your own server or to rely on a third party. For more information, please visit the Farmer\u2019s Dashboard information page at XXX. Automatically uploading the images to the Farmer\u2019s Dashboard (**V3 )** Link You must first create an account on the Farmer\u2019s Dashboard web site. Next, in the account settings, click the \u201cAPI key\u201d tab. You must then enter the name of the web site and the API key in the configuration page of the rover (see \u201c The configuration page of the rover \u201d). Manually uploading the images to the Farmer\u2019s Dashboard (**V3 )** Link It is possible to manually upload the images to the Farmer\u2019s Dashboard web site. First, turn off the rover. Then recover the USB memory stick of the on-board computer of the rover. Put the memory stick in a PC. Finally, follow the upload instructions on the Farmer\u2019s Dashboard web site. CAUTION: Make sure to put the USB memory stick back into the rover. If not, the rover may fail to function properly upon its next use. Supported web browsers Link The interface of the rover should be viewable in any modern, compliant HTML5-compliant web browser with support for ECMAScript 5 (Javascript), XMLHttpRequests, and WebSockets. The following versions and more recent versions of commonly used browsers should work with the rover (TODO: verify!): Browser name Version Release date Chrome 23 Sep 2012 Firefox 21 Apr 2013 IE / Edge 10 Sep 2012 iOS 6 Safari 6 Jul 2012 Opera 15 Jul 2013 \\ The Farmer\u2019s Dashboard web site that is used to browse archived images has its own requirements that are documented on the Farmer\u2019s Dashboard web site. Installing a new SD card Link The rover uses a Raspberry Pi as its main computer. The control software of the rover is installed on an SD card that sits in the card slot of the Raspberry Pi 4. You can replace the SD card with a new one, following steps below: TODO","title":"Manual"},{"location":"Rover/manual/#user-manual","text":">>>>> gd2md-html alert: inline image link here (to images/image2.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> Draft - September 2020 \u00a9 Sony Computer Science Laboratories - CC BY-SA 4.0 License ROMI has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 773875. \\ This manual describes the usage of a fully assembled and ready to use rover. The information on how to build your own rover can be found in a separate document. Visit the ROMI web site at https://docs.romi-project.eu for more information.","title":"User Manual"},{"location":"Rover/manual/#short-description","text":"The ROMI Rover is a farming tool that assists vegetable farmers in maintaining the vegetable beds free from weeds. It does this by regularly hoeing the surface of the soil and thus preventing small weeds from taking roots. It can do this task mostly autonomously and requires only minor changes to the organization of the farm. It is designed for vegetable beds between 70 cm and 120 cm wide (not including the passage ways) and currently handles two types of crops, lettuce and carrots. The lettuce can be planted out in any layout, most likely in a quincunx pattern. In this configuration the rover uses a precision rotary hoe to clean the soil both between the rows and the plants. For the carrots, the rover uses classical mechanical tools, such as stirrup hoe, to regularly clean the soil in between the rows. In this configuration, the carrots should be sown in line. A weekly passage of the robot should be sufficient to keep the population of weeds under control. In addition to the weeding, the rover provides the following useful functions. It can draw quincunx patterns or straight lines in empty vegetable beds to speed up seeding and planting out. The embedded camera can be used to collect images of the vegetable beds. It can also be used as a motorized tray. The ROMI Rover is targeted at farms that grow lettuce and carrots on a relatively small surface, between 200 m\u00b2 and 5 ha (Utilized Agricultural Area).","title":"Short description"},{"location":"Rover/manual/#_1","text":"","title":""},{"location":"Rover/manual/#table-of-content","text":"User Manual Short description Table of Content Technical specifications Functional specifications and requirements Examples for different farm sizes Operation instructions **Overview of the components **V3 Overview of the rover\u2019s usage Setting up the vegetable beds Laying out the rails Maximizing the use of rails (TODO) Setting up the Wi-Fi access point Charging the rover Protection cover Disassembling and reassembling the rover (V3) Adjusting the width of the rover (V3) Adjusting the height of the rover (V3) Attendance (TODO: regulations?) Storage Emergency button Engaging/disengaging the motor lock levers (freewheeling mode and drive mode) The control panel Control panel state message Start-up procedure Shut-down procedure **Switching from drive to freewheeling mode ** **Switching from freewheeling to drive mode ** First time configuration (**V2)** The configuration page of the rover (**V2)** Changing the password of the interface (**V2)** Changing the rover\u2019s name and address (**V2)** Using a Wi-Fi access point / Enabling Internet access (**V2)** Verifying the Wi-Fi connection (**V3)** Connecting a phone, tablet, or computer to the rover Moving the rover to the field Positioning the rover on a vegetable bed Remote control mode and software control mode Emergency control recovery with the remote control Using the remote control **Calibrating the CNC-**to-Camera mapping (V2) Change the weeding tool head Controlling the rover through the control panel (V2) Controlling the rover through the web interface Using the tool carrier Remote maintenance (V3) Remote maintenance (V3) The USB memory stick (**V2)** Manually editing the configuration Editing the rover buttons and actions Farmer\u2019s Dashboard (V3) Automatically uploading the images to the Farmer\u2019s Dashboard (**V3)** Manually uploading the images to the Farmer\u2019s Dashboard (**V3)** Supported web browsers Installing a new SD card Legend V2 The feature will be implemented in the second prototype (deadline: December 2020) V2 The feature refers to the second prototype and will be different in the third prototype. V3 The feature will be implemented in the third prototype (deadline: May 2021). V3 The feature refers to the third prototype and exists in a different form in the second prototype.","title":"Table of Content"},{"location":"Rover/manual/#_2","text":"","title":""},{"location":"Rover/manual/#technical-specifications","text":"Size V3 (Width x Length x Height) Min.: 1.45 m x 1.60 m x 1.46 m Max.: 1.70 m x 1.60 m x 1.46 m Weight V2 80 kg (estimate) Battery life V3 8 h (TODO: that\u2019s the target) Charging time TODO Weeding speed Precision weeding: 600 m\u00b2/day V3 (235 m\u00b2/day V2 ) Classical weeding: 7200 m\u00b2/day V3 (6400 m\u00b2/day V2 ) Width vegetable beds V3 Min.: 0.70 m Max.: 1.2 m Handled crops Precision weeding: Lettuce Classical weeding: Carrots Turning space at end of bed 2 m (TODO: verify)","title":"Technical specifications"},{"location":"Rover/manual/#functional-specifications-and-requirements","text":"The following configuration is required for the use of the ROMI rover. Profile Price Features/Function Bed of vegetables Width of the bed: min 0.7 m max 1.2 m V3 Handled crops Lettuce : The lettuce can be planted out in any layout, most likely in a quincunx pattern Carrots : The carrots should be sown in line. ROMI rover with a remote control, a battery charger, and a protective cover >>>>> gd2md-html alert: inline image link here (to images/image3.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> V3 >>>>> gd2md-html alert: inline image link here (to images/image4.jpg). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> >>>>> gd2md-html alert: inline image link here (to images/image5.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> 5000 \u20ac (estimate) Prevents weed development Draw patterns for seeding and planting out Takes image scans of the beds Tools carrier Photo 1000 \u20ac (estimate) Used with the rover to carry the mechanical weeding tools (optional but needed in most configurations) Mechanical weeding tools (from Terrateck or other manufacturers) >>>>> gd2md-html alert: inline image link here (to images/image6.jpg). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> >>>>> gd2md-html alert: inline image link here (to images/image7.jpg). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> >>>>> gd2md-html alert: inline image link here (to images/image8.jpg). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> >>>>> gd2md-html alert: inline image link here (to images/image9.jpg). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> 400-1000\u20ac (estimate) The weeding tools that will be fixed on the tool carrier. Sold by other manufacturers, for example, by [Terrateck](https://www.terrateck.com) ([catalogue](https://www.terrateck.com/en/16-houes-pousse-pousse)) **TOTAL cost of an equipped rover** 6400 \u2013 7000 \u20ac **Guides** Photo Stainless steel tubes (45x1.5): 3 \u20ac/m Tuyau irrigation PE HD 50mm : 4.4 \u20ac/m The guides consist of either stainless metal tubes (preferred) or wooden boards. They facilitate the navigation of the rover along the bed so that it can operate accurately and reliably. **A mobile phone** >>>>> gd2md-html alert: inline image link here (to images/image10.jpg). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> ![alt_text](images/image10.jpg \"image_tooltip\") Existing phone: 0\u20ac Dedicated phone: 200 \u20ac * Used to control the rover * To browse the archived images of the online service **WiFi** >>>>> gd2md-html alert: inline image link here (to images/image11.jpg). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> ![alt_text](images/image11.jpg \"image_tooltip\") Rover as hotspot: 0 \u20ac/month Existing WiFi: 0 \u20ac/month GSM WiFi router: 120\u20ac (router) + 10 \u20ac/month (SIM card) WiFi connectivity in the field * To connect the mobile phone to the rover (required) * To archives the images taken by the rover (optional) * To provide remote assistance (optional) **Romi Online Service V3** >>>>> gd2md-html alert: inline image link here (to images/image12.jpg). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> ![alt_text](images/image12.jpg \"image_tooltip\") 1 \u20ac/month Monthly fee for the use of the Romi remote server (optional): * To archive and browse the images taken by the rover * To receive remote assistance **Training** 70 \u20ac/day Two days of training for the rover (optional) **Preprogramming according to the configuration of the farm** 50 \u20ac Assistance to fine-tune the rover\u2019s configuration to the farm (optional) **Maintenance costs** 5 \u20ac/month/rover Maintenance contract with the ROMI association (or a local distributor) (optional)","title":"Functional specifications and requirements"},{"location":"Rover/manual/#examples-for-different-farm-sizes","text":"For an urban farm of 5 000 m2 (lettuces or carrots) 83 beds of 50 meters (beds of 80 cm wide, pathway of 40 cm between beds, 3320 m\u00b2 cultivated) 4150 meters of guides (12 450 \u20ac\u2026) 1 rover with tool carrier and mechanical weeding tools (7000 \u20ac) Investment costs: 7000 to 7510 + 12450 \u20ac + 16 \u20ac/months (optional: SIM card, remote maintenance, server archiving) For a vegetable farm of 2 ha 124 beds of 100 meters (beds of 1.1 m wide, pathway of 0.50 m between beds, 13640 m\u00b2 cultivated) 12 400 meters of guides (37.2 k\u20ac) Lettuces (180 000 plants, 54 k\u20ac-129 k\u20ac revenu): 4 rovers Investment costs: 20.5 k\u20ac + 37.2 k\u20ac + 10 to 31 \u20ac/months (wifi and maintenance) Alternative solution: Plastic mulching film:** **12 x 226,50 \u20ac = 2712 \u20ac/year [ REF ] Carrots (1.24M plants, 60-130 tonnes, 31k\u20ac-130k\u20ac revenue) 1 rover Investment costs: 7000 to 7510 \u20ac + 49.6 k\u20ac + 10 to 16 \u20ac/months (wifi and maintenance) Sur carottes, l\u2019ensemble des op\u00e9rations de d\u00e9sherbage varie de 120 \u00e0 plus de 900 heures/ha. [ http://itab.asso.fr/downloads/fiches-lpc/lpc-carotte.pdf ] Prix [ https://rnm.franceagrimer.fr/prix?CAROTTE&12MOIS ] For a vegetable farm of 5 ha XXX beds XXX meters of guides XXX rovers Investment costs: XXX \u20ac + \u20ac/months (wifi and maintenance) It has to be pointed out that in many countries, there are public financial supports for farmers which want to invest in robotics.","title":"Examples for different farm sizes"},{"location":"Rover/manual/#operation-instructions","text":"","title":"Operation instructions"},{"location":"Rover/manual/#overview-of-the-components-v3","text":">>>>> gd2md-html alert: inline image link here (to images/image13.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> Figure 1: The main components of the rover. TODO: Motor lock lever, Carrier tool, Carrier tool clamp, Control panel, Protection cover, Top camera, Recharging plug, Motor sockets (power & data), CNC socket (power & data), Tool carrier socket (power & data), Components inside the electronics box.","title":"**Overview of the components **V3"},{"location":"Rover/manual/#overview-of-the-rovers-usage","text":"The basic usage of the rover is to position it on a vegetable bed and let the machine clean the top-soil with a rotating precision hoe. The rover must be taken to the field using the remote control or by simply pushing it. The robot expects the vegetables to be grown in \u201cbeds\u201d of 0.7 to 1.2 m wide V3 . The robot is designed for smaller market farms of less than 5 ha but the size of the farm depends on the number of rovers that you will use, and the amount of crop you want to cover. To assist the rover in navigating along a bed, it is necessary to install rails (tubes or wooden boards) along the bed. Without the rails, the risk exists that the rover deviates from its course and drives into the vegetable bed. Once the rover is positioned along the rails in the beginning of a bed, it hoes the surface of the soil so that small weeds cannot take roots. It can perform this action all along the length of the bed. Note that the rover cannot remove mature weeds that have already established themselves. It is therefore necessary to start with a vegetable bed that has been cleaned from all weeds. This can be done with various classical techniques to prepare the vegetable beds. Once the beds are clean, the rover can be used to keep them clean. Two weeding methods are available. First, a precision weeding method in which the top-soil is turned over in between the plants and the rows. Second, a classical weeding method in which standard weeding tools are pulled between the rows of vegetables. For the precision weeding method, the rover uses a camera to detect the plants that are underneath the rover. It then moves the precision weeding tool over the surface whilst passing closely around the detected vegetables. Although the rover is autonomous for weeding a single bed, it is important to stay in proximity of the rover. You must also manually perform the U-turn at the end of the bed and reposition the rover on the rails of the next bed.","title":"Overview of the rover\u2019s usage"},{"location":"Rover/manual/#setting-up-the-vegetable-beds","text":"The use of the rover requires relatively flat beds. The precision weeding works best if the surface of the culture beds is flat. Ideally, the alleys between the beds should be flat, too, to facilitate the navigation of the rover. The precision weeding tool can mechanically adjust in height for small deviations in the soil level but there is less risk that the tool will detach from the soil or that it will dig into the soil when care has been taken to level the surface. There is no precise measure of how flat the beds should be but small holes in the ground should be avoided. The presence of stones should also be avoided. Small stones (approx. 1 cm) should not perturb the rover very much. NOTE: The width of the vegetable beds should be constant so that the width of the rover remains the same for all the beds.","title":"Setting up the vegetable beds"},{"location":"Rover/manual/#laying-out-the-rails","text":"The rails guide the wheels and allow the rover to navigate straight wards. It greatly increases the reliability of the navigation and the precision of the rover. We recommend the use of stainless-steel tubes with a diameter of approximatively 5 cm and thickness of 1.5 mm. There are two options to lay out the tubes. A rail on both sides of the bed : The most intuitive organisation is to place a tube on each side of the bed. The tubes should be positioned on the edge of the bed, one on each side, along the length of the bed. A bed of 10 m long will require 20 m of tubes. Care should be taken to assure that the tubes are parallel. There is no recommended method for fixing the rails in the soil so they remain in place. A simple solution is to drill a hole in the end of the tube and fix the tube in the soil using reinforcing bars (steel wire used to reinforce concrete). >>>>> gd2md-html alert: inline image link here (to images/image14.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> >>>>> gd2md-html alert: inline image link here (to images/image15.jpg). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> Figure 2: Left: The rover with two tube rails on each side of the bed. Right: Two stainless steel tubes fixed to the ground with a \u201cstaple\u201d of reinforcement steel wire Two rails every two beds : In the second organisation, two rails are positioned every two beds. The rails are laid in parallel in the same alley in between two beds. The spacing between the two tubes must be such that they fit the width of the front and rear wheel. In effect, the tubes squeeze the two wheels of one lateral side of the rover. The two wheels on the other side of the rover run freely. The tubes should be centred in the middle of the alley such that one pair can be used for two beds, on either side of the rails. Two beds of 10 m long (20 m total) will require 20 m of tubes, or twice less than when using rails on both sides of the bed. >>>>> gd2md-html alert: inline image link here (to images/image16.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> Figure 3: The rover with a stainless tube on each side of the wheel. >>>>> gd2md-html alert: inline image link here (to images/image17.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> Figure 4: A comparison of the two options for laying out the rails. Left: two rails be bed. Right: two rails for every two beds. An alternative option is to use wooden boards instead of steel tubes. This solution is less optimal because the wheels tend to get stuck when they slide against the angular edge of the board. The boards can be stuck partially in the ground with about 5 cm sticking out to guide the wheels. The use of plastic tubes, for example PVC or polyethylene irrigation tubes, can also be considered. The width of the rover must be adjusted so that the wheels roll along the outside of the rails (see \u201c Adjusting the width of the rover\u201d ). There must be no space between the wheels and the tubes.","title":"Laying out the rails"},{"location":"Rover/manual/#maximizing-the-use-of-rails-todo","text":"To reduce the number of rails to be purchased, it is possible to buy \u2155 th of the total amount and displace the rails every day. (TODO: How long does it take to place the rails?)","title":"Maximizing the use of rails (TODO)"},{"location":"Rover/manual/#setting-up-the-wi-fi-access-point","text":"The use of a Wi-Fi access point is optional but strongly recommended. The rover must be connected to a Wi-Fi access point with Internet access for the following functionalities: To automatically upload the images taken by the rover to the Farmer\u2019s Dashboard web application. For remote maintenance. Both features are optional and can be left out when the rover is used for weeding only. However, if you decide to use an access point, it is important that the Wi-Fi signal is strong enough in all the zones where the rover will be used. If not, it may be impossible to connect to the rover\u2019s web interface with a phone or tablet to the rover to send instructions to the rover. It will still be possible to send instructions to the rover using the control panel (see \u201c Controlling the rover through the control panel \u201d). The set-up of the Wi-Fi network is not part of the Romi Rover package. In case of doubt, you should seek advice from a professional about the best solution for your premises. However, below, we briefly discuss several options. Using an existing Wi-Fi router : If the zone where you wish to use the rover is adjacent to existing infrastructure (home, barn) and you have the possibility to install an Internet connection at the premises (ADSL modem over a phone line or any other solution), the Wi-Fi capabilities of the modem can be used to offer Internet access to the rover. Expand the reach of an existing Wi-Fi network : An existing Wi-Fi can be extended to increase its reach using Wi-Fi range extenders. They pick up and retransmit an existing Wi-Fi signal. Most extenders require a standard power supply although some can be powered using an USB battery. Using an Ethernet cable of up to 100 meters long, it is possible to position a secondary access point. Some of the Wi-Fi access points can be powered directly over the Ethernet cable (PoE, Power over Ethernet) removing the need for a power socket. It is also possible to send the network signal over existing electricity cables using a technology called power-line communication (PLC). Finally, there exist also long-range wireless outdoor WiFi extenders that transmit the network between two antennas designed for transmission over distances from a 100 meters to over a kilometer. Install a GSM Wi-Fi router : If there is a good mobile phone signal strength in the field, a GSM Wi-Fi router is a viable option. A GSM Wi-Fi router connects to the Internet over mobile data link (GPRS, EDGE, 3G, HSDPA, 4G, \u2026) and provides access to other devices over Wi-Fi. Separate routers with good antennas can be purchased at reasonable prices but generally require a power plug. Smaller, USB-powered routers are available also and can be plugged directly into a USB port inside the rover. A mobile phone configured as a hotspot is an alternative solution (although with a smaller range than a dedicated router with good antennas). The downside of this option is that it requires a SIM card and a subscription with a mobile network operator. Using a USB GSM modem : In contrast to the solution above, a USB GSM modem is not a stand-alone router but, when plugged in, the Raspberry Pi will see the modem as an additional network interface. The rover remains the hotspot for the Wi-Fi network and will route any Internet traffic through the GSM data connection. This solution may require additional changes to the network configuration of the Raspberry Pi.","title":"Setting up the Wi-Fi access point"},{"location":"Rover/manual/#charging-the-rover","text":"The rover uses two 12 V Lithium batteries (the internal working voltage is 24 V). Use the supplied Victron Energy Blue Smart IP67 24V 5A Charger to reload the batteries. Plug 230 V side of the charger in a regular power plug. The 24 V side must be plugged into the POWER CHARGER plug on the battery box. The charger has LED indicators to show the status of the charging cycle. It is also possible to follow the status using a mobile phone using a Bluetooth connection. Check the official manual provided by Victron Energy charger for details.","title":"Charging the rover"},{"location":"Rover/manual/#protection-cover","text":"The rover comes with a PVC protection cover. The cover must always be placed on the rover when the precision weeding is used. If the precision weeding is not used, it can be removed if there is no risk of the CNC becoming wet. The CNC, on its own, is not waterproof. If the CNC is removed, it is possible to use the rover without cover in light rain conditions (TODO: IP level?...)","title":"Protection cover"},{"location":"Rover/manual/#disassembling-and-reassembling-the-rover-v3","text":"The rover can be disassembled into its main components. This is useful for transport. CAUTION: This should be done by two people. CAUTION: Unplugging the power cables when the rover is on may cause sparks and may damage the rover\u2019s control circuits. Make sure the rover is powered off. Take off the protective cover. Unplug all the cables on both boxes. Untighten the screws of the arcs and of the top bar and remove them. Remove the pins that fix the CNC to the main frame and remove the CNC. Untighten the U-brackets of the wheel modules and remove them. Following the steps above, the main components are now separated: two wheel modules, the main frame, the CNC, the top bars, the arcs, and the protective cover.","title":"Disassembling and reassembling the rover (V3)"},{"location":"Rover/manual/#adjusting-the-width-of-the-rover-v3","text":"The wheel-base of the rover can be adjusted to fit the width of the rails and beds. Loosen the four U-brackets (see Figure 1) that fix the wheel modules to the main frame and slide the wheel modules to the desired position. Assure that the position of the modules is symmetric relative to the main frame. After a change to the width of the wheel-base, the CNC must be calibrated (see \u201c Calibrating the CNC \u201d)","title":"Adjusting the width of the rover (V3)"},{"location":"Rover/manual/#adjusting-the-height-of-the-rover-v3","text":"TODO","title":"Adjusting the height of the rover (V3)"},{"location":"Rover/manual/#attendance-todo-regulations","text":"IMPORTANT: The rover must be used only in the presence of an operator (TODO). The operator must be within a distance of XXX meters of the rover and must be able to reach the rover quickly in case of an emergency (TODO). The operator must carry the remote control with her at all times when the rover is On in order to be able to recover the navigation control of the rover in all circumstances (TODO: add emergency button on the remote control). The rover should not be used in proximity to people who have not been instructed to use the rover (TODO). IMPORTANT: The rover must be used only during the day in good light conditions.","title":"Attendance (TODO: regulations?)"},{"location":"Rover/manual/#storage","text":"The rover should be kept in a covered and not too humid space when not in use.","title":"Storage"},{"location":"Rover/manual/#emergency-button","text":"The emergency button on the back of the rover can be used to cut the power to the motors and CNC at any time. To cut the power, push the red button. To power up the motors, the button must be reactivated. This can be done by pulling the button out again. CAUTION: Before reactivating the button, make sure that the CNC and wheel motors are not moving (TODO: how?\u2026)","title":"Emergency button"},{"location":"Rover/manual/#engagingdisengaging-the-motor-lock-levers-freewheeling-mode-and-drive-mode","text":"The two wheel motors each have a lock lever that allows them to switch between freewheeling mode or motor drive mode. When the lock lever is in the horizontal position the wheels are freewheeling. In freewheeling mode, the robot can be moved simply by pushing it. Turn the lever 90\u00b0 into the vertical position to switch the drive mode. In the drive mode, the wheels are powered by the motors and to move the rover you must use the remote control or the command interface. CAUTION: Only switch to the drive mode when the rover is \u201coff\u201d to assure that the motors are powered off. (TODO: is there no simpler way to power off the motors without shutting down the rover?) >>>>> gd2md-html alert: inline image link here (to images/image18.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> >>>>> gd2md-html alert: inline image link here (to images/image19.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> Figure 5: Lock lever horizontal : Freewheeling mode Lock lever vertical: Motor drive mode","title":"Engaging/disengaging the motor lock levers (freewheeling mode and drive mode)"},{"location":"Rover/manual/#the-control-panel","text":"The control panel provides a means to turn the rover on or off, and to view status messages. >>>>> gd2md-html alert: inline image link here (to images/image20.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> Figure 6: The control panel It has a display and five buttons, including the On/Off button. Please skip to the section \u201c Controlling the rover through the control panel \u201d for more information on how to use the control panel.","title":"The control panel"},{"location":"Rover/manual/#control-panel-state-message","text":"The display of the control panel is divided in two lines. The upper line shows current status of the rover: State display (1 st line) Description ... The control panel has been powered up and is initializing. Off The on-board computer is powered down and the motors are off. Starting up The on-board computer is starting up. The power to the motors is cut. On The rover is ready for use. The on-board computer is running and the motors can be powered up. Shutting down The power to the motors has been cut and the on-board computer is shutting down. Error The rover is in an error state.","title":"Control panel state message"},{"location":"Rover/manual/#start-up-procedure","text":"Before starting up, the rover should be in the following state: Verify that the emergency button is deactivated (pushed in). Verify that the rover is off (the control panel display shows \u201c** Off \u201d). If not \u2026see \u201cXXX\u201d Verify that the lock levers on the motor are disengaged to put the motors into freewheeling mode. The start-up can now proceed: Engage the lock levers on the motor to put the motors into drive mode. Activate the security button by pulling it out. Turn the rover on by holding the on/off button pressed for 5 seconds (see Fig. 1). At that point, the rover begins the start-up sequence and the display says \u201c** Starting up \u201d. When the start-up is completed, the display will show \u201c On \u201d. The motors of the wheels and the CNC are now powered up. The start-up is now finished. You can either do the following: If it is the first usage of the rover, you should go to the section \u201c First time configuration \u201d. You can use the remote control to move the rover (see \u201c Remote control mode and software control mode \u201d), or Use the control interface to send commands to the rover (see \u201c Controlling the rover through the control panel \u201d).","title":"Start-up procedure"},{"location":"Rover/manual/#shut-down-procedure","text":"To turn off the rover, press the On/Off button for 5 seconds. The display will briefly display the message \u201c** Shutting down \u201d, followed by the message \u201c Off \u201d.","title":"Shut-down procedure"},{"location":"Rover/manual/#switching-from-drive-to-freewheeling-mode","text":"When the rover is in drive mode (motors powered on, the lock levers on the motor engaged/vertical), it is possible to go to freewheeling mode as follows: Turn off the power of the motors by pressing the red emergency button. Turn the motor lock levers in the horizontal position (disengaged). Once these steps are completed, you can move the rover by pushing it.","title":"**Switching from drive to freewheeling mode **"},{"location":"Rover/manual/#switching-from-freewheeling-to-drive-mode","text":"To switch from freewheeling to drive mode, the rover\u2019s state should be \u201dOn\u201d. Then do the following operations: Assure that the motors are not moving by sending a STOP command (TODO). Turn the motor lock levers in the vertical position (engaged). Pull the red security button to power the motors. CAUTION: Make sure that the speed and direction controllers of the remote control are in the neutral position.","title":"**Switching from freewheeling to drive mode **"},{"location":"Rover/manual/#first-time-configuration-v2","text":"To configure the robot, you need a mobile phone, tablet, or computer with Wi-Fi capabilities, a recent web browser (see \u201c Supported web browsers \u201d) and a screen of minimum 320x240 (TEST) pixels. Connect your device to the \u201cRomi\u201d wireless network, using the password \u201crover\u201d. Once you are connected, open a web browser and navigate to the page https://romirover.local .","title":"First time configuration (**V2)**"},{"location":"Rover/manual/#the-configuration-page-of-the-rover-v2","text":">>>>> gd2md-html alert: inline image link here (to images/image21.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> Figure 2: The configuration page. This page will be shown on the first connection to the rover\u2019s web address, or when you select the \u201cConfiguration\u201d button in the main page on subsequent access to the rover\u2019s interface. The configuration page lets you: Change the password of interface Change the name of the rover Change the WiFi settings Change the settings for the Farmer\u2019s Dashboard.","title":"The configuration page of the rover (**V2)**"},{"location":"Rover/manual/#changing-the-password-of-the-interface-v2","text":"The first thing you should do is change the default password to something more secure but still easy to remember.","title":"Changing the password of the interface (**V2)**"},{"location":"Rover/manual/#changing-the-rovers-name-and-address-v2","text":"The name of the rover can remain unchanged unless you have several rovers. In that latter case, you should give each a distinct name in order to access the web interface of each. In the configuration page you can enter the following two strings: Short name: A short string that satisfies the following constraints: minimum 5 characters and maximum 32 characters long, only letters (a-z, A_Z), digits (0-9), and the underscore character (_) are allowed. The name should start with a letter. Name: A free-form name of maximum 64 characters. When the name of the rover is changed, the address of the web interface will change to https://NEWNAME.localhost . The value of NEWNAME must be replaced with the short name you have given to the rover. This change will be active after the rover has been turned off and then turned on again.","title":"Changing the rover\u2019s name and address (**V2)**"},{"location":"Rover/manual/#using-a-wi-fi-access-point-enabling-internet-access-v2","text":"To change the Wi-Fi configuration, open the web page https://romirover.local (TODO: does Raspian use Rendez-Vous/Avahi by default?) (or its new location, see \u201c Changing the rover\u2019s name and address \u201d). In the configuration page, check \u201cUse access point\u201d and enter the name of the Wi-Fi network (the SSID) and the password. In case the rover fails to connect to the access point, the configuration will revert to the default configuration that initializes the rover as a local access point. (TODO: verify/configure with Raspian, similar to Ubuntu)","title":"Using a Wi-Fi access point / Enabling Internet access (**V2)**"},{"location":"Rover/manual/#verifying-the-wi-fi-connection-v3","text":"If the Wi-Fi fails to connect, the control panel will display \u201cNo network\u201d. In that case, please verify the network name and password as in the previous section. This status message is not a problem and can be ignored if this happens occasionally, for example, when the rover is far away from the access point. As soon as the rover will be in proximity of the access point, the connection will be re-established. If the message continues to appear when the rover is in proximity of the access point and after the rover has been turned off and on again, then you should verify the Wi-Fi configuration.","title":"Verifying the Wi-Fi connection (**V3)**"},{"location":"Rover/manual/#connecting-a-phone-tablet-or-computer-to-the-rover","text":"Adapt the Wi-Fi settings of the device such that it connects to the same Wi-Fi network as the rover. The interface of the rover is accessible through a web browser. On the mobile device, open up your preferred web browser (see \u201c Supported web browsers \u201d) and in the address field enter the following URL: https://ROVER_SHORT_NAME.local . By default, the ROVER_SHORT_NAME is \u201cromirover\u201d. If you changed the name of the rover, you must use the new short name instead (see \u201c Changing the rover\u2019s name and address \u201d). To facilitate the access to the interface, you can add the address to your bookmarks.","title":"Connecting a phone, tablet, or computer to the rover"},{"location":"Rover/manual/#moving-the-rover-to-the-field","text":"The rover can be moved to the field either by simply pushing it with the motors in the freewheeling mode or by using the remote control to steer the rover. CAUTION: When you use the remote controller, you must stay close to the rover (less than 3m away?). TODO: regulations?","title":"Moving the rover to the field"},{"location":"Rover/manual/#positioning-the-rover-on-a-vegetable-bed","text":"Manually push the rover onto the beginning of the vegetable bed. Make sure that the front wheels (caster wheels) and the rear wheels slide nicely along the rails of the bed.","title":"Positioning the rover on a vegetable bed"},{"location":"Rover/manual/#remote-control-mode-and-software-control-mode","text":"The navigation of the rover can be controlled either by the remote control or by the rover\u2019s software. TODO: Must add an indicator to know what mode the rover is in. TODO: How do we switch from one to another mode?","title":"Remote control mode and software control mode"},{"location":"Rover/manual/#emergency-control-recovery-with-the-remote-control","text":"When the rover is in software control mode it is possible at any time to switch back to remote control mode by pushing the speed or direction controller to the maximum position. The rover will stop and remain immobilized for 3 seconds before listening to the commands of the remote control again.","title":"Emergency control recovery with the remote control"},{"location":"Rover/manual/#using-the-remote-control","text":"The rover comes with the Spektrum STX3 remote controller (RC). >>>>> gd2md-html alert: inline image link here (to images/image22.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> Figure 7: The remote control. The RC\u2019s steering wheel lets you define the direction in which the rover moves. The trigger lets you set the speed, both backward and forward (TODO: is this true with this controller?). Please check the Spektrum STX3 official user manual for detailed information on its use. The RC is powered by 4 AA batteries.","title":"Using the remote control"},{"location":"Rover/manual/#calibrating-the-cnc-to-camera-mapping-v2","text":"The weeding algorithm must be able to map the position of a pixel in the image to a XY coordinate of the CNC. The following steps Replace weeding tool with TODO","title":"**Calibrating the CNC-**to-Camera mapping (V2)"},{"location":"Rover/manual/#change-the-weeding-tool-head","text":"Pin","title":"Change the weeding tool head"},{"location":"Rover/manual/#controlling-the-rover-through-the-control-panel-v2","text":"You can send commands to the rover using the control panel as follows. >>>>> gd2md-html alert: inline image link here (to images/image23.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> MAIN SCREEN The rover must be in the On state. Then press the MENU button. >>>>> gd2md-html alert: inline image link here (to images/image24.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> MENU SCREEN The name of the first task will appear on the bottom line of the display. Use the UP and DOWN buttons to navigate in the list of possible tasks. To cancel and return to the main screen, press MENU. >>>>> gd2md-html alert: inline image link here (to images/image25.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> To start the task, press the SELECT button. To return to the menu screen, press MENU. >>>>> gd2md-html alert: inline image link here (to images/image26.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> CONFIRM SCREEN Press the SEL button a second time to confirm the action, or press MENU to cancel the start of the action and return to the menu screen. >>>>> gd2md-html alert: inline image link here (to images/image27.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> PROGRESS SCREEN If the action is confirmed, the display will show the progress status. When the task is finished, the display will return to the main screen.","title":"Controlling the rover through the control panel (V2)"},{"location":"Rover/manual/#controlling-the-rover-through-the-web-interface","text":"The web interface is a convenient way to send commands to the rover. The interface consists of large buttons to facilitate its use on a mobile phone in the field. The buttons and associated actions are programmable (see \u201c Editing the rover scripts \u201d). By default, a couple of generic buttons are provided (see Fig. 3). When you press a button, a confirmation will be asked to avoid launching an action inadvertently. Upon confirmation, a progress screen will be shown with information on the advancement of the action. >>>>> gd2md-html alert: inline image link here (to images/image28.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> Figure 8: The web interface. Left: The main screen. Centre: The confirmation screen. Right: The progress screen. The list of buttons and associated actions can be programmed. This is useful to adapt them to your needs. See \u201c Editing the rover buttons and actions\u201d for more information.","title":"Controlling the rover through the web interface"},{"location":"Rover/manual/#using-the-tool-carrier","text":"The tool carrier can pull classical weeding tools along the soil. It is best adapted for cultivars that are grown in dense lines, such as carrots. The weeding tools are not part of the Romi Rover package and must be purchased separately. The tool carrier must be attached to the main frame of the rover using the clamps that are welded on the carrier. Attach the power cable to the frame using the available clamps and plug it into the TOOL CARRIER socket on the control box of the rover. The tools must be fixed to one of the horizontal bars of the tool carrier using the dedicated clamps.","title":"Using the tool carrier"},{"location":"Rover/manual/#remote-maintenance-v3","text":"The rover automatically connects to the remote service provided by Dataplicity.","title":"Remote maintenance (V3)"},{"location":"Rover/manual/#remote-maintenance-v3_1","text":"The rover automatically connects to the remote service provided by Dataplicity.","title":"Remote maintenance (V3)"},{"location":"Rover/manual/#the-usb-memory-stick-v2","text":"The on-board computer stores all editable and generated data on a USB memory stick. When the rover is turned off, it is possible to remove the stick in the electronics box of the rover and connect it to another computer. This should normally not be necessary but can be convenient to make a backup of the recorded data or to edit the configuration files manually. The default organisation looks as follows. bin/ config/ +- config.json +- scripts.json +- wifi.json (TODO) database/ lib/ sessions/ +- 2020-08-15_08-47-51/ +- logs/ +- dumps/ +- 2020-08-16_08-53-03/ +- logs/ +- dumps/ The config directory contains several configuration files including: Filename Description config.json The main configuration file of the rover. It is not advised to change this file. scripts.json This file defines the list of buttons and their associated actions that are shown on the user interface (web interface and control panel). wifi.json The wifi set-up of the rover. (TODO) The database directory contains the images and analyses generated either by the weeding tasks or by the camera recorder. It is not advised to make any changes in this directory. The sessions directory contains the log and dump files that are used for the rover\u2019s maintenance. It is not advised to make any changes in this directory.","title":"The USB memory stick (**V2)**"},{"location":"Rover/manual/#manually-editing-the-configuration","text":"In some cases, it may be necessary or more convenient to edit the rover\u2019s configuration file directly instead of using the web interface (see \u201c The configuration page of the rover \u201d). TODO: continue","title":"Manually editing the configuration"},{"location":"Rover/manual/#editing-the-rover-buttons-and-actions","text":"Turn off the rover and open the box with the electronic components to recover the USB stick (see \u201cThe USB memory stick\u201d ). Open the file config/scripts.json on the memory stick using a plain text editor (On Windows, use Notepad, for example). The file uses the JSON format to describe the list of scripts and the associated sequences of actions. The general structure is as follows: [ { \"name\": \"move-forward\", \"display_name\": \"Forward\", \"script\": [ { \"action\": \"move\", \"distance\": 0.60 } ] }, { \"name\": \"move-backward\", \"display_name\": \"Backward\", \"script\": [ { \"action\": \"move\", \"distance\": -0.60 } ] }, { \"name\": \"scan\", \"display_name\": \"Scan\", \"script\": [ { \"action\": \"start_recording\" }, { \"action\": \"move\", \"distance\": 3.6 }, { \"action\": \"stop_recording\" } ] } ] The file contains a list of scripts. Each script has a name that is used to identify the script, a display_name that is shown in the user interface, and a script field that consists of a list of actions. The list of available actions and their parameters is out of the scope of this manual. Please refer to the online documentation at https://docs.romi-project.eu/Rover/configuration/ for details. If you make modifications to the file, it is very important that the new content is a valid JSON file. If not, the rover will fail to load the file and no buttons will be shown in the user interface.","title":"Editing the rover buttons and actions"},{"location":"Rover/manual/#farmers-dashboard-v3","text":"The Farmer\u2019s Dashboard is an online website to view and analyse images scans of the crops. The images collected by the rover can also be uploaded to the Farmer\u2019s Dashboard. The web site offers a means to browse the archive of images. The Farmer\u2019s Dashboard is a complementary service offered by the Romi Organisation. Since the software for the Farmer\u2019s Dashboard is Free and Open Source Software, it is possible to install it on your own server or to rely on a third party. For more information, please visit the Farmer\u2019s Dashboard information page at XXX.","title":"Farmer\u2019s Dashboard (V3)"},{"location":"Rover/manual/#automatically-uploading-the-images-to-the-farmers-dashboard-v3","text":"You must first create an account on the Farmer\u2019s Dashboard web site. Next, in the account settings, click the \u201cAPI key\u201d tab. You must then enter the name of the web site and the API key in the configuration page of the rover (see \u201c The configuration page of the rover \u201d).","title":"Automatically uploading the images to the Farmer\u2019s Dashboard (**V3)**"},{"location":"Rover/manual/#manually-uploading-the-images-to-the-farmers-dashboard-v3","text":"It is possible to manually upload the images to the Farmer\u2019s Dashboard web site. First, turn off the rover. Then recover the USB memory stick of the on-board computer of the rover. Put the memory stick in a PC. Finally, follow the upload instructions on the Farmer\u2019s Dashboard web site. CAUTION: Make sure to put the USB memory stick back into the rover. If not, the rover may fail to function properly upon its next use.","title":"Manually uploading the images to the Farmer\u2019s Dashboard (**V3)**"},{"location":"Rover/manual/#supported-web-browsers","text":"The interface of the rover should be viewable in any modern, compliant HTML5-compliant web browser with support for ECMAScript 5 (Javascript), XMLHttpRequests, and WebSockets. The following versions and more recent versions of commonly used browsers should work with the rover (TODO: verify!): Browser name Version Release date Chrome 23 Sep 2012 Firefox 21 Apr 2013 IE / Edge 10 Sep 2012 iOS 6 Safari 6 Jul 2012 Opera 15 Jul 2013 \\ The Farmer\u2019s Dashboard web site that is used to browse archived images has its own requirements that are documented on the Farmer\u2019s Dashboard web site.","title":"Supported web browsers"},{"location":"Rover/manual/#installing-a-new-sd-card","text":"The rover uses a Raspberry Pi as its main computer. The control software of the rover is installed on an SD card that sits in the card slot of the Raspberry Pi 4. You can replace the SD card with a new one, following steps below: TODO","title":"Installing a new SD card"},{"location":"Rover/software/","text":"Software Installation Link Overview Link This document describes how to run and compile the software for the ROMI Rover. If you are a developer looking for details on the source code then have a look at the separate Developer Documentation . Prerequisites Link The software of the rover runs on Linux. It is not tied to a specific Linux distribution but we have tested it mostly on recent versions of Debian (includin Raspian) and Ubuntu. The software is mostly writen in C and depends on the following libraries: libr : Common code for the rcom and the libromi libraries. It provides some OS abstraction (for example for threads, memory allocation, file system, networking), some core functionality (logging, time), and some base classes (variable-size memory buffers, json parser, lists, serial connections). Code rcom : An inter-process communication framework. It provides real-time communication using UDP messages and high-level communication based on web protocols (HTTP, Websockets). It also includes several utilities to develop and manage rcom applications. Code libromi : Base classes for the romi rover: fsdb (database with filesystem back-end), image loading and manipulations, \u2026) Code romi-brush-motor-controller : The motor controller. Code romi-rover : All of the apps for the Romi rover. Code By default, the rover uses a USB camera. It is possible to use the Intel Realsense camera on the Picamera instead. In that case, you will have to install additional libraries (see XXX). Installing a Raspberry Pi from scratch Link We use the Lite version of Raspbian. You can download it at https://www.raspberrypi.org/downloads/raspbian/ . There are several ways to prepare the disk image for the RPi. Check the page at https://www.raspberrypi.org/documentation/installation/installing-images/ (there\u2019s lots of information available on this topic online) and follow the instructions that suit you best. Once you have the SD card, connect RPi to screen, keyboard and network (ethernet), power up the board and log in (user pi , password raspberry ). The first thing you want to do is change some of the default settings using the raspi-config tool. In the console type: $ sudo raspi-config The list of settings that you may want to look at includes: 1 Change User Password 2 Network Options Hostname WiFi 4 Localisation Options Change locales Change keyboard layout 5 Interfacing Options Enable SSH 8 Update Next, create the user \u2018romi\u2019: $ sudo adduser romi $ sudo adduser romi dialout $ sudo adduser romi video $ sudo adduser romi sudo After that, quit the current session and login again as user \u2018romi\u2019. The nano text editor is installed by default but if you prefer anoher editor, now is a good time to install it: $ sudo apt install emacs-nox ( ... or any editor you like : ) Install the developer tools: $ sudo apt install build-essential cmake git Install the software dependencies: $ sudo apt install libpng-dev libjpeg9-dev That's it. You should be ready. Quick install #!/bin/bash # Install the dependencies sudo apt install build-essential cmake git libpng-dev libjpeg9-dev # Download, compile and install the libraries & apps for id in libr rcom libromi romi-rover ; do echo ---------------------------------------------- echo Compiling $id # Download or update the github repository if [ -d $id ] ; then cd $id git pull else git clone https://github.com/romi/ $id .git cd $id fi # Standard cmake build sequence mkdir -p build cd build cmake .. make sudo make install sudo ldconfig # Get ready for the next component cd ../.. done Installing the romi-rover apps Link You should first install the libr , rcom , and libromi libraries. Check out their the Github pages for the installation instruction and the API documentation. You should also flash the motor controller to the Arduino (instructions are available on Github ) too. Once that is done, the installation of the romi-rover apps is straight-forward. First, check out the code: $ git clone https://github.com/romi/romi-rover.git Then proceed to the compilation and installation: $ cd romi-rover $ mkdir build $ cd build $ cmake .. $ make $ sudo make install Compiling the picamera app Link Although not currently used by the ROMI Rover, we have an Rcom app to access the Picamera. To get it working, you will first have to install the raspicam library: $ git clone https://github.com/cedricve/raspicam.git $ cd raspicam/ $ mkdir build $ cd build/ $ cmake .. $ make $ sudo make install $ sudo ldconfig Once raspicam is installed, you must re-run cmake to enable the compilation of the picamera app: $ cd romi-rover/build/ $ rm CMakeCache.txt $ cmake .. -DWITH_PICAMERA = ON $ make $ sudo make install Compiling the realsense app Link The fonctionality of the Realsense camera app is not complete. You can use it to obtain RGB images and depth images (as BW PNG images). You will first have to install librealsense2 . When librealsense is installed, re-run cmake to enable the compilation of the realsense app: $ cd romi-rover/build/ $ rm CMakeCache.txt $ cmake .. -DWITH_REALSENSE = ON $ make $ sudo make install Configuration Link Configuring the romi-rover apps Link In the directory /home/romi, create the following directories and copy the default configuration and script files: $ cd /home/romi $ mkdir sessions $ mkdir config $ cp <romi-rover>/config/config-romi-rover.json config/ $ mkdir scripts $ cp <romi-rover>/script/config-default.json scripts/ \"html\": \"<romi-rover>/interface/html\", Starting the apps on boot Link Currently we are still using the old rc.local mechanism. The file /etc/rc.local is no longer included in more recent Ubuntu versions. If ls /etc/rc.local returns an error, you will have to create the file as follows: $ sudo nano /etc/rc.local Copy the following contents: #!/bin/sh -e # # rc.local # # This script is executed at the end of each multiuser runlevel. # Make sure that the script will \"exit 0\" on success or any other # value on error. # # In order to enable or disable this script just change the execution # bits. # # By default this script does nothing. exit 0 Finally, make the script executable. $ sudo chmod +x /etc/rc.local To enable the apps on start-up, add the following line in /etc/rc.local, above the exit 0 line: /usr/local/bin/rclaunch /home/romi/config/config-romi-rover.json & Configuring the image uploads Link Annex: the apps and their options Link \"fake_camera\": { \"image\": \"data/camera.jpg\" },","title":"Software"},{"location":"Rover/software/#software-installation","text":"","title":"Software Installation"},{"location":"Rover/software/#overview","text":"This document describes how to run and compile the software for the ROMI Rover. If you are a developer looking for details on the source code then have a look at the separate Developer Documentation .","title":"Overview"},{"location":"Rover/software/#prerequisites","text":"The software of the rover runs on Linux. It is not tied to a specific Linux distribution but we have tested it mostly on recent versions of Debian (includin Raspian) and Ubuntu. The software is mostly writen in C and depends on the following libraries: libr : Common code for the rcom and the libromi libraries. It provides some OS abstraction (for example for threads, memory allocation, file system, networking), some core functionality (logging, time), and some base classes (variable-size memory buffers, json parser, lists, serial connections). Code rcom : An inter-process communication framework. It provides real-time communication using UDP messages and high-level communication based on web protocols (HTTP, Websockets). It also includes several utilities to develop and manage rcom applications. Code libromi : Base classes for the romi rover: fsdb (database with filesystem back-end), image loading and manipulations, \u2026) Code romi-brush-motor-controller : The motor controller. Code romi-rover : All of the apps for the Romi rover. Code By default, the rover uses a USB camera. It is possible to use the Intel Realsense camera on the Picamera instead. In that case, you will have to install additional libraries (see XXX).","title":"Prerequisites"},{"location":"Rover/software/#installing-a-raspberry-pi-from-scratch","text":"We use the Lite version of Raspbian. You can download it at https://www.raspberrypi.org/downloads/raspbian/ . There are several ways to prepare the disk image for the RPi. Check the page at https://www.raspberrypi.org/documentation/installation/installing-images/ (there\u2019s lots of information available on this topic online) and follow the instructions that suit you best. Once you have the SD card, connect RPi to screen, keyboard and network (ethernet), power up the board and log in (user pi , password raspberry ). The first thing you want to do is change some of the default settings using the raspi-config tool. In the console type: $ sudo raspi-config The list of settings that you may want to look at includes: 1 Change User Password 2 Network Options Hostname WiFi 4 Localisation Options Change locales Change keyboard layout 5 Interfacing Options Enable SSH 8 Update Next, create the user \u2018romi\u2019: $ sudo adduser romi $ sudo adduser romi dialout $ sudo adduser romi video $ sudo adduser romi sudo After that, quit the current session and login again as user \u2018romi\u2019. The nano text editor is installed by default but if you prefer anoher editor, now is a good time to install it: $ sudo apt install emacs-nox ( ... or any editor you like : ) Install the developer tools: $ sudo apt install build-essential cmake git Install the software dependencies: $ sudo apt install libpng-dev libjpeg9-dev That's it. You should be ready. Quick install #!/bin/bash # Install the dependencies sudo apt install build-essential cmake git libpng-dev libjpeg9-dev # Download, compile and install the libraries & apps for id in libr rcom libromi romi-rover ; do echo ---------------------------------------------- echo Compiling $id # Download or update the github repository if [ -d $id ] ; then cd $id git pull else git clone https://github.com/romi/ $id .git cd $id fi # Standard cmake build sequence mkdir -p build cd build cmake .. make sudo make install sudo ldconfig # Get ready for the next component cd ../.. done","title":"Installing a Raspberry Pi from scratch"},{"location":"Rover/software/#installing-the-romi-rover-apps","text":"You should first install the libr , rcom , and libromi libraries. Check out their the Github pages for the installation instruction and the API documentation. You should also flash the motor controller to the Arduino (instructions are available on Github ) too. Once that is done, the installation of the romi-rover apps is straight-forward. First, check out the code: $ git clone https://github.com/romi/romi-rover.git Then proceed to the compilation and installation: $ cd romi-rover $ mkdir build $ cd build $ cmake .. $ make $ sudo make install","title":"Installing the romi-rover apps"},{"location":"Rover/software/#compiling-the-picamera-app","text":"Although not currently used by the ROMI Rover, we have an Rcom app to access the Picamera. To get it working, you will first have to install the raspicam library: $ git clone https://github.com/cedricve/raspicam.git $ cd raspicam/ $ mkdir build $ cd build/ $ cmake .. $ make $ sudo make install $ sudo ldconfig Once raspicam is installed, you must re-run cmake to enable the compilation of the picamera app: $ cd romi-rover/build/ $ rm CMakeCache.txt $ cmake .. -DWITH_PICAMERA = ON $ make $ sudo make install","title":"Compiling the picamera app"},{"location":"Rover/software/#compiling-the-realsense-app","text":"The fonctionality of the Realsense camera app is not complete. You can use it to obtain RGB images and depth images (as BW PNG images). You will first have to install librealsense2 . When librealsense is installed, re-run cmake to enable the compilation of the realsense app: $ cd romi-rover/build/ $ rm CMakeCache.txt $ cmake .. -DWITH_REALSENSE = ON $ make $ sudo make install","title":"Compiling the realsense app"},{"location":"Rover/software/#configuration","text":"","title":"Configuration"},{"location":"Rover/software/#configuring-the-romi-rover-apps","text":"In the directory /home/romi, create the following directories and copy the default configuration and script files: $ cd /home/romi $ mkdir sessions $ mkdir config $ cp <romi-rover>/config/config-romi-rover.json config/ $ mkdir scripts $ cp <romi-rover>/script/config-default.json scripts/ \"html\": \"<romi-rover>/interface/html\",","title":"Configuring the romi-rover apps"},{"location":"Rover/software/#starting-the-apps-on-boot","text":"Currently we are still using the old rc.local mechanism. The file /etc/rc.local is no longer included in more recent Ubuntu versions. If ls /etc/rc.local returns an error, you will have to create the file as follows: $ sudo nano /etc/rc.local Copy the following contents: #!/bin/sh -e # # rc.local # # This script is executed at the end of each multiuser runlevel. # Make sure that the script will \"exit 0\" on success or any other # value on error. # # In order to enable or disable this script just change the execution # bits. # # By default this script does nothing. exit 0 Finally, make the script executable. $ sudo chmod +x /etc/rc.local To enable the apps on start-up, add the following line in /etc/rc.local, above the exit 0 line: /usr/local/bin/rclaunch /home/romi/config/config-romi-rover.json &","title":"Starting the apps on boot"},{"location":"Rover/software/#configuring-the-image-uploads","text":"","title":"Configuring the image uploads"},{"location":"Rover/software/#annex-the-apps-and-their-options","text":"\"fake_camera\": { \"image\": \"data/camera.jpg\" },","title":"Annex: the apps and their options"},{"location":"Rover/UserManual/","text":"Romi Rover User Manual Draft - September 2020 Contents Link This manual describes the usage of a fully assembled and ready to use rover. The information on how to build your own rover can be found in a separate document. Visit the ROMI web site at https://docs.romi-project.eu for more information. This manual consists of the following chapters: Copyright Legend Short Description Technical Specifications Functional Specifications and Requirements Operating Instructions","title":"Index"},{"location":"Rover/UserManual/#contents","text":"This manual describes the usage of a fully assembled and ready to use rover. The information on how to build your own rover can be found in a separate document. Visit the ROMI web site at https://docs.romi-project.eu for more information. This manual consists of the following chapters: Copyright Legend Short Description Technical Specifications Functional Specifications and Requirements Operating Instructions","title":"Contents"},{"location":"Rover/UserManual/copyright/","text":"Copyright Link Copyright \u00a9 Sony Computer Science Laboratories License Link The documentation is available under the CC BY-SA 4.0 License Acknowledgements Link ROMI has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 773875. Credits Link TODO Disclaimers Link TODO","title":"Copyright"},{"location":"Rover/UserManual/copyright/#copyright","text":"Copyright \u00a9 Sony Computer Science Laboratories","title":"Copyright"},{"location":"Rover/UserManual/copyright/#license","text":"The documentation is available under the CC BY-SA 4.0 License","title":"License"},{"location":"Rover/UserManual/copyright/#acknowledgements","text":"ROMI has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 773875.","title":"Acknowledgements"},{"location":"Rover/UserManual/copyright/#credits","text":"TODO","title":"Credits"},{"location":"Rover/UserManual/copyright/#disclaimers","text":"TODO","title":"Disclaimers"},{"location":"Rover/UserManual/description/","text":"Short Description Link The ROMI Rover is a farming tool that assists vegetable farmers in maintaining the vegetable beds free from weeds. It does this by regularly hoeing the surface of the soil and thus preventing small weeds from taking roots. It can do this task mostly autonomously and requires only minor changes to the organization of the farm. It is designed for vegetable beds between 70 cm and 120 cm wide (not including the passage ways) and currently handles two types of crops, lettuce and carrots. The lettuce can be planted out in any layout, most likely in a quincunx pattern. In this configuration the rover uses a precision rotary hoe to clean the soil both between the rows and the plants. For the carrots, the rover uses classical mechanical tools, such as stirrup hoe, to regularly clean the soil in between the rows. In this configuration, the carrots should be sown in line. A weekly passage of the robot should be sufficient to keep the population of weeds under control. In addition to the weeding, the rover provides the following useful functions. It can draw quincunx patterns or straight lines in empty vegetable beds to speed up seeding and planting out. The embedded camera can be used to collect images of the vegetable beds. It can also be used as a motorized tray. The ROMI Rover is targeted at farms that grow lettuce and carrots on a relatively small surface, between 200 m\u00b2 and 5 ha (Utilized Agricultural Area).","title":"Description"},{"location":"Rover/UserManual/description/#short-description","text":"The ROMI Rover is a farming tool that assists vegetable farmers in maintaining the vegetable beds free from weeds. It does this by regularly hoeing the surface of the soil and thus preventing small weeds from taking roots. It can do this task mostly autonomously and requires only minor changes to the organization of the farm. It is designed for vegetable beds between 70 cm and 120 cm wide (not including the passage ways) and currently handles two types of crops, lettuce and carrots. The lettuce can be planted out in any layout, most likely in a quincunx pattern. In this configuration the rover uses a precision rotary hoe to clean the soil both between the rows and the plants. For the carrots, the rover uses classical mechanical tools, such as stirrup hoe, to regularly clean the soil in between the rows. In this configuration, the carrots should be sown in line. A weekly passage of the robot should be sufficient to keep the population of weeds under control. In addition to the weeding, the rover provides the following useful functions. It can draw quincunx patterns or straight lines in empty vegetable beds to speed up seeding and planting out. The embedded camera can be used to collect images of the vegetable beds. It can also be used as a motorized tray. The ROMI Rover is targeted at farms that grow lettuce and carrots on a relatively small surface, between 200 m\u00b2 and 5 ha (Utilized Agricultural Area).","title":"Short Description"},{"location":"Rover/UserManual/legend/","text":"Legend Link The draft of the manual uses the following labels to indicate the status of the development: [v2] The feature refers to the second prototype and its implementation will be different in the third prototype. [v3] The feature refers to the third prototype and exists in a different form in the second prototype. [v2++] The feature will be implemented in the second prototype (deadline: December 2020) [v3++] The feature will be implemented in the third prototype (deadline: May 2021).","title":"Legend"},{"location":"Rover/UserManual/legend/#legend","text":"The draft of the manual uses the following labels to indicate the status of the development: [v2] The feature refers to the second prototype and its implementation will be different in the third prototype. [v3] The feature refers to the third prototype and exists in a different form in the second prototype. [v2++] The feature will be implemented in the second prototype (deadline: December 2020) [v3++] The feature will be implemented in the third prototype (deadline: May 2021).","title":"Legend"},{"location":"Rover/UserManual/requirements/","text":"Functional Specifications and Requirements Link The following configuration is required for the use of the ROMI rover. Profile Price Features/Function Bed of vegetables Width of the bed: min 0.7 m max 1.2 m [v3] Handled crops Lettuce : The lettuce can be planted out in any layout, most likely in a quincunx pattern Carrots : The carrots should be sown in line. ROMI rover with a remote control, a battery charger, and a protective cover [v3] 5000 \u20ac (estimate) Prevents weed development Draw patterns for seeding and planting out Takes image scans of the beds Tool carrier Photo 1000 \u20ac (estimate) Used with the rover to carry the mechanical weeding tools (optional but needed in most configurations) Mechanical weeding tools (from Terrateck or other manufacturers) 400-1000\u20ac (estimate) The weeding tools that will be fixed on the tool carrier. Sold by other manufacturers, for example, by [Terrateck](https://www.terrateck.com) ([catalogue](https://www.terrateck.com/en/16-houes-pousse-pousse)) TOTAL cost of an equipped rover 6400 \u2013 7000 \u20ac Guides Photo Stainless steel tubes (45x1.5): 3 \u20ac/m Tuyau irrigation PE HD 50mm : 4.4 \u20ac/m The guides consist of either stainless metal tubes (preferred) or wooden boards. They facilitate the navigation of the rover along the bed so that it can operate accurately and reliably. A mobile phone Existing phone: 0\u20ac Dedicated phone: 200 \u20ac Used to control the rover To browse the archived images of the online service WiFi Rover as hotspot: 0 \u20ac/month Existing WiFi: 0 \u20ac/month GSM WiFi router: 120\u20ac (router) + 10 \u20ac/month (SIM card) WiFi connectivity in the field To connect the mobile phone to the rover (required) To archives the images taken by the rover (optional) To provide remote assistance (optional) Romi's Online Farmer's Dashboard [v3++] 1 \u20ac/month Monthly fee for the use of the Romi remote server (optional): To archive and browse the images taken by the rover To receive remote assistance Training 70 \u20ac/day Two days of training for the rover (optional) Preprogramming according to the configuration of the farm 50 \u20ac Assistance to fine-tune the rover\u2019s configuration to the farm (optional) Maintenance costs 5 \u20ac/month/rover Maintenance contract with the ROMI association (or a local distributor) (optional) Examples for different farm sizes Link For an urban farm of 5 000 m2 (lettuces or carrots) 83 beds of 50 meters (beds of 80 cm wide, pathway of 40 cm between beds, 3320 m\u00b2 cultivated) 4150 meters of guides (12 450 \u20ac\u2026) 1 rover with tool carrier and mechanical weeding tools (7000 \u20ac) Investment costs: 7000 to 7510 + 12450 \u20ac + 16 \u20ac/months (optional: SIM card, remote maintenance, server archiving) For a vegetable farm of 2 ha 124 beds of 100 meters (beds of 1.1 m wide, pathway of 0.50 m between beds, 13640 m\u00b2 cultivated) 12 400 meters of guides (37.2 k\u20ac) Lettuces (180 000 plants, 54 k\u20ac-129 k\u20ac revenu): 4 rovers Investment costs: 20.5 k\u20ac + 37.2 k\u20ac + 10 to 31 \u20ac/months (wifi and maintenance) Alternative solution: Plastic mulching film: 12 x 226,50 \u20ac = 2712 \u20ac/year [ REF ] Carrots (1.24M plants, 60-130 tonnes, 31k\u20ac-130k\u20ac revenue) 1 rover Investment costs: 7000 to 7510 \u20ac + 49.6 k\u20ac + 10 to 16 \u20ac/months (wifi and maintenance) Sur carottes, l\u2019ensemble des op\u00e9rations de d\u00e9sherbage varie de 120 \u00e0 plus de 900 heures/ha. [ http://itab.asso.fr/downloads/fiches-lpc/lpc-carotte.pdf ] Prix [ https://rnm.franceagrimer.fr/prix?CAROTTE&12MOIS ] For a vegetable farm of 5 ha XXX beds XXX meters of guides XXX rovers Investment costs: XXX \u20ac + \u20ac/months (wifi and maintenance) It has to be pointed out that in many countries, there are public financial supports for farmers which want to invest in robotics.","title":"Functional Specifications and Requirements"},{"location":"Rover/UserManual/requirements/#functional-specifications-and-requirements","text":"The following configuration is required for the use of the ROMI rover. Profile Price Features/Function Bed of vegetables Width of the bed: min 0.7 m max 1.2 m [v3] Handled crops Lettuce : The lettuce can be planted out in any layout, most likely in a quincunx pattern Carrots : The carrots should be sown in line. ROMI rover with a remote control, a battery charger, and a protective cover [v3] 5000 \u20ac (estimate) Prevents weed development Draw patterns for seeding and planting out Takes image scans of the beds Tool carrier Photo 1000 \u20ac (estimate) Used with the rover to carry the mechanical weeding tools (optional but needed in most configurations) Mechanical weeding tools (from Terrateck or other manufacturers) 400-1000\u20ac (estimate) The weeding tools that will be fixed on the tool carrier. Sold by other manufacturers, for example, by [Terrateck](https://www.terrateck.com) ([catalogue](https://www.terrateck.com/en/16-houes-pousse-pousse)) TOTAL cost of an equipped rover 6400 \u2013 7000 \u20ac Guides Photo Stainless steel tubes (45x1.5): 3 \u20ac/m Tuyau irrigation PE HD 50mm : 4.4 \u20ac/m The guides consist of either stainless metal tubes (preferred) or wooden boards. They facilitate the navigation of the rover along the bed so that it can operate accurately and reliably. A mobile phone Existing phone: 0\u20ac Dedicated phone: 200 \u20ac Used to control the rover To browse the archived images of the online service WiFi Rover as hotspot: 0 \u20ac/month Existing WiFi: 0 \u20ac/month GSM WiFi router: 120\u20ac (router) + 10 \u20ac/month (SIM card) WiFi connectivity in the field To connect the mobile phone to the rover (required) To archives the images taken by the rover (optional) To provide remote assistance (optional) Romi's Online Farmer's Dashboard [v3++] 1 \u20ac/month Monthly fee for the use of the Romi remote server (optional): To archive and browse the images taken by the rover To receive remote assistance Training 70 \u20ac/day Two days of training for the rover (optional) Preprogramming according to the configuration of the farm 50 \u20ac Assistance to fine-tune the rover\u2019s configuration to the farm (optional) Maintenance costs 5 \u20ac/month/rover Maintenance contract with the ROMI association (or a local distributor) (optional)","title":"Functional Specifications and Requirements"},{"location":"Rover/UserManual/requirements/#examples-for-different-farm-sizes","text":"For an urban farm of 5 000 m2 (lettuces or carrots) 83 beds of 50 meters (beds of 80 cm wide, pathway of 40 cm between beds, 3320 m\u00b2 cultivated) 4150 meters of guides (12 450 \u20ac\u2026) 1 rover with tool carrier and mechanical weeding tools (7000 \u20ac) Investment costs: 7000 to 7510 + 12450 \u20ac + 16 \u20ac/months (optional: SIM card, remote maintenance, server archiving) For a vegetable farm of 2 ha 124 beds of 100 meters (beds of 1.1 m wide, pathway of 0.50 m between beds, 13640 m\u00b2 cultivated) 12 400 meters of guides (37.2 k\u20ac) Lettuces (180 000 plants, 54 k\u20ac-129 k\u20ac revenu): 4 rovers Investment costs: 20.5 k\u20ac + 37.2 k\u20ac + 10 to 31 \u20ac/months (wifi and maintenance) Alternative solution: Plastic mulching film: 12 x 226,50 \u20ac = 2712 \u20ac/year [ REF ] Carrots (1.24M plants, 60-130 tonnes, 31k\u20ac-130k\u20ac revenue) 1 rover Investment costs: 7000 to 7510 \u20ac + 49.6 k\u20ac + 10 to 16 \u20ac/months (wifi and maintenance) Sur carottes, l\u2019ensemble des op\u00e9rations de d\u00e9sherbage varie de 120 \u00e0 plus de 900 heures/ha. [ http://itab.asso.fr/downloads/fiches-lpc/lpc-carotte.pdf ] Prix [ https://rnm.franceagrimer.fr/prix?CAROTTE&12MOIS ] For a vegetable farm of 5 ha XXX beds XXX meters of guides XXX rovers Investment costs: XXX \u20ac + \u20ac/months (wifi and maintenance) It has to be pointed out that in many countries, there are public financial supports for farmers which want to invest in robotics.","title":"Examples for different farm sizes"},{"location":"Rover/UserManual/specifications/","text":"Technical Specifications Link Feature Value Size [v3] (Width x Length x Height) Min.: 1.45 m x 1.60 m x 1.46 m Max.: 1.70 m x 1.60 m x 1.46 m Weight [v2] 80 kg (estimate) Battery life [v3++] 8 h Charging time TODO Weeding speed Precision weeding: 600 m\u00b2/day [v3++] (235 m\u00b2/day [v2]) Classical weeding 7200 m\u00b2/day [v3] (6400 m\u00b2/day [v2]) Width vegetable beds [v3] Min.: 0.70 m Max.: 1.2 m Handled crops Precision weeding: Lettuce Classical weeding: Carrots Turning space at end of bed 2 m (TODO: verify)","title":"Technical Specifications"},{"location":"Rover/UserManual/specifications/#technical-specifications","text":"Feature Value Size [v3] (Width x Length x Height) Min.: 1.45 m x 1.60 m x 1.46 m Max.: 1.70 m x 1.60 m x 1.46 m Weight [v2] 80 kg (estimate) Battery life [v3++] 8 h Charging time TODO Weeding speed Precision weeding: 600 m\u00b2/day [v3++] (235 m\u00b2/day [v2]) Classical weeding 7200 m\u00b2/day [v3] (6400 m\u00b2/day [v2]) Width vegetable beds [v3] Min.: 0.70 m Max.: 1.2 m Handled crops Precision weeding: Lettuce Classical weeding: Carrots Turning space at end of bed 2 m (TODO: verify)","title":"Technical Specifications"},{"location":"Scanner/","text":"Plant Phenotyping Link Within the ROMI project, some work packages are oriented towards the development of a 3D plant phenotyping platform adapted to single potted plants. To achieve this goal, the team developed a suite of affordable open-source tools (hardware & software) presented hereafter. We aim at making our software architecture modular to ensure the required flexibility and adaptability to most of the robotic & research applications from the ROMI project when possible. Todo simplify module interaction overview Modules Link PlantDB Plant 3D Explorer Plant Imager Plant 3D Vision Usage Link Tutorials","title":"Home"},{"location":"Scanner/#plant-phenotyping","text":"Within the ROMI project, some work packages are oriented towards the development of a 3D plant phenotyping platform adapted to single potted plants. To achieve this goal, the team developed a suite of affordable open-source tools (hardware & software) presented hereafter. We aim at making our software architecture modular to ensure the required flexibility and adaptability to most of the robotic & research applications from the ROMI project when possible. Todo simplify module interaction overview","title":"Plant Phenotyping"},{"location":"Scanner/#modules","text":"PlantDB Plant 3D Explorer Plant Imager Plant 3D Vision","title":"Modules"},{"location":"Scanner/#usage","text":"Tutorials","title":"Usage"},{"location":"Scanner/build/","text":"Build instructions for the ROMI plant scanner Link Build overview Link You will have to achieve the following steps: build the aluminium frame; build the CNC; attach the CNC the to aluminium frame; choose a camera & build the camera mount; wiring it all. Open Hardware Link We use \"widely available\" materials and provides all the required information, so you can reproduce and modify our work. If you want to modify it, please note the following important points: The width and depth of the aluminium frame is dependent of the CNC frame size, make sure the CNC frame fits inside the aluminium frame! The height of the aluminium frame will determine the maximum observable plant height. Wiring & communication overview Link The plant imager electronics can be divided in two functional groups: the \"CNC group\", responsible for moving the camera around; the \"Gimbal & Camera group\", allowing to rotate the camera around the z-axis to face the object (plant) to acquire. Note that the CNC group has 3 axis movement (X, Y & Z), the gimbal add a forth one. Overview of the plant imager's electronics. Note that the PiCamera act as a wifi hotspot to which the controller computer is registered. The two groups communicate with the main controller by USB. Note that since the gimbal & camera are located at the end of the arm on the z-axis, the USB cable powering & controlling them are going through the cable rails with those dedicated to the axis motors. This may create interference, so you will need a (long) properly shielded USB cable!","title":"Home"},{"location":"Scanner/build/#build-instructions-for-the-romi-plant-scanner","text":"","title":"Build instructions for the ROMI plant scanner"},{"location":"Scanner/build/#build-overview","text":"You will have to achieve the following steps: build the aluminium frame; build the CNC; attach the CNC the to aluminium frame; choose a camera & build the camera mount; wiring it all.","title":"Build overview"},{"location":"Scanner/build/#open-hardware","text":"We use \"widely available\" materials and provides all the required information, so you can reproduce and modify our work. If you want to modify it, please note the following important points: The width and depth of the aluminium frame is dependent of the CNC frame size, make sure the CNC frame fits inside the aluminium frame! The height of the aluminium frame will determine the maximum observable plant height.","title":"Open Hardware"},{"location":"Scanner/build/#wiring-communication-overview","text":"The plant imager electronics can be divided in two functional groups: the \"CNC group\", responsible for moving the camera around; the \"Gimbal & Camera group\", allowing to rotate the camera around the z-axis to face the object (plant) to acquire. Note that the CNC group has 3 axis movement (X, Y & Z), the gimbal add a forth one. Overview of the plant imager's electronics. Note that the PiCamera act as a wifi hotspot to which the controller computer is registered. The two groups communicate with the main controller by USB. Note that since the gimbal & camera are located at the end of the arm on the z-axis, the USB cable powering & controlling them are going through the cable rails with those dedicated to the axis motors. This may create interference, so you will need a (long) properly shielded USB cable!","title":"Wiring &amp; communication overview"},{"location":"Scanner/build/alu_frame/","text":"Building the aluminium frame Link Important All units are in millimeters! BOM Link To build the aluminium frame you will need: 4x 1800mm 3030 profiles (A) 4x 1200mm 3030 profiles (B) 4x 1007mm 3030 profiles (C) 16x brackets (30x60) for 3030 profiles 64x M6 T-nuts 64x M6x12 screws Note that: Parts A defines the height of the scanner; Parts B & C depends on the external size of the chosen CNC frame! Build instructions Link Start by building the smallest sides of the frame using two A parts & two C parts per side. To ease the build, lay them flat on the floor. Prepare all A parts with their brackets at the right position: Positioning the 30x60 brackets on the 1800mm 3030 profiles (A). Add two C parts per side to construct the \"small sides\". Make sure they are horizontal. Then add the two lower B parts & finally the two upper B parts. Assembled aluminium frame schematic & isometric view. Optional Link Add wood corner brackets to rigidify the structure. Assembled aluminium frame with wood corners schematic. Isometric view of assembled aluminium frame with wood corners.","title":"Aluminium frame"},{"location":"Scanner/build/alu_frame/#building-the-aluminium-frame","text":"Important All units are in millimeters!","title":"Building the aluminium frame"},{"location":"Scanner/build/alu_frame/#bom","text":"To build the aluminium frame you will need: 4x 1800mm 3030 profiles (A) 4x 1200mm 3030 profiles (B) 4x 1007mm 3030 profiles (C) 16x brackets (30x60) for 3030 profiles 64x M6 T-nuts 64x M6x12 screws Note that: Parts A defines the height of the scanner; Parts B & C depends on the external size of the chosen CNC frame!","title":"BOM"},{"location":"Scanner/build/alu_frame/#build-instructions","text":"Start by building the smallest sides of the frame using two A parts & two C parts per side. To ease the build, lay them flat on the floor. Prepare all A parts with their brackets at the right position: Positioning the 30x60 brackets on the 1800mm 3030 profiles (A). Add two C parts per side to construct the \"small sides\". Make sure they are horizontal. Then add the two lower B parts & finally the two upper B parts. Assembled aluminium frame schematic & isometric view.","title":"Build instructions"},{"location":"Scanner/build/alu_frame/#optional","text":"Add wood corner brackets to rigidify the structure. Assembled aluminium frame with wood corners schematic. Isometric view of assembled aluminium frame with wood corners.","title":"Optional"},{"location":"Scanner/build/cnc_calibration/","text":"CNC setup & calibration Link After building & wiring the CNC and fitting it into the aluminium frame it is required to calibrate some Grbl software parameters like the homing directions or the acceleration rates . Using Grbl Link Except if you are familiar with Grbl, if you want to know more, have a look at the official Grbl wiki . Connect to the Arduino Link Then you can use picocom to connect to the arduino: picocom /dev/ttyACM0 -b 115200 Note See here how to find the right USB port. Getting help Link You can type $ (and press enter) to get help. You should not see any local echo of the $ and enter. Grbl should respond with: [HLP:$$ $# $G $I $N $x=val $Nx=line $J=line $SLP $C $X $H ~ ! ? ctrl-x] ok Accessing the saved configuration Link To access the saved configuration, type $$ to obtain the parameter values. For example our config is: $0=10 $1=255 $2=0 $3=5 $4=0 $5=0 $6=0 $10=1 $11=0.020 $12=0.002 $13=0 $20=1 $21=0 $22=1 $23=0 $24=25.000 $25=5000.000 $26=250 $27=1.000 $30=12000 $31=0 $32=0 $100=40.000 $101=40.000 $102=188.947 $110=8000.000 $111=8000.000 $112=1000.000 $120=100.000 $121=100.000 $122=50.000 $130=780.000 $131=790.000 $132=150.000 Note See the official Grbl wiki for details & meaning of parameter. Setup & calibration Link Now we will: verify the cnc respond correctly to homing instruction calibrate if needed Homing the X-carve Link Once you are sure that everything is connected properly (especially the limit switches) you can try to \"home\" the X-Carve manually using $H in the previous terminal connected to Grbl. Warning Be ready to use the emergency stop button in case the axes move in the opposite direction of your limit switches! Note If you don't see any response in the terminal when you type the commands, it is perfectly normal! Change homing direction: Link Parameters named $3 control the homing direction. From the official wiki: Quote By default, Grbl assumes that the axes move in a positive direction when the direction pin signal is low, and a negative direction when the pin is high. To configure the homing direction, you simply need to send the value for the axes you want to invert using the table below. Setting Value Mask Invert X Invert Y Invert Z 0 00000000 N N N 1 00000001 Y N N 2 00000010 N Y N 3 00000011 Y Y N 4 00000100 N N Y 5 00000101 Y N Y 6 00000110 N Y Y 7 00000111 Y Y Y For example, if want to invert the Y axis direction only, you'd send $3=2 to Grbl, and the setting should now read $3=2 (dir port invert mask:00000010) Edit the acceleration rates Link Parameters named $120 , $121 & $123 control the axes acceleration in mm/second/second. From the official wiki: Quote Simplistically, a lower value makes Grbl ease slower into motion, while a higher value yields tighter moves and reaches the desired feed rates much quicker. Much like the max rate setting, each axis has its own acceleration value and are independent of each other. This means that a multi-axis motion will only accelerate as quickly as the lowest contributing axis can. Since the Z-axis arm is long and have a \"heavy weight\" (gimbal + camera) at its lower end, it is probably a good to keep low values to avoid blurry image due to shaking! To determine optimal values, the official wiki is clear, you will have to run some tests: Quote Again, like the max rate setting, the simplest way to determine the values for this setting is to individually test each axis with slowly increasing values until the motor stalls. Then finalize your acceleration setting with a value 10-20% below this absolute max value. This should account for wear, friction, and mass inertia. We highly recommend that you dry test some G-code programs with your new settings before committing to them. Sometimes the loading on your machine is different when moving in all axes together. For example, you can send $120=100.0 to set the X axis acceleration rate to 100 mm/second\u00b2. Note Deceleration rates may also create shaking!","title":"CNC calibration"},{"location":"Scanner/build/cnc_calibration/#cnc-setup-calibration","text":"After building & wiring the CNC and fitting it into the aluminium frame it is required to calibrate some Grbl software parameters like the homing directions or the acceleration rates .","title":"CNC setup &amp; calibration"},{"location":"Scanner/build/cnc_calibration/#using-grbl","text":"Except if you are familiar with Grbl, if you want to know more, have a look at the official Grbl wiki .","title":"Using Grbl"},{"location":"Scanner/build/cnc_calibration/#connect-to-the-arduino","text":"Then you can use picocom to connect to the arduino: picocom /dev/ttyACM0 -b 115200 Note See here how to find the right USB port.","title":"Connect to the Arduino"},{"location":"Scanner/build/cnc_calibration/#getting-help","text":"You can type $ (and press enter) to get help. You should not see any local echo of the $ and enter. Grbl should respond with: [HLP:$$ $# $G $I $N $x=val $Nx=line $J=line $SLP $C $X $H ~ ! ? ctrl-x] ok","title":"Getting help"},{"location":"Scanner/build/cnc_calibration/#accessing-the-saved-configuration","text":"To access the saved configuration, type $$ to obtain the parameter values. For example our config is: $0=10 $1=255 $2=0 $3=5 $4=0 $5=0 $6=0 $10=1 $11=0.020 $12=0.002 $13=0 $20=1 $21=0 $22=1 $23=0 $24=25.000 $25=5000.000 $26=250 $27=1.000 $30=12000 $31=0 $32=0 $100=40.000 $101=40.000 $102=188.947 $110=8000.000 $111=8000.000 $112=1000.000 $120=100.000 $121=100.000 $122=50.000 $130=780.000 $131=790.000 $132=150.000 Note See the official Grbl wiki for details & meaning of parameter.","title":"Accessing the saved configuration"},{"location":"Scanner/build/cnc_calibration/#setup-calibration","text":"Now we will: verify the cnc respond correctly to homing instruction calibrate if needed","title":"Setup &amp; calibration"},{"location":"Scanner/build/cnc_calibration/#homing-the-x-carve","text":"Once you are sure that everything is connected properly (especially the limit switches) you can try to \"home\" the X-Carve manually using $H in the previous terminal connected to Grbl. Warning Be ready to use the emergency stop button in case the axes move in the opposite direction of your limit switches! Note If you don't see any response in the terminal when you type the commands, it is perfectly normal!","title":"Homing the X-carve"},{"location":"Scanner/build/cnc_calibration/#change-homing-direction","text":"Parameters named $3 control the homing direction. From the official wiki: Quote By default, Grbl assumes that the axes move in a positive direction when the direction pin signal is low, and a negative direction when the pin is high. To configure the homing direction, you simply need to send the value for the axes you want to invert using the table below. Setting Value Mask Invert X Invert Y Invert Z 0 00000000 N N N 1 00000001 Y N N 2 00000010 N Y N 3 00000011 Y Y N 4 00000100 N N Y 5 00000101 Y N Y 6 00000110 N Y Y 7 00000111 Y Y Y For example, if want to invert the Y axis direction only, you'd send $3=2 to Grbl, and the setting should now read $3=2 (dir port invert mask:00000010)","title":"Change homing direction:"},{"location":"Scanner/build/cnc_calibration/#edit-the-acceleration-rates","text":"Parameters named $120 , $121 & $123 control the axes acceleration in mm/second/second. From the official wiki: Quote Simplistically, a lower value makes Grbl ease slower into motion, while a higher value yields tighter moves and reaches the desired feed rates much quicker. Much like the max rate setting, each axis has its own acceleration value and are independent of each other. This means that a multi-axis motion will only accelerate as quickly as the lowest contributing axis can. Since the Z-axis arm is long and have a \"heavy weight\" (gimbal + camera) at its lower end, it is probably a good to keep low values to avoid blurry image due to shaking! To determine optimal values, the official wiki is clear, you will have to run some tests: Quote Again, like the max rate setting, the simplest way to determine the values for this setting is to individually test each axis with slowly increasing values until the motor stalls. Then finalize your acceleration setting with a value 10-20% below this absolute max value. This should account for wear, friction, and mass inertia. We highly recommend that you dry test some G-code programs with your new settings before committing to them. Sometimes the loading on your machine is different when moving in all axes together. For example, you can send $120=100.0 to set the X axis acceleration rate to 100 mm/second\u00b2. Note Deceleration rates may also create shaking!","title":"Edit the acceleration rates"},{"location":"Scanner/build/cnc_communication/","text":"Communicating with the CNC Link Connect to the Arduino UNO Link First you need to find which USB port your arduino is connected to. To do so, you can use dmesg : make sure the usb cable from the arduino is unplugged run dmesg -w in a terminal connect the usb and see something like: [ 70480 .940181 ] usb 1 -2: new full-speed USB device number 31 using xhci_hcd [ 70481 .090857 ] usb 1 -2: New USB device found, idVendor = 2a03, idProduct = 0043 , bcdDevice = 0 .01 [ 70481 .090862 ] usb 1 -2: New USB device strings: Mfr = 1 , Product = 2 , SerialNumber = 220 [ 70481 .090865 ] usb 1 -2: Product: Arduino Uno [ 70481 .090868 ] usb 1 -2: Manufacturer: Arduino Srl [ 70481 .090871 ] usb 1 -2: SerialNumber: 554313131383512001F0 [ 70481 .093408 ] cdc_acm 1 -2:1.0: ttyACM0: USB ACM device Important The important info here is ttyACM0 ! Then you can use picocom to connect to the arduino: picocom /dev/ttyACM0 -b 115200 Note -b 115200 is the baudrate of the connection, read the picocom man page for more info. Once connected you should see something like: picocom v2.2 port is : /dev/ttyACM0 flowcontrol : none baudrate is : 115200 parity is : none databits are : 8 stopbits are : 1 escape is : C-a local echo is : no noinit is : no noreset is : no nolock is : no send_cmd is : sz -vv receive_cmd is : rz -vv -E imap is : omap is : emap is : crcrlf,delbs, Type [ C-a ] [ C-h ] to see available commands Terminal ready Grbl 1 .1f [ '$' for help ] [ MSG: '$H' | '$X' to unlock ] This mean you now have access to a Grbl terminal ( Grbl 1.1f ) to communicate, notably send instructions, to the CNC! Troubleshooting Link Serial access denied Link Look here if you can not communicate with the scanner using usb.","title":"CNC communication"},{"location":"Scanner/build/cnc_communication/#communicating-with-the-cnc","text":"","title":"Communicating with the CNC"},{"location":"Scanner/build/cnc_communication/#connect-to-the-arduino-uno","text":"First you need to find which USB port your arduino is connected to. To do so, you can use dmesg : make sure the usb cable from the arduino is unplugged run dmesg -w in a terminal connect the usb and see something like: [ 70480 .940181 ] usb 1 -2: new full-speed USB device number 31 using xhci_hcd [ 70481 .090857 ] usb 1 -2: New USB device found, idVendor = 2a03, idProduct = 0043 , bcdDevice = 0 .01 [ 70481 .090862 ] usb 1 -2: New USB device strings: Mfr = 1 , Product = 2 , SerialNumber = 220 [ 70481 .090865 ] usb 1 -2: Product: Arduino Uno [ 70481 .090868 ] usb 1 -2: Manufacturer: Arduino Srl [ 70481 .090871 ] usb 1 -2: SerialNumber: 554313131383512001F0 [ 70481 .093408 ] cdc_acm 1 -2:1.0: ttyACM0: USB ACM device Important The important info here is ttyACM0 ! Then you can use picocom to connect to the arduino: picocom /dev/ttyACM0 -b 115200 Note -b 115200 is the baudrate of the connection, read the picocom man page for more info. Once connected you should see something like: picocom v2.2 port is : /dev/ttyACM0 flowcontrol : none baudrate is : 115200 parity is : none databits are : 8 stopbits are : 1 escape is : C-a local echo is : no noinit is : no noreset is : no nolock is : no send_cmd is : sz -vv receive_cmd is : rz -vv -E imap is : omap is : emap is : crcrlf,delbs, Type [ C-a ] [ C-h ] to see available commands Terminal ready Grbl 1 .1f [ '$' for help ] [ MSG: '$H' | '$X' to unlock ] This mean you now have access to a Grbl terminal ( Grbl 1.1f ) to communicate, notably send instructions, to the CNC!","title":"Connect to the Arduino UNO"},{"location":"Scanner/build/cnc_communication/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"Scanner/build/cnc_communication/#serial-access-denied","text":"Look here if you can not communicate with the scanner using usb.","title":"Serial access denied"},{"location":"Scanner/build/cnc_electronics/","text":"Wiring the CNC Link If you have the recent version, follow the official wiring instructions here . Here is link to the post 2015 version of the \"wiring\" instructions. Here is link to the post 2015 version of the \"electronic assembly\" instructions. BOM Link If you are familiar with the Arduino world, the electronic is pretty straightforward: Arduino UNO (official buy here ) Synthetos gShield (buy here ) Power converter 220V ac. - 24V dc. (buy @ Farnell ) Emergency stop button (buy @ Farnell ) Optional: 24V fan 220V power cord Note Before 2017 X-Carve shipped 400W power units, now they use a 320W unit. The link is for a 350W unit. Wiring instructions Link Wire the Stepper Cable to the gShield Link Once you\u2019ve determined which stepper cable belongs to which axis, you can wire them into the gShield. First loosen all the screws on the gShield (they will jump a thread when they are fully loose, but they won\u2019t come out of the terminal blocks.) The gShield is marked \"X,\" \"Y,\" and \"Z\". Wire the stepper cable according to the markings on the shield and order your wires (from left to right) black, green, white, red. Check out this diagram for clarification. Mount the gShield Link Now push the gShield onto the Arduino. There are pins on the gShield that go into the headers of the Arduino. 24V Fan mount (optional) Link If you want, you can add a 24V fan to cool the shield. Uses the 24V pins on the gShield as shown in the picture below: Loosen the screws in the power terminal of the gShield and insert the red twisted pair into Vmot and the black twisted pair into GND . Connect Limit Switches to gShield Link Crimp the white ends of each limit switch wire pair. The order of the white wires from left to right is X, Y, Z. The first, sixth, and eighth slots are left EMPTY. Pin mapping: D9 : x-limit (red) D10 : y-limit (red) D12 : z-limit (red) GND : ground (all 3) Power the gShield Link Loosen the screws in the power terminal of the gShield and insert the red twisted pair into Vmot and the black twisted pair into GND . This is similar to the optional 24V fan. Note This will also power the Arduino.","title":"CNC electronics"},{"location":"Scanner/build/cnc_electronics/#wiring-the-cnc","text":"If you have the recent version, follow the official wiring instructions here . Here is link to the post 2015 version of the \"wiring\" instructions. Here is link to the post 2015 version of the \"electronic assembly\" instructions.","title":"Wiring the CNC"},{"location":"Scanner/build/cnc_electronics/#bom","text":"If you are familiar with the Arduino world, the electronic is pretty straightforward: Arduino UNO (official buy here ) Synthetos gShield (buy here ) Power converter 220V ac. - 24V dc. (buy @ Farnell ) Emergency stop button (buy @ Farnell ) Optional: 24V fan 220V power cord Note Before 2017 X-Carve shipped 400W power units, now they use a 320W unit. The link is for a 350W unit.","title":"BOM"},{"location":"Scanner/build/cnc_electronics/#wiring-instructions","text":"","title":"Wiring instructions"},{"location":"Scanner/build/cnc_electronics/#wire-the-stepper-cable-to-the-gshield","text":"Once you\u2019ve determined which stepper cable belongs to which axis, you can wire them into the gShield. First loosen all the screws on the gShield (they will jump a thread when they are fully loose, but they won\u2019t come out of the terminal blocks.) The gShield is marked \"X,\" \"Y,\" and \"Z\". Wire the stepper cable according to the markings on the shield and order your wires (from left to right) black, green, white, red. Check out this diagram for clarification.","title":"Wire the Stepper Cable to the gShield"},{"location":"Scanner/build/cnc_electronics/#mount-the-gshield","text":"Now push the gShield onto the Arduino. There are pins on the gShield that go into the headers of the Arduino.","title":"Mount the gShield"},{"location":"Scanner/build/cnc_electronics/#24v-fan-mount-optional","text":"If you want, you can add a 24V fan to cool the shield. Uses the 24V pins on the gShield as shown in the picture below: Loosen the screws in the power terminal of the gShield and insert the red twisted pair into Vmot and the black twisted pair into GND .","title":"24V Fan mount (optional)"},{"location":"Scanner/build/cnc_electronics/#connect-limit-switches-to-gshield","text":"Crimp the white ends of each limit switch wire pair. The order of the white wires from left to right is X, Y, Z. The first, sixth, and eighth slots are left EMPTY. Pin mapping: D9 : x-limit (red) D10 : y-limit (red) D12 : z-limit (red) GND : ground (all 3)","title":"Connect Limit Switches to gShield"},{"location":"Scanner/build/cnc_electronics/#power-the-gshield","text":"Loosen the screws in the power terminal of the gShield and insert the red twisted pair into Vmot and the black twisted pair into GND . This is similar to the optional 24V fan. Note This will also power the Arduino.","title":"Power the gShield"},{"location":"Scanner/build/cnc_frame/","text":"Building the CNC frame Link We chose the X-Carve 1000 mm as basis for the CNC gantry, feel free to choose another one but beware to check the aluminium frame still fit! X-Carve 1000mm. BOM Link To build the CNC you will need: X-Carve 1000mmm (US buy here ; EU buy here ) Note We chose the X-Carve because initially you could customize your order and for example not order the work area (wood plate). Apparently it is not possible anymore! Instructions Link Follow the official build instructions for the X-Carve here . You should follow these steps: Gantry - Side Plates Gantry - X-Carriage Gantry - Z-Axis Assemble Gantry Rails Belting Note Do NOT follow the steps indicating how to: build the work area fix the spindle fix the gantry and rails over the work area (wood plate) Once you followed the official instructions, attach the CNC to the aluminium frame before continuing with the next section: wiring the CNC electronics!","title":"CNC frame"},{"location":"Scanner/build/cnc_frame/#building-the-cnc-frame","text":"We chose the X-Carve 1000 mm as basis for the CNC gantry, feel free to choose another one but beware to check the aluminium frame still fit! X-Carve 1000mm.","title":"Building the CNC frame"},{"location":"Scanner/build/cnc_frame/#bom","text":"To build the CNC you will need: X-Carve 1000mmm (US buy here ; EU buy here ) Note We chose the X-Carve because initially you could customize your order and for example not order the work area (wood plate). Apparently it is not possible anymore!","title":"BOM"},{"location":"Scanner/build/cnc_frame/#instructions","text":"Follow the official build instructions for the X-Carve here . You should follow these steps: Gantry - Side Plates Gantry - X-Carriage Gantry - Z-Axis Assemble Gantry Rails Belting Note Do NOT follow the steps indicating how to: build the work area fix the spindle fix the gantry and rails over the work area (wood plate) Once you followed the official instructions, attach the CNC to the aluminium frame before continuing with the next section: wiring the CNC electronics!","title":"Instructions"},{"location":"Scanner/build/gimball_communication/","text":"Communicating & controlling the gimball Link Connect to the Feather M0 Link First you need to find which USB port your Feather M0 is connected to. To do so, you can use dmesg : make sure the usb cable from the Feather M0 is unplugged run dmesg -w in a terminal connect the usb and see something like: [ 70481 .093408 ] cdc_acm 1 -2:1.0: ttyACM1: USB ACM device Important The important info here is ttyACM1 ! Then you can use picocom to connect to the Feather M0: picocom /dev/ttyACM1 -b 115200 Usage Link Send plain text-formatted commands for the position or speed commands. See the command list below: x : set velocity (unit: RPM) X : set target position (units: degrees) c : start the calibration of the encoder C : print results of the calibration v : print target velocity p : print PWM value Troubleshooting Link Serial access denied Link Look here if you can not communicate with the scanner using usb.","title":"Gimball communication"},{"location":"Scanner/build/gimball_communication/#communicating-controlling-the-gimball","text":"","title":"Communicating &amp; controlling the gimball"},{"location":"Scanner/build/gimball_communication/#connect-to-the-feather-m0","text":"First you need to find which USB port your Feather M0 is connected to. To do so, you can use dmesg : make sure the usb cable from the Feather M0 is unplugged run dmesg -w in a terminal connect the usb and see something like: [ 70481 .093408 ] cdc_acm 1 -2:1.0: ttyACM1: USB ACM device Important The important info here is ttyACM1 ! Then you can use picocom to connect to the Feather M0: picocom /dev/ttyACM1 -b 115200","title":"Connect to the Feather M0"},{"location":"Scanner/build/gimball_communication/#usage","text":"Send plain text-formatted commands for the position or speed commands. See the command list below: x : set velocity (unit: RPM) X : set target position (units: degrees) c : start the calibration of the encoder C : print results of the calibration v : print target velocity p : print PWM value","title":"Usage"},{"location":"Scanner/build/gimball_communication/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"Scanner/build/gimball_communication/#serial-access-denied","text":"Look here if you can not communicate with the scanner using usb.","title":"Serial access denied"},{"location":"Scanner/build/gimball_setup/","text":"Building the the gimball Link BOM Link The bill of material is short: Adafruit Feather M0 Custom made ROMI gimball controller hat Sources Link The sources of the custom made gimball controller are available here . The source code to compile & upload with Arduino IDE to the Feather M0 is here . The bracket for the picamera that also hold the M0 & the hat should be 3D printed. The design is available here . Resources Link An introduction to the Adafruit Feather M0 Basic is available on Adafruits website here .","title":"Gimball setup"},{"location":"Scanner/build/gimball_setup/#building-the-the-gimball","text":"","title":"Building the the gimball"},{"location":"Scanner/build/gimball_setup/#bom","text":"The bill of material is short: Adafruit Feather M0 Custom made ROMI gimball controller hat","title":"BOM"},{"location":"Scanner/build/gimball_setup/#sources","text":"The sources of the custom made gimball controller are available here . The source code to compile & upload with Arduino IDE to the Feather M0 is here . The bracket for the picamera that also hold the M0 & the hat should be 3D printed. The design is available here .","title":"Sources"},{"location":"Scanner/build/gimball_setup/#resources","text":"An introduction to the Adafruit Feather M0 Basic is available on Adafruits website here .","title":"Resources"},{"location":"Scanner/build/picamera_setup/","text":"PiCamera setup Link Warning This is a work in progress, Peter Hanappe is developing a new version! Getting started Link To setup the PiCamera you first need to install a fresh OS on your Rapsberry Pi Zero W. Burn a new Raspberry Pi OS Lite image Link We recommend the Raspberry Pi Imager tool to burn a new Raspberry Pi OS (32-bit) Lite. You can find it here . Or you can download an official ZIP here . Accessing the PiZero Link A - Enable SSH remote access Link After installing the Raspberry Pi OS, add an empty file ssh in the boot partition to enable SSH access. Warning As we will later use the wifi from the PiZero to create an Access Point, we recommend to use the ethernet port to SSH in the PiZero. B - Use local access Link Otherwise, use a screen and keyboard to access the PiZero. Raspberry Pi OS setup Link Before installing the software, you have to configure some Raspberry Pi OS settings & change the password. Change the password Link For security reasons, you have to change the pi user password because the default is known by everyone! Use the raspi-config` tool to do it: sudo raspi-config Warning The default password is raspberry ! Edit the timezone & locales Link Change the timezone to get the right time from the PiZero clock! You may also change the locales to suits your needs. Create the WiFi Access Point Link We will now create the AP using command lines following the tutorial from: https://learn.sparkfun.com/tutorials/setting-up-a-raspberry-pi-3-as-an-access-point/all . You may also be interested in doing this with a graphical interface, and we would highly recommend raspap-webgui . Install Packages Link To install the required packages, enter the following into the console: sudo apt-get -y install hostapd dnsmasq Set Static IP Address Link Edit the dhcpcd.conf file: sudo nano /etc/dhcpcd.conf At the bottom of the file, add: denyinterfaces wlan0 Save and exit by pressing Ctrl + X and Y when asked. Next, we need to tell the Raspberry Pi to set a static IP address for the WiFi interface. Open the interfaces file with the following command: sudo nano /etc/network/interfaces At the bottom of that file, add the following: auto lo iface lo inet loopback auto eth0 iface eth0 inet dhcp allow-hotplug wlan0 iface wlan0 inet static address 192.168.0.1 netmask 255.255.255.0 network 192.168.0.0 broadcast 192.168.0.255 Configure Hostapd Link We need to set up hostapd to tell it to broadcast a particular SSID and allow WiFi connections on a certain channel. Edit the hostapd.conf file (this will create a new file, as one likely does not exist yet) with this command: sudo nano /etc/hostapd/hostapd.conf Enter the following into that file. Feel fee to change the ssid (WiFi network name) and the wpa_passphrase (password to join the network) to whatever you'd like. You can also change the channel to something in the 1-11 range (if channel 6 is too crowded in your area). interface=wlan0 driver=nl80211 ssid=romi_hotspot hw_mode=g channel=6 ieee80211n=1 wmm_enabled=1 ht_capab=[HT40][SHORT-GI-20][DSSS_CCK-40] macaddr_acl=0 auth_algs=1 ignore_broadcast_ssid=0 wpa=2 wpa_key_mgmt=WPA-PSK wpa_passphrase=raspberry rsn_pairwise=CCMP Save and exit by pressing Ctrl + X and Y when asked. Unfortunately, hostapd does not know where to find this configuration file, so we need to provide its location to the hostapd startup script. Open /etc/default/hostapd : sudo nano /etc/default/hostapd Find the line #DAEMON_CONF=\"\" and replace it with: DAEMON_CONF=\"/etc/hostapd/hostapd.conf\" Save and exit by pressing Ctrl + X and Y when asked. Configure Dnsmasq Link Dnsmasq will help us automatically assign IP addresses as new devices connect to our network as well as work as a translation between network names and IP addresses. The .conf file that comes with Dnsmasq has a lot of good information in it, so it might be worthwhile to save it (as a backup) rather than delete it. After saving it, open a new one for editing: sudo mv /etc/dnsmasq.conf /etc/dnsmasq.conf.bak sudo nano /etc/dnsmasq.conf In the blank file, paste in the text below. interface=wlan0 listen-address=192.168.0.1 bind-interfaces server=8.8.8.8 domain-needed bogus-priv dhcp-range=192.168.0.100,192.168.0.200,24h Test WiFi connection Link Restart the Raspberry Pi using the following command: sudo reboot After your Pi restarts (no need to log in), you should see romi_hotspot appear as a potential wireless network from your computer. Connect to it (the network password is raspberry, unless you changed it in the hostapd.conf file). Then try to SSH it with: ssh pi@192.138.0.1 Install Python3 & pip Link You will need the Python3 interpreter (Python>=3.6) to run the PiCamera server & pip to install the PiCamera package. sudo apt update && sudo apt upgrade -y sudo apt install python3 python3-pip This will update the package manager, upgrade the system libraries & install Python3 & pip. Install the PiCamera package Link Once you have pip you can install the picamera Python3 package: pip3 install picamera Note We do not create an isolated environment in this case since the sole purpose of the PiZero will be to act as a responsive image server. Camera serve Python code Link To capture and serve the images from the PiCamera, we use this Python script: To upload it to your PiZero, from a terminal: wget https://gist.github.com/jlegrand62/c24e454922f0cf203d6f9ed49f95ecc1/raw/c4cda40ff56984188dca928693852f3f7a317fa4/picamera_server.py Now (test) start the server with: python3 picamera_server.py Todo Explain how to execute python3 picamera_server.py command at PiZero boot.","title":"PiCamera setup"},{"location":"Scanner/build/picamera_setup/#picamera-setup","text":"Warning This is a work in progress, Peter Hanappe is developing a new version!","title":"PiCamera setup"},{"location":"Scanner/build/picamera_setup/#getting-started","text":"To setup the PiCamera you first need to install a fresh OS on your Rapsberry Pi Zero W.","title":"Getting started"},{"location":"Scanner/build/picamera_setup/#burn-a-new-raspberry-pi-os-lite-image","text":"We recommend the Raspberry Pi Imager tool to burn a new Raspberry Pi OS (32-bit) Lite. You can find it here . Or you can download an official ZIP here .","title":"Burn a new Raspberry Pi OS Lite image"},{"location":"Scanner/build/picamera_setup/#accessing-the-pizero","text":"","title":"Accessing the PiZero"},{"location":"Scanner/build/picamera_setup/#a-enable-ssh-remote-access","text":"After installing the Raspberry Pi OS, add an empty file ssh in the boot partition to enable SSH access. Warning As we will later use the wifi from the PiZero to create an Access Point, we recommend to use the ethernet port to SSH in the PiZero.","title":"A - Enable SSH remote access"},{"location":"Scanner/build/picamera_setup/#b-use-local-access","text":"Otherwise, use a screen and keyboard to access the PiZero.","title":"B - Use local access"},{"location":"Scanner/build/picamera_setup/#raspberry-pi-os-setup","text":"Before installing the software, you have to configure some Raspberry Pi OS settings & change the password.","title":"Raspberry Pi OS setup"},{"location":"Scanner/build/picamera_setup/#change-the-password","text":"For security reasons, you have to change the pi user password because the default is known by everyone! Use the raspi-config` tool to do it: sudo raspi-config Warning The default password is raspberry !","title":"Change the password"},{"location":"Scanner/build/picamera_setup/#edit-the-timezone-locales","text":"Change the timezone to get the right time from the PiZero clock! You may also change the locales to suits your needs.","title":"Edit the timezone &amp; locales"},{"location":"Scanner/build/picamera_setup/#create-the-wifi-access-point","text":"We will now create the AP using command lines following the tutorial from: https://learn.sparkfun.com/tutorials/setting-up-a-raspberry-pi-3-as-an-access-point/all . You may also be interested in doing this with a graphical interface, and we would highly recommend raspap-webgui .","title":"Create the WiFi Access Point"},{"location":"Scanner/build/picamera_setup/#install-packages","text":"To install the required packages, enter the following into the console: sudo apt-get -y install hostapd dnsmasq","title":"Install Packages"},{"location":"Scanner/build/picamera_setup/#set-static-ip-address","text":"Edit the dhcpcd.conf file: sudo nano /etc/dhcpcd.conf At the bottom of the file, add: denyinterfaces wlan0 Save and exit by pressing Ctrl + X and Y when asked. Next, we need to tell the Raspberry Pi to set a static IP address for the WiFi interface. Open the interfaces file with the following command: sudo nano /etc/network/interfaces At the bottom of that file, add the following: auto lo iface lo inet loopback auto eth0 iface eth0 inet dhcp allow-hotplug wlan0 iface wlan0 inet static address 192.168.0.1 netmask 255.255.255.0 network 192.168.0.0 broadcast 192.168.0.255","title":"Set Static IP Address"},{"location":"Scanner/build/picamera_setup/#configure-hostapd","text":"We need to set up hostapd to tell it to broadcast a particular SSID and allow WiFi connections on a certain channel. Edit the hostapd.conf file (this will create a new file, as one likely does not exist yet) with this command: sudo nano /etc/hostapd/hostapd.conf Enter the following into that file. Feel fee to change the ssid (WiFi network name) and the wpa_passphrase (password to join the network) to whatever you'd like. You can also change the channel to something in the 1-11 range (if channel 6 is too crowded in your area). interface=wlan0 driver=nl80211 ssid=romi_hotspot hw_mode=g channel=6 ieee80211n=1 wmm_enabled=1 ht_capab=[HT40][SHORT-GI-20][DSSS_CCK-40] macaddr_acl=0 auth_algs=1 ignore_broadcast_ssid=0 wpa=2 wpa_key_mgmt=WPA-PSK wpa_passphrase=raspberry rsn_pairwise=CCMP Save and exit by pressing Ctrl + X and Y when asked. Unfortunately, hostapd does not know where to find this configuration file, so we need to provide its location to the hostapd startup script. Open /etc/default/hostapd : sudo nano /etc/default/hostapd Find the line #DAEMON_CONF=\"\" and replace it with: DAEMON_CONF=\"/etc/hostapd/hostapd.conf\" Save and exit by pressing Ctrl + X and Y when asked.","title":"Configure Hostapd"},{"location":"Scanner/build/picamera_setup/#configure-dnsmasq","text":"Dnsmasq will help us automatically assign IP addresses as new devices connect to our network as well as work as a translation between network names and IP addresses. The .conf file that comes with Dnsmasq has a lot of good information in it, so it might be worthwhile to save it (as a backup) rather than delete it. After saving it, open a new one for editing: sudo mv /etc/dnsmasq.conf /etc/dnsmasq.conf.bak sudo nano /etc/dnsmasq.conf In the blank file, paste in the text below. interface=wlan0 listen-address=192.168.0.1 bind-interfaces server=8.8.8.8 domain-needed bogus-priv dhcp-range=192.168.0.100,192.168.0.200,24h","title":"Configure Dnsmasq"},{"location":"Scanner/build/picamera_setup/#test-wifi-connection","text":"Restart the Raspberry Pi using the following command: sudo reboot After your Pi restarts (no need to log in), you should see romi_hotspot appear as a potential wireless network from your computer. Connect to it (the network password is raspberry, unless you changed it in the hostapd.conf file). Then try to SSH it with: ssh pi@192.138.0.1","title":"Test WiFi connection"},{"location":"Scanner/build/picamera_setup/#install-python3-pip","text":"You will need the Python3 interpreter (Python>=3.6) to run the PiCamera server & pip to install the PiCamera package. sudo apt update && sudo apt upgrade -y sudo apt install python3 python3-pip This will update the package manager, upgrade the system libraries & install Python3 & pip.","title":"Install Python3 &amp; pip"},{"location":"Scanner/build/picamera_setup/#install-the-picamera-package","text":"Once you have pip you can install the picamera Python3 package: pip3 install picamera Note We do not create an isolated environment in this case since the sole purpose of the PiZero will be to act as a responsive image server.","title":"Install the PiCamera package"},{"location":"Scanner/build/picamera_setup/#camera-serve-python-code","text":"To capture and serve the images from the PiCamera, we use this Python script: To upload it to your PiZero, from a terminal: wget https://gist.github.com/jlegrand62/c24e454922f0cf203d6f9ed49f95ecc1/raw/c4cda40ff56984188dca928693852f3f7a317fa4/picamera_server.py Now (test) start the server with: python3 picamera_server.py Todo Explain how to execute python3 picamera_server.py command at PiZero boot.","title":"Camera serve Python code"},{"location":"Scanner/build/troubleshooting/","text":"Troubleshooting Link Serial access denied Link If you get an error about permission access: Check in what group you are with: groups ${ USER } If you are not in dialout : sudo gpasswd --add ${ USER } dialout Then log out and back in to see changes!","title":"Troubleshooting"},{"location":"Scanner/build/troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"Scanner/build/troubleshooting/#serial-access-denied","text":"If you get an error about permission access: Check in what group you are with: groups ${ USER } If you are not in dialout : sudo gpasswd --add ${ USER } dialout Then log out and back in to see changes!","title":"Serial access denied"},{"location":"Scanner/developer/","text":"","title":"Home"},{"location":"Scanner/developer/conda/","text":"Conda Link Recipes to build conda packages can be found here . Follow these instructions to build conda packages. Warning Conda packages should be built from the base environment. conda activate base Requirements Link Install conda-build : Link Install conda-build , in the base environment, to be able to build conda package: conda install conda-build WARNING: For macOS, follow these instructions to install the required macOS 10.9 SDK . Optional - Install anaconda-client : Link To be able to upload your package on anaconda cloud you need to install anaconda-client : conda install anaconda-client Build conda packages: Link Build lettucethink : Link Using the given recipe, it is easy to build the lettucethink-python conda package: cd conda_recipes/ conda build lettucethink/ --user romi-eu Build plantdb : Link Using the given recipe, it is easy to build the plantdb conda package: cd conda_recipes/ conda build plantdb/ -c romi-eu -c open3d-admin --user romi-eu Build plant3dvision : Link Using the given recipe, it is easy to build the plant3dvision conda package: cd conda_recipes/ conda build plant3dvision/ -c romi-eu -c conda-forge -c open3d-admin --user romi-eu Build romi-plantviz : Link Using the given recipe, it is easy to build the romi-plantviz conda package: cd conda_recipes/ conda build romi-plantviz/ -c romi-eu -c conda-forge --user romi-eu Optional - Build dirsync package: Link To build dirsync you have to install hgsvn : sudo apt install hgsvn Using the given recipe, it is easy to build the dirsync conda package: cd conda_recipes conda build dirsync/recipe/ --user romi-eu Optional - Build opencv-python package: Link To build opencv-python you have to install qt4-qmake : sudo apt install qt4-qmake qt4-default Using the given recipe, it is easy to build the opencv-python conda package: cd conda_recipes conda build opencv-python/ -c conda-forge --user romi-eu Conda useful commands: Link Purge built packages: Link conda build purge Clean cache & unused packages: Link conda clean --all","title":"Conda"},{"location":"Scanner/developer/conda/#conda","text":"Recipes to build conda packages can be found here . Follow these instructions to build conda packages. Warning Conda packages should be built from the base environment. conda activate base","title":"Conda"},{"location":"Scanner/developer/conda/#requirements","text":"","title":"Requirements"},{"location":"Scanner/developer/conda/#install-conda-build","text":"Install conda-build , in the base environment, to be able to build conda package: conda install conda-build WARNING: For macOS, follow these instructions to install the required macOS 10.9 SDK .","title":"Install conda-build:"},{"location":"Scanner/developer/conda/#optional-install-anaconda-client","text":"To be able to upload your package on anaconda cloud you need to install anaconda-client : conda install anaconda-client","title":"Optional - Install anaconda-client:"},{"location":"Scanner/developer/conda/#build-conda-packages","text":"","title":"Build conda packages:"},{"location":"Scanner/developer/conda/#build-lettucethink","text":"Using the given recipe, it is easy to build the lettucethink-python conda package: cd conda_recipes/ conda build lettucethink/ --user romi-eu","title":"Build lettucethink:"},{"location":"Scanner/developer/conda/#build-plantdb","text":"Using the given recipe, it is easy to build the plantdb conda package: cd conda_recipes/ conda build plantdb/ -c romi-eu -c open3d-admin --user romi-eu","title":"Build plantdb:"},{"location":"Scanner/developer/conda/#build-plant3dvision","text":"Using the given recipe, it is easy to build the plant3dvision conda package: cd conda_recipes/ conda build plant3dvision/ -c romi-eu -c conda-forge -c open3d-admin --user romi-eu","title":"Build plant3dvision:"},{"location":"Scanner/developer/conda/#build-romi-plantviz","text":"Using the given recipe, it is easy to build the romi-plantviz conda package: cd conda_recipes/ conda build romi-plantviz/ -c romi-eu -c conda-forge --user romi-eu","title":"Build romi-plantviz:"},{"location":"Scanner/developer/conda/#optional-build-dirsync-package","text":"To build dirsync you have to install hgsvn : sudo apt install hgsvn Using the given recipe, it is easy to build the dirsync conda package: cd conda_recipes conda build dirsync/recipe/ --user romi-eu","title":"Optional - Build dirsync package:"},{"location":"Scanner/developer/conda/#optional-build-opencv-python-package","text":"To build opencv-python you have to install qt4-qmake : sudo apt install qt4-qmake qt4-default Using the given recipe, it is easy to build the opencv-python conda package: cd conda_recipes conda build opencv-python/ -c conda-forge --user romi-eu","title":"Optional - Build opencv-python package:"},{"location":"Scanner/developer/conda/#conda-useful-commands","text":"","title":"Conda useful commands:"},{"location":"Scanner/developer/conda/#purge-built-packages","text":"conda build purge","title":"Purge built packages:"},{"location":"Scanner/developer/conda/#clean-cache-unused-packages","text":"conda clean --all","title":"Clean cache &amp; unused packages:"},{"location":"Scanner/developer/git_docReview/","text":"Contributing to docs Link From romi-robots-docs , we incorporate changes using a typical git workflow with commits and pull requests. The documentation is generated using MkDocs . Objectives Link At the end of this tutorial, you should be able to: create content for romi-robots-docs review modifications suggested for romi-robots-docs by you colleagues Prerequisite Link Install git Example in Linux system with a Debian-based distribution (e.g. Ubuntu): git --version #verify that you have git installed: it should return the version (e.g. git version 2.25.1) #if git is not installed: $ sudo apt install git-all git clone romi-robots-docs git clone https://github.com/romi/romi-robots-docs.git cd romi-robots-docs #enter the cloned repository so that all git actions are available Step-by-step tutorial Link if you only review an existing branch (without adding or modifying content), go directly to steps 8/9. 1. Create your local branch Link The default branch is master , that directly incorporate all changes. There is no dev branch. Never ever work on master : create a local branch to make changes, using the following commands: git checkout master # go to master git pull # update it with last changes git checkout -b my_branch # create local branch `my_branch` (it will derived from the last master) git push --set-upstream origin my_branch # attach branch `my_branch` to `origin/my_branch`. GitHub login/password will be asked for. 2. Visualize your changes locally on a web browser in an interactive manner Link mkdocs serve # reads the mkdocs.yml file to generate the web page. The terminal gives you information. The programs starts by building the documentation. As soon as you can read: [I 210323 08:58:22 server:335] Serving on http://127.0.0.1:8000 ..., you can connect your favorite web browser by copy-pasting the url or just (ctrl+click) on the address to open it in your default browser. The program mkdocs serve constantly watches for changes and refreshes the build as soon as they are detected, as indicated by the terminal: [I 210323 08:58:22 handlers:62] Start watching changes INFO - Start watching changes [I 210323 08:58:22 handlers:64] Start detecting changes INFO - Start detecting changes Since the refresh is very rapid upon changes, you can then see in live the effect of you modifications. In the terminal, possible issues are listed (INFO and WARNING),pointing to problems that should be fixed: In this case, pages should be added in the nav section of the mkdocs.yml file (see later point 4.). In the interactive browser, you cannot see and display pages that does not exist in the \"nav\" configuration. internal hyperlink issues (WARNING): WARNING - Documentation file 'xxx/xxxx/file1.md' contains a link to 'xxx/yyy/otherfile.md' which is not found in the documentation files. In this case, check and modify the hyperlink in file1.md to provide good redirection to otherfile.md 3. Adding images in the content Link store the image files in assets/. You can also directly provide html address for third party images if you are sure that the link will be stable over time. To have more flexibility and options for images layout, use the html command in your markdown file: <img src=\"/assets/images/my_image.png\" alt=\"name_displayed_if_error\" width=\"600\" style=\"display:block; margin-left: auto; margin-right: auto;\"> # here the style centers the picture 4. Modify the navigation in mkdocs.yml Link Open mkdocs.yml at the root of romi-robots-docs repo. Some changes must be reported in this file in the nav section: when you create a new page (a new file.md ) or a new directory, or modify the name of an existing file.md/directory. In the nav section, you can also enter the name given to pages in the menu. 5. Commit your changes Link This follow the classical git commit procedure: git status #list all files affected by changes git add/restore/rm <file> #do as many action as listed in red by the previous command ( git status ) #optional: verify that all changes are staged and ready for commit) git commit -m \"my awesome commit\" git push # push modification to `origin/my_branch` ( git log ) #optional: verify that your commit is recorded 6. Merge your working branch with current master: rebase may be needed Link Assuming that your working branch is called my_branch git checkout master #switch to master git pull #update in case changes were made in the meantime: now you have the latest master branch git checkout my_branch #switch again to the branch to merge git rebase master # rebase `master` branch onto `my_branch` if the last command indicates conflicts, it means that master and my_branch have diverged during your work. For each files listed by git: fix the conflicts by directly editing the file stage your changes with: git add file1 continue the rebase with: git rebase --continue Finally, once all conflicts have been resolved and changes staged, Push the rebasing to remote central repo: git push -f origin my_branch #-f (force) implies that login/password will be asked for. 7. Prepare a PR on GitHub webpage Link Go to the distant romi repository : https://github.com/romi/romi-robots-docs select you branch and prepare a PR: open a pull request (green button), enter a brief text to explain the modifications, assign reviewers (in the right column of the page), and press the green button 'create pull request'. 8. test a distant branch (e.g. for a pull request review from your colleagues) Link git checkout test_branch #switch to 'test_branch'. /!\\ do not switch to origin/test_branch since your working locally git fetch #Download objects and refs from the remote branch on /origin git pull #Incorporates changes from the remote repository into the current local `test_branch` mkdocs serve #serve the docs website on the local server You can view the display, test the links, etc... You can also create a new branch from it to modify it. 9. make your review on GitHub web interface Link Comment the pull requests (PR), file by file. Point to issues if any. 10. Use the project board (Kanban type) Link Go to: https://github.com/orgs/romi/projects/10 link your PR to existing issues move the corresponding note to the appropriate column (To do / In progress / Test / Done)","title":"Contribute to docs"},{"location":"Scanner/developer/git_docReview/#contributing-to-docs","text":"From romi-robots-docs , we incorporate changes using a typical git workflow with commits and pull requests. The documentation is generated using MkDocs .","title":"Contributing to docs"},{"location":"Scanner/developer/git_docReview/#objectives","text":"At the end of this tutorial, you should be able to: create content for romi-robots-docs review modifications suggested for romi-robots-docs by you colleagues","title":"Objectives"},{"location":"Scanner/developer/git_docReview/#prerequisite","text":"Install git Example in Linux system with a Debian-based distribution (e.g. Ubuntu): git --version #verify that you have git installed: it should return the version (e.g. git version 2.25.1) #if git is not installed: $ sudo apt install git-all git clone romi-robots-docs git clone https://github.com/romi/romi-robots-docs.git cd romi-robots-docs #enter the cloned repository so that all git actions are available","title":"Prerequisite"},{"location":"Scanner/developer/git_docReview/#step-by-step-tutorial","text":"if you only review an existing branch (without adding or modifying content), go directly to steps 8/9.","title":"Step-by-step tutorial"},{"location":"Scanner/developer/git_docReview/#1-create-your-local-branch","text":"The default branch is master , that directly incorporate all changes. There is no dev branch. Never ever work on master : create a local branch to make changes, using the following commands: git checkout master # go to master git pull # update it with last changes git checkout -b my_branch # create local branch `my_branch` (it will derived from the last master) git push --set-upstream origin my_branch # attach branch `my_branch` to `origin/my_branch`. GitHub login/password will be asked for.","title":"1. Create your local branch"},{"location":"Scanner/developer/git_docReview/#2-visualize-your-changes-locally-on-a-web-browser-in-an-interactive-manner","text":"mkdocs serve # reads the mkdocs.yml file to generate the web page. The terminal gives you information. The programs starts by building the documentation. As soon as you can read: [I 210323 08:58:22 server:335] Serving on http://127.0.0.1:8000 ..., you can connect your favorite web browser by copy-pasting the url or just (ctrl+click) on the address to open it in your default browser. The program mkdocs serve constantly watches for changes and refreshes the build as soon as they are detected, as indicated by the terminal: [I 210323 08:58:22 handlers:62] Start watching changes INFO - Start watching changes [I 210323 08:58:22 handlers:64] Start detecting changes INFO - Start detecting changes Since the refresh is very rapid upon changes, you can then see in live the effect of you modifications. In the terminal, possible issues are listed (INFO and WARNING),pointing to problems that should be fixed: In this case, pages should be added in the nav section of the mkdocs.yml file (see later point 4.). In the interactive browser, you cannot see and display pages that does not exist in the \"nav\" configuration. internal hyperlink issues (WARNING): WARNING - Documentation file 'xxx/xxxx/file1.md' contains a link to 'xxx/yyy/otherfile.md' which is not found in the documentation files. In this case, check and modify the hyperlink in file1.md to provide good redirection to otherfile.md","title":"2. Visualize your changes locally on a web browser in an interactive manner"},{"location":"Scanner/developer/git_docReview/#3-adding-images-in-the-content","text":"store the image files in assets/. You can also directly provide html address for third party images if you are sure that the link will be stable over time. To have more flexibility and options for images layout, use the html command in your markdown file: <img src=\"/assets/images/my_image.png\" alt=\"name_displayed_if_error\" width=\"600\" style=\"display:block; margin-left: auto; margin-right: auto;\"> # here the style centers the picture","title":"3. Adding images in the content"},{"location":"Scanner/developer/git_docReview/#4-modify-the-navigation-in-mkdocsyml","text":"Open mkdocs.yml at the root of romi-robots-docs repo. Some changes must be reported in this file in the nav section: when you create a new page (a new file.md ) or a new directory, or modify the name of an existing file.md/directory. In the nav section, you can also enter the name given to pages in the menu.","title":"4. Modify the navigation in mkdocs.yml"},{"location":"Scanner/developer/git_docReview/#5-commit-your-changes","text":"This follow the classical git commit procedure: git status #list all files affected by changes git add/restore/rm <file> #do as many action as listed in red by the previous command ( git status ) #optional: verify that all changes are staged and ready for commit) git commit -m \"my awesome commit\" git push # push modification to `origin/my_branch` ( git log ) #optional: verify that your commit is recorded","title":"5. Commit your changes"},{"location":"Scanner/developer/git_docReview/#6-merge-your-working-branch-with-current-master-rebase-may-be-needed","text":"Assuming that your working branch is called my_branch git checkout master #switch to master git pull #update in case changes were made in the meantime: now you have the latest master branch git checkout my_branch #switch again to the branch to merge git rebase master # rebase `master` branch onto `my_branch` if the last command indicates conflicts, it means that master and my_branch have diverged during your work. For each files listed by git: fix the conflicts by directly editing the file stage your changes with: git add file1 continue the rebase with: git rebase --continue Finally, once all conflicts have been resolved and changes staged, Push the rebasing to remote central repo: git push -f origin my_branch #-f (force) implies that login/password will be asked for.","title":"6. Merge your working branch with current master: rebase may be needed"},{"location":"Scanner/developer/git_docReview/#7-prepare-a-pr-on-github-webpage","text":"Go to the distant romi repository : https://github.com/romi/romi-robots-docs select you branch and prepare a PR: open a pull request (green button), enter a brief text to explain the modifications, assign reviewers (in the right column of the page), and press the green button 'create pull request'.","title":"7. Prepare a PR on GitHub webpage"},{"location":"Scanner/developer/git_docReview/#8-test-a-distant-branch-eg-for-a-pull-request-review-from-your-colleagues","text":"git checkout test_branch #switch to 'test_branch'. /!\\ do not switch to origin/test_branch since your working locally git fetch #Download objects and refs from the remote branch on /origin git pull #Incorporates changes from the remote repository into the current local `test_branch` mkdocs serve #serve the docs website on the local server You can view the display, test the links, etc... You can also create a new branch from it to modify it.","title":"8. test a distant branch (e.g. for a pull request review from your colleagues)"},{"location":"Scanner/developer/git_docReview/#9-make-your-review-on-github-web-interface","text":"Comment the pull requests (PR), file by file. Point to issues if any.","title":"9. make your review on GitHub web interface"},{"location":"Scanner/developer/git_docReview/#10-use-the-project-board-kanban-type","text":"Go to: https://github.com/orgs/romi/projects/10 link your PR to existing issues move the corresponding note to the appropriate column (To do / In progress / Test / Done)","title":"10. Use the project board (Kanban type)"},{"location":"Scanner/developer/git_submodules_workflow/","text":"Git submodules in plant-3d-vision Link We make use of git submodules in the plant-3d-vision repository to tightly control the version of the other ROMI libraries used as dependencies. To help its use we detail how to manage those submodules, especially how to update them. Getting started Link Clone the sources Link If you are joining the project start by cloning the sources: git clone https://github.com/romi/plant-3d-vision.git Initialize the submodules Link If you just cloned the repository or if the submodules folders ( romitask , romicgal , plantdb ...) are empty, you have to initialize the submodules in the plant-3d-vision folder with: cd plant-3d-vision git submodule init git submodule update You should now have submodules folders ( romitask , romicgal , plantdb ...) filled with the contents for the associated \"fixed commit\". To know the latest commit associated to a submodule, move to its folder and look-up the git log : cd plant-3d-vision/<submodule_root_dir> git log Tips Press key q to quit the log. Update the plant-3d-vision integration branch Link From the plant-3d-vision folder, checkout the dev branch (integration branch): git checkout dev git fetch git pull git submodule update git status Update the submodule branch Link To check the latest commit of a submodule do: git log To update a submodule branch cd <submodule_root_dir> git checkout <submodule_branch> git fetch git pull git status git log Create a branch for local merge Link Move to the root folder of the submodule you want to update: cd <submodule_root_dir> git checkout -b update/<submodule_name> For example to update romicgal : cd romicgal git checkout -b update/romicgal Commit the changes Link You may now commit the changes to plant-3d-vision : cd .. git checkout -b update/romicgal","title":"Git Submodules"},{"location":"Scanner/developer/git_submodules_workflow/#git-submodules-in-plant-3d-vision","text":"We make use of git submodules in the plant-3d-vision repository to tightly control the version of the other ROMI libraries used as dependencies. To help its use we detail how to manage those submodules, especially how to update them.","title":"Git submodules in plant-3d-vision"},{"location":"Scanner/developer/git_submodules_workflow/#getting-started","text":"","title":"Getting started"},{"location":"Scanner/developer/git_submodules_workflow/#clone-the-sources","text":"If you are joining the project start by cloning the sources: git clone https://github.com/romi/plant-3d-vision.git","title":"Clone the sources"},{"location":"Scanner/developer/git_submodules_workflow/#initialize-the-submodules","text":"If you just cloned the repository or if the submodules folders ( romitask , romicgal , plantdb ...) are empty, you have to initialize the submodules in the plant-3d-vision folder with: cd plant-3d-vision git submodule init git submodule update You should now have submodules folders ( romitask , romicgal , plantdb ...) filled with the contents for the associated \"fixed commit\". To know the latest commit associated to a submodule, move to its folder and look-up the git log : cd plant-3d-vision/<submodule_root_dir> git log Tips Press key q to quit the log.","title":"Initialize the submodules"},{"location":"Scanner/developer/git_submodules_workflow/#update-the-plant-3d-vision-integration-branch","text":"From the plant-3d-vision folder, checkout the dev branch (integration branch): git checkout dev git fetch git pull git submodule update git status","title":"Update the plant-3d-vision integration branch"},{"location":"Scanner/developer/git_submodules_workflow/#update-the-submodule-branch","text":"To check the latest commit of a submodule do: git log To update a submodule branch cd <submodule_root_dir> git checkout <submodule_branch> git fetch git pull git status git log","title":"Update the submodule branch"},{"location":"Scanner/developer/git_submodules_workflow/#create-a-branch-for-local-merge","text":"Move to the root folder of the submodule you want to update: cd <submodule_root_dir> git checkout -b update/<submodule_name> For example to update romicgal : cd romicgal git checkout -b update/romicgal","title":"Create a branch for local merge"},{"location":"Scanner/developer/git_submodules_workflow/#commit-the-changes","text":"You may now commit the changes to plant-3d-vision : cd .. git checkout -b update/romicgal","title":"Commit the changes"},{"location":"Scanner/developer/git_workflow/","text":"Git Workflow Link Many workflows are possible when using a version control system like git . To clarify how we use it in the ROMI project we hereafter details our choices and show ho to performs the most basic tasks to participate in the development of the ROMI libraries. Rules Link Here are some very important rules, be sure to understand them first! NEVER EVER work on master or dev , always on a branch! ALWAYS rebase your destination branch onto the one you want to merge before doing it! dev is the integration branch, master is the release branch Clone & configure the repository Link It all starts by cloning the repository you want to contribute to, e.g. plant3dvision : git clone https://github.com/romi/plant3dvision.git # clone the repository To use all possible git actions on this repository ('repo'), go the location of this local clone cd plant3dvision #the repo is cloned at the point where you executed the previous command (git clone). If you moved the clone repo, prefix with path like: cd path/to/yourcloned/plant3dvision Create development branch Link To contribute to development you have to create a branch on which you will work. Let's start by pulling the latest developments by updating our local dev branch git checkout dev # switch to your -local- `dev` branch git fetch # fetch changes from remote (`origin/dev`) git pull # pull changes (if any) from remote to local Then create your new branch <my_branch> and set tracking to remote central repo ( origin/<my_branch> ) git checkout -b <my_branch> # create local branch `<my_branch>` git push --set-upstream origin <my_branch> # attach local branch `<my_branch>` to remote `origin/<my_branch>`. Login/password will be asked for. Note Setting the branch tracking can be done later, even after committing changes to local repository! Work on your modifications Link We advise to use a proper IDE (like PyCharm or Atom) with an integrated or plugin based git tool for this part, as manually adding a lot of files can be time-consuming. Overall you will benefit from a nicer and faster integration of this particular step. Nevertheless, for the sake of clarity, hereafter we detail how to do that with the git command-line interface. Tracking new files Link If you create a new file, you will have to tell git to track its changes with: git add <my_new_file.py> Adding changes to local repository Link After editing your files ( e.g. <my_file1.py> <my_file2.py> ), tell git to validate the changes to these files by adding them to the list of tracked changes with: git add <my_file1.py> <my_file2.py> Then commit them to your local repository: git commit -m \"This is my awesome commit!\" Pushing changes to remote repository Link Once you are satisfied with the state of your work, you can push the locally committed changes to the remote central repository: git push # push modification to `origin/<my_branch>` Important Try to do this add/commit/push sequence as often as you can!! How do I not forget changes for committing ? Link Commits that affect only a limited number of files are preferable to track changes and history. However, some work require to modify several files. Especially when working with an IDE allowing easy exploring and modifications of all the repository content, you may forget some changes you did. To quickly identify all current files with uncommitted changes in your current branch, just simply check with git status on your local branch. The terminal lists in a red color all files requiring an action (git add, git restore, git remove): modified files, deleted files, newly added files. Note after acting on listed red files, typing git status should turn all previous files in green. Your branch is ready for committing. Then just proceed as above with git commit and git push . check your commit has been pushed Link After git push , you can get the list of pushed commits related to your current branch with git log (press q to exit the list in the terminal). Prepare your work for merging Link Once you are ready for creating a \"Pull Request\", let's update ( rebase in git) our branch with potential remote changes ( origin/dev ) since branching occurred. Important Start with step 1 & 2 and performs step 3 only if the branch where you are trying to integrate your work have diverged! Get the latest version of origin/dev : git checkout dev # checkout your local `dev` branch git fetch # fetch remote changes git pull # pull remote changes (if any) to local Rebase of origin/dev onto <my_branch> : git checkout <my_branch> # checkout the branch to rebase git rebase dev # rebase `dev` branch onto `<my_branch>` If dev has diverged during your work: if you have conflicts: fix them using an IDE say to git that conflicts are resolved ( e.g. for <my_file1.py> ): git add <my_file1.py> continue rebase until all changes are applied: git rebase --continue # to finish rebase push all changes (your rebased modifications) to the remote repository: git push -f origin <my_branch> Warning Using the -f option is necessary after a rebase as local and remote are now different (as show by a git status ). This will force push the changes done after rebasing. Then you can create your \"Pull Request\" from this latest commit using the GitHub interface. Don't forget to add reviewer. Now if you have a CI job checking the instability and performing tests, you may have to wait it all goes well before merging. Finalization: delete integrated branches. Link Check if you find all yours commits on origin/dev (either on GitHub interface of using git log in this branch), if yes: git branch --delete my_branch # delete local development branch git push origin :my_branch # delete development branch on origin Note git branch -a lists all the local branches first (with the current branch in green), followed by the remote-tracking branches in red. git branch only lists your local branches. Revert a commit Link You can revert the last commit with: git reset HEAD^ Update the project board (Kanban type) Link Go to: https://github.com/orgs/romi/projects choose the project board corresponding to your pull request (PR) link your PR to existing issues move the corresponding note to the appropriate column","title":"Git Workflow"},{"location":"Scanner/developer/git_workflow/#git-workflow","text":"Many workflows are possible when using a version control system like git . To clarify how we use it in the ROMI project we hereafter details our choices and show ho to performs the most basic tasks to participate in the development of the ROMI libraries.","title":"Git Workflow"},{"location":"Scanner/developer/git_workflow/#rules","text":"Here are some very important rules, be sure to understand them first! NEVER EVER work on master or dev , always on a branch! ALWAYS rebase your destination branch onto the one you want to merge before doing it! dev is the integration branch, master is the release branch","title":"Rules"},{"location":"Scanner/developer/git_workflow/#clone-configure-the-repository","text":"It all starts by cloning the repository you want to contribute to, e.g. plant3dvision : git clone https://github.com/romi/plant3dvision.git # clone the repository To use all possible git actions on this repository ('repo'), go the location of this local clone cd plant3dvision #the repo is cloned at the point where you executed the previous command (git clone). If you moved the clone repo, prefix with path like: cd path/to/yourcloned/plant3dvision","title":"Clone &amp; configure the repository"},{"location":"Scanner/developer/git_workflow/#create-development-branch","text":"To contribute to development you have to create a branch on which you will work. Let's start by pulling the latest developments by updating our local dev branch git checkout dev # switch to your -local- `dev` branch git fetch # fetch changes from remote (`origin/dev`) git pull # pull changes (if any) from remote to local Then create your new branch <my_branch> and set tracking to remote central repo ( origin/<my_branch> ) git checkout -b <my_branch> # create local branch `<my_branch>` git push --set-upstream origin <my_branch> # attach local branch `<my_branch>` to remote `origin/<my_branch>`. Login/password will be asked for. Note Setting the branch tracking can be done later, even after committing changes to local repository!","title":"Create development branch"},{"location":"Scanner/developer/git_workflow/#work-on-your-modifications","text":"We advise to use a proper IDE (like PyCharm or Atom) with an integrated or plugin based git tool for this part, as manually adding a lot of files can be time-consuming. Overall you will benefit from a nicer and faster integration of this particular step. Nevertheless, for the sake of clarity, hereafter we detail how to do that with the git command-line interface.","title":"Work on your modifications"},{"location":"Scanner/developer/git_workflow/#tracking-new-files","text":"If you create a new file, you will have to tell git to track its changes with: git add <my_new_file.py>","title":"Tracking new files"},{"location":"Scanner/developer/git_workflow/#adding-changes-to-local-repository","text":"After editing your files ( e.g. <my_file1.py> <my_file2.py> ), tell git to validate the changes to these files by adding them to the list of tracked changes with: git add <my_file1.py> <my_file2.py> Then commit them to your local repository: git commit -m \"This is my awesome commit!\"","title":"Adding changes to local repository"},{"location":"Scanner/developer/git_workflow/#pushing-changes-to-remote-repository","text":"Once you are satisfied with the state of your work, you can push the locally committed changes to the remote central repository: git push # push modification to `origin/<my_branch>` Important Try to do this add/commit/push sequence as often as you can!!","title":"Pushing changes to remote repository"},{"location":"Scanner/developer/git_workflow/#how-do-i-not-forget-changes-for-committing","text":"Commits that affect only a limited number of files are preferable to track changes and history. However, some work require to modify several files. Especially when working with an IDE allowing easy exploring and modifications of all the repository content, you may forget some changes you did. To quickly identify all current files with uncommitted changes in your current branch, just simply check with git status on your local branch. The terminal lists in a red color all files requiring an action (git add, git restore, git remove): modified files, deleted files, newly added files. Note after acting on listed red files, typing git status should turn all previous files in green. Your branch is ready for committing. Then just proceed as above with git commit and git push .","title":"How do I not forget changes for committing ?"},{"location":"Scanner/developer/git_workflow/#check-your-commit-has-been-pushed","text":"After git push , you can get the list of pushed commits related to your current branch with git log (press q to exit the list in the terminal).","title":"check your commit has been pushed"},{"location":"Scanner/developer/git_workflow/#prepare-your-work-for-merging","text":"Once you are ready for creating a \"Pull Request\", let's update ( rebase in git) our branch with potential remote changes ( origin/dev ) since branching occurred. Important Start with step 1 & 2 and performs step 3 only if the branch where you are trying to integrate your work have diverged! Get the latest version of origin/dev : git checkout dev # checkout your local `dev` branch git fetch # fetch remote changes git pull # pull remote changes (if any) to local Rebase of origin/dev onto <my_branch> : git checkout <my_branch> # checkout the branch to rebase git rebase dev # rebase `dev` branch onto `<my_branch>` If dev has diverged during your work: if you have conflicts: fix them using an IDE say to git that conflicts are resolved ( e.g. for <my_file1.py> ): git add <my_file1.py> continue rebase until all changes are applied: git rebase --continue # to finish rebase push all changes (your rebased modifications) to the remote repository: git push -f origin <my_branch> Warning Using the -f option is necessary after a rebase as local and remote are now different (as show by a git status ). This will force push the changes done after rebasing. Then you can create your \"Pull Request\" from this latest commit using the GitHub interface. Don't forget to add reviewer. Now if you have a CI job checking the instability and performing tests, you may have to wait it all goes well before merging.","title":"Prepare your work for merging"},{"location":"Scanner/developer/git_workflow/#finalization-delete-integrated-branches","text":"Check if you find all yours commits on origin/dev (either on GitHub interface of using git log in this branch), if yes: git branch --delete my_branch # delete local development branch git push origin :my_branch # delete development branch on origin Note git branch -a lists all the local branches first (with the current branch in green), followed by the remote-tracking branches in red. git branch only lists your local branches.","title":"Finalization: delete integrated branches."},{"location":"Scanner/developer/git_workflow/#revert-a-commit","text":"You can revert the last commit with: git reset HEAD^","title":"Revert a commit"},{"location":"Scanner/developer/git_workflow/#update-the-project-board-kanban-type","text":"Go to: https://github.com/orgs/romi/projects choose the project board corresponding to your pull request (PR) link your PR to existing issues move the corresponding note to the appropriate column","title":"Update the project board (Kanban type)"},{"location":"Scanner/developer/pipeline_repeatability/","text":"Testing the reconstruction pipelines repeatability Link Objective Link The reconstruction pipeline aims to convert the series of RGB images output of the plant-imager to a reconstructed 3d object, here a plant, with the ultimate goal to obtain quantitative phenotype information. It is composed of a sequence of different tasks, each having a specific function. Some algorithms used during these tasks may be stochastic, hence their output might vary even tough we provide the same input. As it can impact the results of the analysis and is often not easily traceable, it is of interest to be able to quantify it. Two main things can be tested: * With a proper metric, estimate how much different the outputs of a task can be given the same input. * Evaluation of the repercussion in the phenotypic traits extraction of a pipeline including randomness. Prerequisite Link install romi plant-3d-vision (from source or using a docker image ) & read install procedure Create and activate isolated python environment (see the procedure here ) Step-by-step tutorial Link The robustness_comparison script has been developed to quantify randomness in the pipeline and has 2 modes, it can either test the stochasticity of one task or of the full pipeline. Basically it compares outputs of a task given the same input (previous task output or acquisition output depending on the mode) on a fixed parameterizable number of replicates. robustness_comparison -h usage: robustness_comparison [-h] [-n REPLICATE_NUMBER] [-f] [-np] [-db TEST_DATABASE] [--models MODELS] scan {Clean,Colmap,Undistorted,Masks,Segmentation2D,Voxels,PointCloud,TriangleMesh,CurveSkeleton,TreeGraph,AnglesAndInternodes,Segmentation2d,SegmentedPointCloud,ClusteredMesh,OrganSegmentation} config_file ROMI reconstruction & analysis pipeline repeatability test procedure. Analyse the repeatability of a reconstruction & analysis pipeline by: 1. duplicating the scan in a temporary folder (and cleaning it if necessary) 2. running the pipeline up to the previous task of the task to test 3. copying this result to a new database and replicate the dataset 4. repeating the task to test for each replicate 5. comparing the results pair by pair. Comparison can be done at the scale of the files but also with metrics if a reference can be set. To create fully independent tests, we run the pipeline up to the task to test on each replicate. Note that in order to use the ML pipeline, you will first have to: 1. create an output directory 2. use the `--models` argument to copy the CNN trained models required to run the pipeline. positional arguments: scan scan to use for repeatability analysis {Clean,Colmap,Undistorted,Masks,Segmentation2D,Voxels,PointCloud,TriangleMesh,CurveSkeleton,TreeGraph,AnglesAndInternodes,Segmentation2d,SegmentedPointCloud,ClusteredMesh,OrganSegmentation} task to test, should be in: Clean, Colmap, Undistorted, Masks, Segmentation2D, Voxels, PointCloud, TriangleMesh, CurveSkeleton, TreeGraph, AnglesAndInternodes, Segmentation2d, SegmentedPointCloud, ClusteredMesh, OrganSegmentation config_file path to the TOML config file of the analysis pipeline optional arguments: -h, --help show this help message and exit -n REPLICATE_NUMBER, --replicate_number REPLICATE_NUMBER number of replicate to use for repeatability analysis -f, --full_pipe run the analysis pipeline on each replicate independently -np, --no_pipeline do not run the pipeline, only compare tasks outputs -db TEST_DATABASE, --test_database TEST_DATABASE test database location to use. Use at your own risks! --models MODELS models database location to use with ML pipeline. The metrics used are the same as the ones for an evaluation against ground truth 1. Test of a single task Link Example with the task TriangleMesh (whose goal is to compute a mesh from a point cloud): robustness_comparison /path/db/scan TriangleMesh plant-3d-vision/config/pipeline.toml -n 10 Resulting: path/ \u251c\u2500\u2500 20210628123840_rep_test_TriangleMesh/ \u2502 \u251c\u2500\u2500 scan_0 \u2502 \u251c\u2500\u2500 scan_1 \u2502 \u251c\u2500\u2500 scan_2 \u2502 \u251c\u2500\u2500 scan_3 \u2502 \u251c\u2500\u2500 scan_4 \u2502 \u251c\u2500\u2500 scan_5 \u2502 \u251c\u2500\u2500 scan_6 \u2502 \u251c\u2500\u2500 scan_7 \u2502 \u251c\u2500\u2500 scan_8 \u2502 \u251c\u2500\u2500 scan_9 \u2502 \u251c\u2500\u2500 filebyfile_comparison.json \u2502 \u251c\u2500\u2500 romidb \u2502 \u251c\u2500\u2500 TriangleMesh_comparison.json \u2514\u2500\u2500 db/scan The scan datasets are identical up to PointCloud then the TriangleMesh task is run separately on each one. Results with the appropriate metric are in the TriangleMesh_comparison.json file. 2. Independent tests Link If the goal is to see what are the impacts of randomness through the pipeline in the output of the task TriangleMesh , perform an independent test thanks to the -f parameter: robustness_comparison /path/db/scan TriangleMesh plant-3d-vision/config/pipeline.toml -n 10 -f With a similar tree result: path/ \u251c\u2500\u2500 20210628123840_rep_test_TriangleMesh/ \u2502 \u251c\u2500\u2500 scan_0 \u2502 \u251c\u2500\u2500 scan_1 \u2502 \u251c\u2500\u2500 scan_2 \u2502 \u251c\u2500\u2500 scan_3 \u2502 \u251c\u2500\u2500 scan_4 \u2502 \u251c\u2500\u2500 scan_5 \u2502 \u251c\u2500\u2500 scan_6 \u2502 \u251c\u2500\u2500 scan_7 \u2502 \u251c\u2500\u2500 scan_8 \u2502 \u251c\u2500\u2500 scan_9 \u2502 \u251c\u2500\u2500 filebyfile_comparison.json \u2502 \u251c\u2500\u2500 romidb \u2502 \u251c\u2500\u2500 TriangleMesh_comparison.json \u2514\u2500\u2500 db/scan Note To run tests on an existing database the -db parameter is configurable but be careful of what is to be tested","title":"Pipelines repeatability"},{"location":"Scanner/developer/pipeline_repeatability/#testing-the-reconstruction-pipelines-repeatability","text":"","title":"Testing the reconstruction pipelines repeatability"},{"location":"Scanner/developer/pipeline_repeatability/#objective","text":"The reconstruction pipeline aims to convert the series of RGB images output of the plant-imager to a reconstructed 3d object, here a plant, with the ultimate goal to obtain quantitative phenotype information. It is composed of a sequence of different tasks, each having a specific function. Some algorithms used during these tasks may be stochastic, hence their output might vary even tough we provide the same input. As it can impact the results of the analysis and is often not easily traceable, it is of interest to be able to quantify it. Two main things can be tested: * With a proper metric, estimate how much different the outputs of a task can be given the same input. * Evaluation of the repercussion in the phenotypic traits extraction of a pipeline including randomness.","title":"Objective"},{"location":"Scanner/developer/pipeline_repeatability/#prerequisite","text":"install romi plant-3d-vision (from source or using a docker image ) & read install procedure Create and activate isolated python environment (see the procedure here )","title":"Prerequisite"},{"location":"Scanner/developer/pipeline_repeatability/#step-by-step-tutorial","text":"The robustness_comparison script has been developed to quantify randomness in the pipeline and has 2 modes, it can either test the stochasticity of one task or of the full pipeline. Basically it compares outputs of a task given the same input (previous task output or acquisition output depending on the mode) on a fixed parameterizable number of replicates. robustness_comparison -h usage: robustness_comparison [-h] [-n REPLICATE_NUMBER] [-f] [-np] [-db TEST_DATABASE] [--models MODELS] scan {Clean,Colmap,Undistorted,Masks,Segmentation2D,Voxels,PointCloud,TriangleMesh,CurveSkeleton,TreeGraph,AnglesAndInternodes,Segmentation2d,SegmentedPointCloud,ClusteredMesh,OrganSegmentation} config_file ROMI reconstruction & analysis pipeline repeatability test procedure. Analyse the repeatability of a reconstruction & analysis pipeline by: 1. duplicating the scan in a temporary folder (and cleaning it if necessary) 2. running the pipeline up to the previous task of the task to test 3. copying this result to a new database and replicate the dataset 4. repeating the task to test for each replicate 5. comparing the results pair by pair. Comparison can be done at the scale of the files but also with metrics if a reference can be set. To create fully independent tests, we run the pipeline up to the task to test on each replicate. Note that in order to use the ML pipeline, you will first have to: 1. create an output directory 2. use the `--models` argument to copy the CNN trained models required to run the pipeline. positional arguments: scan scan to use for repeatability analysis {Clean,Colmap,Undistorted,Masks,Segmentation2D,Voxels,PointCloud,TriangleMesh,CurveSkeleton,TreeGraph,AnglesAndInternodes,Segmentation2d,SegmentedPointCloud,ClusteredMesh,OrganSegmentation} task to test, should be in: Clean, Colmap, Undistorted, Masks, Segmentation2D, Voxels, PointCloud, TriangleMesh, CurveSkeleton, TreeGraph, AnglesAndInternodes, Segmentation2d, SegmentedPointCloud, ClusteredMesh, OrganSegmentation config_file path to the TOML config file of the analysis pipeline optional arguments: -h, --help show this help message and exit -n REPLICATE_NUMBER, --replicate_number REPLICATE_NUMBER number of replicate to use for repeatability analysis -f, --full_pipe run the analysis pipeline on each replicate independently -np, --no_pipeline do not run the pipeline, only compare tasks outputs -db TEST_DATABASE, --test_database TEST_DATABASE test database location to use. Use at your own risks! --models MODELS models database location to use with ML pipeline. The metrics used are the same as the ones for an evaluation against ground truth","title":"Step-by-step tutorial"},{"location":"Scanner/developer/pipeline_repeatability/#1-test-of-a-single-task","text":"Example with the task TriangleMesh (whose goal is to compute a mesh from a point cloud): robustness_comparison /path/db/scan TriangleMesh plant-3d-vision/config/pipeline.toml -n 10 Resulting: path/ \u251c\u2500\u2500 20210628123840_rep_test_TriangleMesh/ \u2502 \u251c\u2500\u2500 scan_0 \u2502 \u251c\u2500\u2500 scan_1 \u2502 \u251c\u2500\u2500 scan_2 \u2502 \u251c\u2500\u2500 scan_3 \u2502 \u251c\u2500\u2500 scan_4 \u2502 \u251c\u2500\u2500 scan_5 \u2502 \u251c\u2500\u2500 scan_6 \u2502 \u251c\u2500\u2500 scan_7 \u2502 \u251c\u2500\u2500 scan_8 \u2502 \u251c\u2500\u2500 scan_9 \u2502 \u251c\u2500\u2500 filebyfile_comparison.json \u2502 \u251c\u2500\u2500 romidb \u2502 \u251c\u2500\u2500 TriangleMesh_comparison.json \u2514\u2500\u2500 db/scan The scan datasets are identical up to PointCloud then the TriangleMesh task is run separately on each one. Results with the appropriate metric are in the TriangleMesh_comparison.json file.","title":"1. Test of a single task"},{"location":"Scanner/developer/pipeline_repeatability/#2-independent-tests","text":"If the goal is to see what are the impacts of randomness through the pipeline in the output of the task TriangleMesh , perform an independent test thanks to the -f parameter: robustness_comparison /path/db/scan TriangleMesh plant-3d-vision/config/pipeline.toml -n 10 -f With a similar tree result: path/ \u251c\u2500\u2500 20210628123840_rep_test_TriangleMesh/ \u2502 \u251c\u2500\u2500 scan_0 \u2502 \u251c\u2500\u2500 scan_1 \u2502 \u251c\u2500\u2500 scan_2 \u2502 \u251c\u2500\u2500 scan_3 \u2502 \u251c\u2500\u2500 scan_4 \u2502 \u251c\u2500\u2500 scan_5 \u2502 \u251c\u2500\u2500 scan_6 \u2502 \u251c\u2500\u2500 scan_7 \u2502 \u251c\u2500\u2500 scan_8 \u2502 \u251c\u2500\u2500 scan_9 \u2502 \u251c\u2500\u2500 filebyfile_comparison.json \u2502 \u251c\u2500\u2500 romidb \u2502 \u251c\u2500\u2500 TriangleMesh_comparison.json \u2514\u2500\u2500 db/scan Note To run tests on an existing database the -db parameter is configurable but be careful of what is to be tested","title":"2. Independent tests"},{"location":"Scanner/developer/tutorial_template/","text":"How to make a good romi tutorial ? Link Format : starts with \"How\", is a question, should be focused to a user-oriented question content : try to build a streamline procedure. If the procedure is too complicated (like different case scenarios, if you need to explain 'if you want to ... and/or if you want to...' ), consider splitting your tutorial in as many tutorials as needed to obtain a streamline procedure. Here a suggested sections for our romi tutorials: Objectives Link At the end of this tutorial, you should be able to: write a good tutorial review others' tutorials Prerequisite Link Installations link(s) to other tutorials Short theoretical primer Link (if needed) Step-by-step tutorial Link 1. This is step 1 Link For non-computer experts, please remember to provide detailed information each time a command line is needed: explain if a new terminal should be open or not ; precise from where the command(s) should be executed ; indicate whether a particular isolated environment should be activated (e.g. conda environment) ; give precise and detailed command lines needed (make sure that all commands are given) cd my repo #precise from where the command should be executed command 1 #code comments are welcome ! command 2 # this is for.... Provide ideas of the expected results to check that the commands were successfully run. examples: a new folder/object is created the terminal says \"useful information\" 2. This is step 2 Link","title":"Make a tutorial"},{"location":"Scanner/developer/tutorial_template/#how-to-make-a-good-romi-tutorial","text":"Format : starts with \"How\", is a question, should be focused to a user-oriented question content : try to build a streamline procedure. If the procedure is too complicated (like different case scenarios, if you need to explain 'if you want to ... and/or if you want to...' ), consider splitting your tutorial in as many tutorials as needed to obtain a streamline procedure. Here a suggested sections for our romi tutorials:","title":"How to make a good romi tutorial ?"},{"location":"Scanner/developer/tutorial_template/#objectives","text":"At the end of this tutorial, you should be able to: write a good tutorial review others' tutorials","title":"Objectives"},{"location":"Scanner/developer/tutorial_template/#prerequisite","text":"Installations link(s) to other tutorials","title":"Prerequisite"},{"location":"Scanner/developer/tutorial_template/#short-theoretical-primer","text":"(if needed)","title":"Short theoretical primer"},{"location":"Scanner/developer/tutorial_template/#step-by-step-tutorial","text":"","title":"Step-by-step tutorial"},{"location":"Scanner/developer/tutorial_template/#1-this-is-step-1","text":"For non-computer experts, please remember to provide detailed information each time a command line is needed: explain if a new terminal should be open or not ; precise from where the command(s) should be executed ; indicate whether a particular isolated environment should be activated (e.g. conda environment) ; give precise and detailed command lines needed (make sure that all commands are given) cd my repo #precise from where the command should be executed command 1 #code comments are welcome ! command 2 # this is for.... Provide ideas of the expected results to check that the commands were successfully run. examples: a new folder/object is created the terminal says \"useful information\"","title":"1. This is step 1"},{"location":"Scanner/developer/tutorial_template/#2-this-is-step-2","text":"","title":"2. This is step 2"},{"location":"Scanner/developer/user_case_scenarios/","text":"User case scenarios for the plant scanner Link (or the biologists wish list!) Last edited on 7 Nov 2018. We here describe the scenario followed by a biologist experimenter to acquire and reconstruct the plant architecture using the phenotyping station or 3D scanner. Minimal scenario: command line interface (CLI) Link We here define a minimal scenario using simple CLI to develop and test the workflow. the experimenter prepares the plant if needed; the plant is placed (upright) at the centre of the phenotyping station; the experimenter checks the JSON file defining biological metadata the experimenter starts a \"circular scan\" using a CLI: images are acquired and saved locally (computer controlling the 3D scanner); they are later organised using the Database API; the reconstruction is automatically started after the previous step; Final scenario: graphical user interface (GUI) Link This is the final scenario we want to set up. the experimenter prepares the plant if needed; the plant is placed (upright) at the centre of the phenotyping station; the experimenter login to the GUI, if not done already; under the \"metadata\" tab, the experimenter defines the biological metadata related to its plan and should be able to check the used hardware metadata : he or she defines the biological metadata using predefined fields and values (should be possible to add more); he or she validates by clicking a \"save\" button; under the \"acquisition\" tab, the experimenter defines the acquisition method and settings (the type of scan, number of images, ...) and initiate the acquisition: he or she defines the acquisition settings using predefined fields and values (should be possible to add more); he or she starts this step by clicking an \"acquire\" button; under the \"reconstruction\" tab, the experimenter can access a list of datasets (he owns or accessible to him) and initiate reconstruction(s): he or she can select one dataset (or more?), he or she selects a 3D reconstruction method, he or she defines its settings, he or she starts this step by clicking a \"reconstruct\" button; under the \"quantification\" tab, the experimenter can access a list of 3D structures (he owns or accessible to him) and initiate quantification(s): he or she can select one 3D structure (or more?), he or she selects a quantification method, he or she defines its settings, he or she starts this step by clicking a \"quantify\" button; We would probably need an \"OMERO\" tab to: select/change the \"group\" to which the dataset should be sent to; change the URL and port (change the used OMERO database); change the user logged to the OMERO database;","title":"User case scenarios"},{"location":"Scanner/developer/user_case_scenarios/#user-case-scenarios-for-the-plant-scanner","text":"(or the biologists wish list!) Last edited on 7 Nov 2018. We here describe the scenario followed by a biologist experimenter to acquire and reconstruct the plant architecture using the phenotyping station or 3D scanner.","title":"User case scenarios for the plant scanner"},{"location":"Scanner/developer/user_case_scenarios/#minimal-scenario-command-line-interface-cli","text":"We here define a minimal scenario using simple CLI to develop and test the workflow. the experimenter prepares the plant if needed; the plant is placed (upright) at the centre of the phenotyping station; the experimenter checks the JSON file defining biological metadata the experimenter starts a \"circular scan\" using a CLI: images are acquired and saved locally (computer controlling the 3D scanner); they are later organised using the Database API; the reconstruction is automatically started after the previous step;","title":"Minimal scenario: command line interface (CLI)"},{"location":"Scanner/developer/user_case_scenarios/#final-scenario-graphical-user-interface-gui","text":"This is the final scenario we want to set up. the experimenter prepares the plant if needed; the plant is placed (upright) at the centre of the phenotyping station; the experimenter login to the GUI, if not done already; under the \"metadata\" tab, the experimenter defines the biological metadata related to its plan and should be able to check the used hardware metadata : he or she defines the biological metadata using predefined fields and values (should be possible to add more); he or she validates by clicking a \"save\" button; under the \"acquisition\" tab, the experimenter defines the acquisition method and settings (the type of scan, number of images, ...) and initiate the acquisition: he or she defines the acquisition settings using predefined fields and values (should be possible to add more); he or she starts this step by clicking an \"acquire\" button; under the \"reconstruction\" tab, the experimenter can access a list of datasets (he owns or accessible to him) and initiate reconstruction(s): he or she can select one dataset (or more?), he or she selects a 3D reconstruction method, he or she defines its settings, he or she starts this step by clicking a \"reconstruct\" button; under the \"quantification\" tab, the experimenter can access a list of 3D structures (he owns or accessible to him) and initiate quantification(s): he or she can select one 3D structure (or more?), he or she selects a quantification method, he or she defines its settings, he or she starts this step by clicking a \"quantify\" button; We would probably need an \"OMERO\" tab to: select/change the \"group\" to which the dataset should be sent to; change the URL and port (change the used OMERO database); change the user logged to the OMERO database;","title":"Final scenario: graphical user interface (GUI)"},{"location":"Scanner/docker/","text":"Docker containers for ROMI Link The official dockerhub repository for the ROMI project is: https://hub.docker.com/orgs/roboticsmicrofarms/repositories List of docker containers Link We hereafter list the docker containers, their availability and provides link to their location & usage instructions: plantdb is available here and explanations there plantimager is not available yet and explanations there plant-3d-vision is not available yet and explanations there plant-3d-explorer is available here and explanations there Use cases with docker-compose Link In this section we reference the \"real-life\" use cases of our software. Use the plant 3d explorer on a local database directory Link The easiest way to use the plant-3d-explorer on a local database directory without installing the ROMI libraries (and their dependencies) is to use the pre-built docker image and add a docker-compose YAML recipe. See here for more details. DockerHub Link roboticsmicrofarms Link The Docker hub repository for the ROMI project is here: https://hub.docker.com/orgs/roboticsmicrofarms Colmap Link Docker images for the Colmap open source project: https://hub.docker.com/r/colmap/colmap nvidia/cuda with Colmap - (compatible with Driver Version: 418.67 CUDA Version: 10.1) https://hub.docker.com/r/geki/colmap","title":"Home"},{"location":"Scanner/docker/#docker-containers-for-romi","text":"The official dockerhub repository for the ROMI project is: https://hub.docker.com/orgs/roboticsmicrofarms/repositories","title":"Docker containers for ROMI"},{"location":"Scanner/docker/#list-of-docker-containers","text":"We hereafter list the docker containers, their availability and provides link to their location & usage instructions: plantdb is available here and explanations there plantimager is not available yet and explanations there plant-3d-vision is not available yet and explanations there plant-3d-explorer is available here and explanations there","title":"List of docker containers"},{"location":"Scanner/docker/#use-cases-with-docker-compose","text":"In this section we reference the \"real-life\" use cases of our software.","title":"Use cases with docker-compose"},{"location":"Scanner/docker/#use-the-plant-3d-explorer-on-a-local-database-directory","text":"The easiest way to use the plant-3d-explorer on a local database directory without installing the ROMI libraries (and their dependencies) is to use the pre-built docker image and add a docker-compose YAML recipe. See here for more details.","title":"Use the plant 3d explorer on a local database directory"},{"location":"Scanner/docker/#dockerhub","text":"","title":"DockerHub"},{"location":"Scanner/docker/#roboticsmicrofarms","text":"The Docker hub repository for the ROMI project is here: https://hub.docker.com/orgs/roboticsmicrofarms","title":"roboticsmicrofarms"},{"location":"Scanner/docker/#colmap","text":"Docker images for the Colmap open source project: https://hub.docker.com/r/colmap/colmap nvidia/cuda with Colmap - (compatible with Driver Version: 418.67 CUDA Version: 10.1) https://hub.docker.com/r/geki/colmap","title":"Colmap"},{"location":"Scanner/docker/docker_compose/","text":"Docker compose Link In the following sections we will propose several use cases combining docker images thanks to docker-compose . Note You need docker-compose installed, see here . Database & plant 3D explorer Link To use your own local database, we provide a docker compose recipe that: start a database container using roboticsmicrofarms/plantdb start a plant-3d-explorer container using roboticsmicrofarms/plant-3d-explorer Use pre-built docker image Link You can use the pre-built images plantdb & plantviewer , accessible from the ROMI dockerhub, to easily test & use the plant 3d explorer with your own database 1 . The docker-compose.yml look like this: version : '3' services : db : image : \"roboticsmicrofarms/plantdb\" volumes : - ${ROMI_DB}:/home/scanner/db expose : - \"5000\" healthcheck : test : \"exit 0\" viewer : image : \"roboticsmicrofarms/plant-3d-explorer\" depends_on : - db environment : REACT_APP_API_URL : http://172.21.0.2:5000 ports : - \"3000:3000\" From the root directory of plant-3d-explorer containing the docker-compose.yml in a terminal: export ROMI_DB = <path/to/db> docker-compose up -d Important Do not forget to set the path to the database. Warning If you have other containers running it might not work since it assumes the plantdb container will have the 172.21.0.2 IP address! To stop the containers: docker-compose stop Note To use local builds, change the image YAML parameter to match your images names & tag. Force local builds Link To force builds at compose startup, for development or debugging purposes, use the build YAML parameter instead of image 2 . It is possible to keep the image YAML parameter to tag the built images 3 . The docker-compose.yml should look like this: version : '3' services : db : build : ../plantdb/. image : db:debug volumes : - ${ROMI_DB}:/home/scanner/db expose : - \"5000\" healthcheck : test : \"exit 0\" viewer : build : ../developer image : viewer:debug depends_on : - db environment : REACT_APP_API_URL : http://172.21.0.2:5000 ports : - \"3000:3000\" Warning This assumes that you have to plantdb repository cloned next to the one of plant-3d-explorer . https://docs.docker.com/compose/compose-file/#image \u21a9 https://docs.docker.com/compose/gettingstarted/ \u21a9 https://docs.docker.com/compose/compose-file/#build \u21a9","title":"Docker compose"},{"location":"Scanner/docker/docker_compose/#docker-compose","text":"In the following sections we will propose several use cases combining docker images thanks to docker-compose . Note You need docker-compose installed, see here .","title":"Docker compose"},{"location":"Scanner/docker/docker_compose/#database-plant-3d-explorer","text":"To use your own local database, we provide a docker compose recipe that: start a database container using roboticsmicrofarms/plantdb start a plant-3d-explorer container using roboticsmicrofarms/plant-3d-explorer","title":"Database &amp; plant 3D explorer"},{"location":"Scanner/docker/docker_compose/#use-pre-built-docker-image","text":"You can use the pre-built images plantdb & plantviewer , accessible from the ROMI dockerhub, to easily test & use the plant 3d explorer with your own database 1 . The docker-compose.yml look like this: version : '3' services : db : image : \"roboticsmicrofarms/plantdb\" volumes : - ${ROMI_DB}:/home/scanner/db expose : - \"5000\" healthcheck : test : \"exit 0\" viewer : image : \"roboticsmicrofarms/plant-3d-explorer\" depends_on : - db environment : REACT_APP_API_URL : http://172.21.0.2:5000 ports : - \"3000:3000\" From the root directory of plant-3d-explorer containing the docker-compose.yml in a terminal: export ROMI_DB = <path/to/db> docker-compose up -d Important Do not forget to set the path to the database. Warning If you have other containers running it might not work since it assumes the plantdb container will have the 172.21.0.2 IP address! To stop the containers: docker-compose stop Note To use local builds, change the image YAML parameter to match your images names & tag.","title":"Use pre-built docker image"},{"location":"Scanner/docker/docker_compose/#force-local-builds","text":"To force builds at compose startup, for development or debugging purposes, use the build YAML parameter instead of image 2 . It is possible to keep the image YAML parameter to tag the built images 3 . The docker-compose.yml should look like this: version : '3' services : db : build : ../plantdb/. image : db:debug volumes : - ${ROMI_DB}:/home/scanner/db expose : - \"5000\" healthcheck : test : \"exit 0\" viewer : build : ../developer image : viewer:debug depends_on : - db environment : REACT_APP_API_URL : http://172.21.0.2:5000 ports : - \"3000:3000\" Warning This assumes that you have to plantdb repository cloned next to the one of plant-3d-explorer . https://docs.docker.com/compose/compose-file/#image \u21a9 https://docs.docker.com/compose/gettingstarted/ \u21a9 https://docs.docker.com/compose/compose-file/#build \u21a9","title":"Force local builds"},{"location":"Scanner/docker/plant3dexplorer_docker/","text":"Docker container for ROMI plant 3d explorer Link The plant visualizer is a webapp that dialog with the database to display images & some quantitative traits. It is based on Ubuntu 18.04. Note that we tag the different versions, the default is to use the latest, but you can also specify a specific version by changing the value of the environment variable $VTAG , e.g. export VTAG=\"2.1\" . Look here for a list of available tags: https://hub.docker.com/repository/docker/roboticsmicrofarms/plantviewer Requirements Link The docker image does not contain any plant scans and does not come with a working ROMI local database. To quickly create an example DB you can use: wget https://db.romi-project.eu/models/test_db.tar.gz tar -xf test_db.tar.gz This will create a integration_tests folder with a ready to use test database. To create a local ROMI database: python package install, look here . plantdb docker image, look here . docker-compose YAML recipe (start both a plantdb & a plant-3d-explorer docker image connected to the db), look here . Use pre-built docker image Link You can easily download and start the pre-built plant-3d-explorer docker image with: docker run -p 3000 :3000 roboticsmicrofarms/plant-3d-explorer: $VTAG By default, the docker image will create a container pointing toward the official ROMI database https://db.romi-project.eu . To change that, e.g. to a local running database at '0.0.0.0', do 1 : docker run --env REACT_APP_API_URL = '0.0.0.0' -p 3000 :3000 roboticsmicrofarms/plant-3d-explorer: $VTAG Build docker image Link To build the image, from the plant-3d-explorer root directory, run: export VTAG = \"latest\" docker build -t roboticsmicrofarms/plant-3d-explorer: $VTAG . To start the container using the built image: docker run -p 3000 :3000 roboticsmicrofarms/plant-3d-explorer: $VTAG Once it's up and running, you should be able to access the viewer using a browser here: http://localhost:3000/ Note If you omit the -p 3000:3000 you can still access the interface using the docker ip, something like http://172.17.0.2:3000/ Important Use chrome as firefox has some issues with the used JavaScript libraries! Publish docker image Link To push it on the roboticsmicrofarms docker hub: docker push roboticsmicrofarms/plant-3d-explorer: $VTAG This requires a valid account and token on dockerhub! https://docs.docker.com/engine/reference/commandline/run/#set-environment-variables--e---env---env-file \u21a9","title":"plant-3d-explorer"},{"location":"Scanner/docker/plant3dexplorer_docker/#docker-container-for-romi-plant-3d-explorer","text":"The plant visualizer is a webapp that dialog with the database to display images & some quantitative traits. It is based on Ubuntu 18.04. Note that we tag the different versions, the default is to use the latest, but you can also specify a specific version by changing the value of the environment variable $VTAG , e.g. export VTAG=\"2.1\" . Look here for a list of available tags: https://hub.docker.com/repository/docker/roboticsmicrofarms/plantviewer","title":"Docker container for ROMI plant 3d explorer"},{"location":"Scanner/docker/plant3dexplorer_docker/#requirements","text":"The docker image does not contain any plant scans and does not come with a working ROMI local database. To quickly create an example DB you can use: wget https://db.romi-project.eu/models/test_db.tar.gz tar -xf test_db.tar.gz This will create a integration_tests folder with a ready to use test database. To create a local ROMI database: python package install, look here . plantdb docker image, look here . docker-compose YAML recipe (start both a plantdb & a plant-3d-explorer docker image connected to the db), look here .","title":"Requirements"},{"location":"Scanner/docker/plant3dexplorer_docker/#use-pre-built-docker-image","text":"You can easily download and start the pre-built plant-3d-explorer docker image with: docker run -p 3000 :3000 roboticsmicrofarms/plant-3d-explorer: $VTAG By default, the docker image will create a container pointing toward the official ROMI database https://db.romi-project.eu . To change that, e.g. to a local running database at '0.0.0.0', do 1 : docker run --env REACT_APP_API_URL = '0.0.0.0' -p 3000 :3000 roboticsmicrofarms/plant-3d-explorer: $VTAG","title":"Use pre-built docker image"},{"location":"Scanner/docker/plant3dexplorer_docker/#build-docker-image","text":"To build the image, from the plant-3d-explorer root directory, run: export VTAG = \"latest\" docker build -t roboticsmicrofarms/plant-3d-explorer: $VTAG . To start the container using the built image: docker run -p 3000 :3000 roboticsmicrofarms/plant-3d-explorer: $VTAG Once it's up and running, you should be able to access the viewer using a browser here: http://localhost:3000/ Note If you omit the -p 3000:3000 you can still access the interface using the docker ip, something like http://172.17.0.2:3000/ Important Use chrome as firefox has some issues with the used JavaScript libraries!","title":"Build docker image"},{"location":"Scanner/docker/plant3dexplorer_docker/#publish-docker-image","text":"To push it on the roboticsmicrofarms docker hub: docker push roboticsmicrofarms/plant-3d-explorer: $VTAG This requires a valid account and token on dockerhub! https://docs.docker.com/engine/reference/commandline/run/#set-environment-variables--e---env---env-file \u21a9","title":"Publish docker image"},{"location":"Scanner/docker/plantdb_docker/","text":"Docker container for ROMI database Link Important An existing local database directory is required , it will be mounted at container startup. To see how to create a local database directory, look here . Use pre-built docker image Link Assuming you have a valid ROMI database directory under /data/ROMI/DB , you can easily download and start the pre-built plantdb docker image with: export ROMI_DB = /data/ROMI/DB docker run -it -p 5000 :5000 \\ -v $ROMI_DB :/home/scanner/db \\ roboticsmicrofarms/plantdb:latest You should be able to access it here: http://localhost:5000/ Note -v $ROMI_DB:/home/scanner/db performs a bind mount to enable access to the local database by the docker image. See the official documentation . Build docker image Link We provide a convenience bash script to ease the build of plantdb docker image. You can choose to use this script OR to \"manually\" call the docker build command. Provided convenience build.sh script Link To build the image with the provided build script, from the plantdb/docker directory: ./build.sh You can also pass some options, use ./build.sh -h to get more details about usage, options and default values. Manually call the docker build command Link To build the image, from the plantdb root directory: export VTAG = \"latest\" docker build -t roboticsmicrofarms/plantdb: $VTAG . You can use the following optional arguments: --build-arg USER_NAME=<user> : change the default user in container; --build-arg PLANTDB_BRANCH=<git_branch> : change the cloned git branch from plantdb . Publish docker image Link Push it on docker hub: docker push roboticsmicrofarms/plantdb: $VTAG This requires a valid account & token on dockerhub! Usage Link Requirements Link To run it, you need to have a valid local ROMI database, look here for instructions and here for an example database. Starting the plantdb docker image Link Provided run.sh script Link To start the container with the provided run script in plantdb/docker , use: ./run.sh You can also pass some options, use ./run.sh -h to get more details about usage and options. Manually Link Assuming you extracted it in your home folder ( /home/$USER/integration_tests ), you can start the plantdb docker image with: docker run -it -p 5000 :5000 -v /home/ $USER /integration_tests:/home/scanner/db plantdb: $VTAG In both cases, you should see something like: n scans = 2 * Serving Flask app \"romi_scanner_rest_api\" (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit) Tip -v /home/$USER/integration_tests:/home/scanner/db performs a bind mount to enable access to the local database by the docker image. See the official Docker documentation . Accessing the REST API Link Once it's up, you should be able to access the REST API here: http://localhost:5000/ To access the REST API, open your favorite browser and use URLs to access: the list of all scans: http://0.0.0.0:5000/scans the '2018-12-17_17-05-35' dataset: http://0.0.0.0:5000/scans/2018-12-17_17-05-35 You should see JSON formatted text.","title":"Database"},{"location":"Scanner/docker/plantdb_docker/#docker-container-for-romi-database","text":"Important An existing local database directory is required , it will be mounted at container startup. To see how to create a local database directory, look here .","title":"Docker container for ROMI database"},{"location":"Scanner/docker/plantdb_docker/#use-pre-built-docker-image","text":"Assuming you have a valid ROMI database directory under /data/ROMI/DB , you can easily download and start the pre-built plantdb docker image with: export ROMI_DB = /data/ROMI/DB docker run -it -p 5000 :5000 \\ -v $ROMI_DB :/home/scanner/db \\ roboticsmicrofarms/plantdb:latest You should be able to access it here: http://localhost:5000/ Note -v $ROMI_DB:/home/scanner/db performs a bind mount to enable access to the local database by the docker image. See the official documentation .","title":"Use pre-built docker image"},{"location":"Scanner/docker/plantdb_docker/#build-docker-image","text":"We provide a convenience bash script to ease the build of plantdb docker image. You can choose to use this script OR to \"manually\" call the docker build command.","title":"Build docker image"},{"location":"Scanner/docker/plantdb_docker/#provided-convenience-buildsh-script","text":"To build the image with the provided build script, from the plantdb/docker directory: ./build.sh You can also pass some options, use ./build.sh -h to get more details about usage, options and default values.","title":"Provided convenience build.sh script"},{"location":"Scanner/docker/plantdb_docker/#manually-call-the-docker-build-command","text":"To build the image, from the plantdb root directory: export VTAG = \"latest\" docker build -t roboticsmicrofarms/plantdb: $VTAG . You can use the following optional arguments: --build-arg USER_NAME=<user> : change the default user in container; --build-arg PLANTDB_BRANCH=<git_branch> : change the cloned git branch from plantdb .","title":"Manually call the docker build command"},{"location":"Scanner/docker/plantdb_docker/#publish-docker-image","text":"Push it on docker hub: docker push roboticsmicrofarms/plantdb: $VTAG This requires a valid account & token on dockerhub!","title":"Publish docker image"},{"location":"Scanner/docker/plantdb_docker/#usage","text":"","title":"Usage"},{"location":"Scanner/docker/plantdb_docker/#requirements","text":"To run it, you need to have a valid local ROMI database, look here for instructions and here for an example database.","title":"Requirements"},{"location":"Scanner/docker/plantdb_docker/#starting-the-plantdb-docker-image","text":"","title":"Starting the plantdb docker image"},{"location":"Scanner/docker/plantdb_docker/#provided-runsh-script","text":"To start the container with the provided run script in plantdb/docker , use: ./run.sh You can also pass some options, use ./run.sh -h to get more details about usage and options.","title":"Provided run.sh script"},{"location":"Scanner/docker/plantdb_docker/#manually","text":"Assuming you extracted it in your home folder ( /home/$USER/integration_tests ), you can start the plantdb docker image with: docker run -it -p 5000 :5000 -v /home/ $USER /integration_tests:/home/scanner/db plantdb: $VTAG In both cases, you should see something like: n scans = 2 * Serving Flask app \"romi_scanner_rest_api\" (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit) Tip -v /home/$USER/integration_tests:/home/scanner/db performs a bind mount to enable access to the local database by the docker image. See the official Docker documentation .","title":"Manually"},{"location":"Scanner/docker/plantdb_docker/#accessing-the-rest-api","text":"Once it's up, you should be able to access the REST API here: http://localhost:5000/ To access the REST API, open your favorite browser and use URLs to access: the list of all scans: http://0.0.0.0:5000/scans the '2018-12-17_17-05-35' dataset: http://0.0.0.0:5000/scans/2018-12-17_17-05-35 You should see JSON formatted text.","title":"Accessing the REST API"},{"location":"Scanner/docker/plantimager_docker/","text":"Docker container for ROMI plant-imager Link","title":"Plant Imager"},{"location":"Scanner/docker/plantimager_docker/#docker-container-for-romi-plant-imager","text":"","title":"Docker container for ROMI plant-imager"},{"location":"Scanner/docker/plantinterpreter_docker/","text":"Docker container for ROMI plantinterpreter Link Important An existing local database directory is required , it will be mounted at container startup. To see how to create a local database directory, look here . Use pre-built docker image Link Assuming you have a valid ROMI database directory under /data/ROMI/DB , you can easily download and start the pre-built roboticsmicrofarms/plant3dvision docker image with: export ROMI_DB = /data/ROMI/DB docker run --runtime = nvidia --gpus all \\ --env PYOPENCL_CTX = '0' \\ -v $ROMI_DB :/home/scanner/db \\ -it roboticsmicrofarms/plant3dvision:latest This should start the latest pre-built roboticsmicrofarms/plant3dvision docker image in interactive mode. The database location inside the docker container is ~/db . Note -v $ROMI_DB:/home/scanner/db performs a bind mount to enable access to the local database by the docker image. See the official documentation . Build docker image Link We provide a convenience bash script to ease the build of roboticsmicrofarms/plant3dvision docker image. You can choose to use this script OR to \"manually\" call the docker build command. Provided convenience build.sh script Link To build the image with the provided build script, from the plant3dvision/docker directory: ./build.sh You can also pass some options, use ./build.sh -h to get more details about usage, options and default values. Tips To be sure to always pull the latest parent image, you may add the --pull option! Manually call the docker build command Link To build the image, from the plant3dvision/docker directory: export VTAG = \"latest\" docker build -t roboticsmicrofarms/plant3dvision: $VTAG . You can use the following optional arguments: --build-arg PLANT3DVISION_BRANCH=<git_branch> : change the cloned git branch from plant3dvision . --build-arg PLANTDB_BRANCH=<git_branch> : change the cloned git branch from plantdb . --build-arg PLANTIMAGER_BRANCH=<git_branch> : change the cloned git branch from plantimager . Publish docker image Link Push it on docker hub: docker push roboticsmicrofarms/plantdb: $VTAG This requires a valid account & token on dockerhub! Usage Link Requirements Link To run it, you need to have a valid local ROMI database, look here for instructions and here for an example database. Starting the plant3dvision docker image Link Provided run.sh script Link To start the container, in interactive mode, with the provided run script in plant3dvision/docker , use: ./run.sh You can also pass some options, use ./run.sh -h to get more details about usage and options, notably to mount a local romi database. NVIDIA GPU test Link To make sure the started container will be able to access the host GPU, use: ./run.sh --gpu_test Pipelines tests Link To performs test reconstructions, you have several possibilities: test the geometric pipeline: ./run.sh --geom_pipeline_test test the machine learning pipeline: ./run.sh --ml_pipeline_test test both pipelines: ./run.sh --pipeline_test Note This use test data & test models (for ML) provided with plant3dvision in plant3dvision/tests/testdata . Manually Link Assuming you have a valid ROMI database directory under /data/ROMI/DB , you can start the roboticsmicrofarms/plant3dvision docker image with: export ROMI_DB = /data/ROMI/DB docker run --runtime = nvidia --gpus all \\ --env PYOPENCL_CTX = '0' \\ -v $ROMI_DB :/home/scanner/db \\ -it roboticsmicrofarms/plant3dvision: $VTAG bash This should start the built roboticsmicrofarms/plant3dvision docker image in interactive mode. The database location inside the docker container is ~/db . Note that: you are using the docker image roboticsmicrofarms/plant3dvision:$VTAG you mount the host directory $ROMI_DB \"inside\" the running container in the ~/db directory you activate all GPUs within the container with --gpus all declaring the environment variable PYOPENCL_CTX='0' select the first CUDA GPU capable -it & bash returns an interactive bash shell You may want to name the running container (with --name <my_name> ) if you \"demonize\" it (with -d ). Executing a ROMI task Link Once you are inside the running docker container, you may call the ROMI tasks. romi_run_task AnglesAndInternodes db/<my_scan_000>/ --config plant3dvision/configs/original_pipe_0.toml Note You may have to source the .profile file before calling romi_run_task . You can give a command to execute at container start-up using the -c, --cmd option. For example: export ROMI_DB = /data/ROMI/DB ./run.sh -p $ROMI_DB -u scanner -c \"source .profile && romi_run_task AnglesAndInternodes db/<my_scan_000>/ --config plant3dvision/configs/original_pipe_0.toml\" Important source .profile is important to add .local/bin/ to the $PATH environment variable. If you don't do this, you might not be able to access the romi_run_task binary from bash in the docker container.","title":"Plant Interpreter"},{"location":"Scanner/docker/plantinterpreter_docker/#docker-container-for-romi-plantinterpreter","text":"Important An existing local database directory is required , it will be mounted at container startup. To see how to create a local database directory, look here .","title":"Docker container for ROMI plantinterpreter"},{"location":"Scanner/docker/plantinterpreter_docker/#use-pre-built-docker-image","text":"Assuming you have a valid ROMI database directory under /data/ROMI/DB , you can easily download and start the pre-built roboticsmicrofarms/plant3dvision docker image with: export ROMI_DB = /data/ROMI/DB docker run --runtime = nvidia --gpus all \\ --env PYOPENCL_CTX = '0' \\ -v $ROMI_DB :/home/scanner/db \\ -it roboticsmicrofarms/plant3dvision:latest This should start the latest pre-built roboticsmicrofarms/plant3dvision docker image in interactive mode. The database location inside the docker container is ~/db . Note -v $ROMI_DB:/home/scanner/db performs a bind mount to enable access to the local database by the docker image. See the official documentation .","title":"Use pre-built docker image"},{"location":"Scanner/docker/plantinterpreter_docker/#build-docker-image","text":"We provide a convenience bash script to ease the build of roboticsmicrofarms/plant3dvision docker image. You can choose to use this script OR to \"manually\" call the docker build command.","title":"Build docker image"},{"location":"Scanner/docker/plantinterpreter_docker/#provided-convenience-buildsh-script","text":"To build the image with the provided build script, from the plant3dvision/docker directory: ./build.sh You can also pass some options, use ./build.sh -h to get more details about usage, options and default values. Tips To be sure to always pull the latest parent image, you may add the --pull option!","title":"Provided convenience build.sh script"},{"location":"Scanner/docker/plantinterpreter_docker/#manually-call-the-docker-build-command","text":"To build the image, from the plant3dvision/docker directory: export VTAG = \"latest\" docker build -t roboticsmicrofarms/plant3dvision: $VTAG . You can use the following optional arguments: --build-arg PLANT3DVISION_BRANCH=<git_branch> : change the cloned git branch from plant3dvision . --build-arg PLANTDB_BRANCH=<git_branch> : change the cloned git branch from plantdb . --build-arg PLANTIMAGER_BRANCH=<git_branch> : change the cloned git branch from plantimager .","title":"Manually call the docker build command"},{"location":"Scanner/docker/plantinterpreter_docker/#publish-docker-image","text":"Push it on docker hub: docker push roboticsmicrofarms/plantdb: $VTAG This requires a valid account & token on dockerhub!","title":"Publish docker image"},{"location":"Scanner/docker/plantinterpreter_docker/#usage","text":"","title":"Usage"},{"location":"Scanner/docker/plantinterpreter_docker/#requirements","text":"To run it, you need to have a valid local ROMI database, look here for instructions and here for an example database.","title":"Requirements"},{"location":"Scanner/docker/plantinterpreter_docker/#starting-the-plant3dvision-docker-image","text":"","title":"Starting the plant3dvision docker image"},{"location":"Scanner/docker/plantinterpreter_docker/#provided-runsh-script","text":"To start the container, in interactive mode, with the provided run script in plant3dvision/docker , use: ./run.sh You can also pass some options, use ./run.sh -h to get more details about usage and options, notably to mount a local romi database.","title":"Provided run.sh script"},{"location":"Scanner/docker/plantinterpreter_docker/#nvidia-gpu-test","text":"To make sure the started container will be able to access the host GPU, use: ./run.sh --gpu_test","title":"NVIDIA GPU test"},{"location":"Scanner/docker/plantinterpreter_docker/#pipelines-tests","text":"To performs test reconstructions, you have several possibilities: test the geometric pipeline: ./run.sh --geom_pipeline_test test the machine learning pipeline: ./run.sh --ml_pipeline_test test both pipelines: ./run.sh --pipeline_test Note This use test data & test models (for ML) provided with plant3dvision in plant3dvision/tests/testdata .","title":"Pipelines tests"},{"location":"Scanner/docker/plantinterpreter_docker/#manually","text":"Assuming you have a valid ROMI database directory under /data/ROMI/DB , you can start the roboticsmicrofarms/plant3dvision docker image with: export ROMI_DB = /data/ROMI/DB docker run --runtime = nvidia --gpus all \\ --env PYOPENCL_CTX = '0' \\ -v $ROMI_DB :/home/scanner/db \\ -it roboticsmicrofarms/plant3dvision: $VTAG bash This should start the built roboticsmicrofarms/plant3dvision docker image in interactive mode. The database location inside the docker container is ~/db . Note that: you are using the docker image roboticsmicrofarms/plant3dvision:$VTAG you mount the host directory $ROMI_DB \"inside\" the running container in the ~/db directory you activate all GPUs within the container with --gpus all declaring the environment variable PYOPENCL_CTX='0' select the first CUDA GPU capable -it & bash returns an interactive bash shell You may want to name the running container (with --name <my_name> ) if you \"demonize\" it (with -d ).","title":"Manually"},{"location":"Scanner/docker/plantinterpreter_docker/#executing-a-romi-task","text":"Once you are inside the running docker container, you may call the ROMI tasks. romi_run_task AnglesAndInternodes db/<my_scan_000>/ --config plant3dvision/configs/original_pipe_0.toml Note You may have to source the .profile file before calling romi_run_task . You can give a command to execute at container start-up using the -c, --cmd option. For example: export ROMI_DB = /data/ROMI/DB ./run.sh -p $ROMI_DB -u scanner -c \"source .profile && romi_run_task AnglesAndInternodes db/<my_scan_000>/ --config plant3dvision/configs/original_pipe_0.toml\" Important source .profile is important to add .local/bin/ to the $PATH environment variable. If you don't do this, you might not be able to access the romi_run_task binary from bash in the docker container.","title":"Executing a ROMI task"},{"location":"Scanner/docker/virtualplantimager_docker/","text":"Docker container for ROMI virtual plant imager Link Objective Link The following sections aim to show you how to build the docker image and run the corresponding container of the Virtual Plant Imager Prerequisites Link In addition to having docker installed in your system, you must also install the nvidia gpu drivers, nvidia-docker (v2.0) and nvidia-container-toolkit. This docker image has been tested successfully on: docker --version=19.03.6 | nvidia driver version=450.102.04 | CUDA version=11.0 Building the Docker image Link In this repository, you will find a script build.sh in the docker directory. git clone https://github.com/romi/plant-imager.git cd plant-imager/ cd docker/ ./build.sh This will create by default a docker image plantimager:latest . Inside the docker image, a user is created and named as the one currently used by your system. If you want more build options (specific branches, tags...etc), type ./build.sh --help . Running the docker container Link In the docker directory, you will find also a script named run.sh . To show more options, type ./run.sh --help Pre-requisites Link For clarity let us defines some variables here: ROMI_DB : the ROMI database root directory (should contain a plantdb file); ROMI_CFG : the directory containing the ROMI configurations (TOML files); To defines these variable, in a terminal: export ROMI_DB = /data/ROMI/DB export ROMI_CFG = /data/ROMI/configs Get an example archive with arabidopsis model Link Download & extract the example archive at the root directory of the romi database: wget --progress = bar -P $ROMI_DB https://media.romi-project.eu/data/vscan_data.tar.xz tar -C $ROMI_DB / -xvJf $ROMI_DB /vscan_data.tar.xz TOML config Link Use the following configuration, replacing <my_vscan> with the name of the virtual scan dataset to create, e.g. vscan_007 . [ObjFileset] scan_id = \"<my_vscan>\" [HdriFileset] scan_id = \"vscan_data\" [LpyFileset] scan_id = \"vscan_data\" [PaletteFileset] scan_id = \"vscan_data\" [ScanPath] class_name = \"Circle\" [ScanPath.kwargs] center_x = -2 center_y = 3 z = 34.17519302880196 tilt = 8 radius = 30 n_points = 72 [VirtualScan] obj_fileset = \"ObjFileset\" use_palette = true use_hdri = true load_scene = false scene_file_id = \"pot\" render_ground_truth = true [VirtualScan.scanner] width = 896 height = 896 focal = 24 flash = true add_leaf_displacement = true [Voxels] type = \"averaging\" voxel_size = 0.05 Virtual scan of a model plant Link Start the docker container Link Use the roboticsmicrofarms/plantimager docker image: export ROMI_DB = /data/ROMI/DB export ROMI_CFG = /data/ROMI/configs docker run --runtime = nvidia --gpus all \\ -v $ROMI_DB :/home/scanner/db \\ -v $ROMI_CFG :/home/scanner/configs \\ -it roboticsmicrofarms/plantimager:latest bash Initialize a scan dataset Link Use the romi_import_folder tool to import the required data into a new scan dataset, e.g. vscan_007 : romi_import_folder ~/db/vscan_data/data/ ~/db/vscan_007/ --metadata ~/db/vscan_data/files.json Start a VirtualScan romi task Link cd plantimager/bin romi_run_task VirtualScan ~/db/vscan_007 --config ~/plantimager/config/vscan_obj.toml","title":"Virtual Plant Imager"},{"location":"Scanner/docker/virtualplantimager_docker/#docker-container-for-romi-virtual-plant-imager","text":"","title":"Docker container for ROMI virtual plant imager"},{"location":"Scanner/docker/virtualplantimager_docker/#objective","text":"The following sections aim to show you how to build the docker image and run the corresponding container of the Virtual Plant Imager","title":"Objective"},{"location":"Scanner/docker/virtualplantimager_docker/#prerequisites","text":"In addition to having docker installed in your system, you must also install the nvidia gpu drivers, nvidia-docker (v2.0) and nvidia-container-toolkit. This docker image has been tested successfully on: docker --version=19.03.6 | nvidia driver version=450.102.04 | CUDA version=11.0","title":"Prerequisites"},{"location":"Scanner/docker/virtualplantimager_docker/#building-the-docker-image","text":"In this repository, you will find a script build.sh in the docker directory. git clone https://github.com/romi/plant-imager.git cd plant-imager/ cd docker/ ./build.sh This will create by default a docker image plantimager:latest . Inside the docker image, a user is created and named as the one currently used by your system. If you want more build options (specific branches, tags...etc), type ./build.sh --help .","title":"Building the Docker image"},{"location":"Scanner/docker/virtualplantimager_docker/#running-the-docker-container","text":"In the docker directory, you will find also a script named run.sh . To show more options, type ./run.sh --help","title":"Running the docker container"},{"location":"Scanner/docker/virtualplantimager_docker/#pre-requisites","text":"For clarity let us defines some variables here: ROMI_DB : the ROMI database root directory (should contain a plantdb file); ROMI_CFG : the directory containing the ROMI configurations (TOML files); To defines these variable, in a terminal: export ROMI_DB = /data/ROMI/DB export ROMI_CFG = /data/ROMI/configs","title":"Pre-requisites"},{"location":"Scanner/docker/virtualplantimager_docker/#get-an-example-archive-with-arabidopsis-model","text":"Download & extract the example archive at the root directory of the romi database: wget --progress = bar -P $ROMI_DB https://media.romi-project.eu/data/vscan_data.tar.xz tar -C $ROMI_DB / -xvJf $ROMI_DB /vscan_data.tar.xz","title":"Get an example archive with arabidopsis model"},{"location":"Scanner/docker/virtualplantimager_docker/#toml-config","text":"Use the following configuration, replacing <my_vscan> with the name of the virtual scan dataset to create, e.g. vscan_007 . [ObjFileset] scan_id = \"<my_vscan>\" [HdriFileset] scan_id = \"vscan_data\" [LpyFileset] scan_id = \"vscan_data\" [PaletteFileset] scan_id = \"vscan_data\" [ScanPath] class_name = \"Circle\" [ScanPath.kwargs] center_x = -2 center_y = 3 z = 34.17519302880196 tilt = 8 radius = 30 n_points = 72 [VirtualScan] obj_fileset = \"ObjFileset\" use_palette = true use_hdri = true load_scene = false scene_file_id = \"pot\" render_ground_truth = true [VirtualScan.scanner] width = 896 height = 896 focal = 24 flash = true add_leaf_displacement = true [Voxels] type = \"averaging\" voxel_size = 0.05","title":"TOML config"},{"location":"Scanner/docker/virtualplantimager_docker/#virtual-scan-of-a-model-plant","text":"","title":"Virtual scan of a model plant"},{"location":"Scanner/docker/virtualplantimager_docker/#start-the-docker-container","text":"Use the roboticsmicrofarms/plantimager docker image: export ROMI_DB = /data/ROMI/DB export ROMI_CFG = /data/ROMI/configs docker run --runtime = nvidia --gpus all \\ -v $ROMI_DB :/home/scanner/db \\ -v $ROMI_CFG :/home/scanner/configs \\ -it roboticsmicrofarms/plantimager:latest bash","title":"Start the docker container"},{"location":"Scanner/docker/virtualplantimager_docker/#initialize-a-scan-dataset","text":"Use the romi_import_folder tool to import the required data into a new scan dataset, e.g. vscan_007 : romi_import_folder ~/db/vscan_data/data/ ~/db/vscan_007/ --metadata ~/db/vscan_data/files.json","title":"Initialize a scan dataset"},{"location":"Scanner/docker/virtualplantimager_docker/#start-a-virtualscan-romi-task","text":"cd plantimager/bin romi_run_task VirtualScan ~/db/vscan_007 --config ~/plantimager/config/vscan_obj.toml","title":"Start a VirtualScan romi task"},{"location":"Scanner/explanations/general_design/","text":"Overview of the modules interactions Link The following figure shows a use case of the ROMI modules, and the way they interact, to design an efficient plant phenotyping platform used in research. PlantDB Link Should be totally independent of the rest since it could be uses in other parts of the ROMI project (Rover, Cable bot, ...) through the abstract class DB or even the local database class FSDB . Plant Imager Link It requires a physical connection to the hardware ( pyserial ) to control. It also needs an active ROMI database to export acquired datasets (plant images). Virtual Plant Imager Link It requires a connection to an active ROMI database to export generated datasets (virtual plant images). In case of machine learning methods, a database would also provide training datasets. Plant 3D Vision Link It requires connection to an active ROMI database to import datasets to process and export the results. Two plant reconstruction approaches are available in the SmartInterpreter: Geometry based, try to infer the plant's geometry using structure from motion algorithms and space carving to first reconstruct a point cloud. Machine learning based, try to infer the plant's geometry using semantic (organ) segmentation of pictures and space carving to first reconstruct a labelled point cloud. Then meshing and skeletonization finally enables to extract the plant's phyllotaxis. Plant 3D Explorer Link It requires a database with datasets to browse and represent. Research oriented user story Link The user put his/her plant inside the scanner and run acquisitions , which returns a set of images per plant. These images are uploaded to a central database . The user defines a pipeline to reconstruct and quantify plants architecture by choosing among a set of predefined methods and algorithms. These instructions may be run by a distant server. Finally, the user can access the acquisitions, reconstructions & quantitative data by connecting to a visualization server using his/her computer","title":"General design"},{"location":"Scanner/explanations/general_design/#overview-of-the-modules-interactions","text":"The following figure shows a use case of the ROMI modules, and the way they interact, to design an efficient plant phenotyping platform used in research.","title":"Overview of the modules interactions"},{"location":"Scanner/explanations/general_design/#plantdb","text":"Should be totally independent of the rest since it could be uses in other parts of the ROMI project (Rover, Cable bot, ...) through the abstract class DB or even the local database class FSDB .","title":"PlantDB"},{"location":"Scanner/explanations/general_design/#plant-imager","text":"It requires a physical connection to the hardware ( pyserial ) to control. It also needs an active ROMI database to export acquired datasets (plant images).","title":"Plant Imager"},{"location":"Scanner/explanations/general_design/#virtual-plant-imager","text":"It requires a connection to an active ROMI database to export generated datasets (virtual plant images). In case of machine learning methods, a database would also provide training datasets.","title":"Virtual Plant Imager"},{"location":"Scanner/explanations/general_design/#plant-3d-vision","text":"It requires connection to an active ROMI database to import datasets to process and export the results. Two plant reconstruction approaches are available in the SmartInterpreter: Geometry based, try to infer the plant's geometry using structure from motion algorithms and space carving to first reconstruct a point cloud. Machine learning based, try to infer the plant's geometry using semantic (organ) segmentation of pictures and space carving to first reconstruct a labelled point cloud. Then meshing and skeletonization finally enables to extract the plant's phyllotaxis.","title":"Plant 3D Vision"},{"location":"Scanner/explanations/general_design/#plant-3d-explorer","text":"It requires a database with datasets to browse and represent.","title":"Plant 3D Explorer"},{"location":"Scanner/explanations/general_design/#research-oriented-user-story","text":"The user put his/her plant inside the scanner and run acquisitions , which returns a set of images per plant. These images are uploaded to a central database . The user defines a pipeline to reconstruct and quantify plants architecture by choosing among a set of predefined methods and algorithms. These instructions may be run by a distant server. Finally, the user can access the acquisitions, reconstructions & quantitative data by connecting to a visualization server using his/her computer","title":"Research oriented user story"},{"location":"Scanner/explanations/segmentation/","text":"Segmentation of images Link The segmentation of an image consists in assigning a label to each of its pixels. For the 3d reconstruction of a plant, we need at least the segmentation of the images into 2 classes: plant and background . For a reconstruction with semantic labeling of the point cloud, we will need a semantic segmentation of the images giving one label for each organ type (e.g. { leaf , stem , pedicel , flower , fruit }). The figure below shows the binary and multi-class segmentations for a virtual plant. Example image of virtual arabidopsis (left) with binary (middle) and multi-class segmentation (right). Binary segmentation Link The binary segmentation of an image into plant and background is performed with the following command: romi_run_task Masks scan_id --config myconfig.toml with upstream task being ImagesFilesetExists when processing the raw RGB images or Undistorded when processing images corrected using the intrinsic parameters of the camera. The task takes this set of images as an input and produce one binary mask for each image. There are 2 methods available to compute indices for binary segmentation: Excess Green Index and Linear SVM. For each method, we provide an example configuration file in the Index computation section. Index computation Link Linear support vector machine (SVM) Link A linear combination of R, G and B is used to compute the index for pixel (i,j) (i,j) : S_{ij}=w_0 R_{ij} + w_1 G_{ij} +w_2 B_{ij} S_{ij}=w_0 R_{ij} + w_1 G_{ij} +w_2 B_{ij} where w w is the parameters vector specified in the configuration file. A simple vector, like w=(0,1,0) w=(0,1,0) may be used for example. Alternatively, you can train an SVM to learn those weights, and the threshold to be provided in the configuration file. For this, we consider you have a sample image and a ground truth binary mask. A ground truth may be produced using a manual annotation tool like LabelMe . Using for example a list of N randomly selected pixels as X_{train} X_{train} (array of size [N,3]) and their corresponding labels as Y_{train} Y_{train} (array of size N), a linear SVM is trained using from sklearn import svm X_train , Y_train = ... clf = svm . SVC ( kernel = 'linear' ) clf . fit ( X_train , y_train ) the weights can then be retrieved as clf.coef_ and the threshold as -clf.intercept_ Configuration file Link [Masks] upstream_task = \"ImagesFilesetExists\" # other option \"Undistorted\" type = \"linear\" parameters = \"[0,1,0]\" threshold = 0.5 Excess green Link This segmentation method is assuming the plant is green and the background is not. It has no parameter but it may be less robust than the linear SVM. We compute the normalized RGB values x \\in {r,g,b} x \\in {r,g,b} for each pixel (i,j) (i,j) : x_{ij} = \\frac{X_{ij}}{R_{ij}+G_{ij}+B_{ij}} x_{ij} = \\frac{X_{ij}}{R_{ij}+G_{ij}+B_{ij}} where X \\in {R, G, B} X \\in {R, G, B} is the red, green or blue image Then, the green excess index is computed as: \\operatorname{ExG}=2g-r-b \\operatorname{ExG}=2g-r-b Configuration file Link [Masks] upstream_task = \"ImagesFilesetExists\" # other option \"Undistorted\" type = \"excess_green\" threshold = 0.2 Multi-class segmentation Link The Segmentation2D task performs the semantic segmentation of images using a deep neural network (DNN). The command to run this task is: romi_run_task Segmentation2D scan_id my_config.toml This will produce a series of binary masks, one for each class on which the network was trained. Generic encoder/decoder architecture for semantic segmentation (U-net). The architecture of the network is inspired from the U-net 1 , with a ResNet encoder 2 . It consists in encoding and decoding pathways with skip connections between the 2. Along the encoding pathways, there is a sequence of convolutions and the image signal is upsampled along the decoding pathway. The network is trained for segmenting images of a size (S_x,S_y) (S_x,S_y) which is not necessarily the image size of the acquired images. Those parameters Sx and Sy should be provided in the configuration file. The images will be cropped to (S_x,S_y) (S_x,S_y) before being fed to the DNN and it is then resized to the original size as an output of the task. Configuration File Link [Segmentation2D] model_id = \"Resnetdataset_gl_png_896_896_epoch50\" # no default value Sx = 896 Sy = 896 threshold = 0.01 DNN model Link The neural architecture weights are obtained through training on an annotated dataset (see How to train a DNN for semantic segmentation). Those weights should be stored in the database (at <database>/models/models ) and the name of the weights file should be provided as the model_id parameter in the configuration. You can use our model trained on virtual arabidopsis here Binarization Link A binary mask m m is produced from the index or from the output of the DNN, I , by applying a threshold \\theta \\theta on I for each pixel (i,j) (i,j) : \\begin{equation} m_{ij} = \\begin{cases} 255 & \\text{if $I_{ij}>\\theta$}\\\\ 0 & \\text{otherwise} \\end{cases} \\end{equation} \\begin{equation} m_{ij} = \\begin{cases} 255 & \\text{if $I_{ij}>\\theta$}\\\\ 0 & \\text{otherwise} \\end{cases} \\end{equation} This threshold may be chosen empirically, or it may be learnt from annotated data (see linear SVM section). Dilation Link If the integer dilation parameter is non-zero a morphological dilation is applied to the image using the function binary_dilation from the skimage.morphology module. The dilation parameter sets the number of times binary_dilation is iteratively applied. For a faithful reconstruction this parameter should be set to 0 0 but in practice you may want to have a coarser point cloud. This is true when your segmentation is not perfect, dilation will fill the holes or when the reconstructed mesh is broken because the point-cloud is too thin. Working with data from the virtual scanner Link When working with data generated with the virtual scanner, the images folder contains multiple channels corresponding to the various class for which images were generated ( stem , flower , fruit , leaf , pedicel ). You have to select the rgb channel using the query parameter. Configuration File Link [Masks] type = \"excess_green\" threshold = 0.2 query = \"{\\\"channel\\\":\\\"rgb\\\"}\" Ronneberger, O., Fischer, P., & Brox, T. (2015, October). U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention (pp. 234-241). Springer, Cham. \u21a9 Zhang, Z., Liu, Q., & Wang, Y. (2018). Road extraction by deep residual u-net. IEEE Geoscience and Remote Sensing Letters, 15(5), 749-753. \u21a9","title":"Segmentation of images"},{"location":"Scanner/explanations/segmentation/#segmentation-of-images","text":"The segmentation of an image consists in assigning a label to each of its pixels. For the 3d reconstruction of a plant, we need at least the segmentation of the images into 2 classes: plant and background . For a reconstruction with semantic labeling of the point cloud, we will need a semantic segmentation of the images giving one label for each organ type (e.g. { leaf , stem , pedicel , flower , fruit }). The figure below shows the binary and multi-class segmentations for a virtual plant. Example image of virtual arabidopsis (left) with binary (middle) and multi-class segmentation (right).","title":"Segmentation of images"},{"location":"Scanner/explanations/segmentation/#binary-segmentation","text":"The binary segmentation of an image into plant and background is performed with the following command: romi_run_task Masks scan_id --config myconfig.toml with upstream task being ImagesFilesetExists when processing the raw RGB images or Undistorded when processing images corrected using the intrinsic parameters of the camera. The task takes this set of images as an input and produce one binary mask for each image. There are 2 methods available to compute indices for binary segmentation: Excess Green Index and Linear SVM. For each method, we provide an example configuration file in the Index computation section.","title":"Binary segmentation"},{"location":"Scanner/explanations/segmentation/#index-computation","text":"","title":"Index computation"},{"location":"Scanner/explanations/segmentation/#linear-support-vector-machine-svm","text":"A linear combination of R, G and B is used to compute the index for pixel (i,j) (i,j) : S_{ij}=w_0 R_{ij} + w_1 G_{ij} +w_2 B_{ij} S_{ij}=w_0 R_{ij} + w_1 G_{ij} +w_2 B_{ij} where w w is the parameters vector specified in the configuration file. A simple vector, like w=(0,1,0) w=(0,1,0) may be used for example. Alternatively, you can train an SVM to learn those weights, and the threshold to be provided in the configuration file. For this, we consider you have a sample image and a ground truth binary mask. A ground truth may be produced using a manual annotation tool like LabelMe . Using for example a list of N randomly selected pixels as X_{train} X_{train} (array of size [N,3]) and their corresponding labels as Y_{train} Y_{train} (array of size N), a linear SVM is trained using from sklearn import svm X_train , Y_train = ... clf = svm . SVC ( kernel = 'linear' ) clf . fit ( X_train , y_train ) the weights can then be retrieved as clf.coef_ and the threshold as -clf.intercept_","title":"Linear support vector machine (SVM)"},{"location":"Scanner/explanations/segmentation/#configuration-file","text":"[Masks] upstream_task = \"ImagesFilesetExists\" # other option \"Undistorted\" type = \"linear\" parameters = \"[0,1,0]\" threshold = 0.5","title":"Configuration file"},{"location":"Scanner/explanations/segmentation/#excess-green","text":"This segmentation method is assuming the plant is green and the background is not. It has no parameter but it may be less robust than the linear SVM. We compute the normalized RGB values x \\in {r,g,b} x \\in {r,g,b} for each pixel (i,j) (i,j) : x_{ij} = \\frac{X_{ij}}{R_{ij}+G_{ij}+B_{ij}} x_{ij} = \\frac{X_{ij}}{R_{ij}+G_{ij}+B_{ij}} where X \\in {R, G, B} X \\in {R, G, B} is the red, green or blue image Then, the green excess index is computed as: \\operatorname{ExG}=2g-r-b \\operatorname{ExG}=2g-r-b","title":"Excess green"},{"location":"Scanner/explanations/segmentation/#configuration-file_1","text":"[Masks] upstream_task = \"ImagesFilesetExists\" # other option \"Undistorted\" type = \"excess_green\" threshold = 0.2","title":"Configuration file"},{"location":"Scanner/explanations/segmentation/#multi-class-segmentation","text":"The Segmentation2D task performs the semantic segmentation of images using a deep neural network (DNN). The command to run this task is: romi_run_task Segmentation2D scan_id my_config.toml This will produce a series of binary masks, one for each class on which the network was trained. Generic encoder/decoder architecture for semantic segmentation (U-net). The architecture of the network is inspired from the U-net 1 , with a ResNet encoder 2 . It consists in encoding and decoding pathways with skip connections between the 2. Along the encoding pathways, there is a sequence of convolutions and the image signal is upsampled along the decoding pathway. The network is trained for segmenting images of a size (S_x,S_y) (S_x,S_y) which is not necessarily the image size of the acquired images. Those parameters Sx and Sy should be provided in the configuration file. The images will be cropped to (S_x,S_y) (S_x,S_y) before being fed to the DNN and it is then resized to the original size as an output of the task.","title":"Multi-class segmentation"},{"location":"Scanner/explanations/segmentation/#configuration-file_2","text":"[Segmentation2D] model_id = \"Resnetdataset_gl_png_896_896_epoch50\" # no default value Sx = 896 Sy = 896 threshold = 0.01","title":"Configuration File"},{"location":"Scanner/explanations/segmentation/#dnn-model","text":"The neural architecture weights are obtained through training on an annotated dataset (see How to train a DNN for semantic segmentation). Those weights should be stored in the database (at <database>/models/models ) and the name of the weights file should be provided as the model_id parameter in the configuration. You can use our model trained on virtual arabidopsis here","title":"DNN model"},{"location":"Scanner/explanations/segmentation/#binarization","text":"A binary mask m m is produced from the index or from the output of the DNN, I , by applying a threshold \\theta \\theta on I for each pixel (i,j) (i,j) : \\begin{equation} m_{ij} = \\begin{cases} 255 & \\text{if $I_{ij}>\\theta$}\\\\ 0 & \\text{otherwise} \\end{cases} \\end{equation} \\begin{equation} m_{ij} = \\begin{cases} 255 & \\text{if $I_{ij}>\\theta$}\\\\ 0 & \\text{otherwise} \\end{cases} \\end{equation} This threshold may be chosen empirically, or it may be learnt from annotated data (see linear SVM section).","title":"Binarization"},{"location":"Scanner/explanations/segmentation/#dilation","text":"If the integer dilation parameter is non-zero a morphological dilation is applied to the image using the function binary_dilation from the skimage.morphology module. The dilation parameter sets the number of times binary_dilation is iteratively applied. For a faithful reconstruction this parameter should be set to 0 0 but in practice you may want to have a coarser point cloud. This is true when your segmentation is not perfect, dilation will fill the holes or when the reconstructed mesh is broken because the point-cloud is too thin.","title":"Dilation"},{"location":"Scanner/explanations/segmentation/#working-with-data-from-the-virtual-scanner","text":"When working with data generated with the virtual scanner, the images folder contains multiple channels corresponding to the various class for which images were generated ( stem , flower , fruit , leaf , pedicel ). You have to select the rgb channel using the query parameter.","title":"Working with data from the virtual scanner"},{"location":"Scanner/explanations/segmentation/#configuration-file_3","text":"[Masks] type = \"excess_green\" threshold = 0.2 query = \"{\\\"channel\\\":\\\"rgb\\\"}\" Ronneberger, O., Fischer, P., & Brox, T. (2015, October). U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention (pp. 234-241). Springer, Cham. \u21a9 Zhang, Z., Liu, Q., & Wang, Y. (2018). Road extraction by deep residual u-net. IEEE Geoscience and Remote Sensing Letters, 15(5), 749-753. \u21a9","title":"Configuration File"},{"location":"Scanner/how_to/","text":"","title":"Home"},{"location":"Scanner/how_to/create_new_evaluation_task/","text":"How-to create a new ROMI evaluation task Link In order to evaluate the reconstruction and quantification tasks accuracy , we offer the possibility to also create evaluation tasks . The idea is to use a digital twin to generates the expected outcome of a task and use it as ground truth to challenge the reconstruction task. To do so, you will have to create two tasks: ground truth task : it should generate the expected outcome of the evaluated task from the digital twin; evaluation task : it will compare the output of the evaluated task against the ground truth. For example, the Voxels task has a VoxelGroundTruth task and a VoxelEvaluation task. Ground truth task Link Ground truth tasks should be defined in plant-3d-vision/plant3dvision/tasks/ground_truth.py . It should inherit from RomiTask and define a run method exporting the ground truth later use as reference in the evaluation task. Warning Do not forget to reference the task in romitask/romitask/modules.py . Evaluation task Link Evaluation tasks should be defined in plant-3d-vision/plant3dvision/tasks/evaluation.py . The evaluation task that you will write should inherit from EvaluationTask that defines: the requires method to use an upstream_task and ground_truth ; the output method to create the corresponding evaluation dataset the evaluate method that you should override; the run method that call evaluate and save the results as a JSON file. Warning Do not forget to reference the task in romitask/romitask/modules.py .","title":"Create a new evaluation task"},{"location":"Scanner/how_to/create_new_evaluation_task/#how-to-create-a-new-romi-evaluation-task","text":"In order to evaluate the reconstruction and quantification tasks accuracy , we offer the possibility to also create evaluation tasks . The idea is to use a digital twin to generates the expected outcome of a task and use it as ground truth to challenge the reconstruction task. To do so, you will have to create two tasks: ground truth task : it should generate the expected outcome of the evaluated task from the digital twin; evaluation task : it will compare the output of the evaluated task against the ground truth. For example, the Voxels task has a VoxelGroundTruth task and a VoxelEvaluation task.","title":"How-to create a new ROMI evaluation task"},{"location":"Scanner/how_to/create_new_evaluation_task/#ground-truth-task","text":"Ground truth tasks should be defined in plant-3d-vision/plant3dvision/tasks/ground_truth.py . It should inherit from RomiTask and define a run method exporting the ground truth later use as reference in the evaluation task. Warning Do not forget to reference the task in romitask/romitask/modules.py .","title":"Ground truth task"},{"location":"Scanner/how_to/create_new_evaluation_task/#evaluation-task","text":"Evaluation tasks should be defined in plant-3d-vision/plant3dvision/tasks/evaluation.py . The evaluation task that you will write should inherit from EvaluationTask that defines: the requires method to use an upstream_task and ground_truth ; the output method to create the corresponding evaluation dataset the evaluate method that you should override; the run method that call evaluate and save the results as a JSON file. Warning Do not forget to reference the task in romitask/romitask/modules.py .","title":"Evaluation task"},{"location":"Scanner/how_to/create_new_task/","text":"How-to create a new ROMI task Link We hereafter details how you can make your own algorithms available to the ROMI reconstruction and analysis pipeline by creating a task and registering it as an available module. For the sake of clarity and to illustrate how-to create a ROMI task from scratch, in this guide we will assume you want to add something quite different from what is already there. Important ROMI task usually have a semantic meaning and for example, the task AnglesAndInternodes may take several types of object in input (mesh, point-cloud & skeletons) but always output the JSON file with the obtained measures. So, to decide if you have to create a new task or add your algorithm to an existing task, following this rule should help: at a given step of the pipeline, if the output change, this is a NEW task! Add your algorithm to plant3dvision Link You first have to add a file (or append to an existing one), e.g. named algo.py , under the plant-3d-vision/plant3dvision directory. Let's assume the previously added file has a main function called my_algo like this: def my_algo ( data , * params , ** kwargs ): # Do something to data with given parameters to return transformed data `out_data` return out_data , error It has: data input(s) ( e.g. images, point clouds, meshes, ...) that will often be the output of a previous task in the pipeline parameter(s) , specific to the algorithm you want to add output(s) , the transformed dataset that will often be the input of a following task in the pipeline Create a ROMI task Link Dependency to luigi Link We use luigi to manage the pipeline execution and handle requirements & tasks dependencies. To create a task you will thus have to create a new Python class MyTask inheriting from the RomiTask class and creates a few methods and at least a run method used by luigi . Dependency to plantdb Link To manage the files, inputs and outputs, we use the plantdb package implementing a local file system database written in pure python. It provides classes and methods that simplifies and normalize the creation and use of the tasks outputs and inputs. New RomiTask template Link You will create a new python file my_task.py in the tasks submodule: plant-3d-vision/plant3dvision/tasks/my_task.py #!/usr/bin/env python # -*- coding: utf-8 -*- \"\"\" Briefly describe your module here. \"\"\" import luigi from plantdb import RomiTask from plantdb import io from plant3dvision.log import logger # Use this as logging method from plant3dvision.tasks.proc3d import SegmentedPointCloud # Now import your main method: from plant3dvision.algo import my_algo def MyTask ( RomiTask ): \"\"\"My algorithm is the best! Attributes ---------- upstream_task : luigi.TaskParameter Upstream task that will provides the data to your algorithm, here `SegmentedPointCloud`. param1 : luigi.FloatParameter An example float parameter parsed from the TOML config file. Set to `2.0` by default. param2 : luigi.IntParameter An example float parameter parsed from the TOML config file. Set to `5` by default. log : luigi.BoolParameter An example boolean parameter. \"\"\" # No need to write an `__init__` section, declare your class attributes as task parameters: upstream_task = luigi . TaskParameter ( default = SegmentedPointCloud ) param1 = luigi . FloatParameter ( default = 2.0 ) param2 = luigi . IntParameter ( default = 5 ) log = luigi . BoolParameter ( default = False ) def requires ( self ): \"\"\"Used by luigi to check you task dependencies.\"\"\" # By default a RomiTask requires a luigi.TaskParameter called `upstream_task`. # So no need to declare this method if you don't requires more than one upstream task! # Else you can override with something like (should be of type `luigi.TaskParameter`!): #return [self.upstream_task1(), self.upstream_task1()] pass def run ( self ): \"\"\"Called by luigi, it will run your algorithm. Usually consist of 3 steps: 1. Get the input(s) data from the previous task, eg. images or point clouds 2. Run you algorithm on input data 3. Save the result(s) of your method, eg. as a JSON file Notes ----- The parameters for your algorithms have been declared at class instantiation! \"\"\" # -1- Get the input(s) data from the previous task # To access the single file output of the upstream task use: uptask_input_file = self . input_file () # Read it with the proper reader, here a point-cloud reader (SegmentedPointCloud): in_data = io . read_point_cloud ( uptask_input_file ) # -2- Run you algorithm on input data out_data , error = my_algo ( in_data ) # Use example for boolean parameter & logger with 'info' level if self . log : logger . info ( \"My task ran perfectly!\" ) # -3- Write a single output (eg. a JSON file)... # Create the output `File` object task_output_file = self . output_file () # Write a JSON file with your method results io . write_json ( task_output_file , out_data ) # Add metadata to your file, eg. some error measure you don't want to include in the main output file: task_output_file . set_metadata ( \"my_error\" , error ) The corresponding TOML configuration file ( my_pipeline.toml ) controlling your task behaviour would look like this: [MyTask] upstream_task = 'SegmentedPointCloud' param1 = 6.0 param2 = 3 log = true Note You may need to add methods to read and write data, this should be done in the plantdb library using the plantdb/plantdb/io.py file! Multiple I/O for a task Link Your method (or the upstream task) may produce a set of object you want to save as separates files. In such case, use Filset objects. For example to output multiple JSON files: list_of_jsonifyable = [ ... ] task_output_fs = self . output () . get () for i , json_data in enumerate ( list_of_jsonifyable ): f = task_output_fs . create_file ( f \"my_json_ { i } \" ) # no extension! io . write_json ( f , json_data ) # Add some metadata to this `File` object f . set_metadata ( \"foo\" , f \"bar { i } \" ) Test your task Link You should now be able to test your newly created task MyTask with romi_run_task : romi_run_task MyTask /path/to/dataset --config /path/to/my_pipeline.toml --module plant3dvision.tasks.my_algo Using the --module option you can test your task without registering it. Register your task Link Once you are satisfied, you can add it to romitask/modules.py by referring to the task class name & its python module location: MODULES = { # ... \"MyTask\" : \"plant3dvision.tasks.my_algo\" , # ... } Use your newly created task Link Finally, you should now be able to use your newly created task MyTask with romi_run_task : romi_run_task MyTask /path/to/dataset --config /path/to/my_pipeline.toml Warning Use of absolute path is highly recommended as you may experience some difficulties from luigi otherwise!","title":"Create a new task"},{"location":"Scanner/how_to/create_new_task/#how-to-create-a-new-romi-task","text":"We hereafter details how you can make your own algorithms available to the ROMI reconstruction and analysis pipeline by creating a task and registering it as an available module. For the sake of clarity and to illustrate how-to create a ROMI task from scratch, in this guide we will assume you want to add something quite different from what is already there. Important ROMI task usually have a semantic meaning and for example, the task AnglesAndInternodes may take several types of object in input (mesh, point-cloud & skeletons) but always output the JSON file with the obtained measures. So, to decide if you have to create a new task or add your algorithm to an existing task, following this rule should help: at a given step of the pipeline, if the output change, this is a NEW task!","title":"How-to create a new ROMI task"},{"location":"Scanner/how_to/create_new_task/#add-your-algorithm-to-plant3dvision","text":"You first have to add a file (or append to an existing one), e.g. named algo.py , under the plant-3d-vision/plant3dvision directory. Let's assume the previously added file has a main function called my_algo like this: def my_algo ( data , * params , ** kwargs ): # Do something to data with given parameters to return transformed data `out_data` return out_data , error It has: data input(s) ( e.g. images, point clouds, meshes, ...) that will often be the output of a previous task in the pipeline parameter(s) , specific to the algorithm you want to add output(s) , the transformed dataset that will often be the input of a following task in the pipeline","title":"Add your algorithm to plant3dvision"},{"location":"Scanner/how_to/create_new_task/#create-a-romi-task","text":"","title":"Create a ROMI task"},{"location":"Scanner/how_to/create_new_task/#dependency-to-luigi","text":"We use luigi to manage the pipeline execution and handle requirements & tasks dependencies. To create a task you will thus have to create a new Python class MyTask inheriting from the RomiTask class and creates a few methods and at least a run method used by luigi .","title":"Dependency to luigi"},{"location":"Scanner/how_to/create_new_task/#dependency-to-plantdb","text":"To manage the files, inputs and outputs, we use the plantdb package implementing a local file system database written in pure python. It provides classes and methods that simplifies and normalize the creation and use of the tasks outputs and inputs.","title":"Dependency to plantdb"},{"location":"Scanner/how_to/create_new_task/#new-romitask-template","text":"You will create a new python file my_task.py in the tasks submodule: plant-3d-vision/plant3dvision/tasks/my_task.py #!/usr/bin/env python # -*- coding: utf-8 -*- \"\"\" Briefly describe your module here. \"\"\" import luigi from plantdb import RomiTask from plantdb import io from plant3dvision.log import logger # Use this as logging method from plant3dvision.tasks.proc3d import SegmentedPointCloud # Now import your main method: from plant3dvision.algo import my_algo def MyTask ( RomiTask ): \"\"\"My algorithm is the best! Attributes ---------- upstream_task : luigi.TaskParameter Upstream task that will provides the data to your algorithm, here `SegmentedPointCloud`. param1 : luigi.FloatParameter An example float parameter parsed from the TOML config file. Set to `2.0` by default. param2 : luigi.IntParameter An example float parameter parsed from the TOML config file. Set to `5` by default. log : luigi.BoolParameter An example boolean parameter. \"\"\" # No need to write an `__init__` section, declare your class attributes as task parameters: upstream_task = luigi . TaskParameter ( default = SegmentedPointCloud ) param1 = luigi . FloatParameter ( default = 2.0 ) param2 = luigi . IntParameter ( default = 5 ) log = luigi . BoolParameter ( default = False ) def requires ( self ): \"\"\"Used by luigi to check you task dependencies.\"\"\" # By default a RomiTask requires a luigi.TaskParameter called `upstream_task`. # So no need to declare this method if you don't requires more than one upstream task! # Else you can override with something like (should be of type `luigi.TaskParameter`!): #return [self.upstream_task1(), self.upstream_task1()] pass def run ( self ): \"\"\"Called by luigi, it will run your algorithm. Usually consist of 3 steps: 1. Get the input(s) data from the previous task, eg. images or point clouds 2. Run you algorithm on input data 3. Save the result(s) of your method, eg. as a JSON file Notes ----- The parameters for your algorithms have been declared at class instantiation! \"\"\" # -1- Get the input(s) data from the previous task # To access the single file output of the upstream task use: uptask_input_file = self . input_file () # Read it with the proper reader, here a point-cloud reader (SegmentedPointCloud): in_data = io . read_point_cloud ( uptask_input_file ) # -2- Run you algorithm on input data out_data , error = my_algo ( in_data ) # Use example for boolean parameter & logger with 'info' level if self . log : logger . info ( \"My task ran perfectly!\" ) # -3- Write a single output (eg. a JSON file)... # Create the output `File` object task_output_file = self . output_file () # Write a JSON file with your method results io . write_json ( task_output_file , out_data ) # Add metadata to your file, eg. some error measure you don't want to include in the main output file: task_output_file . set_metadata ( \"my_error\" , error ) The corresponding TOML configuration file ( my_pipeline.toml ) controlling your task behaviour would look like this: [MyTask] upstream_task = 'SegmentedPointCloud' param1 = 6.0 param2 = 3 log = true Note You may need to add methods to read and write data, this should be done in the plantdb library using the plantdb/plantdb/io.py file!","title":"New RomiTask template"},{"location":"Scanner/how_to/create_new_task/#multiple-io-for-a-task","text":"Your method (or the upstream task) may produce a set of object you want to save as separates files. In such case, use Filset objects. For example to output multiple JSON files: list_of_jsonifyable = [ ... ] task_output_fs = self . output () . get () for i , json_data in enumerate ( list_of_jsonifyable ): f = task_output_fs . create_file ( f \"my_json_ { i } \" ) # no extension! io . write_json ( f , json_data ) # Add some metadata to this `File` object f . set_metadata ( \"foo\" , f \"bar { i } \" )","title":"Multiple I/O for a task"},{"location":"Scanner/how_to/create_new_task/#test-your-task","text":"You should now be able to test your newly created task MyTask with romi_run_task : romi_run_task MyTask /path/to/dataset --config /path/to/my_pipeline.toml --module plant3dvision.tasks.my_algo Using the --module option you can test your task without registering it.","title":"Test your task"},{"location":"Scanner/how_to/create_new_task/#register-your-task","text":"Once you are satisfied, you can add it to romitask/modules.py by referring to the task class name & its python module location: MODULES = { # ... \"MyTask\" : \"plant3dvision.tasks.my_algo\" , # ... }","title":"Register your task"},{"location":"Scanner/how_to/create_new_task/#use-your-newly-created-task","text":"Finally, you should now be able to use your newly created task MyTask with romi_run_task : romi_run_task MyTask /path/to/dataset --config /path/to/my_pipeline.toml Warning Use of absolute path is highly recommended as you may experience some difficulties from luigi otherwise!","title":"Use your newly created task"},{"location":"Scanner/how_to/import_files/","text":"How-to import files in ROMI database Link Importing external images as a dataset Link In order to be able to use external images, i.e. images that were not acquired with the software & hardware developed by ROMI, we provide tools to import them as a scan dataset in the ROMI database. One example could be a set of pictures (of a plant) acquired with your phone that you would like to reconstruct and maybe analyse with our software. To do so, you may use the romi_import_folder or romi_import_file executables from plantdb . For example, you have a set of 10 RGB pictures named img_00*.jpg in a folder my_plant/ that you would like to import as outdoor_plant_1 in a romi database located under /data/romi/db . First you have to move the pictures to an \u00ecmages sub-directory & create a metadata.json describing the object under study: cd my_plant mkdir images mv *.jpg images/. touch metadata.json An example of a metadata.json : { \"object\" : { \"age\" : \"N/A\" , \"culture\" : \"N/A\" , \"environment\" : \"outdoor\" , \"experiment_id\" : \"romi demo outdoor plant\" , \"object\" : \"plant\" , \"plant_id\" : \"Chirsuta_1\" , \"sample\" : \"whole plant\" , \"species\" : \"Cardamine hirsuta\" , \"stock\" : \"WT\" , \"treatment\" : \"none\" } } To summarize you now should have the following folder structure: my_plant/ \u251c\u2500\u2500 images/ \u2502 \u251c\u2500\u2500 img_001.jpg \u2502 \u251c\u2500\u2500 [...] \u2502 \u2514\u2500\u2500 img_010.jpg \u2514\u2500\u2500 metadata.json Then you can perform the 'import to the database' operation with romi_import_folder : romi_import_folder my_plant/\u00ecmages/ /data/romi/db/outdoor_plant_1 --metadata my_plant/metadata.json That's it! Your manual acquisition is ready to be used by the romi_run_task tool for reconstruction.","title":"Import external ressources"},{"location":"Scanner/how_to/import_files/#how-to-import-files-in-romi-database","text":"","title":"How-to import files in ROMI database"},{"location":"Scanner/how_to/import_files/#importing-external-images-as-a-dataset","text":"In order to be able to use external images, i.e. images that were not acquired with the software & hardware developed by ROMI, we provide tools to import them as a scan dataset in the ROMI database. One example could be a set of pictures (of a plant) acquired with your phone that you would like to reconstruct and maybe analyse with our software. To do so, you may use the romi_import_folder or romi_import_file executables from plantdb . For example, you have a set of 10 RGB pictures named img_00*.jpg in a folder my_plant/ that you would like to import as outdoor_plant_1 in a romi database located under /data/romi/db . First you have to move the pictures to an \u00ecmages sub-directory & create a metadata.json describing the object under study: cd my_plant mkdir images mv *.jpg images/. touch metadata.json An example of a metadata.json : { \"object\" : { \"age\" : \"N/A\" , \"culture\" : \"N/A\" , \"environment\" : \"outdoor\" , \"experiment_id\" : \"romi demo outdoor plant\" , \"object\" : \"plant\" , \"plant_id\" : \"Chirsuta_1\" , \"sample\" : \"whole plant\" , \"species\" : \"Cardamine hirsuta\" , \"stock\" : \"WT\" , \"treatment\" : \"none\" } } To summarize you now should have the following folder structure: my_plant/ \u251c\u2500\u2500 images/ \u2502 \u251c\u2500\u2500 img_001.jpg \u2502 \u251c\u2500\u2500 [...] \u2502 \u2514\u2500\u2500 img_010.jpg \u2514\u2500\u2500 metadata.json Then you can perform the 'import to the database' operation with romi_import_folder : romi_import_folder my_plant/\u00ecmages/ /data/romi/db/outdoor_plant_1 --metadata my_plant/metadata.json That's it! Your manual acquisition is ready to be used by the romi_run_task tool for reconstruction.","title":"Importing external images as a dataset"},{"location":"Scanner/install/","text":"Installing the ROMI software Link Use cases Link In the following subsections we will detail how to install ROMI software for a few usage cases: Create a database server here . Plant scans acquisition using the ROMI plant scanner to a database here . Plant reconstruction pipelines from existing plant scans in a database here . Virtual plant creation (3D modelling of plant architecture with LPY), virtual scan (mimic plant scanner with blender) & reconstruction (same as 2.) here . Create a web server hosting the plant 3d explorer GUI here . Note You can find docker images for use cases #1, #3 & #5 in the dockerhub repository of the ROMI project here . General requirements Link Cloning sources Link To clone the git repository, you will need: git ca-certificates Start with these system dependencies: sudo apt-get install git ca-certificates Downloading from URLs Link Sometimes the documentation will provide commands with wget to download archives or other types of files, here is the command line to install it if you do not have it: sudo apt install wget Creating isolated Python environments Link Important We recommend using conda to create isolated environments as some packages, like openalea.lpy , are available as conda packages but not from pip and can be tricky to install from sources! Follow this link to learn how to install miniconda3 & create isolated Python environments with conda . If you have no idea why you should use isolated Python environments, here is a quote from the official Python documentation: Quote venv (for Python 3) and virtualenv (for Python 2) allow you to manage separate package installations for different projects. They essentially allow you to create a \"virtual\" isolated Python installation and install packages into that virtual installation. When you switch projects, you can simply create a new virtual environment and not have to worry about breaking the packages installed in the other environments. It is always recommended using a virtual environment while developing Python applications. List of sources Link For the ROMI projects, several libraries have been developed in various languages and made available on GitHub. Here is a list of the important repositories for the plant scanner project: plantdb : the database module is accessible here ; plantimager : the scanner interface, and the virtual scanner is accessible here ; plant3dvision : the computer vision algorithms to reconstruct the plants is accessible here ; romiseg : the ML-based plant segmentation models is accessible here ; plant-3d-explorer : the Node JS web viewer for plant scan, reconstruction and quantification is accessible here Additionally, we also have: romicgal : some CGAL bindings used for skeletonization & meshing is accessible here bldc_featherwing : the controller for BLDC motor on a feather wing is accessible here","title":"Home"},{"location":"Scanner/install/#installing-the-romi-software","text":"","title":"Installing the ROMI software"},{"location":"Scanner/install/#use-cases","text":"In the following subsections we will detail how to install ROMI software for a few usage cases: Create a database server here . Plant scans acquisition using the ROMI plant scanner to a database here . Plant reconstruction pipelines from existing plant scans in a database here . Virtual plant creation (3D modelling of plant architecture with LPY), virtual scan (mimic plant scanner with blender) & reconstruction (same as 2.) here . Create a web server hosting the plant 3d explorer GUI here . Note You can find docker images for use cases #1, #3 & #5 in the dockerhub repository of the ROMI project here .","title":"Use cases"},{"location":"Scanner/install/#general-requirements","text":"","title":"General requirements"},{"location":"Scanner/install/#cloning-sources","text":"To clone the git repository, you will need: git ca-certificates Start with these system dependencies: sudo apt-get install git ca-certificates","title":"Cloning sources"},{"location":"Scanner/install/#downloading-from-urls","text":"Sometimes the documentation will provide commands with wget to download archives or other types of files, here is the command line to install it if you do not have it: sudo apt install wget","title":"Downloading from URLs"},{"location":"Scanner/install/#creating-isolated-python-environments","text":"Important We recommend using conda to create isolated environments as some packages, like openalea.lpy , are available as conda packages but not from pip and can be tricky to install from sources! Follow this link to learn how to install miniconda3 & create isolated Python environments with conda . If you have no idea why you should use isolated Python environments, here is a quote from the official Python documentation: Quote venv (for Python 3) and virtualenv (for Python 2) allow you to manage separate package installations for different projects. They essentially allow you to create a \"virtual\" isolated Python installation and install packages into that virtual installation. When you switch projects, you can simply create a new virtual environment and not have to worry about breaking the packages installed in the other environments. It is always recommended using a virtual environment while developing Python applications.","title":"Creating isolated Python environments"},{"location":"Scanner/install/#list-of-sources","text":"For the ROMI projects, several libraries have been developed in various languages and made available on GitHub. Here is a list of the important repositories for the plant scanner project: plantdb : the database module is accessible here ; plantimager : the scanner interface, and the virtual scanner is accessible here ; plant3dvision : the computer vision algorithms to reconstruct the plants is accessible here ; romiseg : the ML-based plant segmentation models is accessible here ; plant-3d-explorer : the Node JS web viewer for plant scan, reconstruction and quantification is accessible here Additionally, we also have: romicgal : some CGAL bindings used for skeletonization & meshing is accessible here bldc_featherwing : the controller for BLDC motor on a feather wing is accessible here","title":"List of sources"},{"location":"Scanner/install/create_env/","text":"Creating isolated Python environments Link You can use venv or conda to create isolated Python environments. Warning Some ROMI libraries have dependencies relying on specific Python versions. Make sure that the isolated environment you create match these requirements! Isolated environments with venv Link Requirements Link Python 3 & pip are required. On Debian-like OS, use the following command to install them: sudo apt-get install python3 python3-pip For more details & explanations, follow this official guide to learn how to install packages using pip and virtual environments. Environment creation Link To create a new environment, named plant_imager , use python3 and the venv module: python3 -m venv plant_imager Note This will create a plant_imager folder in the current working directory and place the \"environment files\" there! We thus advise to gather all your environment in a common folder like ~/envs . To activate it: source plant_imager/bin/activate Usage Link Now you can easily install Python packages, for example NumPy , as follow: pip3 install numpy Note Use deactivate or kill terminal to leave it! You can now use this environment to install the ROMI software & dependencies. Isolated environments with miniconda Link Requirements Link In this case you do not need Python to be installed on your system, all you need it to install miniconda3 . You can download the latest miniconda3 version with: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh On Debian-like OS, use the following command to install it: bash Miniconda3-latest-Linux-x86_64.sh For more details & explanations, follow this official guide to learn how to install miniconda . Environment creation Link To create a new conda environment, named plant_imager with Python 3.7: conda create --name plant_imager python == 3 .7 To activate it: conda activate plant_imager Usage Link Now you can now easily install Python packages, for example NumPy , as follow: conda install numpy Note Use conda deactivate or kill terminal to leave it! You can now use this environment to install the ROMI software & dependencies.","title":"Create python environments"},{"location":"Scanner/install/create_env/#creating-isolated-python-environments","text":"You can use venv or conda to create isolated Python environments. Warning Some ROMI libraries have dependencies relying on specific Python versions. Make sure that the isolated environment you create match these requirements!","title":"Creating isolated Python environments"},{"location":"Scanner/install/create_env/#isolated-environments-with-venv","text":"","title":"Isolated environments with venv"},{"location":"Scanner/install/create_env/#requirements","text":"Python 3 & pip are required. On Debian-like OS, use the following command to install them: sudo apt-get install python3 python3-pip For more details & explanations, follow this official guide to learn how to install packages using pip and virtual environments.","title":"Requirements"},{"location":"Scanner/install/create_env/#environment-creation","text":"To create a new environment, named plant_imager , use python3 and the venv module: python3 -m venv plant_imager Note This will create a plant_imager folder in the current working directory and place the \"environment files\" there! We thus advise to gather all your environment in a common folder like ~/envs . To activate it: source plant_imager/bin/activate","title":"Environment creation"},{"location":"Scanner/install/create_env/#usage","text":"Now you can easily install Python packages, for example NumPy , as follow: pip3 install numpy Note Use deactivate or kill terminal to leave it! You can now use this environment to install the ROMI software & dependencies.","title":"Usage"},{"location":"Scanner/install/create_env/#isolated-environments-with-miniconda","text":"","title":"Isolated environments with miniconda"},{"location":"Scanner/install/create_env/#requirements_1","text":"In this case you do not need Python to be installed on your system, all you need it to install miniconda3 . You can download the latest miniconda3 version with: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh On Debian-like OS, use the following command to install it: bash Miniconda3-latest-Linux-x86_64.sh For more details & explanations, follow this official guide to learn how to install miniconda .","title":"Requirements"},{"location":"Scanner/install/create_env/#environment-creation_1","text":"To create a new conda environment, named plant_imager with Python 3.7: conda create --name plant_imager python == 3 .7 To activate it: conda activate plant_imager","title":"Environment creation"},{"location":"Scanner/install/create_env/#usage_1","text":"Now you can now easily install Python packages, for example NumPy , as follow: conda install numpy Note Use conda deactivate or kill terminal to leave it! You can now use this environment to install the ROMI software & dependencies.","title":"Usage"},{"location":"Scanner/install/plant3dexplorer_setup/","text":"Install the ROMI Plant 3d explorer Link To follow this guide you should have a conda environment, see here . For the sake of clarity it will be called Plant 3d explorer . Note If you do not want the hassle of having to create environment & install python libraries, there is a pre-built docker image, with usage instructions here . Pre-requisite Link The Plant 3d explorer relies on: node npm Install node and npm , on ubuntu: sudo apt install npm The packaged version ot npm is probably out of date (require npm>=5 ), to update it: npm install npm@latest -g Install ROMI packages & their dependencies: Link Activate your plant_imager environment! Clone the visualizer git repository : git clone https://github.com/romi/plant-3d-explorer.git cd plant-3d-explorer Install node packages and build the pages: npm install Use the Plant 3d explorer Link With the official ROMI database Link You can use the ROMI database to test the installation of the Plant 3d explorer : export REACT_APP_API_URL = 'https://db.romi-project.eu' npm start With a running local database Link If you have followed the installation instructions of the ROMI database ( here ), you can use it with the Plant 3d explorer : export REACT_APP_API_URL = '0.0.0.0' npm start Tip To permanently set this URL as the location of the DB, add it to your ~/.bashrc file. echo 'export REACT_APP_API_URL=0.0.0.0' >> ~/.bashrc","title":"plant-3d-explorer software setup"},{"location":"Scanner/install/plant3dexplorer_setup/#install-the-romi-plant-3d-explorer","text":"To follow this guide you should have a conda environment, see here . For the sake of clarity it will be called Plant 3d explorer . Note If you do not want the hassle of having to create environment & install python libraries, there is a pre-built docker image, with usage instructions here .","title":"Install the ROMI Plant 3d explorer"},{"location":"Scanner/install/plant3dexplorer_setup/#pre-requisite","text":"The Plant 3d explorer relies on: node npm Install node and npm , on ubuntu: sudo apt install npm The packaged version ot npm is probably out of date (require npm>=5 ), to update it: npm install npm@latest -g","title":"Pre-requisite"},{"location":"Scanner/install/plant3dexplorer_setup/#install-romi-packages-their-dependencies","text":"Activate your plant_imager environment! Clone the visualizer git repository : git clone https://github.com/romi/plant-3d-explorer.git cd plant-3d-explorer Install node packages and build the pages: npm install","title":"Install ROMI packages &amp; their dependencies:"},{"location":"Scanner/install/plant3dexplorer_setup/#use-the-plant-3d-explorer","text":"","title":"Use the Plant 3d explorer"},{"location":"Scanner/install/plant3dexplorer_setup/#with-the-official-romi-database","text":"You can use the ROMI database to test the installation of the Plant 3d explorer : export REACT_APP_API_URL = 'https://db.romi-project.eu' npm start","title":"With the official ROMI database"},{"location":"Scanner/install/plant3dexplorer_setup/#with-a-running-local-database","text":"If you have followed the installation instructions of the ROMI database ( here ), you can use it with the Plant 3d explorer : export REACT_APP_API_URL = '0.0.0.0' npm start Tip To permanently set this URL as the location of the DB, add it to your ~/.bashrc file. echo 'export REACT_APP_API_URL=0.0.0.0' >> ~/.bashrc","title":"With a running local database"},{"location":"Scanner/install/plant_imager_setup/","text":"Install ROMI software for the plant imager Link To follows this guide you should have a conda or a Python venv , see here . For the sake of clarity the environment will be called plant_imager . Install ROMI packages with pip : Link Activate your plant_imager environment! conda activate plant_imager Note Since this is still under development, the packages are installed in \"editable mode\" with the -e option. Install plant-imager sources: Link To pilot the hardware you have to install the plant-imager package: python3 -m pip install -e git+https://github.com/romi/plant-imager.git@dev Install plant3dvision sources: Link To start \"acquisition jobs\", you have to install the plant3dvision package: python3 -m pip install -e git+https://github.com/romi/plant3dvision.git@dev Install plantdb sources: Link Since we will need an active database to export the acquisitions, you have to install the plantdb package: python3 -m pip install -e git+https://github.com/romi/plantdb.git@dev Example database Link To quickly create an example DB you can use: wget https://db.romi-project.eu/models/test_db.tar.gz tar -xf test_db.tar.gz This will create a integration_tests folder with a ready to use test database. You should now be ready to perform \"plant acquisitions\" following the dedicated user guide. Troubleshooting Link Serial access denied Link Look here if you can not communicate with the scanner using usb.","title":"Plant imager software setup"},{"location":"Scanner/install/plant_imager_setup/#install-romi-software-for-the-plant-imager","text":"To follows this guide you should have a conda or a Python venv , see here . For the sake of clarity the environment will be called plant_imager .","title":"Install ROMI software for the plant imager"},{"location":"Scanner/install/plant_imager_setup/#install-romi-packages-with-pip","text":"Activate your plant_imager environment! conda activate plant_imager Note Since this is still under development, the packages are installed in \"editable mode\" with the -e option.","title":"Install ROMI packages with pip:"},{"location":"Scanner/install/plant_imager_setup/#install-plant-imager-sources","text":"To pilot the hardware you have to install the plant-imager package: python3 -m pip install -e git+https://github.com/romi/plant-imager.git@dev","title":"Install plant-imager sources:"},{"location":"Scanner/install/plant_imager_setup/#install-plant3dvision-sources","text":"To start \"acquisition jobs\", you have to install the plant3dvision package: python3 -m pip install -e git+https://github.com/romi/plant3dvision.git@dev","title":"Install plant3dvision sources:"},{"location":"Scanner/install/plant_imager_setup/#install-plantdb-sources","text":"Since we will need an active database to export the acquisitions, you have to install the plantdb package: python3 -m pip install -e git+https://github.com/romi/plantdb.git@dev","title":"Install plantdb sources:"},{"location":"Scanner/install/plant_imager_setup/#example-database","text":"To quickly create an example DB you can use: wget https://db.romi-project.eu/models/test_db.tar.gz tar -xf test_db.tar.gz This will create a integration_tests folder with a ready to use test database. You should now be ready to perform \"plant acquisitions\" following the dedicated user guide.","title":"Example database"},{"location":"Scanner/install/plant_imager_setup/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"Scanner/install/plant_imager_setup/#serial-access-denied","text":"Look here if you can not communicate with the scanner using usb.","title":"Serial access denied"},{"location":"Scanner/install/plant_reconstruction_setup/","text":"Install ROMI software for plants reconstruction Link To follows this guide you should have an existing conda or a Python venv , see here . We prefer to use conda and will use examples command lines with it. Simply replace the conda activate plant_imager commands with source plant_imager/bin/activate . For the sake of clarity the environment will be called plant_imager . Requirements: Link Colmap Link To save you the hassle of installing Colmap & its dependencies, we wrote a mechanism allowing you to run the Colmap command straight into a docker container where it is already done! Note Choose between option A (recommended) OR B! A - Use of docker image Link You can use a pre-built docker image with Colmap & its dependencies installed (named geki/colmap ). This requires to install the docker-engine. To do so, follows the official instructions here: https://docs.docker.com/get-docker/ It uses the Python docker SDK docker available on PyPi , to learn more read the official documentation. If you upgrade from an older install, you may have to install the Python docker SDK: conda activate plant_imager python -m pip install docker Important Make sure you can access the docker engine as a non-root user! On Linux: 1. Create the docker group : `$ sudo groupadd docker ` 2. Add your user to the docker group : `$ sudo usermod - aG docker $ USER ` 3. Log out and log back in so that your group membership is re - evaluated . Official instructions [ here ] ( https : // docs . docker . com / engine / install / linux - postinstall / ) B - System install Link If you are a warrior or a computer expert, you can follow the procedure from the official documentation here . Make sure to use version 3.6. Note If you are using a conda environment, you can install ceres-solver dependency for Colmap from the conda-forge channel: conda activate plant_imager conda install ceres-solver -c conda-forge Important By default we use the docker mechanism, to enable the system install you need to export the environment variable COLMAP_EXE='colmap' . Install ROMI packages with pip : Link Activate your plant_imager environment! Note Since this is still under development, the packages are installed in \"editable mode\" with the -e option. Important You need an active ROMI database to import the images fileset to reconstruct. If it's not done yet, follow the installation instructions of a ROMI database ( here ). Install plant3dvision sources: Link To start \"reconstruction jobs\", you have to install the plant3dvision package. Here we use the submodules but if you wish to edit other packages than plant3dvision , e.g. plantdb , install them from source! conda activate plant_imager git clone --branch dev https://github.com/romi/plant3dvision.git cd plant3dvision git submodule init git submodule update python3 -m pip install -r requirements.txt python3 -m pip install ./plantdb/ python3 -m pip install ./romiseg/ python3 -m pip install ./plantimager/ python3 -m pip install ./romicgal/ python3 -m pip install -e . You should now be ready to performs \"plant reconstructions\" following the dedicated user guide. Install romicgal sources Link We use some algorithms from CGAL and propose a minimal python wrapper called romicgal . To install it: conda activate plant_imager python3 -m pip install -e git+https://github.com/romi/romicgal.git@master Note This takes some time since it has to download dependencies ( CGAL-0.5 & boost-1.72.0 ) and compile them. Install romiseg sources Link To install the additional Machine Learning based segmentation module: conda activate plant_imager python3 -m pip install -e git+https://github.com/romi/romiseg@dev Warning If not using CUDA 10.*, you have to install the matching pytorch distribution. For example, for CUDA 9.2, use: conda activate plant_imager pip install torch == 1 .4.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html","title":"Plant reconstruction software setup"},{"location":"Scanner/install/plant_reconstruction_setup/#install-romi-software-for-plants-reconstruction","text":"To follows this guide you should have an existing conda or a Python venv , see here . We prefer to use conda and will use examples command lines with it. Simply replace the conda activate plant_imager commands with source plant_imager/bin/activate . For the sake of clarity the environment will be called plant_imager .","title":"Install ROMI software for plants reconstruction"},{"location":"Scanner/install/plant_reconstruction_setup/#requirements","text":"","title":"Requirements:"},{"location":"Scanner/install/plant_reconstruction_setup/#colmap","text":"To save you the hassle of installing Colmap & its dependencies, we wrote a mechanism allowing you to run the Colmap command straight into a docker container where it is already done! Note Choose between option A (recommended) OR B!","title":"Colmap"},{"location":"Scanner/install/plant_reconstruction_setup/#a-use-of-docker-image","text":"You can use a pre-built docker image with Colmap & its dependencies installed (named geki/colmap ). This requires to install the docker-engine. To do so, follows the official instructions here: https://docs.docker.com/get-docker/ It uses the Python docker SDK docker available on PyPi , to learn more read the official documentation. If you upgrade from an older install, you may have to install the Python docker SDK: conda activate plant_imager python -m pip install docker Important Make sure you can access the docker engine as a non-root user! On Linux: 1. Create the docker group : `$ sudo groupadd docker ` 2. Add your user to the docker group : `$ sudo usermod - aG docker $ USER ` 3. Log out and log back in so that your group membership is re - evaluated . Official instructions [ here ] ( https : // docs . docker . com / engine / install / linux - postinstall / )","title":"A - Use of docker image"},{"location":"Scanner/install/plant_reconstruction_setup/#b-system-install","text":"If you are a warrior or a computer expert, you can follow the procedure from the official documentation here . Make sure to use version 3.6. Note If you are using a conda environment, you can install ceres-solver dependency for Colmap from the conda-forge channel: conda activate plant_imager conda install ceres-solver -c conda-forge Important By default we use the docker mechanism, to enable the system install you need to export the environment variable COLMAP_EXE='colmap' .","title":"B - System install"},{"location":"Scanner/install/plant_reconstruction_setup/#install-romi-packages-with-pip","text":"Activate your plant_imager environment! Note Since this is still under development, the packages are installed in \"editable mode\" with the -e option. Important You need an active ROMI database to import the images fileset to reconstruct. If it's not done yet, follow the installation instructions of a ROMI database ( here ).","title":"Install ROMI packages with pip:"},{"location":"Scanner/install/plant_reconstruction_setup/#install-plant3dvision-sources","text":"To start \"reconstruction jobs\", you have to install the plant3dvision package. Here we use the submodules but if you wish to edit other packages than plant3dvision , e.g. plantdb , install them from source! conda activate plant_imager git clone --branch dev https://github.com/romi/plant3dvision.git cd plant3dvision git submodule init git submodule update python3 -m pip install -r requirements.txt python3 -m pip install ./plantdb/ python3 -m pip install ./romiseg/ python3 -m pip install ./plantimager/ python3 -m pip install ./romicgal/ python3 -m pip install -e . You should now be ready to performs \"plant reconstructions\" following the dedicated user guide.","title":"Install plant3dvision sources:"},{"location":"Scanner/install/plant_reconstruction_setup/#install-romicgal-sources","text":"We use some algorithms from CGAL and propose a minimal python wrapper called romicgal . To install it: conda activate plant_imager python3 -m pip install -e git+https://github.com/romi/romicgal.git@master Note This takes some time since it has to download dependencies ( CGAL-0.5 & boost-1.72.0 ) and compile them.","title":"Install romicgal sources"},{"location":"Scanner/install/plant_reconstruction_setup/#install-romiseg-sources","text":"To install the additional Machine Learning based segmentation module: conda activate plant_imager python3 -m pip install -e git+https://github.com/romi/romiseg@dev Warning If not using CUDA 10.*, you have to install the matching pytorch distribution. For example, for CUDA 9.2, use: conda activate plant_imager pip install torch == 1 .4.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html","title":"Install romiseg sources"},{"location":"Scanner/install/plantdb_setup/","text":"Create a ROMI database to host, receive & serve plant scans Link To follow this guide you should have a conda environment, see here . For the sake of clarity it will be called plantscans_db . Note Since this is still under development, the packages are installed in \"editable mode\" with the -e option. Note If you do not want the hassle of having to create environment & install python libraries, there is a pre-built docker image, with usage instructions here . Install plantdb sources Link Activate your plantscans_db environment! conda activate plantscans_db To create an active ROMI database, you have to install the plantdb package: git clone https://github.com/romi/plantdb.git && \\ cd plantdb && \\ git checkout dev && \\ python3.7 -m pip install setuptools setuptools-scm && \\ python3.7 -m pip install luigi pillow && \\ python3.7 -m pip install flask flask-restful flask-cors && \\ python3.7 -m pip install . Initialize a ROMI database Link The FSDB class from the plantdb module is used to manage a local file system for data storage. A database is any folder which contains a file named plantdb . To create an empty database, just create a new folder, and an empty file named plantdb in it. For example: mkdir /data/romi_db touch /data/romi_db/plantdb Then define its location in an environment variable DB_LOCATION : export DB_LOCATION = '/data/ROMI/DB' Note To permanently set this directory as the location of the DB, add it to your ~/.bashrc file. echo 'export DB_LOCATION=/data/ROMI/DB' >> ~/.bashrc Serve the REST API Link Then you can start the REST API with romi_scanner_rest_api : romi_scanner_rest_api You should see something like: n scans = 2 * Serving Flask app \"romi_scanner_rest_api\" (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit) To access the REST API, open your favorite browser and use URLs to access: the list of all scans: http://0.0.0.0:5000/scans the '2018-12-17_17-05-35' dataset: http://0.0.0.0:5000/scans/2018-12-17_17-05-35 You should see JSON formatted text. Troubleshooting : When starting the REST API with romi_scanner_rest_api , if you get an error message about this executable not being found, it may be missing from the $PATH environment variable. Add it with: export PATH = $PATH : \"/home/ $USER /.local/bin\" Note To permanently set this in your bash terminal, add it to your ~/.bashrc file. echo 'export PATH=$PATH:/home/$USER/.local/bin' >> ~/.bashrc","title":"Database setup"},{"location":"Scanner/install/plantdb_setup/#create-a-romi-database-to-host-receive-serve-plant-scans","text":"To follow this guide you should have a conda environment, see here . For the sake of clarity it will be called plantscans_db . Note Since this is still under development, the packages are installed in \"editable mode\" with the -e option. Note If you do not want the hassle of having to create environment & install python libraries, there is a pre-built docker image, with usage instructions here .","title":"Create a ROMI database to host, receive &amp; serve plant scans"},{"location":"Scanner/install/plantdb_setup/#install-plantdb-sources","text":"Activate your plantscans_db environment! conda activate plantscans_db To create an active ROMI database, you have to install the plantdb package: git clone https://github.com/romi/plantdb.git && \\ cd plantdb && \\ git checkout dev && \\ python3.7 -m pip install setuptools setuptools-scm && \\ python3.7 -m pip install luigi pillow && \\ python3.7 -m pip install flask flask-restful flask-cors && \\ python3.7 -m pip install .","title":"Install plantdb sources"},{"location":"Scanner/install/plantdb_setup/#initialize-a-romi-database","text":"The FSDB class from the plantdb module is used to manage a local file system for data storage. A database is any folder which contains a file named plantdb . To create an empty database, just create a new folder, and an empty file named plantdb in it. For example: mkdir /data/romi_db touch /data/romi_db/plantdb Then define its location in an environment variable DB_LOCATION : export DB_LOCATION = '/data/ROMI/DB' Note To permanently set this directory as the location of the DB, add it to your ~/.bashrc file. echo 'export DB_LOCATION=/data/ROMI/DB' >> ~/.bashrc","title":"Initialize a ROMI database"},{"location":"Scanner/install/plantdb_setup/#serve-the-rest-api","text":"Then you can start the REST API with romi_scanner_rest_api : romi_scanner_rest_api You should see something like: n scans = 2 * Serving Flask app \"romi_scanner_rest_api\" (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit) To access the REST API, open your favorite browser and use URLs to access: the list of all scans: http://0.0.0.0:5000/scans the '2018-12-17_17-05-35' dataset: http://0.0.0.0:5000/scans/2018-12-17_17-05-35 You should see JSON formatted text. Troubleshooting : When starting the REST API with romi_scanner_rest_api , if you get an error message about this executable not being found, it may be missing from the $PATH environment variable. Add it with: export PATH = $PATH : \"/home/ $USER /.local/bin\" Note To permanently set this in your bash terminal, add it to your ~/.bashrc file. echo 'export PATH=$PATH:/home/$USER/.local/bin' >> ~/.bashrc","title":"Serve the REST API"},{"location":"Scanner/install/virtual_plant_setup/","text":"Install ROMI software for virtual plants acquisition & reconstruction Link To follows this guide you should have a conda or a Python venv , see here . For the sake of clarity the environment will be called virtual_plants . Notice for using the virtual scanner Link If you want to use the virtual scanner, the modified python version bundled with blender and the environment python version have to match. To obtain the python version bundled with your distribution of blender, type: blender -b --python-expr \"import sys; print(sys.version)\" It will output something like: Blender 2.82 (sub 7) (hash 5b416ffb848e built 2020-02-14 16:19:45) ALSA lib pcm_dmix.c:1089:(snd_pcm_dmix_open) unable to open slave 3.8.1 (default, Jan 22 2020, 06:38:00) [GCC 9.2.0] Blender quit In this case, this means Blender bundle Python 3.8, and you should too. In the following, we will assume that you are using conda environments. If not, adapt with corresponding virtualenv commands. Install ROMI packages with pip : Link Activate your virtual_plants environment! Note Since this is still under development, the packages are installed in \"editable mode\" with the -e option. Install openalea.lpy Link If you're using python>=3.7 and conda , just install lpy from conda: conda install -c conda-forge -c fredboudon openalea.lpy Install romicgal sources Link To pilot the hardware you have to install the plantimager package: python3 -m pip install -e git+https://github.com/romi/romicgal.git@dev Note This takes some time since it has to download dependencies ( CGAL-0.5 & boost-1.72.0 ) and compile them. Install plant3dvision sources Link To start \"acquisition jobs\", you have to install the plant3dvision package: python3 -m pip install -e git+https://github.com/romi/plant3dvision.git@dev Install plantdb sources Link Since we will need an active database to export the acquisitions, you have to install the plantdb package: python3 -m pip install -e git+https://github.com/romi/plantdb.git@dev Install romiseg sources Link To install the additional segmentation module: python3 -m pip install git+https://github.com/romi/romiseg@dev Warning If not using CUDA 10.*, you have to install the matching pytorch distribution. For example, for CUDA 9.2, use: pip install torch == 1 .4.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html Example database Link To quickly create an example DB you can use: wget https://db.romi-project.eu/models/test_db.tar.gz tar -xf test_db.tar.gz This will create a integration_tests folder with a ready to use test database. You should now be ready to perform tasks on virtual plants such as \"creation\", \"acquisition\" & \"reconstruction\" following the dedicated user guide.","title":"Virtual plants scanner software setup"},{"location":"Scanner/install/virtual_plant_setup/#install-romi-software-for-virtual-plants-acquisition-reconstruction","text":"To follows this guide you should have a conda or a Python venv , see here . For the sake of clarity the environment will be called virtual_plants .","title":"Install ROMI software for virtual plants acquisition &amp; reconstruction"},{"location":"Scanner/install/virtual_plant_setup/#notice-for-using-the-virtual-scanner","text":"If you want to use the virtual scanner, the modified python version bundled with blender and the environment python version have to match. To obtain the python version bundled with your distribution of blender, type: blender -b --python-expr \"import sys; print(sys.version)\" It will output something like: Blender 2.82 (sub 7) (hash 5b416ffb848e built 2020-02-14 16:19:45) ALSA lib pcm_dmix.c:1089:(snd_pcm_dmix_open) unable to open slave 3.8.1 (default, Jan 22 2020, 06:38:00) [GCC 9.2.0] Blender quit In this case, this means Blender bundle Python 3.8, and you should too. In the following, we will assume that you are using conda environments. If not, adapt with corresponding virtualenv commands.","title":"Notice for using the virtual scanner"},{"location":"Scanner/install/virtual_plant_setup/#install-romi-packages-with-pip","text":"Activate your virtual_plants environment! Note Since this is still under development, the packages are installed in \"editable mode\" with the -e option.","title":"Install ROMI packages with pip:"},{"location":"Scanner/install/virtual_plant_setup/#install-openalealpy","text":"If you're using python>=3.7 and conda , just install lpy from conda: conda install -c conda-forge -c fredboudon openalea.lpy","title":"Install openalea.lpy"},{"location":"Scanner/install/virtual_plant_setup/#install-romicgal-sources","text":"To pilot the hardware you have to install the plantimager package: python3 -m pip install -e git+https://github.com/romi/romicgal.git@dev Note This takes some time since it has to download dependencies ( CGAL-0.5 & boost-1.72.0 ) and compile them.","title":"Install romicgal sources"},{"location":"Scanner/install/virtual_plant_setup/#install-plant3dvision-sources","text":"To start \"acquisition jobs\", you have to install the plant3dvision package: python3 -m pip install -e git+https://github.com/romi/plant3dvision.git@dev","title":"Install plant3dvision sources"},{"location":"Scanner/install/virtual_plant_setup/#install-plantdb-sources","text":"Since we will need an active database to export the acquisitions, you have to install the plantdb package: python3 -m pip install -e git+https://github.com/romi/plantdb.git@dev","title":"Install plantdb sources"},{"location":"Scanner/install/virtual_plant_setup/#install-romiseg-sources","text":"To install the additional segmentation module: python3 -m pip install git+https://github.com/romi/romiseg@dev Warning If not using CUDA 10.*, you have to install the matching pytorch distribution. For example, for CUDA 9.2, use: pip install torch == 1 .4.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html","title":"Install romiseg sources"},{"location":"Scanner/install/virtual_plant_setup/#example-database","text":"To quickly create an example DB you can use: wget https://db.romi-project.eu/models/test_db.tar.gz tar -xf test_db.tar.gz This will create a integration_tests folder with a ready to use test database. You should now be ready to perform tasks on virtual plants such as \"creation\", \"acquisition\" & \"reconstruction\" following the dedicated user guide.","title":"Example database"},{"location":"Scanner/metadata/","text":"FAIR data for the ROMI plant scanner project Link FAIR data Link We aim at using FAIR data principles in the ROMI plant scanner project. Quoting the GoFAIR website: Quote In 2016, the \" FAIR Guiding Principles for scientific data management and stewardship \" were published in Scientific Data. The authors intended to provide guidelines to improve the Findability , Accessibility , Interoperability , and Reuse of digital assets. The principles emphasise machine-actionability (i.e., the capacity of computational systems to find, access, interoperate, and reuse data with none or minimal human intervention) because humans increasingly rely on computational support to deal with data as a result of the increase in volume, complexity, and creation speed of data. In our context, a biological dataset is a set of RAW images (eg: RGB images), used to reconstruct the plant 3D structure, associated with a set of metadata of different nature: biological, hardware & software. ROMI plant scanner metadata Link Metadata are stored hierarchically. We currently use the JSON format. There are many JSON files containing metadata in the metadata directory attached to a dataset. General metadata Link The first you should consider is metadata/metadata.json . Its top-level entries are: \"scanner\", the hardware metadata (see here ) \"object\", the biological metadata (see here ) \"path\", the parameter values used for the task Scan (see here ) \"computed\", the parameter values used for the task Colmap (see here ) \"measures\", the parameter values used for the task AnglesAndInternodes (see here ) Todo Remove top-level entries \"path\", \"computed\" & \"measures\", they look like duplicates from their respective task metadata. Scanning operation Link Found under the path top level section, it contains: the trajectory of the camera under path Example: \"path\" : { \"args\" : { \"filetype\" : \"jpg\" , \"num_points\" : 72 , \"radius\" : 350 , \"tilt\" : 0.45 , \"xc\" : 400 , \"yc\" : 400 , \"z\" : 0 }, \"id\" : \"circular_72\" , \"type\" : \"circular\" } Todo Potential duplication from the Scan task metadata! Colmap reconstruction Link Found under the computed top level section, it contains the camera_model used by Colmap. Example: \"computed\" : { \"camera_model\" : { \"height\" : 1080 , \"id\" : 1 , \"model\" : \"OPENCV\" , \"params\" : [ 1102.2709767952233 , 1102.2709767952233 , 808.0 , 540.0 , -0.015118876273724434 , -0.015118876273724434 , 0.0 , 0.0 ], \"width\" : 1616 } } Todo Potential duplication from the Colmap task metadata! Measures of angles and internodes Link Found under the measures top level section: Measured angles are under angles Measured internodes are under internodes Example: \"measures\" : { \"angles\" : [ 2.6179938779914944 , 1.3089969389957472 , ... 2.0943951023931953 ], \"internodes\" : [ 41 , 32 , ... 1 ] } Todo Potential duplication from the AnglesAndInternodes task metadata! Tasks metadata Link Then there are the JSON files attached to each task, e.g. Colmap_True____feature_extrac_3bbfcb1413.json , they contain the parameter used to run this task. Their content and meaning is explained in the task metadata section. Images metadata Link The image JSON file metadata/images.json contains ??? and is produced by ???. Example: { \"task_params\" : { \"fileset_id\" : \"images\" , \"output_file_id\" : \"out\" , \"scan_id\" : \"\" } } Found under the metadata/images directory, these JSON files contains ??? and is produced by ???. Note Sub-section camera_model seems redundant with same section in reconstruction . Example: { \"calibrated_pose\" : [ 49.04537654162613 , 401.1470121046677 , -0.10613970524327433 ], \"colmap_camera\" : { \"camera_model\" : { \"height\" : 1080 , \"id\" : 1 , \"model\" : \"OPENCV\" , \"params\" : [ 1106.9593323985682 , 1106.9593323985682 , 808.0 , 540.0 , -0.012379986602455324 , -0.012379986602455324 , 0.0 , 0.0 ], \"width\" : 1616 }, \"rotmat\" : [ [ -0.07758281248083276 , 0.9961595266033345 , 0.040584538496628464 ], [ -0.4230067224604736 , -0.069751540308706 , 0.9034378979087665 ], [ 0.9027991027691664 , 0.05292372040950383 , 0.42679369706827314 ] ], \"tvec\" : [ -397.2357284138349 , 49.50722946972662 , -66.64522999892229 ] }, \"pose\" : [ 50.0 , 400.0 , 0 , 0.0 , 0.45 ] } Visualization metadata Link The visualization JSON file metadata/Visualization.json contains ??? and is produced by ???. Important Run the Visualization task to complete this section! Example: { \"files\" : { \"angles\" : \"AnglesAndInternodes\" , \"images\" : [ \"image_rgb-000\" , \"image_rgb-001\" , \"...\" \"image_rgb-071\" ], \"mesh\" : \"TriangleMesh\" , \"point_cloud\" : \"PointCloud\" , \"poses\" : \"images\" , \"skeleton\" : \"CurveSkeleton\" , \"thumbnails\" : [ \"thumbnail_rgb-000\" , \"thumbnail_rgb-001\" , \"...\" \"thumbnail_rgb-071\" ], \"zip\" : \"scan\" } } Found under the metadata/Visualization/ directory, there are two category of JSON files: image_*.json contains ??? and is produced by ???. thumbnail_*.json contains ??? and is produced by ???. Example for image_*.json : { \"image_id\" : \"rgb-000\" } Example for thumbnail_*.json : { \"image_id\" : \"rgb-000\" } Other metadata Link Todo Explain what are & who produce: - the image JSON file metadata/images.json - the image JSON files found under metadata/images/*.json - the visualization JSON file metadata/Visualization.json - the visualization JSON files found under metadata/Visualization/*.json","title":"Home"},{"location":"Scanner/metadata/#fair-data-for-the-romi-plant-scanner-project","text":"","title":"FAIR data for the ROMI plant scanner project"},{"location":"Scanner/metadata/#fair-data","text":"We aim at using FAIR data principles in the ROMI plant scanner project. Quoting the GoFAIR website: Quote In 2016, the \" FAIR Guiding Principles for scientific data management and stewardship \" were published in Scientific Data. The authors intended to provide guidelines to improve the Findability , Accessibility , Interoperability , and Reuse of digital assets. The principles emphasise machine-actionability (i.e., the capacity of computational systems to find, access, interoperate, and reuse data with none or minimal human intervention) because humans increasingly rely on computational support to deal with data as a result of the increase in volume, complexity, and creation speed of data. In our context, a biological dataset is a set of RAW images (eg: RGB images), used to reconstruct the plant 3D structure, associated with a set of metadata of different nature: biological, hardware & software.","title":"FAIR data"},{"location":"Scanner/metadata/#romi-plant-scanner-metadata","text":"Metadata are stored hierarchically. We currently use the JSON format. There are many JSON files containing metadata in the metadata directory attached to a dataset.","title":"ROMI plant scanner metadata"},{"location":"Scanner/metadata/#general-metadata","text":"The first you should consider is metadata/metadata.json . Its top-level entries are: \"scanner\", the hardware metadata (see here ) \"object\", the biological metadata (see here ) \"path\", the parameter values used for the task Scan (see here ) \"computed\", the parameter values used for the task Colmap (see here ) \"measures\", the parameter values used for the task AnglesAndInternodes (see here ) Todo Remove top-level entries \"path\", \"computed\" & \"measures\", they look like duplicates from their respective task metadata.","title":"General metadata"},{"location":"Scanner/metadata/#scanning-operation","text":"Found under the path top level section, it contains: the trajectory of the camera under path Example: \"path\" : { \"args\" : { \"filetype\" : \"jpg\" , \"num_points\" : 72 , \"radius\" : 350 , \"tilt\" : 0.45 , \"xc\" : 400 , \"yc\" : 400 , \"z\" : 0 }, \"id\" : \"circular_72\" , \"type\" : \"circular\" } Todo Potential duplication from the Scan task metadata!","title":"Scanning operation"},{"location":"Scanner/metadata/#colmap-reconstruction","text":"Found under the computed top level section, it contains the camera_model used by Colmap. Example: \"computed\" : { \"camera_model\" : { \"height\" : 1080 , \"id\" : 1 , \"model\" : \"OPENCV\" , \"params\" : [ 1102.2709767952233 , 1102.2709767952233 , 808.0 , 540.0 , -0.015118876273724434 , -0.015118876273724434 , 0.0 , 0.0 ], \"width\" : 1616 } } Todo Potential duplication from the Colmap task metadata!","title":"Colmap reconstruction"},{"location":"Scanner/metadata/#measures-of-angles-and-internodes","text":"Found under the measures top level section: Measured angles are under angles Measured internodes are under internodes Example: \"measures\" : { \"angles\" : [ 2.6179938779914944 , 1.3089969389957472 , ... 2.0943951023931953 ], \"internodes\" : [ 41 , 32 , ... 1 ] } Todo Potential duplication from the AnglesAndInternodes task metadata!","title":"Measures of angles and internodes"},{"location":"Scanner/metadata/#tasks-metadata","text":"Then there are the JSON files attached to each task, e.g. Colmap_True____feature_extrac_3bbfcb1413.json , they contain the parameter used to run this task. Their content and meaning is explained in the task metadata section.","title":"Tasks metadata"},{"location":"Scanner/metadata/#images-metadata","text":"The image JSON file metadata/images.json contains ??? and is produced by ???. Example: { \"task_params\" : { \"fileset_id\" : \"images\" , \"output_file_id\" : \"out\" , \"scan_id\" : \"\" } } Found under the metadata/images directory, these JSON files contains ??? and is produced by ???. Note Sub-section camera_model seems redundant with same section in reconstruction . Example: { \"calibrated_pose\" : [ 49.04537654162613 , 401.1470121046677 , -0.10613970524327433 ], \"colmap_camera\" : { \"camera_model\" : { \"height\" : 1080 , \"id\" : 1 , \"model\" : \"OPENCV\" , \"params\" : [ 1106.9593323985682 , 1106.9593323985682 , 808.0 , 540.0 , -0.012379986602455324 , -0.012379986602455324 , 0.0 , 0.0 ], \"width\" : 1616 }, \"rotmat\" : [ [ -0.07758281248083276 , 0.9961595266033345 , 0.040584538496628464 ], [ -0.4230067224604736 , -0.069751540308706 , 0.9034378979087665 ], [ 0.9027991027691664 , 0.05292372040950383 , 0.42679369706827314 ] ], \"tvec\" : [ -397.2357284138349 , 49.50722946972662 , -66.64522999892229 ] }, \"pose\" : [ 50.0 , 400.0 , 0 , 0.0 , 0.45 ] }","title":"Images metadata"},{"location":"Scanner/metadata/#visualization-metadata","text":"The visualization JSON file metadata/Visualization.json contains ??? and is produced by ???. Important Run the Visualization task to complete this section! Example: { \"files\" : { \"angles\" : \"AnglesAndInternodes\" , \"images\" : [ \"image_rgb-000\" , \"image_rgb-001\" , \"...\" \"image_rgb-071\" ], \"mesh\" : \"TriangleMesh\" , \"point_cloud\" : \"PointCloud\" , \"poses\" : \"images\" , \"skeleton\" : \"CurveSkeleton\" , \"thumbnails\" : [ \"thumbnail_rgb-000\" , \"thumbnail_rgb-001\" , \"...\" \"thumbnail_rgb-071\" ], \"zip\" : \"scan\" } } Found under the metadata/Visualization/ directory, there are two category of JSON files: image_*.json contains ??? and is produced by ???. thumbnail_*.json contains ??? and is produced by ???. Example for image_*.json : { \"image_id\" : \"rgb-000\" } Example for thumbnail_*.json : { \"image_id\" : \"rgb-000\" }","title":"Visualization metadata"},{"location":"Scanner/metadata/#other-metadata","text":"Todo Explain what are & who produce: - the image JSON file metadata/images.json - the image JSON files found under metadata/images/*.json - the visualization JSON file metadata/Visualization.json - the visualization JSON files found under metadata/Visualization/*.json","title":"Other metadata"},{"location":"Scanner/metadata/biological_metadata/","text":"Biological Metadata Link Biological metadata are informative of the biological object and its growth conditions. Definitions Link Here is a list of biological metadata and their definition: species : the species of the biological object analysed, eg : \"Arabidopsis thaliana\"; seed stock : an identifier of the seed stock used, eg : \"Col-0\", \"186.AV.L1\", ...; plant id : an identifier for the plant, eg : \"GT1\"; growth environment : , eg : \"Lyon - Indoor\"; growth conditions : growth condition used, eg : \"LD\", \"SD\", \"LD+SD\"; treatment : specific treatment applied, if any, eg : \"Auxin 1mM\"; DAG : Days After Germination or age of the plant in days, eg : 40; sample : part of the plant used, if any, eg : \"main stem\"; experiment id : an identifier for the experiment, eg : \"dry plant\"; dataset id : the Omero dataset identifier for the biological dataset, eg : 12; Configuration Link Todo How is it defined in a TOML configuration file ? Database location Link Located in metadata/metadata.json and found under the object top level section, it contains biologically relevant information such as the studied species, its age and growth conditions. This information is not restricted in its format but should contain a minimal set of entries. Todo Defines the minimal set of entries! Use the MIAPPE standard? JSON example Link Example of a metadata/metadata.json file for biological metadata: \"object\" : { \"age\" : \"62d\" , \"culture\" : \"LD\" , \"environment\" : \"Lyon indoor\" , \"experiment_id\" : \"living plant\" , \"object\" : \"plant\" , \"plant_id\" : \"Col0_26_10_2018_B\" , \"sample\" : \"main stem\" , \"species\" : \"Arabidopsis thaliana\" , \"stock\" : \"186AV.L1\" , \"treatment\" : \"none\" }","title":"Biological metadata"},{"location":"Scanner/metadata/biological_metadata/#biological-metadata","text":"Biological metadata are informative of the biological object and its growth conditions.","title":"Biological Metadata"},{"location":"Scanner/metadata/biological_metadata/#definitions","text":"Here is a list of biological metadata and their definition: species : the species of the biological object analysed, eg : \"Arabidopsis thaliana\"; seed stock : an identifier of the seed stock used, eg : \"Col-0\", \"186.AV.L1\", ...; plant id : an identifier for the plant, eg : \"GT1\"; growth environment : , eg : \"Lyon - Indoor\"; growth conditions : growth condition used, eg : \"LD\", \"SD\", \"LD+SD\"; treatment : specific treatment applied, if any, eg : \"Auxin 1mM\"; DAG : Days After Germination or age of the plant in days, eg : 40; sample : part of the plant used, if any, eg : \"main stem\"; experiment id : an identifier for the experiment, eg : \"dry plant\"; dataset id : the Omero dataset identifier for the biological dataset, eg : 12;","title":"Definitions"},{"location":"Scanner/metadata/biological_metadata/#configuration","text":"Todo How is it defined in a TOML configuration file ?","title":"Configuration"},{"location":"Scanner/metadata/biological_metadata/#database-location","text":"Located in metadata/metadata.json and found under the object top level section, it contains biologically relevant information such as the studied species, its age and growth conditions. This information is not restricted in its format but should contain a minimal set of entries. Todo Defines the minimal set of entries! Use the MIAPPE standard?","title":"Database location"},{"location":"Scanner/metadata/biological_metadata/#json-example","text":"Example of a metadata/metadata.json file for biological metadata: \"object\" : { \"age\" : \"62d\" , \"culture\" : \"LD\" , \"environment\" : \"Lyon indoor\" , \"experiment_id\" : \"living plant\" , \"object\" : \"plant\" , \"plant_id\" : \"Col0_26_10_2018_B\" , \"sample\" : \"main stem\" , \"species\" : \"Arabidopsis thaliana\" , \"stock\" : \"186AV.L1\" , \"treatment\" : \"none\" }","title":"JSON example"},{"location":"Scanner/metadata/hardware_metadata/","text":"Hardware metadata & scan settings Link Hardware metadata are informative of the hardware setup like the used camera, its firmwares or the workspace size. Definitions Link Here is a list of hardware metadata and their definition: frame : scanner frame type and version, eg : \"30profile v1\"; X_motor : type of motor used for the X axis, eg : \"X-Carve NEMA23\"; Y_motor : type of motor used for the Y axis, eg : \"X-Carve NEMA23\"; Z_motor : type of motor used for the Z axis, eg : \"X-Carve NEMA23\"; pan_motor : , type of motor used for the camera pan axis, eg : \"Dynamixel\"; tilt_motor : , type of motor used for the camera tilt axis, eg : \"Gimball\"; sensor : type of sensor used during acquisition, eg : \"Sony alpha\"; scan_os : , eg : \"\"; The metadata dictionary made of frame , X_motor , Y_motor , Z_motor , pan_motor & tilt_motor define the hardware_id used in the README.md . The sensor metadata could be more detailed, for example as a dictionary or a reference to a sensor_id database? Configuration Link Todo How is it defined in a TOML configuration file ? Database location Link Located in metadata/metadata.json and found under the scanner top level section, it contains information about the hardware and software used for the scan: the used camera with camera_args , camera_firmware , camera_hardware & camera_lens the model and version of the scanning station with id list of hardware and software components and their versions with cnc_args , cnc_firmware , cnc_hardware , frame , gimbal_args , gimbal_firmware , gimbal_hardware the used workspace with workspace JSON example Link Example of a metadata/metadata.json file for hardware metadata: \"scanner\" : { \"camera_args\" : { \"api_url\" : \"http://192.168.122.1:8080\" }, \"camera_firmware\" : \"sony_wifi\" , \"camera_hardware\" : \"Sony Alpha 5100\" , \"camera_lens\" : \"16-35 stock\" , \"cnc_args\" : { \"homing\" : true , \"port\" : \"/dev/ttyUSB1\" }, \"cnc_firmware\" : \"grbl-v1.1\" , \"cnc_hardware\" : \"xcarve-v2\" , \"frame\" : \"alu 40mm\" , \"gimbal_args\" : { \"baud_rate\" : 57600 , \"dev\" : \"/dev/ttyUSB0\" , \"tilt0\" : 3072 }, \"gimbal_firmware\" : \"dynamixel-usb2dynamixel\" , \"gimbal_hardware\" : \"dynamixel\" , \"id\" : \"lyon_1\" , \"workspace\" : { \"x\" : [ 200 , 600 ], \"y\" : [ 200 , 600 ], \"z\" : [ -180 , 260 ] } } Todo Gather all camera parameters under a camera section? Gather all cnc parameters under a cnc section? Gather all gimbal parameters under a gimbal section?","title":"Hardware metadata"},{"location":"Scanner/metadata/hardware_metadata/#hardware-metadata-scan-settings","text":"Hardware metadata are informative of the hardware setup like the used camera, its firmwares or the workspace size.","title":"Hardware metadata &amp; scan settings"},{"location":"Scanner/metadata/hardware_metadata/#definitions","text":"Here is a list of hardware metadata and their definition: frame : scanner frame type and version, eg : \"30profile v1\"; X_motor : type of motor used for the X axis, eg : \"X-Carve NEMA23\"; Y_motor : type of motor used for the Y axis, eg : \"X-Carve NEMA23\"; Z_motor : type of motor used for the Z axis, eg : \"X-Carve NEMA23\"; pan_motor : , type of motor used for the camera pan axis, eg : \"Dynamixel\"; tilt_motor : , type of motor used for the camera tilt axis, eg : \"Gimball\"; sensor : type of sensor used during acquisition, eg : \"Sony alpha\"; scan_os : , eg : \"\"; The metadata dictionary made of frame , X_motor , Y_motor , Z_motor , pan_motor & tilt_motor define the hardware_id used in the README.md . The sensor metadata could be more detailed, for example as a dictionary or a reference to a sensor_id database?","title":"Definitions"},{"location":"Scanner/metadata/hardware_metadata/#configuration","text":"Todo How is it defined in a TOML configuration file ?","title":"Configuration"},{"location":"Scanner/metadata/hardware_metadata/#database-location","text":"Located in metadata/metadata.json and found under the scanner top level section, it contains information about the hardware and software used for the scan: the used camera with camera_args , camera_firmware , camera_hardware & camera_lens the model and version of the scanning station with id list of hardware and software components and their versions with cnc_args , cnc_firmware , cnc_hardware , frame , gimbal_args , gimbal_firmware , gimbal_hardware the used workspace with workspace","title":"Database location"},{"location":"Scanner/metadata/hardware_metadata/#json-example","text":"Example of a metadata/metadata.json file for hardware metadata: \"scanner\" : { \"camera_args\" : { \"api_url\" : \"http://192.168.122.1:8080\" }, \"camera_firmware\" : \"sony_wifi\" , \"camera_hardware\" : \"Sony Alpha 5100\" , \"camera_lens\" : \"16-35 stock\" , \"cnc_args\" : { \"homing\" : true , \"port\" : \"/dev/ttyUSB1\" }, \"cnc_firmware\" : \"grbl-v1.1\" , \"cnc_hardware\" : \"xcarve-v2\" , \"frame\" : \"alu 40mm\" , \"gimbal_args\" : { \"baud_rate\" : 57600 , \"dev\" : \"/dev/ttyUSB0\" , \"tilt0\" : 3072 }, \"gimbal_firmware\" : \"dynamixel-usb2dynamixel\" , \"gimbal_hardware\" : \"dynamixel\" , \"id\" : \"lyon_1\" , \"workspace\" : { \"x\" : [ 200 , 600 ], \"y\" : [ 200 , 600 ], \"z\" : [ -180 , 260 ] } } Todo Gather all camera parameters under a camera section? Gather all cnc parameters under a cnc section? Gather all gimbal parameters under a gimbal section?","title":"JSON example"},{"location":"Scanner/metadata/software_metadata/","text":"Software metadata: versioning and history Link They aim at keeping track of the algorithm versions, using their git tag, during any ROMI task. They enable to trace back results and compare them.","title":"Software metadata"},{"location":"Scanner/metadata/software_metadata/#software-metadata-versioning-and-history","text":"They aim at keeping track of the algorithm versions, using their git tag, during any ROMI task. They enable to trace back results and compare them.","title":"Software metadata: versioning and history"},{"location":"Scanner/metadata/tasks_metadata/","text":"Tasks metadata: parametrization Link Aim Link They aim at keeping track of the parameters used by algorithms during any ROMI task. They enable to trace back results and compare them. Database location Link Located under the metadata directory of the plant scan dataset, these JSON files contain the parameters used to run the task and are produced by each task. Examples of task metadata JSON file names: AnglesAndInternodes_1_0_2_0_0_1_9e87e344e6.json Colmap_True____feature_extrac_3bbfcb1413.json Masks_True_5_out_9adb9db801.json TreeGraph_out__CurveSkeleton_5dca9a2821.json Undistorted_out_____fb3e3fa0ff.json Voxels_False___background___False_0ac9c133f7.json Visualization.json The tasks ids are a concatenation of the task name, the first values of the first 3 parameters sorted by parameter name and a md5hash of the name/parameters as a cananocalised json (from luigi documentation of task_id_str) AnglesAndInternodes task Link Configuration Link To configure this task, we use the [AnglesAndInternodes] section in the TOML configuration file. For example: [AnglesAndInternodes] upstream_task = \"TreeGraph\" characteristic_length = 1.0 stem_axis_inverted = false Database location Link Found under metadata/AnglesAndInternodes_*.json . JSON example Link Example of metadata/AnglesAndInternodes_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"characteristic_length\" : 1.0 , \"min_elongation_ratio\" : 2.0 , \"min_fruit_size\" : 0.1 , \"number_nn\" : 50 , \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"stem_axis\" : 2 , \"stem_axis_inverted\" : \"False\" , \"upstream_task\" : \"ClusteredMesh\" } } ClusteredMesh task Link Configuration Link To configure this task, we use the [Colmap] section in the TOML configuration file. For example: [ClusteredMesh] upstream_task = \"SegmentedPointCloud\" min_vol = 1.0 min_length = 10.0 Database location Link Found under metadata/ClusteredMesh_*.json . JSON example Link Example of metadata/ClusteredMesh_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"min_length\" : 10.0 , \"min_vol\" : 1.0 , \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"upstream_task\" : \"SegmentedPointCloud\" } } Colmap task Link Configuration Link To configure this task, we use the [Colmap] section in the TOML configuration file. For example: [Colmap] matcher = \"exhaustive\" compute_dense = false [Colmap.cli_args.feature_extractor] \"--ImageReader.single_camera\" = \"1\" \"--SiftExtraction.use_gpu\" = \"1\" [Colmap.cli_args.exhaustive_matcher] \"--SiftMatching.use_gpu\" = \"1\" [Colmap.cli_args.model_aligner] \"--robust_alignment_max_error\" = \"10\" [ Colmap . bounding_box ] # default to None x = [150, 650] y = [150, 650] z = [-90, 300] Database location Link Found under metadata/Colmap_*.json . JSON example Link Example of metadata/Colmap_*.json corresponding to the example TOML configuration file: { \"bounding_box\" : { \"x\" : [ 282.3466418101626 , 590.7798175997629 ], \"y\" : [ 386.77943280470265 , 589.1307313142569 ], \"z\" : [ 22.820168903922998 , 259.7594718279537 ] }, \"task_params\" : { \"align_pcd\" : \"True\" , \"calibration_scan_id\" : \"\" , \"cli_args\" : { \"exhaustive_matcher\" : { \"--SiftMatching.use_gpu\" : \"1\" }, \"feature_extractor\" : { \"--ImageReader.single_camera\" : \"1\" , \"--SiftExtraction.use_gpu\" : \"1\" }, \"model_aligner\" : { \"--robust_alignment_max_error\" : \"10\" } }, \"compute_dense\" : \"False\" , \"matcher\" : \"exhaustive\" , \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"upstream_task\" : \"ImagesFilesetExists\" } } CurveSkeleton task Link Configuration Link To configure this task, we use the [CurveSkeleton] section in the TOML configuration file. For example: [CurveSkeleton] upstream_task = \"TriangleMesh\" Database location Link Found under metadata/CurveSkeleton_*.json . JSON example Link Example of metadata/CurveSkeleton_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"upstream_task\" : \"TriangleMesh\" } } Masks task Link Configuration Link To configure this task, we use the [Masks] section in the TOML configuration file. Default parametrization based on linear masking. For example: [Masks] upstream_task = \"Undistorted\" type = \"linear\" parameters = \"[0,1,0]\" dilation = 5 binarize = true threshold = 0.3 Database location Link Found under metadata/Masks_*.json . JSON example Link Example of metadata/Masks_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"binarize\" : \"True\" , \"dilation\" : 5 , \"output_file_id\" : \"out\" , \"parameters\" : [ 0 , 1 , 0 ], \"query\" : {}, \"scan_id\" : \"\" , \"threshold\" : 0.3 , \"type\" : \"linear\" , \"upstream_task\" : \"ImagesFilesetExists\" } } PointCloud task Link Configuration Link To configure this task, we use the [PointCloud] section in the TOML configuration file. For example: [PointCloud] upstream_task = \"Voxels\" level_set_value = 1.0 log = false background_prior = -200 min_contrast = 10.0 min_score = 0.2 Database location Link Found under metadata/PointCloud_*.json . JSON example Link Example of metadata/PointCloud_*.json : { \"task_params\" : { \"background_prior\" : 0.5 , \"level_set_value\" : 1 , \"log\" : \"False\" , \"min_contrast\" : 10.0 , \"min_score\" : 0.2 , \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"upstream_task\" : \"Voxels\" } } Segmentation2D task Link Configuration Link To configure this task, we use the [Segmentation2D] section in the TOML configuration file. For example: [Segmentation2D] upstream_task = \"Undistorted\" model_fileset = \"ModelFileset\" query = \"{\\\"channel\\\":\\\"rgb\\\"}\" model_id = \"Resnetdataset_gl_png_896_896_epoch50\" resize = true binarize = true dilation = 1 Sx = 896 Sy = 896 epochs = 1 batch = 1 learning_rate = 0.0001 threshold = 0.0035 [ModelFileset] scan_id = \"models\" Database location Link Found under metadata/Segmentation2D_*.json . JSON example Link Example of metadata/Segmentation2D_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"Sx\" : 896 , \"Sy\" : 896 , \"binarize\" : \"True\" , \"dilation\" : 1 , \"inverted_labels\" : [ \"background\" ], \"labels\" : [], \"model_fileset\" : \"ModelFileset\" , \"model_id\" : \"Resnetdataset_gl_png_896_896_epoch50\" , \"output_file_id\" : \"out\" , \"query\" : { \"channel\" : \"rgb\" }, \"resize\" : \"True\" , \"scan_id\" : \"\" , \"threshold\" : 0.0035 , \"upstream_task\" : \"Undistorted\" } } SegmentedPointCloud task Link Configuration Link To configure this task, we use the [SegmentedPointCloud] section in the TOML configuration file. For example: [SegmentedPointCloud] upstream_segmentation = \"Segmentation2D\" upstream_task = \"PointCloud\" use_colmap_poses = true Database location Link Found under metadata/SegmentedPointCloud_*.json . JSON example Link Example of metadata/SegmentedPointCloud_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"upstream_segmentation\" : \"Segmentation2D\" , \"upstream_task\" : \"PointCloud\" , \"use_colmap_poses\" : \"True\" } } TreeGraph task Link Configuration Link To configure this task, we use the [TreeGraph] section in the TOML configuration file. For example: [TreeGraph] upstream_task = \"CurveSkeleton\" z_axis = 2 stem_axis_inverted = false Database location Link Found under metadata/TreeGraph_*.json . JSON example Link Example of metadata/TreeGraph_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"upstream_task\" : \"CurveSkeleton\" , \"z_axis\" : 2 , \"z_orientation\" : 1 } } TriangleMesh task Link Configuration Link To configure this task, we use the [TriangleMesh] section in the TOML configuration file. For example: [TriangleMesh] upstream_task = \"PointCloud\" Database location Link Found under metadata/TriangleMesh_*.json . JSON example Link Example of metadata/TriangleMesh_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"upstream_task\" : \"PointCloud\" } } Undistorted task Link Configuration Link To configure this task, we use the [Undistorted] section in the TOML configuration file. For example: [Undistorted] upstream_task = \"ImagesFilesetExists\" Database location Link Found under metadata/Undistorted_*.json . JSON example Link Example of metadata/Undistorted_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"output_file_id\" : \"out\" , \"query\" : {}, \"scan_id\" : \"\" , \"upstream_task\" : \"ImagesFilesetExists\" } } Visualization task Link Configuration Link To configure this task, we use the [Visualization] section in the TOML configuration file. For example: [Visualization] upstream_point_cloud = \"PointCloud\" upstream_mesh = \"TriangleMesh\" upstream_colmap = \"Colmap\" upstream_angles = \"AnglesAndInternodes\" upstream_skeleton = \"CurveSkeleton\" upstream_images = \"ImagesFilesetExists\" max_image_size = 1500 max_point_cloud_size = 10000 thumbnail_size = 150 Database location Link Found under metadata/Visualization.json . JSON example Link Example of metadata/Visualization.json corresponding to the example TOML configuration file: { \"files\" : { \"angles\" : \"AnglesAndInternodes\" , \"camera\" : \"cameras\" , \"images\" : [ \"image_00000_rgb\" , \"image_00001_rgb\" , \"image_00002_rgb\" , \"image_00003_rgb\" , \"image_00004_rgb\" , \"image_00005_rgb\" , \"image_00006_rgb\" , \"image_00007_rgb\" , \"image_00008_rgb\" , \"image_00009_rgb\" , \"image_00010_rgb\" , \"image_00011_rgb\" , \"image_00012_rgb\" , \"image_00013_rgb\" , \"image_00014_rgb\" , \"image_00015_rgb\" , \"image_00016_rgb\" , \"image_00017_rgb\" , \"image_00018_rgb\" , \"image_00019_rgb\" , \"image_00020_rgb\" , \"image_00021_rgb\" , \"image_00022_rgb\" , \"image_00023_rgb\" , \"image_00024_rgb\" , \"image_00025_rgb\" , \"image_00026_rgb\" , \"image_00027_rgb\" , \"image_00028_rgb\" , \"image_00029_rgb\" , \"image_00030_rgb\" , \"image_00031_rgb\" , \"image_00032_rgb\" , \"image_00033_rgb\" , \"image_00034_rgb\" , \"image_00035_rgb\" , \"image_00036_rgb\" , \"image_00037_rgb\" , \"image_00038_rgb\" , \"image_00039_rgb\" , \"image_00040_rgb\" , \"image_00041_rgb\" , \"image_00042_rgb\" , \"image_00043_rgb\" , \"image_00044_rgb\" , \"image_00045_rgb\" , \"image_00046_rgb\" , \"image_00047_rgb\" , \"image_00048_rgb\" , \"image_00049_rgb\" , \"image_00050_rgb\" , \"image_00051_rgb\" , \"image_00052_rgb\" , \"image_00053_rgb\" , \"image_00054_rgb\" , \"image_00055_rgb\" , \"image_00056_rgb\" , \"image_00057_rgb\" , \"image_00058_rgb\" , \"image_00059_rgb\" ], \"measures\" : \"measures\" , \"mesh\" : \"TriangleMesh\" , \"point_cloud\" : \"PointCloud\" , \"poses\" : \"images\" , \"skeleton\" : \"CurveSkeleton\" , \"thumbnails\" : [ \"thumbnail_00000_rgb\" , \"thumbnail_00001_rgb\" , \"thumbnail_00002_rgb\" , \"thumbnail_00003_rgb\" , \"thumbnail_00004_rgb\" , \"thumbnail_00005_rgb\" , \"thumbnail_00006_rgb\" , \"thumbnail_00007_rgb\" , \"thumbnail_00008_rgb\" , \"thumbnail_00009_rgb\" , \"thumbnail_00010_rgb\" , \"thumbnail_00011_rgb\" , \"thumbnail_00012_rgb\" , \"thumbnail_00013_rgb\" , \"thumbnail_00014_rgb\" , \"thumbnail_00015_rgb\" , \"thumbnail_00016_rgb\" , \"thumbnail_00017_rgb\" , \"thumbnail_00018_rgb\" , \"thumbnail_00019_rgb\" , \"thumbnail_00020_rgb\" , \"thumbnail_00021_rgb\" , \"thumbnail_00022_rgb\" , \"thumbnail_00023_rgb\" , \"thumbnail_00024_rgb\" , \"thumbnail_00025_rgb\" , \"thumbnail_00026_rgb\" , \"thumbnail_00027_rgb\" , \"thumbnail_00028_rgb\" , \"thumbnail_00029_rgb\" , \"thumbnail_00030_rgb\" , \"thumbnail_00031_rgb\" , \"thumbnail_00032_rgb\" , \"thumbnail_00033_rgb\" , \"thumbnail_00034_rgb\" , \"thumbnail_00035_rgb\" , \"thumbnail_00036_rgb\" , \"thumbnail_00037_rgb\" , \"thumbnail_00038_rgb\" , \"thumbnail_00039_rgb\" , \"thumbnail_00040_rgb\" , \"thumbnail_00041_rgb\" , \"thumbnail_00042_rgb\" , \"thumbnail_00043_rgb\" , \"thumbnail_00044_rgb\" , \"thumbnail_00045_rgb\" , \"thumbnail_00046_rgb\" , \"thumbnail_00047_rgb\" , \"thumbnail_00048_rgb\" , \"thumbnail_00049_rgb\" , \"thumbnail_00050_rgb\" , \"thumbnail_00051_rgb\" , \"thumbnail_00052_rgb\" , \"thumbnail_00053_rgb\" , \"thumbnail_00054_rgb\" , \"thumbnail_00055_rgb\" , \"thumbnail_00056_rgb\" , \"thumbnail_00057_rgb\" , \"thumbnail_00058_rgb\" , \"thumbnail_00059_rgb\" ], \"zip\" : \"scan\" }, \"task_params\" : { \"max_image_size\" : 1500 , \"max_point_cloud_size\" : 10000 , \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"thumbnail_size\" : 150 , \"upstream_angles\" : \"AnglesAndInternodes\" , \"upstream_colmap\" : \"Colmap\" , \"upstream_images\" : \"Undistorted\" , \"upstream_mesh\" : \"TriangleMesh\" , \"upstream_point_cloud\" : \"PointCloud\" , \"upstream_skeleton\" : \"CurveSkeleton\" } } Voxels task Link Configuration Link To configure this task, we use the [Voxels] section in the TOML configuration file. For example: [Voxels] upstream_mask = \"Masks\" upstream_colmap = \"Colmap\" use_colmap_poses = true voxel_size = 1.0 type = \"carving\" log = false invert = false labels = \"[]\" Database location Link Found under metadata/Voxels_*.json . JSON example Link Example of metadata/Voxels_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"invert\" : \"False\" , \"labels\" : [ \"background\" ], \"log\" : \"False\" , \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"type\" : \"averaging\" , \"upstream_colmap\" : \"Colmap\" , \"upstream_mask\" : \"Segmentation2D\" , \"use_colmap_poses\" : \"True\" , \"voxel_size\" : 0.01 } } Scan task Link Todo For some reason the parameters are defined in the metadata/metadata.json file. Definition can be found here .","title":"Tasks metadata"},{"location":"Scanner/metadata/tasks_metadata/#tasks-metadata-parametrization","text":"","title":"Tasks metadata: parametrization"},{"location":"Scanner/metadata/tasks_metadata/#aim","text":"They aim at keeping track of the parameters used by algorithms during any ROMI task. They enable to trace back results and compare them.","title":"Aim"},{"location":"Scanner/metadata/tasks_metadata/#database-location","text":"Located under the metadata directory of the plant scan dataset, these JSON files contain the parameters used to run the task and are produced by each task. Examples of task metadata JSON file names: AnglesAndInternodes_1_0_2_0_0_1_9e87e344e6.json Colmap_True____feature_extrac_3bbfcb1413.json Masks_True_5_out_9adb9db801.json TreeGraph_out__CurveSkeleton_5dca9a2821.json Undistorted_out_____fb3e3fa0ff.json Voxels_False___background___False_0ac9c133f7.json Visualization.json The tasks ids are a concatenation of the task name, the first values of the first 3 parameters sorted by parameter name and a md5hash of the name/parameters as a cananocalised json (from luigi documentation of task_id_str)","title":"Database location"},{"location":"Scanner/metadata/tasks_metadata/#anglesandinternodes-task","text":"","title":"AnglesAndInternodes task"},{"location":"Scanner/metadata/tasks_metadata/#configuration","text":"To configure this task, we use the [AnglesAndInternodes] section in the TOML configuration file. For example: [AnglesAndInternodes] upstream_task = \"TreeGraph\" characteristic_length = 1.0 stem_axis_inverted = false","title":"Configuration"},{"location":"Scanner/metadata/tasks_metadata/#database-location_1","text":"Found under metadata/AnglesAndInternodes_*.json .","title":"Database location"},{"location":"Scanner/metadata/tasks_metadata/#json-example","text":"Example of metadata/AnglesAndInternodes_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"characteristic_length\" : 1.0 , \"min_elongation_ratio\" : 2.0 , \"min_fruit_size\" : 0.1 , \"number_nn\" : 50 , \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"stem_axis\" : 2 , \"stem_axis_inverted\" : \"False\" , \"upstream_task\" : \"ClusteredMesh\" } }","title":"JSON example"},{"location":"Scanner/metadata/tasks_metadata/#clusteredmesh-task","text":"","title":"ClusteredMesh task"},{"location":"Scanner/metadata/tasks_metadata/#configuration_1","text":"To configure this task, we use the [Colmap] section in the TOML configuration file. For example: [ClusteredMesh] upstream_task = \"SegmentedPointCloud\" min_vol = 1.0 min_length = 10.0","title":"Configuration"},{"location":"Scanner/metadata/tasks_metadata/#database-location_2","text":"Found under metadata/ClusteredMesh_*.json .","title":"Database location"},{"location":"Scanner/metadata/tasks_metadata/#json-example_1","text":"Example of metadata/ClusteredMesh_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"min_length\" : 10.0 , \"min_vol\" : 1.0 , \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"upstream_task\" : \"SegmentedPointCloud\" } }","title":"JSON example"},{"location":"Scanner/metadata/tasks_metadata/#colmap-task","text":"","title":"Colmap task"},{"location":"Scanner/metadata/tasks_metadata/#configuration_2","text":"To configure this task, we use the [Colmap] section in the TOML configuration file. For example: [Colmap] matcher = \"exhaustive\" compute_dense = false [Colmap.cli_args.feature_extractor] \"--ImageReader.single_camera\" = \"1\" \"--SiftExtraction.use_gpu\" = \"1\" [Colmap.cli_args.exhaustive_matcher] \"--SiftMatching.use_gpu\" = \"1\" [Colmap.cli_args.model_aligner] \"--robust_alignment_max_error\" = \"10\" [ Colmap . bounding_box ] # default to None x = [150, 650] y = [150, 650] z = [-90, 300]","title":"Configuration"},{"location":"Scanner/metadata/tasks_metadata/#database-location_3","text":"Found under metadata/Colmap_*.json .","title":"Database location"},{"location":"Scanner/metadata/tasks_metadata/#json-example_2","text":"Example of metadata/Colmap_*.json corresponding to the example TOML configuration file: { \"bounding_box\" : { \"x\" : [ 282.3466418101626 , 590.7798175997629 ], \"y\" : [ 386.77943280470265 , 589.1307313142569 ], \"z\" : [ 22.820168903922998 , 259.7594718279537 ] }, \"task_params\" : { \"align_pcd\" : \"True\" , \"calibration_scan_id\" : \"\" , \"cli_args\" : { \"exhaustive_matcher\" : { \"--SiftMatching.use_gpu\" : \"1\" }, \"feature_extractor\" : { \"--ImageReader.single_camera\" : \"1\" , \"--SiftExtraction.use_gpu\" : \"1\" }, \"model_aligner\" : { \"--robust_alignment_max_error\" : \"10\" } }, \"compute_dense\" : \"False\" , \"matcher\" : \"exhaustive\" , \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"upstream_task\" : \"ImagesFilesetExists\" } }","title":"JSON example"},{"location":"Scanner/metadata/tasks_metadata/#curveskeleton-task","text":"","title":"CurveSkeleton task"},{"location":"Scanner/metadata/tasks_metadata/#configuration_3","text":"To configure this task, we use the [CurveSkeleton] section in the TOML configuration file. For example: [CurveSkeleton] upstream_task = \"TriangleMesh\"","title":"Configuration"},{"location":"Scanner/metadata/tasks_metadata/#database-location_4","text":"Found under metadata/CurveSkeleton_*.json .","title":"Database location"},{"location":"Scanner/metadata/tasks_metadata/#json-example_3","text":"Example of metadata/CurveSkeleton_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"upstream_task\" : \"TriangleMesh\" } }","title":"JSON example"},{"location":"Scanner/metadata/tasks_metadata/#masks-task","text":"","title":"Masks task"},{"location":"Scanner/metadata/tasks_metadata/#configuration_4","text":"To configure this task, we use the [Masks] section in the TOML configuration file. Default parametrization based on linear masking. For example: [Masks] upstream_task = \"Undistorted\" type = \"linear\" parameters = \"[0,1,0]\" dilation = 5 binarize = true threshold = 0.3","title":"Configuration"},{"location":"Scanner/metadata/tasks_metadata/#database-location_5","text":"Found under metadata/Masks_*.json .","title":"Database location"},{"location":"Scanner/metadata/tasks_metadata/#json-example_4","text":"Example of metadata/Masks_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"binarize\" : \"True\" , \"dilation\" : 5 , \"output_file_id\" : \"out\" , \"parameters\" : [ 0 , 1 , 0 ], \"query\" : {}, \"scan_id\" : \"\" , \"threshold\" : 0.3 , \"type\" : \"linear\" , \"upstream_task\" : \"ImagesFilesetExists\" } }","title":"JSON example"},{"location":"Scanner/metadata/tasks_metadata/#pointcloud-task","text":"","title":"PointCloud task"},{"location":"Scanner/metadata/tasks_metadata/#configuration_5","text":"To configure this task, we use the [PointCloud] section in the TOML configuration file. For example: [PointCloud] upstream_task = \"Voxels\" level_set_value = 1.0 log = false background_prior = -200 min_contrast = 10.0 min_score = 0.2","title":"Configuration"},{"location":"Scanner/metadata/tasks_metadata/#database-location_6","text":"Found under metadata/PointCloud_*.json .","title":"Database location"},{"location":"Scanner/metadata/tasks_metadata/#json-example_5","text":"Example of metadata/PointCloud_*.json : { \"task_params\" : { \"background_prior\" : 0.5 , \"level_set_value\" : 1 , \"log\" : \"False\" , \"min_contrast\" : 10.0 , \"min_score\" : 0.2 , \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"upstream_task\" : \"Voxels\" } }","title":"JSON example"},{"location":"Scanner/metadata/tasks_metadata/#segmentation2d-task","text":"","title":"Segmentation2D task"},{"location":"Scanner/metadata/tasks_metadata/#configuration_6","text":"To configure this task, we use the [Segmentation2D] section in the TOML configuration file. For example: [Segmentation2D] upstream_task = \"Undistorted\" model_fileset = \"ModelFileset\" query = \"{\\\"channel\\\":\\\"rgb\\\"}\" model_id = \"Resnetdataset_gl_png_896_896_epoch50\" resize = true binarize = true dilation = 1 Sx = 896 Sy = 896 epochs = 1 batch = 1 learning_rate = 0.0001 threshold = 0.0035 [ModelFileset] scan_id = \"models\"","title":"Configuration"},{"location":"Scanner/metadata/tasks_metadata/#database-location_7","text":"Found under metadata/Segmentation2D_*.json .","title":"Database location"},{"location":"Scanner/metadata/tasks_metadata/#json-example_6","text":"Example of metadata/Segmentation2D_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"Sx\" : 896 , \"Sy\" : 896 , \"binarize\" : \"True\" , \"dilation\" : 1 , \"inverted_labels\" : [ \"background\" ], \"labels\" : [], \"model_fileset\" : \"ModelFileset\" , \"model_id\" : \"Resnetdataset_gl_png_896_896_epoch50\" , \"output_file_id\" : \"out\" , \"query\" : { \"channel\" : \"rgb\" }, \"resize\" : \"True\" , \"scan_id\" : \"\" , \"threshold\" : 0.0035 , \"upstream_task\" : \"Undistorted\" } }","title":"JSON example"},{"location":"Scanner/metadata/tasks_metadata/#segmentedpointcloud-task","text":"","title":"SegmentedPointCloud task"},{"location":"Scanner/metadata/tasks_metadata/#configuration_7","text":"To configure this task, we use the [SegmentedPointCloud] section in the TOML configuration file. For example: [SegmentedPointCloud] upstream_segmentation = \"Segmentation2D\" upstream_task = \"PointCloud\" use_colmap_poses = true","title":"Configuration"},{"location":"Scanner/metadata/tasks_metadata/#database-location_8","text":"Found under metadata/SegmentedPointCloud_*.json .","title":"Database location"},{"location":"Scanner/metadata/tasks_metadata/#json-example_7","text":"Example of metadata/SegmentedPointCloud_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"upstream_segmentation\" : \"Segmentation2D\" , \"upstream_task\" : \"PointCloud\" , \"use_colmap_poses\" : \"True\" } }","title":"JSON example"},{"location":"Scanner/metadata/tasks_metadata/#treegraph-task","text":"","title":"TreeGraph task"},{"location":"Scanner/metadata/tasks_metadata/#configuration_8","text":"To configure this task, we use the [TreeGraph] section in the TOML configuration file. For example: [TreeGraph] upstream_task = \"CurveSkeleton\" z_axis = 2 stem_axis_inverted = false","title":"Configuration"},{"location":"Scanner/metadata/tasks_metadata/#database-location_9","text":"Found under metadata/TreeGraph_*.json .","title":"Database location"},{"location":"Scanner/metadata/tasks_metadata/#json-example_8","text":"Example of metadata/TreeGraph_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"upstream_task\" : \"CurveSkeleton\" , \"z_axis\" : 2 , \"z_orientation\" : 1 } }","title":"JSON example"},{"location":"Scanner/metadata/tasks_metadata/#trianglemesh-task","text":"","title":"TriangleMesh task"},{"location":"Scanner/metadata/tasks_metadata/#configuration_9","text":"To configure this task, we use the [TriangleMesh] section in the TOML configuration file. For example: [TriangleMesh] upstream_task = \"PointCloud\"","title":"Configuration"},{"location":"Scanner/metadata/tasks_metadata/#database-location_10","text":"Found under metadata/TriangleMesh_*.json .","title":"Database location"},{"location":"Scanner/metadata/tasks_metadata/#json-example_9","text":"Example of metadata/TriangleMesh_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"upstream_task\" : \"PointCloud\" } }","title":"JSON example"},{"location":"Scanner/metadata/tasks_metadata/#undistorted-task","text":"","title":"Undistorted task"},{"location":"Scanner/metadata/tasks_metadata/#configuration_10","text":"To configure this task, we use the [Undistorted] section in the TOML configuration file. For example: [Undistorted] upstream_task = \"ImagesFilesetExists\"","title":"Configuration"},{"location":"Scanner/metadata/tasks_metadata/#database-location_11","text":"Found under metadata/Undistorted_*.json .","title":"Database location"},{"location":"Scanner/metadata/tasks_metadata/#json-example_10","text":"Example of metadata/Undistorted_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"output_file_id\" : \"out\" , \"query\" : {}, \"scan_id\" : \"\" , \"upstream_task\" : \"ImagesFilesetExists\" } }","title":"JSON example"},{"location":"Scanner/metadata/tasks_metadata/#visualization-task","text":"","title":"Visualization task"},{"location":"Scanner/metadata/tasks_metadata/#configuration_11","text":"To configure this task, we use the [Visualization] section in the TOML configuration file. For example: [Visualization] upstream_point_cloud = \"PointCloud\" upstream_mesh = \"TriangleMesh\" upstream_colmap = \"Colmap\" upstream_angles = \"AnglesAndInternodes\" upstream_skeleton = \"CurveSkeleton\" upstream_images = \"ImagesFilesetExists\" max_image_size = 1500 max_point_cloud_size = 10000 thumbnail_size = 150","title":"Configuration"},{"location":"Scanner/metadata/tasks_metadata/#database-location_12","text":"Found under metadata/Visualization.json .","title":"Database location"},{"location":"Scanner/metadata/tasks_metadata/#json-example_11","text":"Example of metadata/Visualization.json corresponding to the example TOML configuration file: { \"files\" : { \"angles\" : \"AnglesAndInternodes\" , \"camera\" : \"cameras\" , \"images\" : [ \"image_00000_rgb\" , \"image_00001_rgb\" , \"image_00002_rgb\" , \"image_00003_rgb\" , \"image_00004_rgb\" , \"image_00005_rgb\" , \"image_00006_rgb\" , \"image_00007_rgb\" , \"image_00008_rgb\" , \"image_00009_rgb\" , \"image_00010_rgb\" , \"image_00011_rgb\" , \"image_00012_rgb\" , \"image_00013_rgb\" , \"image_00014_rgb\" , \"image_00015_rgb\" , \"image_00016_rgb\" , \"image_00017_rgb\" , \"image_00018_rgb\" , \"image_00019_rgb\" , \"image_00020_rgb\" , \"image_00021_rgb\" , \"image_00022_rgb\" , \"image_00023_rgb\" , \"image_00024_rgb\" , \"image_00025_rgb\" , \"image_00026_rgb\" , \"image_00027_rgb\" , \"image_00028_rgb\" , \"image_00029_rgb\" , \"image_00030_rgb\" , \"image_00031_rgb\" , \"image_00032_rgb\" , \"image_00033_rgb\" , \"image_00034_rgb\" , \"image_00035_rgb\" , \"image_00036_rgb\" , \"image_00037_rgb\" , \"image_00038_rgb\" , \"image_00039_rgb\" , \"image_00040_rgb\" , \"image_00041_rgb\" , \"image_00042_rgb\" , \"image_00043_rgb\" , \"image_00044_rgb\" , \"image_00045_rgb\" , \"image_00046_rgb\" , \"image_00047_rgb\" , \"image_00048_rgb\" , \"image_00049_rgb\" , \"image_00050_rgb\" , \"image_00051_rgb\" , \"image_00052_rgb\" , \"image_00053_rgb\" , \"image_00054_rgb\" , \"image_00055_rgb\" , \"image_00056_rgb\" , \"image_00057_rgb\" , \"image_00058_rgb\" , \"image_00059_rgb\" ], \"measures\" : \"measures\" , \"mesh\" : \"TriangleMesh\" , \"point_cloud\" : \"PointCloud\" , \"poses\" : \"images\" , \"skeleton\" : \"CurveSkeleton\" , \"thumbnails\" : [ \"thumbnail_00000_rgb\" , \"thumbnail_00001_rgb\" , \"thumbnail_00002_rgb\" , \"thumbnail_00003_rgb\" , \"thumbnail_00004_rgb\" , \"thumbnail_00005_rgb\" , \"thumbnail_00006_rgb\" , \"thumbnail_00007_rgb\" , \"thumbnail_00008_rgb\" , \"thumbnail_00009_rgb\" , \"thumbnail_00010_rgb\" , \"thumbnail_00011_rgb\" , \"thumbnail_00012_rgb\" , \"thumbnail_00013_rgb\" , \"thumbnail_00014_rgb\" , \"thumbnail_00015_rgb\" , \"thumbnail_00016_rgb\" , \"thumbnail_00017_rgb\" , \"thumbnail_00018_rgb\" , \"thumbnail_00019_rgb\" , \"thumbnail_00020_rgb\" , \"thumbnail_00021_rgb\" , \"thumbnail_00022_rgb\" , \"thumbnail_00023_rgb\" , \"thumbnail_00024_rgb\" , \"thumbnail_00025_rgb\" , \"thumbnail_00026_rgb\" , \"thumbnail_00027_rgb\" , \"thumbnail_00028_rgb\" , \"thumbnail_00029_rgb\" , \"thumbnail_00030_rgb\" , \"thumbnail_00031_rgb\" , \"thumbnail_00032_rgb\" , \"thumbnail_00033_rgb\" , \"thumbnail_00034_rgb\" , \"thumbnail_00035_rgb\" , \"thumbnail_00036_rgb\" , \"thumbnail_00037_rgb\" , \"thumbnail_00038_rgb\" , \"thumbnail_00039_rgb\" , \"thumbnail_00040_rgb\" , \"thumbnail_00041_rgb\" , \"thumbnail_00042_rgb\" , \"thumbnail_00043_rgb\" , \"thumbnail_00044_rgb\" , \"thumbnail_00045_rgb\" , \"thumbnail_00046_rgb\" , \"thumbnail_00047_rgb\" , \"thumbnail_00048_rgb\" , \"thumbnail_00049_rgb\" , \"thumbnail_00050_rgb\" , \"thumbnail_00051_rgb\" , \"thumbnail_00052_rgb\" , \"thumbnail_00053_rgb\" , \"thumbnail_00054_rgb\" , \"thumbnail_00055_rgb\" , \"thumbnail_00056_rgb\" , \"thumbnail_00057_rgb\" , \"thumbnail_00058_rgb\" , \"thumbnail_00059_rgb\" ], \"zip\" : \"scan\" }, \"task_params\" : { \"max_image_size\" : 1500 , \"max_point_cloud_size\" : 10000 , \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"thumbnail_size\" : 150 , \"upstream_angles\" : \"AnglesAndInternodes\" , \"upstream_colmap\" : \"Colmap\" , \"upstream_images\" : \"Undistorted\" , \"upstream_mesh\" : \"TriangleMesh\" , \"upstream_point_cloud\" : \"PointCloud\" , \"upstream_skeleton\" : \"CurveSkeleton\" } }","title":"JSON example"},{"location":"Scanner/metadata/tasks_metadata/#voxels-task","text":"","title":"Voxels task"},{"location":"Scanner/metadata/tasks_metadata/#configuration_12","text":"To configure this task, we use the [Voxels] section in the TOML configuration file. For example: [Voxels] upstream_mask = \"Masks\" upstream_colmap = \"Colmap\" use_colmap_poses = true voxel_size = 1.0 type = \"carving\" log = false invert = false labels = \"[]\"","title":"Configuration"},{"location":"Scanner/metadata/tasks_metadata/#database-location_13","text":"Found under metadata/Voxels_*.json .","title":"Database location"},{"location":"Scanner/metadata/tasks_metadata/#json-example_12","text":"Example of metadata/Voxels_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"invert\" : \"False\" , \"labels\" : [ \"background\" ], \"log\" : \"False\" , \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"type\" : \"averaging\" , \"upstream_colmap\" : \"Colmap\" , \"upstream_mask\" : \"Segmentation2D\" , \"use_colmap_poses\" : \"True\" , \"voxel_size\" : 0.01 } }","title":"JSON example"},{"location":"Scanner/metadata/tasks_metadata/#scan-task","text":"Todo For some reason the parameters are defined in the metadata/metadata.json file. Definition can be found here .","title":"Scan task"},{"location":"Scanner/modules/plant_3d_explorer/","text":"Welcome to the Plant 3D Explorer repository. It is the home of our webapp dedicated to the exploration of single plant acquisitions and reconstruction. For a general documentation on the whole ROMI project, head over here . Requirements Link You will need npm to use this package. It comes with nodejs , as per the official instruction . To install v16.X, for ubuntu: ```shell","title":"Plant 3d explorer"},{"location":"Scanner/modules/plant_3d_explorer/#requirements","text":"You will need npm to use this package. It comes with nodejs , as per the official instruction . To install v16.X, for ubuntu: ```shell","title":"Requirements"},{"location":"Scanner/modules/plant_3d_vision/","text":"Plant 3D Vision v0.9.99 (dev) Link Documentation about the \"Plant Scanner\" project can be found here . Table of Contents Link Building and running with Docker (recommended) Building the image Running the container Performing a task with plant-3d-vision Install requirements for Ubuntu and Conda Environment Conda Environment Install from sources in conda environment Build plant_3d_vision conda package Install plant_3d_vision conda package Docker Link Building the image Link To build the Docker image of plant-3d-vision, you have to clone the repo and run the script docker/build.sh . git clone https://github.com/romi/plant-3d-vision.git cd plant-3d-vision/ git submodule init git submodule update ./docker/build.sh This will create an image docker plant3dvision:latest . If you want to tag your image with a specific one, just pass the tag argument as follows ./docker/build.sh -t mytag To show more options (built-in user...), just type ./docker/build.sh -h . Note that, you must run the script from the root of plant-3d-vision folder as shown previously. Running the container Link In the docker folder, you will find also a run script docker/run.sh . You may want to mount your database folder and an other folder (let's say your configs). This can be done as follows: ./docker/run.sh \\ -v /home/ ${ USER } /my_database:/home/ ${ USER } /database/ \\ -v /home/ ${ USER } /my_configs/:/home/ ${ USER } /config/ Don't forget to change the paths with yours! If you want to run a docker image with an other tag, you can pass the tag name as an argument: ./docker/run.sh -t my_tage . To see more running options (specif tag, command...), type ./docker/run.sh -h Troubleshooting : You must install nvidia gpu drivers, nvidia-docker (v2.0) and nvidia-container-toolkit. To test if everything is okay: ./docker/run.sh --gpu_test This docker image has been tested successfully on: docker --version=19.03.6 | nvidia driver version=450.102.04 | CUDA version=11.0 Performing a task with plant-3d-vision Link Inside the docker image there is a romi_run_task command which performs a task on a database according to a passed config file. In this following example, we will use the test database and config file shipped in this repo: - Run the default docker image ( plant3dvision:latest ) - Mount the database ( plant-3d-vision/tests/testdata/ ) and configs folder (plant-3d-vision/config/) inside the docker container - Perform the task AnglesAndInternodes on the database with geom_pipe_real.toml config file ./docker/run.sh -v /path/to/plant-3d-vision/tests/testdata/:/home/ $USER /database/ -v /path/to/plant-3d-vision/config/:/home/ $USER /config romi_run_task --config ~/config/geom_pipe_real.toml AnglesAndInternodes ~/database/real_plant/ Don't forget to replace the paths path/to/plant-3d-vision by the correct ones. Install requirements Link Colmap is required to run the reconstruction tasks, follow the official install instructions for linux here . This library use pyopencl and thus require the following system libraries: ocl-icd-libopencl1 opencl-headers In addition you will need: git python3-pip python3-wheel As you will be using the romicgal library, which is a minimal wrapper for CGAL 5.0 using pybind11 you will also need: - wget - eigen3 - gmp - mprf - pkg-config On Debian and Ubuntu, you can install them with: sudo apt-get update && sudo apt-get install -y \\ git wget \\ ocl-icd-libopencl1 opencl-headers \\ python3-wheel python3-pip \\ gcc pkg-config \\ libeigen3-dev libgmp3-dev libmpfr-dev If you have an NVIDIA GPU: mkdir -p /etc/OpenCL/vendors echo \"libnvidia-opencl.so.1\" > /etc/OpenCL/vendors/nvidia.icd To avoid troubles during pyopencl install, check /usr/lib/libOpenCL.so exists, if not add it with: ln -s /usr/lib/x86_64-linux-gnu/libOpenCL.so.1 /usr/lib/libOpenCL.so Conda environment Link Install from sources in conda environment Link In this install instructions, we leverage the git submodule functionality to clone the required ROMI libraries. Clone the plant-3d-vision sources: git clone https://github.com/romi/plant-3d-vision.git Create a conda environment named plant_3d_vision_0.9 with Python3.7 for example: conda create --name plant_3d_vision_0.9 python = 3 .7 Install sources and submodules in activated environment: conda activate plant_3d_vision_0.9 cd plant-3d-vision/ git submodule init git submodule update python3 -m pip install -r requirements.txt python3 -m pip install ./plantdb/ python3 -m pip install ./romitask/ python3 -m pip install ./romiseg/ python3 -m pip install ./romicgal/ python3 -m pip install . Test import of plant3dvision library: conda activate scan_0.9 python3 -c 'import plant3dvision' Longer tests using shipped \"test dataset\": cd tests/ bash check_pipe.sh rm testdata/models/models/Resnet_896_896_epoch50.pt Troubleshooting : ImportError: libGL.so.1: cannot open shared object file: No such file or directory can be fixed with: apt-get install libgl1-mesa-glx ImportError: libSM.so.6: cannot open shared object file: No such file or directory can be fixed with: apt-get install libsm6 libxext6 libxrender-dev Build plant-3d-vision conda package Link From the base conda environment, run: conda build conda_recipes/plant_3d_vision/ -c romi-eu -c open3d-admin -c conda-forge --user romi-eu Install plant-3d-vision conda package Link conda create -n plant3dvision plant3dvision -c romi-eu -c open3d-admin --force To test package install, in the activated environment import plant3dvision in python: conda activate plant3dvision python -c 'import plant3dvision'","title":"Plant 3d vision"},{"location":"Scanner/modules/plant_3d_vision/#plant-3d-vision-v0999-dev","text":"Documentation about the \"Plant Scanner\" project can be found here .","title":"Plant 3D Vision v0.9.99 (dev)"},{"location":"Scanner/modules/plant_3d_vision/#table-of-contents","text":"Building and running with Docker (recommended) Building the image Running the container Performing a task with plant-3d-vision Install requirements for Ubuntu and Conda Environment Conda Environment Install from sources in conda environment Build plant_3d_vision conda package Install plant_3d_vision conda package","title":"Table of Contents"},{"location":"Scanner/modules/plant_3d_vision/#docker","text":"","title":"Docker"},{"location":"Scanner/modules/plant_3d_vision/#building-the-image","text":"To build the Docker image of plant-3d-vision, you have to clone the repo and run the script docker/build.sh . git clone https://github.com/romi/plant-3d-vision.git cd plant-3d-vision/ git submodule init git submodule update ./docker/build.sh This will create an image docker plant3dvision:latest . If you want to tag your image with a specific one, just pass the tag argument as follows ./docker/build.sh -t mytag To show more options (built-in user...), just type ./docker/build.sh -h . Note that, you must run the script from the root of plant-3d-vision folder as shown previously.","title":"Building the image"},{"location":"Scanner/modules/plant_3d_vision/#running-the-container","text":"In the docker folder, you will find also a run script docker/run.sh . You may want to mount your database folder and an other folder (let's say your configs). This can be done as follows: ./docker/run.sh \\ -v /home/ ${ USER } /my_database:/home/ ${ USER } /database/ \\ -v /home/ ${ USER } /my_configs/:/home/ ${ USER } /config/ Don't forget to change the paths with yours! If you want to run a docker image with an other tag, you can pass the tag name as an argument: ./docker/run.sh -t my_tage . To see more running options (specif tag, command...), type ./docker/run.sh -h Troubleshooting : You must install nvidia gpu drivers, nvidia-docker (v2.0) and nvidia-container-toolkit. To test if everything is okay: ./docker/run.sh --gpu_test This docker image has been tested successfully on: docker --version=19.03.6 | nvidia driver version=450.102.04 | CUDA version=11.0","title":"Running the container"},{"location":"Scanner/modules/plant_3d_vision/#performing-a-task-with-plant-3d-vision","text":"Inside the docker image there is a romi_run_task command which performs a task on a database according to a passed config file. In this following example, we will use the test database and config file shipped in this repo: - Run the default docker image ( plant3dvision:latest ) - Mount the database ( plant-3d-vision/tests/testdata/ ) and configs folder (plant-3d-vision/config/) inside the docker container - Perform the task AnglesAndInternodes on the database with geom_pipe_real.toml config file ./docker/run.sh -v /path/to/plant-3d-vision/tests/testdata/:/home/ $USER /database/ -v /path/to/plant-3d-vision/config/:/home/ $USER /config romi_run_task --config ~/config/geom_pipe_real.toml AnglesAndInternodes ~/database/real_plant/ Don't forget to replace the paths path/to/plant-3d-vision by the correct ones.","title":"Performing a task with plant-3d-vision"},{"location":"Scanner/modules/plant_3d_vision/#install-requirements","text":"Colmap is required to run the reconstruction tasks, follow the official install instructions for linux here . This library use pyopencl and thus require the following system libraries: ocl-icd-libopencl1 opencl-headers In addition you will need: git python3-pip python3-wheel As you will be using the romicgal library, which is a minimal wrapper for CGAL 5.0 using pybind11 you will also need: - wget - eigen3 - gmp - mprf - pkg-config On Debian and Ubuntu, you can install them with: sudo apt-get update && sudo apt-get install -y \\ git wget \\ ocl-icd-libopencl1 opencl-headers \\ python3-wheel python3-pip \\ gcc pkg-config \\ libeigen3-dev libgmp3-dev libmpfr-dev If you have an NVIDIA GPU: mkdir -p /etc/OpenCL/vendors echo \"libnvidia-opencl.so.1\" > /etc/OpenCL/vendors/nvidia.icd To avoid troubles during pyopencl install, check /usr/lib/libOpenCL.so exists, if not add it with: ln -s /usr/lib/x86_64-linux-gnu/libOpenCL.so.1 /usr/lib/libOpenCL.so","title":"Install requirements"},{"location":"Scanner/modules/plant_3d_vision/#conda-environment","text":"","title":"Conda environment"},{"location":"Scanner/modules/plant_3d_vision/#install-from-sources-in-conda-environment","text":"In this install instructions, we leverage the git submodule functionality to clone the required ROMI libraries. Clone the plant-3d-vision sources: git clone https://github.com/romi/plant-3d-vision.git Create a conda environment named plant_3d_vision_0.9 with Python3.7 for example: conda create --name plant_3d_vision_0.9 python = 3 .7 Install sources and submodules in activated environment: conda activate plant_3d_vision_0.9 cd plant-3d-vision/ git submodule init git submodule update python3 -m pip install -r requirements.txt python3 -m pip install ./plantdb/ python3 -m pip install ./romitask/ python3 -m pip install ./romiseg/ python3 -m pip install ./romicgal/ python3 -m pip install . Test import of plant3dvision library: conda activate scan_0.9 python3 -c 'import plant3dvision' Longer tests using shipped \"test dataset\": cd tests/ bash check_pipe.sh rm testdata/models/models/Resnet_896_896_epoch50.pt Troubleshooting : ImportError: libGL.so.1: cannot open shared object file: No such file or directory can be fixed with: apt-get install libgl1-mesa-glx ImportError: libSM.so.6: cannot open shared object file: No such file or directory can be fixed with: apt-get install libsm6 libxext6 libxrender-dev","title":"Install from sources in conda environment"},{"location":"Scanner/modules/plant_3d_vision/#build-plant-3d-vision-conda-package","text":"From the base conda environment, run: conda build conda_recipes/plant_3d_vision/ -c romi-eu -c open3d-admin -c conda-forge --user romi-eu","title":"Build plant-3d-vision conda package"},{"location":"Scanner/modules/plant_3d_vision/#install-plant-3d-vision-conda-package","text":"conda create -n plant3dvision plant3dvision -c romi-eu -c open3d-admin --force To test package install, in the activated environment import plant3dvision in python: conda activate plant3dvision python -c 'import plant3dvision'","title":"Install plant-3d-vision conda package"},{"location":"Scanner/modules/plant_imager/","text":"-Virtual- Plant Imager Link To be as clear as possible, we define the following names: - Plant Imager : hardware control of the ROMI 3D plant imager . - Virtual Plant Imager : simulate the plant imager using LPY to generate virtual plants and Blender for rendering. We recommend using it inside a docker container. Getting started Link Clone the repository and initialize the sub-modules. In a terminal: git clone https://github.com/romi/plant-imager.git cd plant-imager/ git submodule init # only do this ONCE if the sub-module have not yet been initialized! Docker Link For now, you have to first build the image locally, then you can use it. We will change that soon, so you can use pre-built images! Obviously you first have to follow the getting started instructions. Using the plantimager docker image Link Build the plantimager image Link In this repository, you will find a dedicated script, named build.sh , in the docker/plantimager/ directory. You may need to update the sub-modules before building the image. In a terminal move to the root of the cloned plant-imager repository and update the sub-modules with: git submodule update Now you can use the provided build script. ./docker/plantimager/build.sh By default, this build script will create a docker image named plantimager:latest . If you want more build options (specific branches, tags...etc), type ./docker/plantimager/build.sh --help . by default the docker user is the same as the one currently used by your system. Use echo $USER to check the active username. Run a plantimager container Link In the docker/plantimager/ directory, you will also find a convenience script named run.sh . To get usage information and options list, use ./run.sh -h or from the plant-imager repository root folder: ./docker/plantimager/run.sh -h Using the virtualplantimager docker image Link Build the virtualplantimager image Link In this repository, you will find a dedicated script, named build.sh , in the docker/virtualplantimager/ directory. You may need to update the sub-modules before building the image. In a terminal move to the root of the cloned plant-imager repository and update the sub-modules with: git submodule update Now you can use the provided build script. ./docker/virtualplantimager/build.sh By default, this build script will create a docker image named virtualplantimager:latest . If you want more build options (specific branches, tags...etc), type ./docker/virtualplantimager/build.sh -h . by default the docker user is the same as the one currently used by your system. Use echo $USER to check the active username. Run a virtualplantimager container Link In the docker/virtualplantimager/ directory, you will also find a convenience script named run.sh . To get usage information and options list, use ./run.sh -h or from the plant-imager repository root folder: ./docker/virtualplantimager/run.sh -h Example: generating a virtual plant dataset with lpy and Blender Link Create a temporary copy of the example database Link To avoid generating the dataset in the git repository, we highly recommend to first copy the example database to a temporary folder. On UNIX systems, from the plant-imager/ root directory, do: cd plant-imager cp -r database_example /tmp/ You can check the temporary copy with: ls /tmp/database_example Generate a virtual data Link Following the temporary copy, to generate a virtual Arabidopsis thaliana named generated_dataset , do: cd plant-imager ./docker/virtualplantimager/run.sh \\ -t latest -db /tmp/database_example \\ -c \"romi_run_task VirtualScan db/generated_dataset \\ --config plant-imager/config/vscan_lpy_blender.toml\" This will use a standard configuration for the VirtualScan task defined in plant-imager/config/vscan_lpy_blender.toml . After a while, if the generation has succeeded, you can check the generated dataset on your mapped host machine directory /tmp/database_example/generated_dataset stopping or restarting your system will clean the /tmp directory. If you have an error about a locked database: rm /tmp/generated_dataset/lock and try again! For more information have a look at the official ROMI documentation website . Install from sources Link We highly recommend using the docker image for the virtual plant imager as installing things can get messy! Getting started Link Start by cloning the sources and getting the sub-modules: # Clone plant-imager sources git clone https://github.com/romi/plant-imager cd plant-imager # Get the sub-modules: git submodule init # only do this ONCE if the sub-module have not yet been initialized! git submodule update Install from sources to use the plant imager Link Conda environment creation Link Create a conda environment named plant_imager and install required dependencies: conda create -n plant_imager python = 3 .7 Install the sources Link You may now proceed to install the python requirements and packages: # Don't forget to activate the environment! conda activate plant_imager cd plant-imager # Install `plantdb` from sub-modules: python -m pip install -e ./plantdb/ # Install `romitask` from sub-modules: python -m pip install -e ./romitask/ # Install `plant-imager`: python -m pip install -e . Please notice that we here use the git sub-modules to clone and install romitask and plantdb . Install from sources to use the virtual plant imager Link System requirements Link sudo apt-get install libgl1-mesa-dev libxi6 gcc Conda environment creation Link Create a conda environment named plant_imager and install required dependencies: conda create -n plant_imager python = 3 .7 conda install -n plant_imager -c conda-forge -c fredboudon flask imageio toml luigi boost = 1 .70.0 qhull = 2015 .2 openalea.lpy Blender setup Link In order to use the bpy module we have to do this mess: 1. Download Blender2.81 Link Download Blender2.81, extract the archive and move its contents to /opt/blender : BLENDER_URL = https://download.blender.org/release/Blender2.81/blender-2.81a-linux-glibc217-x86_64.tar.bz2 wget --progress = bar $BLENDER_URL tar -xjf blender-2.81a-linux-glibc217-x86_64.tar.bz2 sudo mv blender-2.81a-linux-glibc217-x86_64 /opt/blender rm blender-2.81a-linux-glibc217-x86_64.tar.bz2 Add blender library path to $LD_LIBRARY_PATH : echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/blender/lib' >> ~/.bashrc 2. Copy required python libraries to blender Link Copy python libraries to the python's blender site-packages: sudo cp -r /opt/conda/envs/plant_imager/lib/python3.7/site-packages/markupsafe/ /opt/blender/2.81/python/lib/python3.7/site-packages/ sudo cp -r /opt/conda/envs/plant_imager/lib/python3.7/site-packages/flask/ /opt/blender/2.81/python/lib/python3.7/site-packages/ sudo cp -r /opt/conda/envs/plant_imager/lib/python3.7/site-packages/itsdangerous/ /opt/blender/2.81/python/lib/python3.7/site-packages/ sudo cp -r /opt/conda/envs/plant_imager/lib/python3.7/site-packages/click/ /opt/blender/2.81/python/lib/python3.7/site-packages/ sudo cp -r /opt/conda/envs/plant_imager/lib/python3.7/site-packages/werkzeug/ /opt/blender/2.81/python/lib/python3.7/site-packages/ sudo cp -r /opt/conda/envs/plant_imager/lib/python3.7/site-packages/jinja2/ /opt/blender/2.81/python/lib/python3.7/site-packages/ sudo cp -r /opt/conda/envs/plant_imager/lib/python3.7/site-packages/imageio/ /opt/blender/2.81/python/lib/python3.7/site-packages/ 3. Install ROMI sources Link We will now finish by cloning and installing the ROMI sources for plant-imager . # Don't forget to activate the environment! conda activate plant_imager cd plant-imager # Install `plantdb` from sub-modules: python -m pip install -e ./plantdb/ # Install `romitask` from sub-modules: python -m pip install -e ./romitask/ # Install `plant-imager`: python -m pip install -e . Please notice that we here use the git sub-modules to clone and install romitask and plantdb . The -e option install the module or sub-module in editable mode. That means you will not have to reinstall them if you make modifications to the sources.","title":"Plant imager"},{"location":"Scanner/modules/plant_imager/#-virtual-plant-imager","text":"To be as clear as possible, we define the following names: - Plant Imager : hardware control of the ROMI 3D plant imager . - Virtual Plant Imager : simulate the plant imager using LPY to generate virtual plants and Blender for rendering. We recommend using it inside a docker container.","title":"-Virtual- Plant Imager"},{"location":"Scanner/modules/plant_imager/#getting-started","text":"Clone the repository and initialize the sub-modules. In a terminal: git clone https://github.com/romi/plant-imager.git cd plant-imager/ git submodule init # only do this ONCE if the sub-module have not yet been initialized!","title":"Getting started"},{"location":"Scanner/modules/plant_imager/#docker","text":"For now, you have to first build the image locally, then you can use it. We will change that soon, so you can use pre-built images! Obviously you first have to follow the getting started instructions.","title":"Docker"},{"location":"Scanner/modules/plant_imager/#using-the-plantimager-docker-image","text":"","title":"Using the plantimager docker image"},{"location":"Scanner/modules/plant_imager/#build-the-plantimager-image","text":"In this repository, you will find a dedicated script, named build.sh , in the docker/plantimager/ directory. You may need to update the sub-modules before building the image. In a terminal move to the root of the cloned plant-imager repository and update the sub-modules with: git submodule update Now you can use the provided build script. ./docker/plantimager/build.sh By default, this build script will create a docker image named plantimager:latest . If you want more build options (specific branches, tags...etc), type ./docker/plantimager/build.sh --help . by default the docker user is the same as the one currently used by your system. Use echo $USER to check the active username.","title":"Build the plantimager image"},{"location":"Scanner/modules/plant_imager/#run-a-plantimager-container","text":"In the docker/plantimager/ directory, you will also find a convenience script named run.sh . To get usage information and options list, use ./run.sh -h or from the plant-imager repository root folder: ./docker/plantimager/run.sh -h","title":"Run a plantimager container"},{"location":"Scanner/modules/plant_imager/#using-the-virtualplantimager-docker-image","text":"","title":"Using the virtualplantimager docker image"},{"location":"Scanner/modules/plant_imager/#build-the-virtualplantimager-image","text":"In this repository, you will find a dedicated script, named build.sh , in the docker/virtualplantimager/ directory. You may need to update the sub-modules before building the image. In a terminal move to the root of the cloned plant-imager repository and update the sub-modules with: git submodule update Now you can use the provided build script. ./docker/virtualplantimager/build.sh By default, this build script will create a docker image named virtualplantimager:latest . If you want more build options (specific branches, tags...etc), type ./docker/virtualplantimager/build.sh -h . by default the docker user is the same as the one currently used by your system. Use echo $USER to check the active username.","title":"Build the virtualplantimager image"},{"location":"Scanner/modules/plant_imager/#run-a-virtualplantimager-container","text":"In the docker/virtualplantimager/ directory, you will also find a convenience script named run.sh . To get usage information and options list, use ./run.sh -h or from the plant-imager repository root folder: ./docker/virtualplantimager/run.sh -h","title":"Run a virtualplantimager container"},{"location":"Scanner/modules/plant_imager/#example-generating-a-virtual-plant-dataset-with-lpy-and-blender","text":"","title":"Example: generating a virtual plant dataset with lpy and Blender"},{"location":"Scanner/modules/plant_imager/#create-a-temporary-copy-of-the-example-database","text":"To avoid generating the dataset in the git repository, we highly recommend to first copy the example database to a temporary folder. On UNIX systems, from the plant-imager/ root directory, do: cd plant-imager cp -r database_example /tmp/ You can check the temporary copy with: ls /tmp/database_example","title":"Create a temporary copy of the example database"},{"location":"Scanner/modules/plant_imager/#generate-a-virtual-data","text":"Following the temporary copy, to generate a virtual Arabidopsis thaliana named generated_dataset , do: cd plant-imager ./docker/virtualplantimager/run.sh \\ -t latest -db /tmp/database_example \\ -c \"romi_run_task VirtualScan db/generated_dataset \\ --config plant-imager/config/vscan_lpy_blender.toml\" This will use a standard configuration for the VirtualScan task defined in plant-imager/config/vscan_lpy_blender.toml . After a while, if the generation has succeeded, you can check the generated dataset on your mapped host machine directory /tmp/database_example/generated_dataset stopping or restarting your system will clean the /tmp directory. If you have an error about a locked database: rm /tmp/generated_dataset/lock and try again! For more information have a look at the official ROMI documentation website .","title":"Generate a virtual data"},{"location":"Scanner/modules/plant_imager/#install-from-sources","text":"We highly recommend using the docker image for the virtual plant imager as installing things can get messy!","title":"Install from sources"},{"location":"Scanner/modules/plant_imager/#getting-started_1","text":"Start by cloning the sources and getting the sub-modules: # Clone plant-imager sources git clone https://github.com/romi/plant-imager cd plant-imager # Get the sub-modules: git submodule init # only do this ONCE if the sub-module have not yet been initialized! git submodule update","title":"Getting started"},{"location":"Scanner/modules/plant_imager/#install-from-sources-to-use-the-plant-imager","text":"","title":"Install from sources to use the plant imager"},{"location":"Scanner/modules/plant_imager/#conda-environment-creation","text":"Create a conda environment named plant_imager and install required dependencies: conda create -n plant_imager python = 3 .7","title":"Conda environment creation"},{"location":"Scanner/modules/plant_imager/#install-the-sources","text":"You may now proceed to install the python requirements and packages: # Don't forget to activate the environment! conda activate plant_imager cd plant-imager # Install `plantdb` from sub-modules: python -m pip install -e ./plantdb/ # Install `romitask` from sub-modules: python -m pip install -e ./romitask/ # Install `plant-imager`: python -m pip install -e . Please notice that we here use the git sub-modules to clone and install romitask and plantdb .","title":"Install the sources"},{"location":"Scanner/modules/plant_imager/#install-from-sources-to-use-the-virtual-plant-imager","text":"","title":"Install from sources to use the virtual plant imager"},{"location":"Scanner/modules/plant_imager/#system-requirements","text":"sudo apt-get install libgl1-mesa-dev libxi6 gcc","title":"System requirements"},{"location":"Scanner/modules/plant_imager/#conda-environment-creation_1","text":"Create a conda environment named plant_imager and install required dependencies: conda create -n plant_imager python = 3 .7 conda install -n plant_imager -c conda-forge -c fredboudon flask imageio toml luigi boost = 1 .70.0 qhull = 2015 .2 openalea.lpy","title":"Conda environment creation"},{"location":"Scanner/modules/plant_imager/#blender-setup","text":"In order to use the bpy module we have to do this mess:","title":"Blender setup"},{"location":"Scanner/modules/plant_imager/#1-download-blender281","text":"Download Blender2.81, extract the archive and move its contents to /opt/blender : BLENDER_URL = https://download.blender.org/release/Blender2.81/blender-2.81a-linux-glibc217-x86_64.tar.bz2 wget --progress = bar $BLENDER_URL tar -xjf blender-2.81a-linux-glibc217-x86_64.tar.bz2 sudo mv blender-2.81a-linux-glibc217-x86_64 /opt/blender rm blender-2.81a-linux-glibc217-x86_64.tar.bz2 Add blender library path to $LD_LIBRARY_PATH : echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/blender/lib' >> ~/.bashrc","title":"1. Download Blender2.81"},{"location":"Scanner/modules/plant_imager/#2-copy-required-python-libraries-to-blender","text":"Copy python libraries to the python's blender site-packages: sudo cp -r /opt/conda/envs/plant_imager/lib/python3.7/site-packages/markupsafe/ /opt/blender/2.81/python/lib/python3.7/site-packages/ sudo cp -r /opt/conda/envs/plant_imager/lib/python3.7/site-packages/flask/ /opt/blender/2.81/python/lib/python3.7/site-packages/ sudo cp -r /opt/conda/envs/plant_imager/lib/python3.7/site-packages/itsdangerous/ /opt/blender/2.81/python/lib/python3.7/site-packages/ sudo cp -r /opt/conda/envs/plant_imager/lib/python3.7/site-packages/click/ /opt/blender/2.81/python/lib/python3.7/site-packages/ sudo cp -r /opt/conda/envs/plant_imager/lib/python3.7/site-packages/werkzeug/ /opt/blender/2.81/python/lib/python3.7/site-packages/ sudo cp -r /opt/conda/envs/plant_imager/lib/python3.7/site-packages/jinja2/ /opt/blender/2.81/python/lib/python3.7/site-packages/ sudo cp -r /opt/conda/envs/plant_imager/lib/python3.7/site-packages/imageio/ /opt/blender/2.81/python/lib/python3.7/site-packages/","title":"2. Copy required python libraries to blender"},{"location":"Scanner/modules/plant_imager/#3-install-romi-sources","text":"We will now finish by cloning and installing the ROMI sources for plant-imager . # Don't forget to activate the environment! conda activate plant_imager cd plant-imager # Install `plantdb` from sub-modules: python -m pip install -e ./plantdb/ # Install `romitask` from sub-modules: python -m pip install -e ./romitask/ # Install `plant-imager`: python -m pip install -e . Please notice that we here use the git sub-modules to clone and install romitask and plantdb . The -e option install the module or sub-module in editable mode. That means you will not have to reinstall them if you make modifications to the sources.","title":"3. Install ROMI sources"},{"location":"Scanner/modules/plantdb/","text":"PlantDB Link Documentation about the \"Plant Scanner\" project can be found here . System requirements Link You will need: git python3-pip On Debian and Ubuntu, you can install them with: sudo apt-get update && apt-get install -y git python3-pip Install from sources in conda environment: Link Clone the sources: git clone https://github.com/romi/plantdb.git Create a conda environment: conda create --name plantdb_dev python = 3 .7 Install sources: conda activate plantdb_dev cd plantdb python3 -m pip install -e . Test import of plantdb library: conda activate plantdb_dev python3 -c 'import plantdb' Test plantdb library: Install dependencies for testing and coverage: conda activate plantdb_dev conda install nose coverage Run all tests with highly verbose output (from the root directory): nosetests tests/ -x -s -v Run all tests with coverage report (from the root directory): nosetests tests/ --with-coverage --cover-package = plantdb Notes : You may change the name plantdb_dev to something else, like an already existing conda environment (then skip step 2.). Conda packaging Link Notes : This is not working YET! Install plantdb conda package: Link conda create -n plantdb plantdb -c romi-eu -c open3d-admin --force To test package install, in the activated environment import plantdb in python: conda activate plantdb python -c 'import plantdb' Build plantdb conda package: Link From the base conda environment, run: conda build conda_recipes/plantdb/ -c romi-eu -c open3d-admin --user romi-eu Usage Link This library is intended to run in the background as a REST API serving JSON informations from the DB. Typically these are used by the 3d-plantviewer or romiscan libraries. Setup Link You need to create a directory where to put the data, e.g. /data/ROMI/DB and add a file called romidb define it in an environment variable DB_LOCATION : mkdir -p /data/ROMI/DB touch /data/ROMI/DB/romidb Then define its location in an environment variable DB_LOCATION : export DB_LOCATION = /data/ROMI/DB Notes : To permanently set this directory as the location of the DB, add it to your ~/.bashrc file. Example datasets Link To populate your DB with example datasets, we provide some examples here . Make sure you have wget : sudo apt-get install wget Then download the test archive and extract it to the location od the DB: wget https://media.romi-project.eu/data/test_db_small.tar.gz tar -xf test_db_small.tar.gz -C $DB_LOCATION Serve the REST API Link Then you can start the REST API with romi_scanner_rest_api : romi_scanner_rest_api You should see something like: n scans = 2 * Serving Flask app \"romi_scanner_rest_api\" (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit) Open your favorite browser here: scans: http://0.0.0.0:5000/scans '2018-12-17_17-05-35' dataset: http://0.0.0.0:5000/scans/2018-12-17_17-05-35 Python API Link Here is a minimal example how to access DB in Python: # Get the environment variable $DB_LOCATION import os db_path = os . environ [ 'DB_LOCATION' ] # Use it to connect to DB: from plantdb import FSDB db = FSDB ( db_path ) db . connect () dataset = db . get_scan ( \"2018-12-17_17-05-35\" ) img_fs = dataset . get_fileset ( 'images' )","title":"Plantdb"},{"location":"Scanner/modules/plantdb/#plantdb","text":"Documentation about the \"Plant Scanner\" project can be found here .","title":"PlantDB"},{"location":"Scanner/modules/plantdb/#system-requirements","text":"You will need: git python3-pip On Debian and Ubuntu, you can install them with: sudo apt-get update && apt-get install -y git python3-pip","title":"System requirements"},{"location":"Scanner/modules/plantdb/#install-from-sources-in-conda-environment","text":"Clone the sources: git clone https://github.com/romi/plantdb.git Create a conda environment: conda create --name plantdb_dev python = 3 .7 Install sources: conda activate plantdb_dev cd plantdb python3 -m pip install -e . Test import of plantdb library: conda activate plantdb_dev python3 -c 'import plantdb' Test plantdb library: Install dependencies for testing and coverage: conda activate plantdb_dev conda install nose coverage Run all tests with highly verbose output (from the root directory): nosetests tests/ -x -s -v Run all tests with coverage report (from the root directory): nosetests tests/ --with-coverage --cover-package = plantdb Notes : You may change the name plantdb_dev to something else, like an already existing conda environment (then skip step 2.).","title":"Install from sources in conda environment:"},{"location":"Scanner/modules/plantdb/#conda-packaging","text":"Notes : This is not working YET!","title":"Conda packaging"},{"location":"Scanner/modules/plantdb/#install-plantdb-conda-package","text":"conda create -n plantdb plantdb -c romi-eu -c open3d-admin --force To test package install, in the activated environment import plantdb in python: conda activate plantdb python -c 'import plantdb'","title":"Install plantdb conda package:"},{"location":"Scanner/modules/plantdb/#build-plantdb-conda-package","text":"From the base conda environment, run: conda build conda_recipes/plantdb/ -c romi-eu -c open3d-admin --user romi-eu","title":"Build plantdb conda package:"},{"location":"Scanner/modules/plantdb/#usage","text":"This library is intended to run in the background as a REST API serving JSON informations from the DB. Typically these are used by the 3d-plantviewer or romiscan libraries.","title":"Usage"},{"location":"Scanner/modules/plantdb/#setup","text":"You need to create a directory where to put the data, e.g. /data/ROMI/DB and add a file called romidb define it in an environment variable DB_LOCATION : mkdir -p /data/ROMI/DB touch /data/ROMI/DB/romidb Then define its location in an environment variable DB_LOCATION : export DB_LOCATION = /data/ROMI/DB Notes : To permanently set this directory as the location of the DB, add it to your ~/.bashrc file.","title":"Setup"},{"location":"Scanner/modules/plantdb/#example-datasets","text":"To populate your DB with example datasets, we provide some examples here . Make sure you have wget : sudo apt-get install wget Then download the test archive and extract it to the location od the DB: wget https://media.romi-project.eu/data/test_db_small.tar.gz tar -xf test_db_small.tar.gz -C $DB_LOCATION","title":"Example datasets"},{"location":"Scanner/modules/plantdb/#serve-the-rest-api","text":"Then you can start the REST API with romi_scanner_rest_api : romi_scanner_rest_api You should see something like: n scans = 2 * Serving Flask app \"romi_scanner_rest_api\" (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit) Open your favorite browser here: scans: http://0.0.0.0:5000/scans '2018-12-17_17-05-35' dataset: http://0.0.0.0:5000/scans/2018-12-17_17-05-35","title":"Serve the REST API"},{"location":"Scanner/modules/plantdb/#python-api","text":"Here is a minimal example how to access DB in Python: # Get the environment variable $DB_LOCATION import os db_path = os . environ [ 'DB_LOCATION' ] # Use it to connect to DB: from plantdb import FSDB db = FSDB ( db_path ) db . connect () dataset = db . get_scan ( \"2018-12-17_17-05-35\" ) img_fs = dataset . get_fileset ( 'images' )","title":"Python API"},{"location":"Scanner/specifications/","text":"","title":"Home"},{"location":"Scanner/specifications/configuration_files/","text":"Tasks configuration Link Introduction Link Each task has some default configuration but to create a working pipeline, you need to set some parameters in a TOML configuration file. There are two types of parameters: those associated with luigi: upstream tasks, requirements & outputs those tuning the used algorithms: variables & parameters For examples look at the configurations examples provided in plant3dvision/configs . I/O tasks Link These tasks refer to the inputs and outputs of each algorithmic tasks. They are often related to task defined in the plantdb library. FilesetExists Link Check that the given Fileset id exists (in dataset). No luigi parameters (no upstream task). List of task variables: fileset_id : the id ( str ) to check for existence. ImagesFilesetExists(FilesetExists) Link Check that the image file set exists. No luigi parameters (no upstream task). List of task variables: fileset_id : the id ( str ) to check for existence 'images' by default. ModelFileset(FilesetExists) Link Check that the trained ML models file set exists. No luigi parameters (no upstream task). List of task variables: fileset_id : the id ( str ) to check for existence 'models' by default. Example: [ModelFileset] scan_id = \"models\" Defines the location of the trained ML models in a dataset named 'models' . Database tasks Link Clean Link Defined in plantdb.task , it is used to clean a scan dataset from the \"processing folders\". No luigi parameters (no upstream task). List of task variables: no_confirm : boolean indicating whether a confirmation is required to clean the dataset, False by default, i.e. confirmation required. Example: [Clean] no_confirm = true Use this to clean without confirmation. Algorithmic tasks Link Colmap task Link Defined in plant3dvision.task.colmap , it is used to match scan images and estimate camera poses. It can also be used to compute a sparse and/or dense point cloud. List of luigi task parameters: upstream_task : task upstream of the Colmap task, default is ImagesFilesetExists List of task variables: matcher : images matching method, can be either \"exhaustive\" (default) or \"sequential\"; compute_dense : boolean indicating whether to run the dense Colmap to obtain a dense point cloud, False by default; cli_args : dictionary of parameters for Colmap command line prompts; align_pcd : boolean indicating whether to align point cloud on calibrated or metadata poses, True by default; calibration_scan_id : ID of the calibration scan, requires ???. Example: [Colmap] matcher = \"exhaustive\" compute_dense = false calibration_scan_id = \"calib_scan_shortpath\" [Colmap.cli_args.feature_extractor] \"--ImageReader.single_camera\" = \"1\" \"--SiftExtraction.use_gpu\" = \"1\" [Colmap.cli_args.exhaustive_matcher] \"--SiftMatching.use_gpu\" = \"1\" [Colmap.cli_args.model_aligner] \"--robust_alignment_max_error\" = \"10\" Todo Add a compute_sparse ? Undistorted task Link Defined in plant3dvision.task.proc2d , it is used to create undistorted images from pre-computed intrinsic camera parameters. Masks task Link Defined in plant3dvision.task.proc2d , it is used to create binary mask of the plant location within each image. List of luigi task parameters: upstream_task : task upstream of the Masks task, default is Undistorted but can be None . List of task variables: type : method to use to compute masks, choices are: 'linear' , 'excess_green' , 'vesselness' , 'invert' ; parameters : list of scalar parameters, depends on type hence no default values ; dilation : integer defining the dilation factor to apply when using a binary mask, no default values ; binarize : boolean indicating whether to binarize the mask, default is True ; threshold : float used as threshold for binarization step, default is 0.0 ; Voxels task Link Defined in plant3dvision.task.cl , it is used to reconstruct the 3D structure of the plant from binary or segmented masks. List of luigi task parameters: upstream_task : task upstream of the Colmap task, default is ImagesFilesetExists List of task variables: type : ; Example: [Voxels] upstream_mask = \"Segmentation2D\" labels = \"[\\\"background\\\"]\" voxel_size = 0.3 type = \"averaging\" invert = false use_colmap_poses = true log = false Note Choose 'Segmentation2D' as ` for ML pipeline or 'Masks'` for geometrical pipeline. PointCloud task Link Example: [PointCloud] level_set_value = 1 background_prior = 0.5 log = false SegmentedPointCloud task Link Example: [SegmentedPointCloud] upstream_segmentation = \"Segmentation2D\" upstream_task = \"PointCloud\" use_colmap_poses = true Segmentation2D task Link Example: [Segmentation2D] upstream_task = \"Undistorted\" query = \"{\\\"channel\\\":\\\"rgb\\\"}\" model_id = \"Resnetdataset_gl_png_896_896_epoch50\" resize = true binarize = true dilation = 1 Sx = 896 Sy = 896 epochs = 1 batch = 1 learning_rate = 0.0001 threshold = 0.0035 Visualization task Link Example: [Visualization] max_image_size = 1500 max_point_cloud_size = 10000 thumbnail_size = 150 pcd_source = \"vox2pcd\" mesh_source = \"delaunay\" ClusteredMesh task Link Example: [ClusteredMesh] upstream_task = \"SegmentedPointCloud\" AnglesAndInternodes task Link Example: [AnglesAndInternodes] upstream_task = \"ClusteredMesh\" Evaluation tasks Link VoxelGroundTruth Link Example: PointCloudGroundTruth Link Example: [PointCloudGroundTruth] pcd_size = 10000 ClusteredMeshGroundTruth Link Example: PointCloudEvaluation task Link Example: [PointCloudEvaluation] max_distance = 0.2 PointCloudSegmentationEvaluation task Link Segmentation2DEvaluation task Link","title":"Configuration files"},{"location":"Scanner/specifications/configuration_files/#tasks-configuration","text":"","title":"Tasks configuration"},{"location":"Scanner/specifications/configuration_files/#introduction","text":"Each task has some default configuration but to create a working pipeline, you need to set some parameters in a TOML configuration file. There are two types of parameters: those associated with luigi: upstream tasks, requirements & outputs those tuning the used algorithms: variables & parameters For examples look at the configurations examples provided in plant3dvision/configs .","title":"Introduction"},{"location":"Scanner/specifications/configuration_files/#io-tasks","text":"These tasks refer to the inputs and outputs of each algorithmic tasks. They are often related to task defined in the plantdb library.","title":"I/O tasks"},{"location":"Scanner/specifications/configuration_files/#filesetexists","text":"Check that the given Fileset id exists (in dataset). No luigi parameters (no upstream task). List of task variables: fileset_id : the id ( str ) to check for existence.","title":"FilesetExists"},{"location":"Scanner/specifications/configuration_files/#imagesfilesetexistsfilesetexists","text":"Check that the image file set exists. No luigi parameters (no upstream task). List of task variables: fileset_id : the id ( str ) to check for existence 'images' by default.","title":"ImagesFilesetExists(FilesetExists)"},{"location":"Scanner/specifications/configuration_files/#modelfilesetfilesetexists","text":"Check that the trained ML models file set exists. No luigi parameters (no upstream task). List of task variables: fileset_id : the id ( str ) to check for existence 'models' by default. Example: [ModelFileset] scan_id = \"models\" Defines the location of the trained ML models in a dataset named 'models' .","title":"ModelFileset(FilesetExists)"},{"location":"Scanner/specifications/configuration_files/#database-tasks","text":"","title":"Database tasks"},{"location":"Scanner/specifications/configuration_files/#clean","text":"Defined in plantdb.task , it is used to clean a scan dataset from the \"processing folders\". No luigi parameters (no upstream task). List of task variables: no_confirm : boolean indicating whether a confirmation is required to clean the dataset, False by default, i.e. confirmation required. Example: [Clean] no_confirm = true Use this to clean without confirmation.","title":"Clean"},{"location":"Scanner/specifications/configuration_files/#algorithmic-tasks","text":"","title":"Algorithmic tasks"},{"location":"Scanner/specifications/configuration_files/#colmap-task","text":"Defined in plant3dvision.task.colmap , it is used to match scan images and estimate camera poses. It can also be used to compute a sparse and/or dense point cloud. List of luigi task parameters: upstream_task : task upstream of the Colmap task, default is ImagesFilesetExists List of task variables: matcher : images matching method, can be either \"exhaustive\" (default) or \"sequential\"; compute_dense : boolean indicating whether to run the dense Colmap to obtain a dense point cloud, False by default; cli_args : dictionary of parameters for Colmap command line prompts; align_pcd : boolean indicating whether to align point cloud on calibrated or metadata poses, True by default; calibration_scan_id : ID of the calibration scan, requires ???. Example: [Colmap] matcher = \"exhaustive\" compute_dense = false calibration_scan_id = \"calib_scan_shortpath\" [Colmap.cli_args.feature_extractor] \"--ImageReader.single_camera\" = \"1\" \"--SiftExtraction.use_gpu\" = \"1\" [Colmap.cli_args.exhaustive_matcher] \"--SiftMatching.use_gpu\" = \"1\" [Colmap.cli_args.model_aligner] \"--robust_alignment_max_error\" = \"10\" Todo Add a compute_sparse ?","title":"Colmap task"},{"location":"Scanner/specifications/configuration_files/#undistorted-task","text":"Defined in plant3dvision.task.proc2d , it is used to create undistorted images from pre-computed intrinsic camera parameters.","title":"Undistorted task"},{"location":"Scanner/specifications/configuration_files/#masks-task","text":"Defined in plant3dvision.task.proc2d , it is used to create binary mask of the plant location within each image. List of luigi task parameters: upstream_task : task upstream of the Masks task, default is Undistorted but can be None . List of task variables: type : method to use to compute masks, choices are: 'linear' , 'excess_green' , 'vesselness' , 'invert' ; parameters : list of scalar parameters, depends on type hence no default values ; dilation : integer defining the dilation factor to apply when using a binary mask, no default values ; binarize : boolean indicating whether to binarize the mask, default is True ; threshold : float used as threshold for binarization step, default is 0.0 ;","title":"Masks task"},{"location":"Scanner/specifications/configuration_files/#voxels-task","text":"Defined in plant3dvision.task.cl , it is used to reconstruct the 3D structure of the plant from binary or segmented masks. List of luigi task parameters: upstream_task : task upstream of the Colmap task, default is ImagesFilesetExists List of task variables: type : ; Example: [Voxels] upstream_mask = \"Segmentation2D\" labels = \"[\\\"background\\\"]\" voxel_size = 0.3 type = \"averaging\" invert = false use_colmap_poses = true log = false Note Choose 'Segmentation2D' as ` for ML pipeline or 'Masks'` for geometrical pipeline.","title":"Voxels task"},{"location":"Scanner/specifications/configuration_files/#pointcloud-task","text":"Example: [PointCloud] level_set_value = 1 background_prior = 0.5 log = false","title":"PointCloud task"},{"location":"Scanner/specifications/configuration_files/#segmentedpointcloud-task","text":"Example: [SegmentedPointCloud] upstream_segmentation = \"Segmentation2D\" upstream_task = \"PointCloud\" use_colmap_poses = true","title":"SegmentedPointCloud task"},{"location":"Scanner/specifications/configuration_files/#segmentation2d-task","text":"Example: [Segmentation2D] upstream_task = \"Undistorted\" query = \"{\\\"channel\\\":\\\"rgb\\\"}\" model_id = \"Resnetdataset_gl_png_896_896_epoch50\" resize = true binarize = true dilation = 1 Sx = 896 Sy = 896 epochs = 1 batch = 1 learning_rate = 0.0001 threshold = 0.0035","title":"Segmentation2D task"},{"location":"Scanner/specifications/configuration_files/#visualization-task","text":"Example: [Visualization] max_image_size = 1500 max_point_cloud_size = 10000 thumbnail_size = 150 pcd_source = \"vox2pcd\" mesh_source = \"delaunay\"","title":"Visualization task"},{"location":"Scanner/specifications/configuration_files/#clusteredmesh-task","text":"Example: [ClusteredMesh] upstream_task = \"SegmentedPointCloud\"","title":"ClusteredMesh task"},{"location":"Scanner/specifications/configuration_files/#anglesandinternodes-task","text":"Example: [AnglesAndInternodes] upstream_task = \"ClusteredMesh\"","title":"AnglesAndInternodes task"},{"location":"Scanner/specifications/configuration_files/#evaluation-tasks","text":"","title":"Evaluation tasks"},{"location":"Scanner/specifications/configuration_files/#voxelgroundtruth","text":"Example:","title":"VoxelGroundTruth"},{"location":"Scanner/specifications/configuration_files/#pointcloudgroundtruth","text":"Example: [PointCloudGroundTruth] pcd_size = 10000","title":"PointCloudGroundTruth"},{"location":"Scanner/specifications/configuration_files/#clusteredmeshgroundtruth","text":"Example:","title":"ClusteredMeshGroundTruth"},{"location":"Scanner/specifications/configuration_files/#pointcloudevaluation-task","text":"Example: [PointCloudEvaluation] max_distance = 0.2","title":"PointCloudEvaluation task"},{"location":"Scanner/specifications/configuration_files/#pointcloudsegmentationevaluation-task","text":"","title":"PointCloudSegmentationEvaluation task"},{"location":"Scanner/specifications/configuration_files/#segmentation2devaluation-task","text":"","title":"Segmentation2DEvaluation task"},{"location":"Scanner/specifications/data/","text":"This page describes how to use the romi package plantdb accessible here . A shared example datasets is accessible here . Getting started Link Installation Link Warning If you intend to contribute to the development of plantdb or want to be able to edit the code and test your changes, you should choose editable mode . Non-editable mode Link Install from GitHub using pip : pip install git+ssh://git@github.com/romi/plantdb.git#dev Note This uses ssh and thus requires to be registered as part of the project and to deploy ssh keys. Editable mode Link Clone from GitHub and install using pip : git clone https://github.com/romi/plantdb.git cd plantdb pip install -e . Minimal working example Link Let's assume you have a list of images of a given object and that you want to add them to a ROMI database as a \"scan\". 1 - Initialize database Link First create the directory for the database and add the romidb marker to it: from os.path import join from tempfile import mkdtemp mydb = mkdtemp ( prefix = 'romidb_' ) open ( join ( mydb , 'romidb' ), 'w' ) . close () Now you can initialize a ROMI FSDB database object: from plantdb.fsdb import FSDB db = FSDB ( mydb ) db . connect () # Locks the database and allows access 2 - Create a new dataset Link To create a new dataset, here named myscan_001 , do: scan = db . create_scan ( \"myscan_001\" ) To add scan metadata ( eg. camera settings, biological metadata, hardware metadata...), do: scan . set_metadata ({ \"scanner\" : { \"harware\" : 'test' }}) This will results in several changes in the local database: Add a myscan_001 sub-directory in the database root directory; Add a metadata sub-directory in myscan_001 and a metadata.json gathering the given scan metadata . 3 - Add images as new fileset Link OPTIONAL - create a list of RGB images If you do not have a scan datasets available, either download a shared datasets here or generate a list of images as follows: import numpy as np # Generate random noise images n_images = 99 imgs = [] for i in range ( n_images ): img = 256 * np . random . rand ( 256 , 256 , 3 ) img = np . array ( img , dtype = np . uint8 ) imgs . append ( img ) Create a new fileset : fileset = scan . create_fileset ( \"images\" ) Add the images to the fileset: Load the file list (or skip if you generated random images): from os import listdir imgs = listdir ( \"</path/to/my/scan/images>\" ) Then loop the images list and add them to the fileset , optionally attach some metadata to each image: from plantdb import io for i , img in enumerate ( imgs ): file = fileset . create_file ( \" %i \" % i ) io . write_image ( file , img ) file . set_metadata ( \"key\" , \" %i \" % i ) This will results in several changes in the local database: Reference the image by its file name by adding an entry in files.json ; Write a scan_img_1.jpeg image in the images sub-directory of the scan \"myscan\" . Add an images sub-directory in the metadata sub-directory, and JSON files with the image id as name to store the image metadata . 4 - Access image files in a fileset Link To access the image files in a fileset (in a datasets, itself in an existing and accessible database), proceed as follows: from plantdb.fsdb import FSDB db = FSDB ( mydb ) db . connect () # Locks the database and allows access scan = db . get_scan ( \"myscan\" ) fileset = scan . get_fileset ( \"images\" ) for f in fileset . get_files (): im = io . read_image ( f ) # reads image data print ( f . get_metadata ( \"key\" )) # i db . disconnect () Examples Link from plantdb.fsdb import FSDB from plantdb import io import numpy as np # Generate random noise images n_images = 100 imgs = [] for i in range ( n_images ): img = 256 * np . random . rand ( 256 , 256 , 3 ) img = np . array ( img , dtype = np . uint8 ) imgs . append ( img ) from os import listdir from os.path import join from tempfile import mkdtemp # Create a temporary DB directory: mydb = mkdtemp ( prefix = 'romidb_' ) # Create the `romidb` file in previously created temporary DB directory: open ( join ( mydb , 'romidb' ), 'w' ) . close () listdir ( mydb ) # Connect to the DB: db = FSDB ( mydb ) db . connect () # Locks the database and allows access # Add a scan datasets to the DB: scan = db . create_scan ( \"myscan_001\" ) listdir ( mydb ) # Add metadata to a scan datasets: scan . set_metadata ({ \"scanner\" : { \"hardware\" : 'test' }}) listdir ( join ( mydb , \"myscan_001\" )) listdir ( join ( mydb , \"myscan_001\" , \"metadata\" )) fileset = scan . create_fileset ( \"images\" ) listdir ( join ( mydb , \"myscan_001\" )) for i , img in enumerate ( imgs ): file = fileset . create_file ( \" %i \" % i ) io . write_image ( file , img ) file . set_metadata ( \"key\" , \" %i \" % i ) # Add some metadata # read files in the fileset: scan = db . get_scan ( \"myscan\" ) fileset = scan . get_fileset ( \"images\" ) for f in fileset . get_files (): im = io . read_image ( f ) # reads image data print ( f . get_metadata ( \"key\" )) # i db . disconnect () Database structure Link Overview Link Hereafter we give an overview of the database structure using the ROMI database terminology: plantdb_root/ \u251c\u2500\u2500 dataset_001/ \u2502 \u251c\u2500\u2500 fileset_A/ \u2502 \u2502 \u251c\u2500\u2500 file_A_001.ext \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2514\u2500\u2500 file_A_009.ext \u2502 \u251c\u2500\u2500 fileset_B/ \u2502 \u2502 \u251c\u2500\u2500 file_B_001.ext \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2514\u2500\u2500 file_B_009.ext \u2502 \u251c\u2500\u2500 metadata \u2502 \u2502 \u251c\u2500\u2500 fileset_A.json \u2502 \u2502 \u251c\u2500\u2500 fileset_B.json \u2502 \u2502 \u2514\u2500\u2500 metadata.json \u2502 \u2514\u2500\u2500 files.json \u251c\u2500\u2500 dataset_002/ \u2502 \u2514\u2500\u2500 [...] \u251c\u2500\u2500 [...] \u251c\u2500\u2500 (lock) \u2514\u2500\u2500 romidb Database root directory Link A root database directory is defined, eg. mydb/ . Inside this directory we need to define (add) the romidb marker, so it may be used by FSDB class. We may also find the lock file used to limit the access to the database to only one user. Note that the database initialization part is manual. To create them, in a terminal: mkdir mydb touch mydb/romidb We just created the following tree structure: mydb/ \u2514\u2500\u2500 romidb Once you have created this root directory and the romidb marker file, you can initialize a ROMI FSDB database object in Python: from plantdb.fsdb import FSDB db = FSDB ( \"mydb\" ) db . connect () The method FSDB.connect() locks the database with a lock file at root directory and allows access. To disconnect and free the database do: db . disconnect () If for some reason the Python terminal unexpectedly terminate without a call to the disconnect method, you may have to remove the lock file manually. Check that no one else is using the database! Within this root database directory you will find other directories corresponding to datasets . Datasets directories Link At the next level, we find the datasets directory(s), eg. named myscan_001 . Their names must be uniques, and you create them as follows: scan = db . create_scan ( \"myscan_001\" ) If you add scan metadata ( eg. camera settings, biological metadata, hardware metadata...) with scan.set_metadata() , you get another directory metadata with a metadata.json file. scan . set_metadata ({ \"scanner\" : { \"hardware\" : 'test' }}) We now have the following tree structure: mydb/ \u251c\u2500\u2500 myscan_001/ \u2502 \u2514\u2500\u2500 metadata/ \u2502 \u2514\u2500\u2500 metadata.json \u2514\u2500\u2500 romidb And the file metadata.json should look like this: { \"scanner\" : { \"hardware\" : \"test\" } } Images directories Link Inside myscan_001/ , we find the datasets or fileset in plantdb terminology. In the case of the \"plant scanner\", this is a list of RGB image files acquired by a camera moving around the plant. To store the datasets, we thus name the created fileset \"images\": fileset = scan . create_fileset ( \"images\" ) This creates an images directory and a files.json at the dataset root directory. We now have the following tree structure: mydb/ \u251c\u2500\u2500 myscan_001/ \u2502 \u251c\u2500\u2500 images/ \u2502 \u251c\u2500\u2500 metadata/ \u2502 \u2502 \u2514\u2500\u2500 metadata.json \u2502 \u2514\u2500\u2500 files.json \u2514\u2500\u2500 romidb The JSON should look like this: { \"filesets\" : [ { \"files\" : [], \"id\" : \"images\" } ] } We then create random RGB images to add to the dataset: import numpy as np # Generate random noise images n_images = 100 imgs = [] for i in range ( n_images ): img = 256 * np . random . rand ( 256 , 256 , 3 ) img = np . array ( img , dtype = np . uint8 ) imgs . append ( img ) And we add them with their metadata to the database: for i , img in enumerate ( imgs ): fname = f \"img_ { str ( i ) . zfill ( 2 ) } .png\" file = fileset . create_file ( fname ) io . write_image ( file , img ) file . set_metadata ( \"key\" , fname ) file . set_metadata ( \"id\" , i ) Inside this images/ directory will reside the images added to the database. At the same time you added images with REF_TO_TUTO , you created an entry in a JSON file referencing the files. If you added metadata along with the files ( eg. camera poses, jpeg metadata...) it should be referenced in metadata/images/ eg. metadata/images/<scan_img_01>.json . mydb/ \u251c\u2500\u2500 myscan_001/ \u2502 \u251c\u2500\u2500 files.json \u2502 \u251c\u2500\u2500 images/ \u2502 \u2502 \u251c\u2500\u2500 scan_img_01.jpg \u2502 \u2502 \u251c\u2500\u2500 scan_img_02.jpg \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2514\u2500\u2500 scan_img_99.jpg \u2502 \u251c\u2500\u2500 metadata/ \u2502 \u2502 \u251c\u2500\u2500 images \u2502 \u2502 \u2502 \u251c\u2500\u2500 scan_img_01.json \u2502 \u2502 \u2502 \u251c\u2500\u2500 scan_img_02.json \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2502 \u2514\u2500\u2500 scan_img_99.json \u2502 \u2502 \u2514\u2500\u2500 metadata.json \u2514\u2500\u2500 romidb Example Link mydb/ \u251c\u2500\u2500 myscan_001/ \u2502 \u251c\u2500\u2500 AnglesAndInternodes_1_0_2_0_0_1_dd8d67653a \u2502 \u2502 \u2514\u2500\u2500 AnglesAndInternodes.json \u2502 \u251c\u2500\u2500 Colmap_True____feature_extrac_3bbfcb1413 \u2502 \u2502 \u251c\u2500\u2500 cameras.json \u2502 \u2502 \u251c\u2500\u2500 images.json \u2502 \u2502 \u251c\u2500\u2500 points3d.json \u2502 \u2502 \u2514\u2500\u2500 sparse.ply \u2502 \u251c\u2500\u2500 CurveSkeleton_out__TriangleMesh_6a92751c20 \u2502 \u2502 \u2514\u2500\u2500 CurveSkeleton.json \u2502 \u251c\u2500\u2500 files.json \u2502 \u251c\u2500\u2500 images \u2502 \u2502 \u251c\u2500\u2500 pict20190201_110110_0.jpg \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2514\u2500\u2500 pict20190201_111209_0.jpg \u2502 \u251c\u2500\u2500 Masks_True_5_out_9adb9db801 \u2502 \u2502 \u251c\u2500\u2500 pict20190201_110110_0.jpg \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2514\u2500\u2500 pict20190201_111209_0.jpg \u2502 \u251c\u2500\u2500 measures.csv \u2502 \u251c\u2500\u2500 metadata \u2502 \u2502 \u251c\u2500\u2500 AnglesAndInternodes_1_0_2_0_0_1_dd8d67653a.json \u2502 \u2502 \u251c\u2500\u2500 Colmap_True____feature_extrac_3bbfcb1413.json \u2502 \u2502 \u251c\u2500\u2500 CurveSkeleton_out__TriangleMesh_6a92751c20.json \u2502 \u2502 \u251c\u2500\u2500 images \u2502 \u2502 \u2502 \u251c\u2500\u2500 pict20190201_110110_0.json \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2502 \u2514\u2500\u2500 pict20190201_111209_0.json \u2502 \u2502 \u251c\u2500\u2500 images.json \u2502 \u2502 \u251c\u2500\u2500 Masks_True_5_out_9adb9db801 \u2502 \u2502 \u2502 \u251c\u2500\u2500 pict20190201_110110_0.json \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2502 \u2514\u2500\u2500 pict20190201_111209_0.json \u2502 \u2502 \u251c\u2500\u2500 Masks_True_5_out_e90d1804eb.json \u2502 \u2502 \u251c\u2500\u2500 metadata.json \u2502 \u2502 \u251c\u2500\u2500 PointCloud_1_0_1_0_False_9ab5a15d9b \u2502 \u2502 \u2502 \u2514\u2500\u2500 PointCloud.json \u2502 \u2502 \u251c\u2500\u2500 PointCloud_1_0_1_0_False_9ab5a15d9b.json \u2502 \u2502 \u251c\u2500\u2500 PointCloud__200_0_1_0_False_4ce2e46446.json \u2502 \u2502 \u251c\u2500\u2500 TreeGraph_out__CurveSkeleton_5dca9a2821.json \u2502 \u2502 \u251c\u2500\u2500 TriangleMesh_out__PointCloud_80dc94ac81.json \u2502 \u2502 \u251c\u2500\u2500 Undistorted_out_____fb3e3fa0ff \u2502 \u2502 \u2502 \u251c\u2500\u2500 pict20190201_110110_0.json \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2502 \u2514\u2500\u2500 pict20190201_111209_0.json \u2502 \u2502 \u251c\u2500\u2500 Undistorted_out_____fb3e3fa0ff.json \u2502 \u2502 \u251c\u2500\u2500 Voxels_False____False_567dc7f48b \u2502 \u2502 \u2502 \u2514\u2500\u2500 Voxels.json \u2502 \u2502 \u251c\u2500\u2500 Voxels_False____False_567dc7f48b.json \u2502 \u2502 \u251c\u2500\u2500 Voxels_False____True_af037e876e.json \u2502 \u2502 \u2514\u2500\u2500 Voxels_False____True_cd9a5ff06b.json \u2502 \u251c\u2500\u2500 pipeline.toml \u2502 \u251c\u2500\u2500 PointCloud_1_0_1_0_False_9ab5a15d9b \u2502 \u2502 \u2514\u2500\u2500 PointCloud.ply \u2502 \u251c\u2500\u2500 TreeGraph_out__CurveSkeleton_5dca9a2821 \u2502 \u2502 \u2514\u2500\u2500 TreeGraph.p \u2502 \u251c\u2500\u2500 TriangleMesh_out__PointCloud_80dc94ac81 \u2502 \u2502 \u2514\u2500\u2500 TriangleMesh.ply \u2502 \u2514\u2500\u2500 Voxels_False____False_567dc7f48b \u2502 \u2514\u2500\u2500 Voxels.npz \u251c\u2500\u2500 colmap_log.txt \u251c\u2500\u2500 lock \u2514\u2500\u2500 romidb","title":"Data"},{"location":"Scanner/specifications/data/#getting-started","text":"","title":"Getting started"},{"location":"Scanner/specifications/data/#installation","text":"Warning If you intend to contribute to the development of plantdb or want to be able to edit the code and test your changes, you should choose editable mode .","title":"Installation"},{"location":"Scanner/specifications/data/#non-editable-mode","text":"Install from GitHub using pip : pip install git+ssh://git@github.com/romi/plantdb.git#dev Note This uses ssh and thus requires to be registered as part of the project and to deploy ssh keys.","title":"Non-editable mode"},{"location":"Scanner/specifications/data/#editable-mode","text":"Clone from GitHub and install using pip : git clone https://github.com/romi/plantdb.git cd plantdb pip install -e .","title":"Editable mode"},{"location":"Scanner/specifications/data/#minimal-working-example","text":"Let's assume you have a list of images of a given object and that you want to add them to a ROMI database as a \"scan\".","title":"Minimal working example"},{"location":"Scanner/specifications/data/#1-initialize-database","text":"First create the directory for the database and add the romidb marker to it: from os.path import join from tempfile import mkdtemp mydb = mkdtemp ( prefix = 'romidb_' ) open ( join ( mydb , 'romidb' ), 'w' ) . close () Now you can initialize a ROMI FSDB database object: from plantdb.fsdb import FSDB db = FSDB ( mydb ) db . connect () # Locks the database and allows access","title":"1 - Initialize database"},{"location":"Scanner/specifications/data/#2-create-a-new-dataset","text":"To create a new dataset, here named myscan_001 , do: scan = db . create_scan ( \"myscan_001\" ) To add scan metadata ( eg. camera settings, biological metadata, hardware metadata...), do: scan . set_metadata ({ \"scanner\" : { \"harware\" : 'test' }}) This will results in several changes in the local database: Add a myscan_001 sub-directory in the database root directory; Add a metadata sub-directory in myscan_001 and a metadata.json gathering the given scan metadata .","title":"2 - Create a new dataset"},{"location":"Scanner/specifications/data/#3-add-images-as-new-fileset","text":"OPTIONAL - create a list of RGB images If you do not have a scan datasets available, either download a shared datasets here or generate a list of images as follows: import numpy as np # Generate random noise images n_images = 99 imgs = [] for i in range ( n_images ): img = 256 * np . random . rand ( 256 , 256 , 3 ) img = np . array ( img , dtype = np . uint8 ) imgs . append ( img ) Create a new fileset : fileset = scan . create_fileset ( \"images\" ) Add the images to the fileset: Load the file list (or skip if you generated random images): from os import listdir imgs = listdir ( \"</path/to/my/scan/images>\" ) Then loop the images list and add them to the fileset , optionally attach some metadata to each image: from plantdb import io for i , img in enumerate ( imgs ): file = fileset . create_file ( \" %i \" % i ) io . write_image ( file , img ) file . set_metadata ( \"key\" , \" %i \" % i ) This will results in several changes in the local database: Reference the image by its file name by adding an entry in files.json ; Write a scan_img_1.jpeg image in the images sub-directory of the scan \"myscan\" . Add an images sub-directory in the metadata sub-directory, and JSON files with the image id as name to store the image metadata .","title":"3 - Add images as new fileset"},{"location":"Scanner/specifications/data/#4-access-image-files-in-a-fileset","text":"To access the image files in a fileset (in a datasets, itself in an existing and accessible database), proceed as follows: from plantdb.fsdb import FSDB db = FSDB ( mydb ) db . connect () # Locks the database and allows access scan = db . get_scan ( \"myscan\" ) fileset = scan . get_fileset ( \"images\" ) for f in fileset . get_files (): im = io . read_image ( f ) # reads image data print ( f . get_metadata ( \"key\" )) # i db . disconnect ()","title":"4 - Access image files in a fileset"},{"location":"Scanner/specifications/data/#examples","text":"from plantdb.fsdb import FSDB from plantdb import io import numpy as np # Generate random noise images n_images = 100 imgs = [] for i in range ( n_images ): img = 256 * np . random . rand ( 256 , 256 , 3 ) img = np . array ( img , dtype = np . uint8 ) imgs . append ( img ) from os import listdir from os.path import join from tempfile import mkdtemp # Create a temporary DB directory: mydb = mkdtemp ( prefix = 'romidb_' ) # Create the `romidb` file in previously created temporary DB directory: open ( join ( mydb , 'romidb' ), 'w' ) . close () listdir ( mydb ) # Connect to the DB: db = FSDB ( mydb ) db . connect () # Locks the database and allows access # Add a scan datasets to the DB: scan = db . create_scan ( \"myscan_001\" ) listdir ( mydb ) # Add metadata to a scan datasets: scan . set_metadata ({ \"scanner\" : { \"hardware\" : 'test' }}) listdir ( join ( mydb , \"myscan_001\" )) listdir ( join ( mydb , \"myscan_001\" , \"metadata\" )) fileset = scan . create_fileset ( \"images\" ) listdir ( join ( mydb , \"myscan_001\" )) for i , img in enumerate ( imgs ): file = fileset . create_file ( \" %i \" % i ) io . write_image ( file , img ) file . set_metadata ( \"key\" , \" %i \" % i ) # Add some metadata # read files in the fileset: scan = db . get_scan ( \"myscan\" ) fileset = scan . get_fileset ( \"images\" ) for f in fileset . get_files (): im = io . read_image ( f ) # reads image data print ( f . get_metadata ( \"key\" )) # i db . disconnect ()","title":"Examples"},{"location":"Scanner/specifications/data/#database-structure","text":"","title":"Database structure"},{"location":"Scanner/specifications/data/#overview","text":"Hereafter we give an overview of the database structure using the ROMI database terminology: plantdb_root/ \u251c\u2500\u2500 dataset_001/ \u2502 \u251c\u2500\u2500 fileset_A/ \u2502 \u2502 \u251c\u2500\u2500 file_A_001.ext \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2514\u2500\u2500 file_A_009.ext \u2502 \u251c\u2500\u2500 fileset_B/ \u2502 \u2502 \u251c\u2500\u2500 file_B_001.ext \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2514\u2500\u2500 file_B_009.ext \u2502 \u251c\u2500\u2500 metadata \u2502 \u2502 \u251c\u2500\u2500 fileset_A.json \u2502 \u2502 \u251c\u2500\u2500 fileset_B.json \u2502 \u2502 \u2514\u2500\u2500 metadata.json \u2502 \u2514\u2500\u2500 files.json \u251c\u2500\u2500 dataset_002/ \u2502 \u2514\u2500\u2500 [...] \u251c\u2500\u2500 [...] \u251c\u2500\u2500 (lock) \u2514\u2500\u2500 romidb","title":"Overview"},{"location":"Scanner/specifications/data/#database-root-directory","text":"A root database directory is defined, eg. mydb/ . Inside this directory we need to define (add) the romidb marker, so it may be used by FSDB class. We may also find the lock file used to limit the access to the database to only one user. Note that the database initialization part is manual. To create them, in a terminal: mkdir mydb touch mydb/romidb We just created the following tree structure: mydb/ \u2514\u2500\u2500 romidb Once you have created this root directory and the romidb marker file, you can initialize a ROMI FSDB database object in Python: from plantdb.fsdb import FSDB db = FSDB ( \"mydb\" ) db . connect () The method FSDB.connect() locks the database with a lock file at root directory and allows access. To disconnect and free the database do: db . disconnect () If for some reason the Python terminal unexpectedly terminate without a call to the disconnect method, you may have to remove the lock file manually. Check that no one else is using the database! Within this root database directory you will find other directories corresponding to datasets .","title":"Database root directory"},{"location":"Scanner/specifications/data/#datasets-directories","text":"At the next level, we find the datasets directory(s), eg. named myscan_001 . Their names must be uniques, and you create them as follows: scan = db . create_scan ( \"myscan_001\" ) If you add scan metadata ( eg. camera settings, biological metadata, hardware metadata...) with scan.set_metadata() , you get another directory metadata with a metadata.json file. scan . set_metadata ({ \"scanner\" : { \"hardware\" : 'test' }}) We now have the following tree structure: mydb/ \u251c\u2500\u2500 myscan_001/ \u2502 \u2514\u2500\u2500 metadata/ \u2502 \u2514\u2500\u2500 metadata.json \u2514\u2500\u2500 romidb And the file metadata.json should look like this: { \"scanner\" : { \"hardware\" : \"test\" } }","title":"Datasets directories"},{"location":"Scanner/specifications/data/#images-directories","text":"Inside myscan_001/ , we find the datasets or fileset in plantdb terminology. In the case of the \"plant scanner\", this is a list of RGB image files acquired by a camera moving around the plant. To store the datasets, we thus name the created fileset \"images\": fileset = scan . create_fileset ( \"images\" ) This creates an images directory and a files.json at the dataset root directory. We now have the following tree structure: mydb/ \u251c\u2500\u2500 myscan_001/ \u2502 \u251c\u2500\u2500 images/ \u2502 \u251c\u2500\u2500 metadata/ \u2502 \u2502 \u2514\u2500\u2500 metadata.json \u2502 \u2514\u2500\u2500 files.json \u2514\u2500\u2500 romidb The JSON should look like this: { \"filesets\" : [ { \"files\" : [], \"id\" : \"images\" } ] } We then create random RGB images to add to the dataset: import numpy as np # Generate random noise images n_images = 100 imgs = [] for i in range ( n_images ): img = 256 * np . random . rand ( 256 , 256 , 3 ) img = np . array ( img , dtype = np . uint8 ) imgs . append ( img ) And we add them with their metadata to the database: for i , img in enumerate ( imgs ): fname = f \"img_ { str ( i ) . zfill ( 2 ) } .png\" file = fileset . create_file ( fname ) io . write_image ( file , img ) file . set_metadata ( \"key\" , fname ) file . set_metadata ( \"id\" , i ) Inside this images/ directory will reside the images added to the database. At the same time you added images with REF_TO_TUTO , you created an entry in a JSON file referencing the files. If you added metadata along with the files ( eg. camera poses, jpeg metadata...) it should be referenced in metadata/images/ eg. metadata/images/<scan_img_01>.json . mydb/ \u251c\u2500\u2500 myscan_001/ \u2502 \u251c\u2500\u2500 files.json \u2502 \u251c\u2500\u2500 images/ \u2502 \u2502 \u251c\u2500\u2500 scan_img_01.jpg \u2502 \u2502 \u251c\u2500\u2500 scan_img_02.jpg \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2514\u2500\u2500 scan_img_99.jpg \u2502 \u251c\u2500\u2500 metadata/ \u2502 \u2502 \u251c\u2500\u2500 images \u2502 \u2502 \u2502 \u251c\u2500\u2500 scan_img_01.json \u2502 \u2502 \u2502 \u251c\u2500\u2500 scan_img_02.json \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2502 \u2514\u2500\u2500 scan_img_99.json \u2502 \u2502 \u2514\u2500\u2500 metadata.json \u2514\u2500\u2500 romidb","title":"Images directories"},{"location":"Scanner/specifications/data/#example","text":"mydb/ \u251c\u2500\u2500 myscan_001/ \u2502 \u251c\u2500\u2500 AnglesAndInternodes_1_0_2_0_0_1_dd8d67653a \u2502 \u2502 \u2514\u2500\u2500 AnglesAndInternodes.json \u2502 \u251c\u2500\u2500 Colmap_True____feature_extrac_3bbfcb1413 \u2502 \u2502 \u251c\u2500\u2500 cameras.json \u2502 \u2502 \u251c\u2500\u2500 images.json \u2502 \u2502 \u251c\u2500\u2500 points3d.json \u2502 \u2502 \u2514\u2500\u2500 sparse.ply \u2502 \u251c\u2500\u2500 CurveSkeleton_out__TriangleMesh_6a92751c20 \u2502 \u2502 \u2514\u2500\u2500 CurveSkeleton.json \u2502 \u251c\u2500\u2500 files.json \u2502 \u251c\u2500\u2500 images \u2502 \u2502 \u251c\u2500\u2500 pict20190201_110110_0.jpg \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2514\u2500\u2500 pict20190201_111209_0.jpg \u2502 \u251c\u2500\u2500 Masks_True_5_out_9adb9db801 \u2502 \u2502 \u251c\u2500\u2500 pict20190201_110110_0.jpg \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2514\u2500\u2500 pict20190201_111209_0.jpg \u2502 \u251c\u2500\u2500 measures.csv \u2502 \u251c\u2500\u2500 metadata \u2502 \u2502 \u251c\u2500\u2500 AnglesAndInternodes_1_0_2_0_0_1_dd8d67653a.json \u2502 \u2502 \u251c\u2500\u2500 Colmap_True____feature_extrac_3bbfcb1413.json \u2502 \u2502 \u251c\u2500\u2500 CurveSkeleton_out__TriangleMesh_6a92751c20.json \u2502 \u2502 \u251c\u2500\u2500 images \u2502 \u2502 \u2502 \u251c\u2500\u2500 pict20190201_110110_0.json \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2502 \u2514\u2500\u2500 pict20190201_111209_0.json \u2502 \u2502 \u251c\u2500\u2500 images.json \u2502 \u2502 \u251c\u2500\u2500 Masks_True_5_out_9adb9db801 \u2502 \u2502 \u2502 \u251c\u2500\u2500 pict20190201_110110_0.json \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2502 \u2514\u2500\u2500 pict20190201_111209_0.json \u2502 \u2502 \u251c\u2500\u2500 Masks_True_5_out_e90d1804eb.json \u2502 \u2502 \u251c\u2500\u2500 metadata.json \u2502 \u2502 \u251c\u2500\u2500 PointCloud_1_0_1_0_False_9ab5a15d9b \u2502 \u2502 \u2502 \u2514\u2500\u2500 PointCloud.json \u2502 \u2502 \u251c\u2500\u2500 PointCloud_1_0_1_0_False_9ab5a15d9b.json \u2502 \u2502 \u251c\u2500\u2500 PointCloud__200_0_1_0_False_4ce2e46446.json \u2502 \u2502 \u251c\u2500\u2500 TreeGraph_out__CurveSkeleton_5dca9a2821.json \u2502 \u2502 \u251c\u2500\u2500 TriangleMesh_out__PointCloud_80dc94ac81.json \u2502 \u2502 \u251c\u2500\u2500 Undistorted_out_____fb3e3fa0ff \u2502 \u2502 \u2502 \u251c\u2500\u2500 pict20190201_110110_0.json \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2502 \u2514\u2500\u2500 pict20190201_111209_0.json \u2502 \u2502 \u251c\u2500\u2500 Undistorted_out_____fb3e3fa0ff.json \u2502 \u2502 \u251c\u2500\u2500 Voxels_False____False_567dc7f48b \u2502 \u2502 \u2502 \u2514\u2500\u2500 Voxels.json \u2502 \u2502 \u251c\u2500\u2500 Voxels_False____False_567dc7f48b.json \u2502 \u2502 \u251c\u2500\u2500 Voxels_False____True_af037e876e.json \u2502 \u2502 \u2514\u2500\u2500 Voxels_False____True_cd9a5ff06b.json \u2502 \u251c\u2500\u2500 pipeline.toml \u2502 \u251c\u2500\u2500 PointCloud_1_0_1_0_False_9ab5a15d9b \u2502 \u2502 \u2514\u2500\u2500 PointCloud.ply \u2502 \u251c\u2500\u2500 TreeGraph_out__CurveSkeleton_5dca9a2821 \u2502 \u2502 \u2514\u2500\u2500 TreeGraph.p \u2502 \u251c\u2500\u2500 TriangleMesh_out__PointCloud_80dc94ac81 \u2502 \u2502 \u2514\u2500\u2500 TriangleMesh.ply \u2502 \u2514\u2500\u2500 Voxels_False____False_567dc7f48b \u2502 \u2514\u2500\u2500 Voxels.npz \u251c\u2500\u2500 colmap_log.txt \u251c\u2500\u2500 lock \u2514\u2500\u2500 romidb","title":"Example"},{"location":"Scanner/specifications/hardware/","text":"Hardware setup and instructions Link Network overview Link The general network design of the ROMI Plant Imager is as the following: The raspberry pi controls the movements of the camera thanks to the CNC (for the x,y,z coordinates) and the Gimbal (pan and tilt). Both of them are connected to the pi by USB cables. For the camera, several configurations exists. It is possible to retrieve the photos either by Wi-Fi (might lead to a lower resolution) or directly via a micro USB. Hardware configuration files Link To gather configuration information on the hardware during an acquisition with the plant imager we use toml files. For example, saving the following lines in a config.toml : [ScanPath] class_name = \"Circle\" # Circle, Line, Cylinder [ScanPath.kwargs] center_x = 375 center_y = 375 z = 80 tilt = 0 radius = 300 n_points = 60 [Scan.scanner.cnc] module = \"plantimager.grbl\" [Scan.scanner.cnc.kwargs] homing = true port = \"/dev/ttyACM0\" [Scan.scanner.gimbal] module = \"plantimager.blgimbal\" [Scan.scanner.gimbal.kwargs] port = \"/dev/ttyACM1\" has_tilt = false zero_pan = 0 invert_rotation = true [Scan.scanner.camera] module = \"plantimager.sony\" [Scan.scanner.camera.kwargs] device_ip = \"192.168.122.1\" api_port = \"10000\" postview = true use_flashair = false rotation = 270 [Scan.metadata.object] species = \"chenopodium album\" seed_stock = \"Col-0\" plant_id = \"3dt_chenoA\" growth_environment = \"Lyon-indoor\" growth_conditions = \"SD+LD\" treatment = \"None\" DAG = 40 sample = \"main_stem\" experiment_id = \"3dt_2021-01\" dataset_id = \"3dt\" [Scan.metadata.hardware] frame = \"30profile v1\" X_motor = \"X-Carve NEMA23\" Y_motor = \"X-Carve NEMA23\" Z_motor = \"X-Carve NEMA23\" pan_motor = \"iPower Motor GM4108H-120T Brushless Gimbal Motor\" tilt_motor = \"None\" sensor = \"RX0\" [Scan.metadata.workspace] x = [200, 600, ] y = [200, 600, ] z = [-100, 300, ] Some arguments in this example have default values and for others (commented \"mandatory\" in the following description) it has to be specified in the configuration file. Here, a more detailed explanation with a full default parameters list: The acquisition path: [ ScanPath ] # mandatory class_name = \"Circle\" class_name defines the type of path the robotic arm will follow. In this case it will be a circle, the other possibilities are commented next to the variable in the example above. [ ScanPath . kwargs ] # mandatory center_x = 375 center_y = 375 z = 80 tilt = 0 radius = 300 n_points = 60 The kwargs related to the path are in this section. The arm will perform a circle of 300 around the point (375, 375) with a fixed z (80) and a tilt of 0\u00b0. The angle between each pose will be 5\u00b0 because the n_points is 60 on a 360\u00b0 circle. The center_x , center_y , z and radius parameters are expressed in mm and are related to the axis of the CNC. To have an idea of valid values for those it's possible to get the limits of the CNC axis with steps described in the cnc calibration description. Needed parameters for connection between hardware components (CNC, Gimbal and camera) and software: [ Scan . scanner . cnc ] # mandatory module = \"plantimager.grbl\" Here for example for the CNC you will have to inform about the python module required to connect to the hardware. It will depend on the type of the device. [Scan.scanner.cnc.kwargs] homing = true port = \"/dev/ttyUSB0\" baud_rate = 115200 x_lims = None y_lims = None z_lims = None safe_start = True invert_x = true invert_y = true invert_z = true The arguments all have default values here, but you might need to change the port (check with dmesg -w ). [ Scan . scanner . gimbal ] # mandatory module = \"plantimager.blgimbal\" [Scan.scanner.gimbal.kwargs] port = \"/dev/ttyUSB0\" has_tilt = True steps_per_turn = 360 zero_pan = 0 zero_tilt = 0 invert_rotation = False Similarly, for the Gimbal, again with default arguments that could be changed depending on your setup [ Scan . scanner . camera ] # mandatory module = \"plantimager.sony\" # or plantimager.gp2 [Scan.scanner.camera.kwargs] device_ip = \"192.168.122.1\" # mandatory api_port = \"10000\" # mandatory timeout : time_s = 10 postview = false use_adb = false use_flashair = false flashair_host = None camera_params = None rotation = 0 Finally, the camera (in this case the SONY RX0 communicating via Wi-Fi) with more specific arguments that will depend on the type of sensor used. A more precise documentation on several cameras and their associated parameters can be found here Object metadata: In principle, you can put any information that appear important as part of an experiment but to have a guideline of relevant parameters in the context of phenotyping you might want to check the biological metadata documentation Hardware metadata: Again here, some guidelines for this section can be found in the hardware metadata description. [ Scan . metadata . workspace ] # mandatory x = [200, 600, ] y = [200, 600, ] z = [-100, 300, ] Concerning the workspace, it is not properly required for the scan to perform but if a reconstruction is to be made it will be needed. As for the path, appropriate coordinates can be collected from information contained in the cnc calibration . To load the config file in python: >> > import toml >> > conf = toml . load ( open ( 'config.toml' )) >> > print ( conf ) { 'Scan' : { 'scanner' : { 'camera_firmware' : 'sony_wifi' , 'cnc_firmware' : 'grbl-v1.1' , 'gimbal_firmware' : 'blgimbal' }}} >> > print ( conf [ \"Scan\" ][ \"scanner\" ][ \"camera_firmware\" ]) \"sony_wifi\" PiZero camera rovercam Link WORK IN PROGRESS!!!!! Configuring the access point host software (hostapd) Link Source: Raspberry Foundation website . 1. General setup Link Switch over to systemd-networkd : # remove classic networking sudo apt --autoremove purge ifupdown dhcpcd5 isc-dhcp-client isc-dhcp-common rm -r /etc/network /etc/dhcp # enable systemd-networkd systemctl enable systemd-networkd.service # setup systemd-resolved systemctl enable systemd-resolved.service apt --autoremove purge avahi-daemon apt install libnss-resolve ln -sf /run/systemd/resolve/stub-resolv.conf /etc/resolv.conf 2. Configure wpa_supplicant as access point Link To configure wpa_supplicant as access point create this file with your settings for country= , ssid= , psk= and maybe frequency= . You can just copy and paste this in one block to your command line beginning with cat and including both EOF (delimiter EOF will not get part of the file): cat > /etc/wpa_supplicant/wpa_supplicant-wlan0.conf <<EOF country=DE ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 network={ ssid=\"RPiNet\" mode=2 frequency=2437 #key_mgmt=NONE # uncomment this for an open hotspot # delete next 3 lines if key_mgmt=NONE key_mgmt=WPA-PSK proto=RSN WPA psk=\"password\" } EOF chmod 600 /etc/wpa_supplicant/wpa_supplicant-wlan0.conf systemctl disable wpa_supplicant.service systemctl enable wpa_supplicant@wlan0.service Setting up a stand alone access point Link Example for this setup: wifi mobile-phone <~.~.~.~.~> ( wlan0 ) RPi ( eth0 ) \\ / ( dhcp ) 192 .168.4.1 Do \"General setup\" then create the following file to configure wlan0 . We only have the access point. There is no ethernet device configured. cat > /etc/systemd/network/08-wlan0.network <<EOF [Match] Name=wlan0 [Network] Address=192.168.4.1/24 MulticastDNS=yes DHCPServer=yes EOF If you want this then reboot. That's it. Otherwise, go on, no need to reboot this time. Troubleshooting Link Serial access denied Link Look here if you can not communicate with the scanner using usb.","title":"Hardware"},{"location":"Scanner/specifications/hardware/#hardware-setup-and-instructions","text":"","title":"Hardware setup and instructions"},{"location":"Scanner/specifications/hardware/#network-overview","text":"The general network design of the ROMI Plant Imager is as the following: The raspberry pi controls the movements of the camera thanks to the CNC (for the x,y,z coordinates) and the Gimbal (pan and tilt). Both of them are connected to the pi by USB cables. For the camera, several configurations exists. It is possible to retrieve the photos either by Wi-Fi (might lead to a lower resolution) or directly via a micro USB.","title":"Network overview"},{"location":"Scanner/specifications/hardware/#hardware-configuration-files","text":"To gather configuration information on the hardware during an acquisition with the plant imager we use toml files. For example, saving the following lines in a config.toml : [ScanPath] class_name = \"Circle\" # Circle, Line, Cylinder [ScanPath.kwargs] center_x = 375 center_y = 375 z = 80 tilt = 0 radius = 300 n_points = 60 [Scan.scanner.cnc] module = \"plantimager.grbl\" [Scan.scanner.cnc.kwargs] homing = true port = \"/dev/ttyACM0\" [Scan.scanner.gimbal] module = \"plantimager.blgimbal\" [Scan.scanner.gimbal.kwargs] port = \"/dev/ttyACM1\" has_tilt = false zero_pan = 0 invert_rotation = true [Scan.scanner.camera] module = \"plantimager.sony\" [Scan.scanner.camera.kwargs] device_ip = \"192.168.122.1\" api_port = \"10000\" postview = true use_flashair = false rotation = 270 [Scan.metadata.object] species = \"chenopodium album\" seed_stock = \"Col-0\" plant_id = \"3dt_chenoA\" growth_environment = \"Lyon-indoor\" growth_conditions = \"SD+LD\" treatment = \"None\" DAG = 40 sample = \"main_stem\" experiment_id = \"3dt_2021-01\" dataset_id = \"3dt\" [Scan.metadata.hardware] frame = \"30profile v1\" X_motor = \"X-Carve NEMA23\" Y_motor = \"X-Carve NEMA23\" Z_motor = \"X-Carve NEMA23\" pan_motor = \"iPower Motor GM4108H-120T Brushless Gimbal Motor\" tilt_motor = \"None\" sensor = \"RX0\" [Scan.metadata.workspace] x = [200, 600, ] y = [200, 600, ] z = [-100, 300, ] Some arguments in this example have default values and for others (commented \"mandatory\" in the following description) it has to be specified in the configuration file. Here, a more detailed explanation with a full default parameters list: The acquisition path: [ ScanPath ] # mandatory class_name = \"Circle\" class_name defines the type of path the robotic arm will follow. In this case it will be a circle, the other possibilities are commented next to the variable in the example above. [ ScanPath . kwargs ] # mandatory center_x = 375 center_y = 375 z = 80 tilt = 0 radius = 300 n_points = 60 The kwargs related to the path are in this section. The arm will perform a circle of 300 around the point (375, 375) with a fixed z (80) and a tilt of 0\u00b0. The angle between each pose will be 5\u00b0 because the n_points is 60 on a 360\u00b0 circle. The center_x , center_y , z and radius parameters are expressed in mm and are related to the axis of the CNC. To have an idea of valid values for those it's possible to get the limits of the CNC axis with steps described in the cnc calibration description. Needed parameters for connection between hardware components (CNC, Gimbal and camera) and software: [ Scan . scanner . cnc ] # mandatory module = \"plantimager.grbl\" Here for example for the CNC you will have to inform about the python module required to connect to the hardware. It will depend on the type of the device. [Scan.scanner.cnc.kwargs] homing = true port = \"/dev/ttyUSB0\" baud_rate = 115200 x_lims = None y_lims = None z_lims = None safe_start = True invert_x = true invert_y = true invert_z = true The arguments all have default values here, but you might need to change the port (check with dmesg -w ). [ Scan . scanner . gimbal ] # mandatory module = \"plantimager.blgimbal\" [Scan.scanner.gimbal.kwargs] port = \"/dev/ttyUSB0\" has_tilt = True steps_per_turn = 360 zero_pan = 0 zero_tilt = 0 invert_rotation = False Similarly, for the Gimbal, again with default arguments that could be changed depending on your setup [ Scan . scanner . camera ] # mandatory module = \"plantimager.sony\" # or plantimager.gp2 [Scan.scanner.camera.kwargs] device_ip = \"192.168.122.1\" # mandatory api_port = \"10000\" # mandatory timeout : time_s = 10 postview = false use_adb = false use_flashair = false flashair_host = None camera_params = None rotation = 0 Finally, the camera (in this case the SONY RX0 communicating via Wi-Fi) with more specific arguments that will depend on the type of sensor used. A more precise documentation on several cameras and their associated parameters can be found here Object metadata: In principle, you can put any information that appear important as part of an experiment but to have a guideline of relevant parameters in the context of phenotyping you might want to check the biological metadata documentation Hardware metadata: Again here, some guidelines for this section can be found in the hardware metadata description. [ Scan . metadata . workspace ] # mandatory x = [200, 600, ] y = [200, 600, ] z = [-100, 300, ] Concerning the workspace, it is not properly required for the scan to perform but if a reconstruction is to be made it will be needed. As for the path, appropriate coordinates can be collected from information contained in the cnc calibration . To load the config file in python: >> > import toml >> > conf = toml . load ( open ( 'config.toml' )) >> > print ( conf ) { 'Scan' : { 'scanner' : { 'camera_firmware' : 'sony_wifi' , 'cnc_firmware' : 'grbl-v1.1' , 'gimbal_firmware' : 'blgimbal' }}} >> > print ( conf [ \"Scan\" ][ \"scanner\" ][ \"camera_firmware\" ]) \"sony_wifi\"","title":"Hardware configuration files"},{"location":"Scanner/specifications/hardware/#pizero-camera-rovercam","text":"WORK IN PROGRESS!!!!!","title":"PiZero camera rovercam"},{"location":"Scanner/specifications/hardware/#configuring-the-access-point-host-software-hostapd","text":"Source: Raspberry Foundation website .","title":"Configuring the access point host software (hostapd)"},{"location":"Scanner/specifications/hardware/#1-general-setup","text":"Switch over to systemd-networkd : # remove classic networking sudo apt --autoremove purge ifupdown dhcpcd5 isc-dhcp-client isc-dhcp-common rm -r /etc/network /etc/dhcp # enable systemd-networkd systemctl enable systemd-networkd.service # setup systemd-resolved systemctl enable systemd-resolved.service apt --autoremove purge avahi-daemon apt install libnss-resolve ln -sf /run/systemd/resolve/stub-resolv.conf /etc/resolv.conf","title":"1. General setup"},{"location":"Scanner/specifications/hardware/#2-configure-wpa_supplicant-as-access-point","text":"To configure wpa_supplicant as access point create this file with your settings for country= , ssid= , psk= and maybe frequency= . You can just copy and paste this in one block to your command line beginning with cat and including both EOF (delimiter EOF will not get part of the file): cat > /etc/wpa_supplicant/wpa_supplicant-wlan0.conf <<EOF country=DE ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 network={ ssid=\"RPiNet\" mode=2 frequency=2437 #key_mgmt=NONE # uncomment this for an open hotspot # delete next 3 lines if key_mgmt=NONE key_mgmt=WPA-PSK proto=RSN WPA psk=\"password\" } EOF chmod 600 /etc/wpa_supplicant/wpa_supplicant-wlan0.conf systemctl disable wpa_supplicant.service systemctl enable wpa_supplicant@wlan0.service","title":"2. Configure wpa_supplicant as access point"},{"location":"Scanner/specifications/hardware/#setting-up-a-stand-alone-access-point","text":"Example for this setup: wifi mobile-phone <~.~.~.~.~> ( wlan0 ) RPi ( eth0 ) \\ / ( dhcp ) 192 .168.4.1 Do \"General setup\" then create the following file to configure wlan0 . We only have the access point. There is no ethernet device configured. cat > /etc/systemd/network/08-wlan0.network <<EOF [Match] Name=wlan0 [Network] Address=192.168.4.1/24 MulticastDNS=yes DHCPServer=yes EOF If you want this then reboot. That's it. Otherwise, go on, no need to reboot this time.","title":"Setting up a stand alone access point"},{"location":"Scanner/specifications/hardware/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"Scanner/specifications/hardware/#serial-access-denied","text":"Look here if you can not communicate with the scanner using usb.","title":"Serial access denied"},{"location":"Scanner/specifications/luigi/","text":"Luigi in the ROMI Plant Scanner project Link Hereafter we explain how we used the luigi Python package in the ROMI Plant Scanner project. If you are not familiar with luigi , let's just say that it is useful to create long-running batch processes where you want to chain many tasks with dependencies and requirements. In the context of the ROMI Plant Scanner project, we faced the challenge of creating complex pipelines, especially for the 3D reconstruction & analysis of a plant structure after its acquisition in the form of a series of RGB images. Several complex and fairly distinct algorithm are required to achieve our goals, and we thus decided to break down this chain of tasks to achieve greater modularity and robustness. Using luigi led us to abstract several concepts like Task , Target & Parameter : a task is limited to a single algorithmic operation with input(s), output(s) & parameter(s); a target is a (set of) file(s) that can be the input required by a task or (one of) its output(s); a parameter is a value controlling the algorithm; Parameters & configuration Link As we run luigi using the command-line tool, and the constructed workflow can be made of many tasks each with several parameters, we use TOML configuration files to define them. In addition to the TOML configuration file, the romi_run_task script requires the definition of two values: the name of the ROMI task to run; the name of the Scan dataset on which to run the ROMI task; As we will see later, each \"computational task\" has an upstream task. Using these tasks dependencies luigi will create the required workflow. Hence you do not have to know or defines the required workflow to run a task, just call it and luigi will do the rest for you! Target subclass Link In order to check for a task requirement(s) or handle its output(s) luigi implement the Target class. As we use our own Python database implementation FSDB from plantdb , and it implements the concept of a set of files as a Fileset class, we subclassed the luigi.Target class as TargetFileset . It is thus used to get/create files from the FSDB database by our luigi.Task subclasses. For example, the raw RGB images set obtained after a Scan task is the 'images' Fileset . Task subclasses Link This is where the computation is done with the run() method and targets are controlled with the requires() & output() methods. Note ROMI tasks do not use a Scan dataset identifier as it is assumed that they only work on one Scan at a time.","title":"Luigi"},{"location":"Scanner/specifications/luigi/#luigi-in-the-romi-plant-scanner-project","text":"Hereafter we explain how we used the luigi Python package in the ROMI Plant Scanner project. If you are not familiar with luigi , let's just say that it is useful to create long-running batch processes where you want to chain many tasks with dependencies and requirements. In the context of the ROMI Plant Scanner project, we faced the challenge of creating complex pipelines, especially for the 3D reconstruction & analysis of a plant structure after its acquisition in the form of a series of RGB images. Several complex and fairly distinct algorithm are required to achieve our goals, and we thus decided to break down this chain of tasks to achieve greater modularity and robustness. Using luigi led us to abstract several concepts like Task , Target & Parameter : a task is limited to a single algorithmic operation with input(s), output(s) & parameter(s); a target is a (set of) file(s) that can be the input required by a task or (one of) its output(s); a parameter is a value controlling the algorithm;","title":"Luigi in the ROMI Plant Scanner project"},{"location":"Scanner/specifications/luigi/#parameters-configuration","text":"As we run luigi using the command-line tool, and the constructed workflow can be made of many tasks each with several parameters, we use TOML configuration files to define them. In addition to the TOML configuration file, the romi_run_task script requires the definition of two values: the name of the ROMI task to run; the name of the Scan dataset on which to run the ROMI task; As we will see later, each \"computational task\" has an upstream task. Using these tasks dependencies luigi will create the required workflow. Hence you do not have to know or defines the required workflow to run a task, just call it and luigi will do the rest for you!","title":"Parameters &amp; configuration"},{"location":"Scanner/specifications/luigi/#target-subclass","text":"In order to check for a task requirement(s) or handle its output(s) luigi implement the Target class. As we use our own Python database implementation FSDB from plantdb , and it implements the concept of a set of files as a Fileset class, we subclassed the luigi.Target class as TargetFileset . It is thus used to get/create files from the FSDB database by our luigi.Task subclasses. For example, the raw RGB images set obtained after a Scan task is the 'images' Fileset .","title":"Target subclass"},{"location":"Scanner/specifications/luigi/#task-subclasses","text":"This is where the computation is done with the run() method and targets are controlled with the requires() & output() methods. Note ROMI tasks do not use a Scan dataset identifier as it is assumed that they only work on one Scan at a time.","title":"Task subclasses"},{"location":"Scanner/specifications/pipelines/","text":"Warning This is a work in progress... the original author has no idea what he is doing! Legend Link Let's start with a description of the used symbols: Note shaped boxes are RomiConfig , they are TOML files that contains parameters for each task. Round shaped boxes are RomiTasks with their name on the first level, then the module names ( --module option in romi_run_task ) and a quick description of the tasks at hand. Folder shaped boxes are RomiTarget , they indicate files input/output and the file extension is given between parenthesis. Acquisitions Link Acquisition of real plant datasets Link Acquisition of virtual plant datasets Link Plant Reconstruction from RGB images Link 3D Plant Phenotyping Link Geometric approach Link Machine Learning approach Link Evaluation Link Mask task evaluation Link Voxel task evaluation Link PointCloud task evaluation Link","title":"Pipelines"},{"location":"Scanner/specifications/pipelines/#legend","text":"Let's start with a description of the used symbols: Note shaped boxes are RomiConfig , they are TOML files that contains parameters for each task. Round shaped boxes are RomiTasks with their name on the first level, then the module names ( --module option in romi_run_task ) and a quick description of the tasks at hand. Folder shaped boxes are RomiTarget , they indicate files input/output and the file extension is given between parenthesis.","title":"Legend"},{"location":"Scanner/specifications/pipelines/#acquisitions","text":"","title":"Acquisitions"},{"location":"Scanner/specifications/pipelines/#acquisition-of-real-plant-datasets","text":"","title":"Acquisition of real plant datasets"},{"location":"Scanner/specifications/pipelines/#acquisition-of-virtual-plant-datasets","text":"","title":"Acquisition of virtual plant datasets"},{"location":"Scanner/specifications/pipelines/#plant-reconstruction-from-rgb-images","text":"","title":"Plant Reconstruction from RGB images"},{"location":"Scanner/specifications/pipelines/#3d-plant-phenotyping","text":"","title":"3D Plant Phenotyping"},{"location":"Scanner/specifications/pipelines/#geometric-approach","text":"","title":"Geometric approach"},{"location":"Scanner/specifications/pipelines/#machine-learning-approach","text":"","title":"Machine Learning approach"},{"location":"Scanner/specifications/pipelines/#evaluation","text":"","title":"Evaluation"},{"location":"Scanner/specifications/pipelines/#mask-task-evaluation","text":"","title":"Mask task evaluation"},{"location":"Scanner/specifications/pipelines/#voxel-task-evaluation","text":"","title":"Voxel task evaluation"},{"location":"Scanner/specifications/pipelines/#pointcloud-task-evaluation","text":"","title":"PointCloud task evaluation"},{"location":"Scanner/specifications/virtual-plant-imager/","text":"Instructions and main specifications for the virtual plant imager Link Warning clearly separate Lpy and the VPI General description Link The Virtual Plant Imager functions as a digital twin of the real romi robot Plant imager : it takes rgb images of one to several virtual 3D plant models and generate all data and metadata necessary to proceed downstream with a training of a neural network or with an analysis pipeline of the plant-3d-vision tool suite. As input , it takes a 3D model ( .obj ) file ; As output , it provides one to several datasets in a romi database format , ready for downstream use (training or analysis). db/ \u251c\u2500\u2500 virtual_imageset/ \u2502 \u251c\u2500\u2500 images/ \u2502 \u251c\u2500\u2500 metadata/ \u2502 \u2502 \u2514\u2500\u2500 images/ \u2502 \u2502 \u2514\u2500\u2500 images.json \u2502 \u2514\u2500\u2500 files.json \u2502 \u2514\u2500\u2500 scan.json \u2514\u2500\u2500 romidb The Virtual Plant Imager relies on Blender v2.81a to generate at set of (2D) RGB images from a plant 3d model. An HTTP server acts as an interface to drive Blender generation scripts. Note The Virtual Plant Imager is closely integrated with the plant generator Lpy . For information related to the generation of virtual 3D model of plants, you will be redirected to other LPy documentation. Input data (for romi_run_task VirtualScan) Link As for all ROMI tools, the Virtual Plant Imager requires a proper database to store, access and generate new data. Let's call virtual_db this database. In particular, it contains data for the virtual plant generation and/or imaging grouped in a so-called vscan_data folder: Legend : (*) the name is fixed and cannot be changed (!) the folder/file must exist (no tag means that the folder is not required for the program to run) virtual_db \u2502 romidb (!*) # a (empty) marker file for recognition by the plantdb module \u2514\u2500\u2500\u2500vscan_data (!*) \u2502 \u2514\u2500\u2500\u2500hdri (*) \u2502 \u2502 hdri_file1.hdr \u2502 \u2502 hdri_file2.hdr \u2502 \u2502 etc... \u2502 \u2514\u2500\u2500\u2500lpy (*) \u2502 \u2502 my_plant_species_model.lpy \u2502 \u2514\u2500\u2500\u2500obj (*) \u2502 \u2502 VirtualPlant.obj \u2502 \u2502 VirtualPlant_mtl \u2502 \u2514\u2500\u2500\u2500metadata (!*) \u2502 \u2502 hdri.json \u2502 \u2502 lpy.json \u2502 \u2502 obj.json \u2502 \u2502 palette.json \u2502 \u2502 scenes.json \u2502 \u2514\u2500\u2500\u2500palette (*) \u2502 \u2502 my_plant_species_model.png \u2502 \u2514\u2500\u2500\u2500scenes (*) \u2502 files.json Following sections detail the content of each files and subfolders. hdri Link If no hdri files are provided, a uniform black background will be applied. Even if they are provided, hdri backgrounds can be deactivated in the .toml file (related romi_run task: VirtualScan ) and the default black background will be applied. New background HDRI files can be downloaded from hdri haven and store in the hdri folder. Limited resolution is enough for downstream applications (we recommend 2K resolution, 6.2 MB total download). ! Note if there are several hdri files -> which one is used ? lpy Link .lpy files contain a parametric model of a plant that Lpy will use to generate a 3D model as an .obj file. With the above data_example, we provide the file arabidopsis_notex.lpy , a Lpy model for the laboratory model plant Arabidopsis thaliana (copyright C. Godin, Inria - RDP Mosaic). You can replace this file with any other Lpy model file. The correct name of the file must then be specified in the configuration .toml file (see below) [VirtualPlant] lpy_file_id = \"arabidopsis_notex\" #base name of the .lpy to be used by Lpy If you do not want to use Lpy to generate your virtual plant, you can also directly import your custom 3D plant (generated elsewhere), which must be at the .obj format. In this case, lpy subfolder is dispensable, store data in the obj subfolder (see next) obj Link Alternatively to Lpy-generated, custom 3D plant model can be provided. Data must consist in a obj and a mtl files. In the .obj , each semantic type of label desired to segment the plant must correspond to a distinct mesh. Each of these meshes must have a single material whose name is the name of the label. (note QUESTION: and then, what are the groundtruth ? how are they provided ?) metadata Link lpy.json { \"task_params\" : { \"output_file_id\" : \"out\" , \"scan_id\" : \"vscan_data\" } } other files (hdri/palette/scenes.json): { \"task_parameters\" : {}, \"task_params\" : { \"output_file_id\" : \"out\" , \"scan_id\" : \"vscan_data\" } } palette Link a .png file containing textures that will LPy will apply on the virtual plant. scenes Link files.json Link fileset descriptor with \"id\" definitions. { \"filesets\" : [ { \"files\" : [ { \"file\" : \"felsenlabyrinth_2k.hdr\" , \"id\" : \"felsenlabyrinth_2k\" }, { \"file\" : \"forest_slope_2k.hdr\" , \"id\" : \"forest_slope_2k\" } ], \"id\" : \"hdri\" }, { \"files\" : [ { \"file\" : \"arabidopsis_notex.lpy\" , \"id\" : \"arabidopsis_notex\" } ], \"id\" : \"lpy\" }, { \"etc...\" } ] } Running the virtual plant imager Link Basic command lines Link Corresponding task with romi_run_task : VirtualScan romi_run_task VirtualScan \\ --config vpi_single_dataset.toml \\ path/to/db/generated_dataset the content of the configuration file vpi_single_dataset.toml will be detailed un the next section ### configuration file for the Task Virtual Scan see also the page on specifications>tasks>VPI (does not exist yet, to be created)","title":"Virtual Plant Imager"},{"location":"Scanner/specifications/virtual-plant-imager/#instructions-and-main-specifications-for-the-virtual-plant-imager","text":"Warning clearly separate Lpy and the VPI","title":"Instructions and main specifications for the virtual plant imager"},{"location":"Scanner/specifications/virtual-plant-imager/#general-description","text":"The Virtual Plant Imager functions as a digital twin of the real romi robot Plant imager : it takes rgb images of one to several virtual 3D plant models and generate all data and metadata necessary to proceed downstream with a training of a neural network or with an analysis pipeline of the plant-3d-vision tool suite. As input , it takes a 3D model ( .obj ) file ; As output , it provides one to several datasets in a romi database format , ready for downstream use (training or analysis). db/ \u251c\u2500\u2500 virtual_imageset/ \u2502 \u251c\u2500\u2500 images/ \u2502 \u251c\u2500\u2500 metadata/ \u2502 \u2502 \u2514\u2500\u2500 images/ \u2502 \u2502 \u2514\u2500\u2500 images.json \u2502 \u2514\u2500\u2500 files.json \u2502 \u2514\u2500\u2500 scan.json \u2514\u2500\u2500 romidb The Virtual Plant Imager relies on Blender v2.81a to generate at set of (2D) RGB images from a plant 3d model. An HTTP server acts as an interface to drive Blender generation scripts. Note The Virtual Plant Imager is closely integrated with the plant generator Lpy . For information related to the generation of virtual 3D model of plants, you will be redirected to other LPy documentation.","title":"General description"},{"location":"Scanner/specifications/virtual-plant-imager/#input-data-for-romi_run_task-virtualscan","text":"As for all ROMI tools, the Virtual Plant Imager requires a proper database to store, access and generate new data. Let's call virtual_db this database. In particular, it contains data for the virtual plant generation and/or imaging grouped in a so-called vscan_data folder: Legend : (*) the name is fixed and cannot be changed (!) the folder/file must exist (no tag means that the folder is not required for the program to run) virtual_db \u2502 romidb (!*) # a (empty) marker file for recognition by the plantdb module \u2514\u2500\u2500\u2500vscan_data (!*) \u2502 \u2514\u2500\u2500\u2500hdri (*) \u2502 \u2502 hdri_file1.hdr \u2502 \u2502 hdri_file2.hdr \u2502 \u2502 etc... \u2502 \u2514\u2500\u2500\u2500lpy (*) \u2502 \u2502 my_plant_species_model.lpy \u2502 \u2514\u2500\u2500\u2500obj (*) \u2502 \u2502 VirtualPlant.obj \u2502 \u2502 VirtualPlant_mtl \u2502 \u2514\u2500\u2500\u2500metadata (!*) \u2502 \u2502 hdri.json \u2502 \u2502 lpy.json \u2502 \u2502 obj.json \u2502 \u2502 palette.json \u2502 \u2502 scenes.json \u2502 \u2514\u2500\u2500\u2500palette (*) \u2502 \u2502 my_plant_species_model.png \u2502 \u2514\u2500\u2500\u2500scenes (*) \u2502 files.json Following sections detail the content of each files and subfolders.","title":"Input data (for romi_run_task VirtualScan)"},{"location":"Scanner/specifications/virtual-plant-imager/#hdri","text":"If no hdri files are provided, a uniform black background will be applied. Even if they are provided, hdri backgrounds can be deactivated in the .toml file (related romi_run task: VirtualScan ) and the default black background will be applied. New background HDRI files can be downloaded from hdri haven and store in the hdri folder. Limited resolution is enough for downstream applications (we recommend 2K resolution, 6.2 MB total download). ! Note if there are several hdri files -> which one is used ?","title":"hdri"},{"location":"Scanner/specifications/virtual-plant-imager/#lpy","text":".lpy files contain a parametric model of a plant that Lpy will use to generate a 3D model as an .obj file. With the above data_example, we provide the file arabidopsis_notex.lpy , a Lpy model for the laboratory model plant Arabidopsis thaliana (copyright C. Godin, Inria - RDP Mosaic). You can replace this file with any other Lpy model file. The correct name of the file must then be specified in the configuration .toml file (see below) [VirtualPlant] lpy_file_id = \"arabidopsis_notex\" #base name of the .lpy to be used by Lpy If you do not want to use Lpy to generate your virtual plant, you can also directly import your custom 3D plant (generated elsewhere), which must be at the .obj format. In this case, lpy subfolder is dispensable, store data in the obj subfolder (see next)","title":"lpy"},{"location":"Scanner/specifications/virtual-plant-imager/#obj","text":"Alternatively to Lpy-generated, custom 3D plant model can be provided. Data must consist in a obj and a mtl files. In the .obj , each semantic type of label desired to segment the plant must correspond to a distinct mesh. Each of these meshes must have a single material whose name is the name of the label. (note QUESTION: and then, what are the groundtruth ? how are they provided ?)","title":"obj"},{"location":"Scanner/specifications/virtual-plant-imager/#metadata","text":"lpy.json { \"task_params\" : { \"output_file_id\" : \"out\" , \"scan_id\" : \"vscan_data\" } } other files (hdri/palette/scenes.json): { \"task_parameters\" : {}, \"task_params\" : { \"output_file_id\" : \"out\" , \"scan_id\" : \"vscan_data\" } }","title":"metadata"},{"location":"Scanner/specifications/virtual-plant-imager/#palette","text":"a .png file containing textures that will LPy will apply on the virtual plant.","title":"palette"},{"location":"Scanner/specifications/virtual-plant-imager/#scenes","text":"","title":"scenes"},{"location":"Scanner/specifications/virtual-plant-imager/#filesjson","text":"fileset descriptor with \"id\" definitions. { \"filesets\" : [ { \"files\" : [ { \"file\" : \"felsenlabyrinth_2k.hdr\" , \"id\" : \"felsenlabyrinth_2k\" }, { \"file\" : \"forest_slope_2k.hdr\" , \"id\" : \"forest_slope_2k\" } ], \"id\" : \"hdri\" }, { \"files\" : [ { \"file\" : \"arabidopsis_notex.lpy\" , \"id\" : \"arabidopsis_notex\" } ], \"id\" : \"lpy\" }, { \"etc...\" } ] }","title":"files.json"},{"location":"Scanner/specifications/virtual-plant-imager/#running-the-virtual-plant-imager","text":"","title":"Running the virtual plant imager"},{"location":"Scanner/specifications/virtual-plant-imager/#basic-command-lines","text":"Corresponding task with romi_run_task : VirtualScan romi_run_task VirtualScan \\ --config vpi_single_dataset.toml \\ path/to/db/generated_dataset the content of the configuration file vpi_single_dataset.toml will be detailed un the next section ### configuration file for the Task Virtual Scan see also the page on specifications>tasks>VPI (does not exist yet, to be created)","title":"Basic command lines"},{"location":"Scanner/specifications/tasks/","text":"Task definitions Link As we have seen previously, we are using the luigi paradigm and have defined a series of tasks to create flexible and modular pipelines. This section is a more general overview, For more details see the reference documentation ( TODO )! Base task class Link RomiTask is the base abstract class for the ROMI Plant Scanner project and subclass luigi.Task . It implements the following methods: requires() , by default an upstream task is required; output() , by default the output of a task is a Fileset with the task's name as an identifier; input_file() , helper method to get the output of the upstream task; output_file() , helper method to create the output of the current task;","title":"Home"},{"location":"Scanner/specifications/tasks/#task-definitions","text":"As we have seen previously, we are using the luigi paradigm and have defined a series of tasks to create flexible and modular pipelines. This section is a more general overview, For more details see the reference documentation ( TODO )!","title":"Task definitions"},{"location":"Scanner/specifications/tasks/#base-task-class","text":"RomiTask is the base abstract class for the ROMI Plant Scanner project and subclass luigi.Task . It implements the following methods: requires() , by default an upstream task is required; output() , by default the output of a task is a Fileset with the task's name as an identifier; input_file() , helper method to get the output of the upstream task; output_file() , helper method to create the output of the current task;","title":"Base task class"},{"location":"Scanner/specifications/tasks/acquisition_tasks/","text":"Acquisition-related tasks Link Scan task Link This task class is used to acquire a set of RGB images from a real plant to study using the Plant imager hardware. It produces a Fileset named 'images' (designated as raw scan) that may go with a metadata dictionary if provided as parameter. VirtualScan task Link This task class is used to acquire a set of RGB images from a mesh computer model (OBJ file) representing a plant using blender. It produces a Fileset named 'images' that may be accompanied by a metadata dictionary if provided as parameter. VirtualPlant task Link This task class is used to generate a mesh computer model (OBJ file) representing a plant using a programmable plant model generator called LPY. It produces ??? ( TODO ) CalibrationScan task Link This task class is used to ??? ( TODO )","title":"Acquisition tasks"},{"location":"Scanner/specifications/tasks/acquisition_tasks/#acquisition-related-tasks","text":"","title":"Acquisition-related tasks"},{"location":"Scanner/specifications/tasks/acquisition_tasks/#scan-task","text":"This task class is used to acquire a set of RGB images from a real plant to study using the Plant imager hardware. It produces a Fileset named 'images' (designated as raw scan) that may go with a metadata dictionary if provided as parameter.","title":"Scan task"},{"location":"Scanner/specifications/tasks/acquisition_tasks/#virtualscan-task","text":"This task class is used to acquire a set of RGB images from a mesh computer model (OBJ file) representing a plant using blender. It produces a Fileset named 'images' that may be accompanied by a metadata dictionary if provided as parameter.","title":"VirtualScan task"},{"location":"Scanner/specifications/tasks/acquisition_tasks/#virtualplant-task","text":"This task class is used to generate a mesh computer model (OBJ file) representing a plant using a programmable plant model generator called LPY. It produces ??? ( TODO )","title":"VirtualPlant task"},{"location":"Scanner/specifications/tasks/acquisition_tasks/#calibrationscan-task","text":"This task class is used to ??? ( TODO )","title":"CalibrationScan task"},{"location":"Scanner/specifications/tasks/evaluation_tasks/","text":"Evaluation-related tasks Link These tasks require the definition of a ground truth . EvaluationTask Link This is the base (abstract) evaluation task. PointCloudSegmentationEvaluation Link PointCloudEvaluation Link Segmentation2DEvaluation Link VoxelsEvaluation Link CylinderRadiusEvaluation Link Thanks to ground truth derived from virtual plants data, most of the reconstruction tasks can be properly evaluated. However, the acquisition made by the plant imager can not be assessed with virtual ground truth and needs some from real world with an adapted comparison method. The 2 main outputs of the plant imager are a set of images and the corresponding positions from the robotic arm. Those data are used later in the reconstruction pipeline, therefore their precision can be evaluated by the precision of the reconstructed object. The evaluation of this acquisition component will be described in this section. Objective Link The goal here is to be able to evaluate the precision of an acquisition by comparing the measurement of a specific trait in an actual object with the same feature in its reconstruction. One basic shape that can often be encountered in plants is the cylinder (stem are often cylindrical, and sometimes so are fruits). It has been therefore chosen as model from which to extract main characteristics (as the height or the radius). In this evaluation we focused on the radius estimation. The several steps are: Make an acquisition of a cylindrical object Generate a point cloud from the collected images Estimate the radius of the reconstructed object and compare it with the \"ground truth\" radius Prerequisite Link install romi plant-imager (from source or using a docker image ) & read install procedure install romi plant-3d-vision (from source or using a docker image ) & read install procedure Create and activate isolated python environment (see the procedure here ) Step-by-step tutorial Link 1. Image a cylindrical object Link In order to have a precise measurement of the object radius, it can be a good idea to use manufactured items with a calibrated size. In our case we chose to evaluate our plant imager with a can. Note Be careful to be able to detect your object from the background in the future reconstruction process. Different segmentation methods are described here and as we use the \"Binary segmentation\" algorithm the can has been painted in a very tasteful green. The procedure to take images is described in the plant imager tutorial but here are the basic steps: DB creation mkdir path/to/db touch path/to/db/romidb Run an acquisition with the Scan task, and a hardware.toml configuration file in the newly created DB romi_run_task Scan /path/to/db/imageset_id/ --config plant-imager/config/hardware.toml The imageset_id fileset is now filled : db/ \u251c\u2500\u2500 imageset_id/ \u2502 \u251c\u2500\u2500 files.json \u2502 \u251c\u2500\u2500 images/ \u2502 \u251c\u2500\u2500 metadata/ \u2502 \u2502 \u2514\u2500\u2500 images/ \u2502 \u2502 \u2514\u2500\u2500 images.json \u2502 \u2514\u2500\u2500 scan.toml \u2514\u2500\u2500 romidb As for the AnglesAndInternodes measurement, in order for the CylinderRadiusEvaluation to retrieve the ground truth radius value, it must be added manually in a imageset_id/measures.json file: { \"radius\" : 7.95 } 2. Point cloud reconstruction Link The next step is to compute a point cloud from the acquired data. As before, the full explanation of the operations concerning the reconstruction pipeline can be found in this tutorial but mainly are: romi_run_task PointCloud /path/to/db/imageset_id/ --config plant-3d-vision/config/pipeline.toml Resulting an equivalent of this tree structure (depending on the used configuration file): db/ \u251c\u2500\u2500 imageset_id/ \u2502 \u251c\u2500\u2500 Colmap__/ \u2502 \u251c\u2500\u2500 files.json \u2502 \u251c\u2500\u2500 images/ \u2502 \u251c\u2500\u2500 Masks__/ \u2502 \u251c\u2500\u2500 measures.json \u2502 \u251c\u2500\u2500 metadata/ \u2502 \u2502 \u2514\u2500\u2500 images/ \u2502 \u2502 \u2514\u2500\u2500 images.json \u2502 \u251c\u2500\u2500 pipeline.toml \u2502 \u251c\u2500\u2500 PointCloud__/ \u2502 \u251c\u2500\u2500 scan.toml \u2502 \u251c\u2500\u2500 Undistorted__/ \u2502 \u2514\u2500\u2500 Voxels__/ \u2514\u2500\u2500 romidb Note Be sure to obtain a proper reconstructed cylinder (as maybe a section of the object) by checking if you are satisfied with the PointCloud_created_fileset/PointCloud.ply point cloud in you favorite 3d software. If not, you can try to modify parameters in the pipeline.toml configuration file as: - the bounding box in the Colmap section - the threshold parameter (or equivalent) of the Masks task - the voxel_size linked to the Voxels task 3. Cylinder Radius Evaluation Link It is now possible to extract the radius from the point cloud using the CylinderRadiusEvaluation . Parameters in the pipeline.toml associated to the task must be defined accordingly: [CylinderRadiusEvaluation] upstream_task = \"PointCloud\" Note By default the upstream_task parameter of CylinderRadiusEvaluation is CylinderRadiusGroundTruth With the following command line: romi_run_task CylinderRadiusEvaluation /path/to/db/imageset_id/ --config plant-3d-vision/config/pipeline.toml The CylinderRadiusEvaluation.json can be found in the CylinderRadiusEvaluation__ fileset with the output results: { \"calculated_radius\" : 8.265318865620745 , \"gt_radius\" : 7.95 , \"err (%)\" : 3.97 } Evaluate the evaluation task Link To see whether the evaluation is actually precise or not, a CylinderRadiusGroundTruth has been developed. It produces a ground truth point cloud cylinder with random dimensions (height and radius) and can be run the same way as any other task: romi_run_task CylinderRadiusGroundTruth /path/to/db/imageset_id/ --config plant-3d-vision/config/pipeline.toml","title":"Evaluation tasks"},{"location":"Scanner/specifications/tasks/evaluation_tasks/#evaluation-related-tasks","text":"These tasks require the definition of a ground truth .","title":"Evaluation-related tasks"},{"location":"Scanner/specifications/tasks/evaluation_tasks/#evaluationtask","text":"This is the base (abstract) evaluation task.","title":"EvaluationTask"},{"location":"Scanner/specifications/tasks/evaluation_tasks/#pointcloudsegmentationevaluation","text":"","title":"PointCloudSegmentationEvaluation"},{"location":"Scanner/specifications/tasks/evaluation_tasks/#pointcloudevaluation","text":"","title":"PointCloudEvaluation"},{"location":"Scanner/specifications/tasks/evaluation_tasks/#segmentation2devaluation","text":"","title":"Segmentation2DEvaluation"},{"location":"Scanner/specifications/tasks/evaluation_tasks/#voxelsevaluation","text":"","title":"VoxelsEvaluation"},{"location":"Scanner/specifications/tasks/evaluation_tasks/#cylinderradiusevaluation","text":"Thanks to ground truth derived from virtual plants data, most of the reconstruction tasks can be properly evaluated. However, the acquisition made by the plant imager can not be assessed with virtual ground truth and needs some from real world with an adapted comparison method. The 2 main outputs of the plant imager are a set of images and the corresponding positions from the robotic arm. Those data are used later in the reconstruction pipeline, therefore their precision can be evaluated by the precision of the reconstructed object. The evaluation of this acquisition component will be described in this section.","title":"CylinderRadiusEvaluation"},{"location":"Scanner/specifications/tasks/evaluation_tasks/#objective","text":"The goal here is to be able to evaluate the precision of an acquisition by comparing the measurement of a specific trait in an actual object with the same feature in its reconstruction. One basic shape that can often be encountered in plants is the cylinder (stem are often cylindrical, and sometimes so are fruits). It has been therefore chosen as model from which to extract main characteristics (as the height or the radius). In this evaluation we focused on the radius estimation. The several steps are: Make an acquisition of a cylindrical object Generate a point cloud from the collected images Estimate the radius of the reconstructed object and compare it with the \"ground truth\" radius","title":"Objective"},{"location":"Scanner/specifications/tasks/evaluation_tasks/#prerequisite","text":"install romi plant-imager (from source or using a docker image ) & read install procedure install romi plant-3d-vision (from source or using a docker image ) & read install procedure Create and activate isolated python environment (see the procedure here )","title":"Prerequisite"},{"location":"Scanner/specifications/tasks/evaluation_tasks/#step-by-step-tutorial","text":"","title":"Step-by-step tutorial"},{"location":"Scanner/specifications/tasks/evaluation_tasks/#1-image-a-cylindrical-object","text":"In order to have a precise measurement of the object radius, it can be a good idea to use manufactured items with a calibrated size. In our case we chose to evaluate our plant imager with a can. Note Be careful to be able to detect your object from the background in the future reconstruction process. Different segmentation methods are described here and as we use the \"Binary segmentation\" algorithm the can has been painted in a very tasteful green. The procedure to take images is described in the plant imager tutorial but here are the basic steps: DB creation mkdir path/to/db touch path/to/db/romidb Run an acquisition with the Scan task, and a hardware.toml configuration file in the newly created DB romi_run_task Scan /path/to/db/imageset_id/ --config plant-imager/config/hardware.toml The imageset_id fileset is now filled : db/ \u251c\u2500\u2500 imageset_id/ \u2502 \u251c\u2500\u2500 files.json \u2502 \u251c\u2500\u2500 images/ \u2502 \u251c\u2500\u2500 metadata/ \u2502 \u2502 \u2514\u2500\u2500 images/ \u2502 \u2502 \u2514\u2500\u2500 images.json \u2502 \u2514\u2500\u2500 scan.toml \u2514\u2500\u2500 romidb As for the AnglesAndInternodes measurement, in order for the CylinderRadiusEvaluation to retrieve the ground truth radius value, it must be added manually in a imageset_id/measures.json file: { \"radius\" : 7.95 }","title":"1. Image a cylindrical object"},{"location":"Scanner/specifications/tasks/evaluation_tasks/#2-point-cloud-reconstruction","text":"The next step is to compute a point cloud from the acquired data. As before, the full explanation of the operations concerning the reconstruction pipeline can be found in this tutorial but mainly are: romi_run_task PointCloud /path/to/db/imageset_id/ --config plant-3d-vision/config/pipeline.toml Resulting an equivalent of this tree structure (depending on the used configuration file): db/ \u251c\u2500\u2500 imageset_id/ \u2502 \u251c\u2500\u2500 Colmap__/ \u2502 \u251c\u2500\u2500 files.json \u2502 \u251c\u2500\u2500 images/ \u2502 \u251c\u2500\u2500 Masks__/ \u2502 \u251c\u2500\u2500 measures.json \u2502 \u251c\u2500\u2500 metadata/ \u2502 \u2502 \u2514\u2500\u2500 images/ \u2502 \u2502 \u2514\u2500\u2500 images.json \u2502 \u251c\u2500\u2500 pipeline.toml \u2502 \u251c\u2500\u2500 PointCloud__/ \u2502 \u251c\u2500\u2500 scan.toml \u2502 \u251c\u2500\u2500 Undistorted__/ \u2502 \u2514\u2500\u2500 Voxels__/ \u2514\u2500\u2500 romidb Note Be sure to obtain a proper reconstructed cylinder (as maybe a section of the object) by checking if you are satisfied with the PointCloud_created_fileset/PointCloud.ply point cloud in you favorite 3d software. If not, you can try to modify parameters in the pipeline.toml configuration file as: - the bounding box in the Colmap section - the threshold parameter (or equivalent) of the Masks task - the voxel_size linked to the Voxels task","title":"2. Point cloud reconstruction"},{"location":"Scanner/specifications/tasks/evaluation_tasks/#3-cylinder-radius-evaluation","text":"It is now possible to extract the radius from the point cloud using the CylinderRadiusEvaluation . Parameters in the pipeline.toml associated to the task must be defined accordingly: [CylinderRadiusEvaluation] upstream_task = \"PointCloud\" Note By default the upstream_task parameter of CylinderRadiusEvaluation is CylinderRadiusGroundTruth With the following command line: romi_run_task CylinderRadiusEvaluation /path/to/db/imageset_id/ --config plant-3d-vision/config/pipeline.toml The CylinderRadiusEvaluation.json can be found in the CylinderRadiusEvaluation__ fileset with the output results: { \"calculated_radius\" : 8.265318865620745 , \"gt_radius\" : 7.95 , \"err (%)\" : 3.97 }","title":"3. Cylinder Radius Evaluation"},{"location":"Scanner/specifications/tasks/evaluation_tasks/#evaluate-the-evaluation-task","text":"To see whether the evaluation is actually precise or not, a CylinderRadiusGroundTruth has been developed. It produces a ground truth point cloud cylinder with random dimensions (height and radius) and can be run the same way as any other task: romi_run_task CylinderRadiusGroundTruth /path/to/db/imageset_id/ --config plant-3d-vision/config/pipeline.toml","title":"Evaluate the evaluation task"},{"location":"Scanner/specifications/tasks/file_tasks/","text":"File-related tasks Link FilesetExists Link This task takes a Fileset identifier as a parameter and makes sure it is found in the Scan instance it is working on. No upstream task definition is required and it returns the Fileset . ImagesFilesetExists Link This is a specific case of the FilesetExists class for 'images' Fileset , i.e. the set of RGB images obtained after the Scan task. ModelFileset Link This is a specific case of the FilesetExists class for 'models' Fileset , i.e. the training file obtained from machine learning. FileByFileTask Link This is an abstract class used to apply a RomiTask on each file of a Fileset . Clean task Link This task class is used to clean a Scan dataset by removing all Fileset s except the 'images' Fileset , i.e. the set of RGB images obtained after the Scan task.","title":"Files tasks"},{"location":"Scanner/specifications/tasks/file_tasks/#file-related-tasks","text":"","title":"File-related tasks"},{"location":"Scanner/specifications/tasks/file_tasks/#filesetexists","text":"This task takes a Fileset identifier as a parameter and makes sure it is found in the Scan instance it is working on. No upstream task definition is required and it returns the Fileset .","title":"FilesetExists"},{"location":"Scanner/specifications/tasks/file_tasks/#imagesfilesetexists","text":"This is a specific case of the FilesetExists class for 'images' Fileset , i.e. the set of RGB images obtained after the Scan task.","title":"ImagesFilesetExists"},{"location":"Scanner/specifications/tasks/file_tasks/#modelfileset","text":"This is a specific case of the FilesetExists class for 'models' Fileset , i.e. the training file obtained from machine learning.","title":"ModelFileset"},{"location":"Scanner/specifications/tasks/file_tasks/#filebyfiletask","text":"This is an abstract class used to apply a RomiTask on each file of a Fileset .","title":"FileByFileTask"},{"location":"Scanner/specifications/tasks/file_tasks/#clean-task","text":"This task class is used to clean a Scan dataset by removing all Fileset s except the 'images' Fileset , i.e. the set of RGB images obtained after the Scan task.","title":"Clean task"},{"location":"Scanner/specifications/tasks/ground_truth_tasks/","text":"Defining ground truth Link To performs evaluation tasks, first you have to defines a ground truth to compare to. This is the aim of the following task classes. VoxelGroundTruth Link PointCloudGroundTruth Link ClusteredMeshGroundTruth Link","title":"Defining ground truth"},{"location":"Scanner/specifications/tasks/ground_truth_tasks/#defining-ground-truth","text":"To performs evaluation tasks, first you have to defines a ground truth to compare to. This is the aim of the following task classes.","title":"Defining ground truth"},{"location":"Scanner/specifications/tasks/ground_truth_tasks/#voxelgroundtruth","text":"","title":"VoxelGroundTruth"},{"location":"Scanner/specifications/tasks/ground_truth_tasks/#pointcloudgroundtruth","text":"","title":"PointCloudGroundTruth"},{"location":"Scanner/specifications/tasks/ground_truth_tasks/#clusteredmeshgroundtruth","text":"","title":"ClusteredMeshGroundTruth"},{"location":"Scanner/specifications/tasks/reconstruction_tasks/","text":"Reconstruction related tasks Link Undistorted Link This task class is used to \"undistort\" images obtained by a camera that may not have a perfect lens. It produces a Fileset of rgb images saved under the task id. Colmap Link This task class is used to estimate camera poses from a set of RGB images. By default, it is downstream ImagesFilesetExists (raw scan) An alternative upstream task choice could be the Undistorted . It produces ??? ( TODO ) Clarification required! don't we use the positions (x, y, z, pan) from the CNC & gimball ? why do we compute the sparse reconstruction (point-cloud) ? Masks Link This task class is used to create a binary mask of each (real or virtual) plant RGB image . By default, it is downstream the Undistorted task. An alternative upstream task choice could be the Undistorted . The following methods are available to compute masks: linear excess_green vesselness invert Clarification required! document mask algorithms! It produces a Fileset of binary images saved under the task id. Voxels Link This task class is used to compute a volume (ref?) from back-projection of the binary (Masks task) or labelled (Segmentation2D task) masks. By default, it is downstream the Masks & Colmap tasks. The following methods are available to compute back-projection: carving averaging Clarification required! what is a volume ?! the difference with \"point-cloud\" is not too clear... * document back-projection algorithms! It produces a 3D array saved as NPZ (compressed numpy array). This array may be binary if downstream of the Masks task, or a labelled array if downstream the Segmentation2D task. PointCloud Link This task class is used to transform the binary volumetric data, from the Voxels tasks into an open3d 3D point-cloud. By default, it is downstream the Voxels task. It uses an Exact Euclidean distance transform method (ref?). It produces a PLY file. TriangleMesh Link This task class is used to transform a 3D point-cloud into an open3d 3D triangulated mesh. By default, it is downstream the PointCloud task. It uses the poisson_mesh method from the CGAL library described here . It produces a PLY file. CurveSkeleton Link This task class is used to compute a skeleton (ref?) from a 3D triangulated mesh. By default, it is downstream the PointCloud task. It uses the skeletonize_mesh method from the CGAL library described here . It produces a PLY file. TreeGraph Link This task class is used to generate a tree graph structure (ref?) from a skeleton . By default, it is downstream the CurveSkeleton task. It uses networkx Python package to compute a minimum spanning tree with ??? ( TODO ) It produces a JSON file ??? ( TODO ) AnglesAndInternodes Link This task class is used to compute angles and internodes between successive organs along the main stem. By default, it is downstream the TreeGraph task. It produces a JSON file ??? ( TODO ) Segmentation2D Link This task class is used to SegmentedPointCloud Link This task class is used to transform the multiclass volumetric data, from the Segmentation2D tasks into an open3d labelled 3D point-cloud. By default, it is downstream the Segmentation2D task. It produces a PLY file. ClusteredMesh Link This task class is used to transform a labelled 3D point-cloud into an open3d 3D triangulated mesh. By default, it is downstream the SegmentedPointCloud task. It produces a PLY file.","title":"Reconstruction tasks"},{"location":"Scanner/specifications/tasks/reconstruction_tasks/#reconstruction-related-tasks","text":"","title":"Reconstruction related tasks"},{"location":"Scanner/specifications/tasks/reconstruction_tasks/#undistorted","text":"This task class is used to \"undistort\" images obtained by a camera that may not have a perfect lens. It produces a Fileset of rgb images saved under the task id.","title":"Undistorted"},{"location":"Scanner/specifications/tasks/reconstruction_tasks/#colmap","text":"This task class is used to estimate camera poses from a set of RGB images. By default, it is downstream ImagesFilesetExists (raw scan) An alternative upstream task choice could be the Undistorted . It produces ??? ( TODO ) Clarification required! don't we use the positions (x, y, z, pan) from the CNC & gimball ? why do we compute the sparse reconstruction (point-cloud) ?","title":"Colmap"},{"location":"Scanner/specifications/tasks/reconstruction_tasks/#masks","text":"This task class is used to create a binary mask of each (real or virtual) plant RGB image . By default, it is downstream the Undistorted task. An alternative upstream task choice could be the Undistorted . The following methods are available to compute masks: linear excess_green vesselness invert Clarification required! document mask algorithms! It produces a Fileset of binary images saved under the task id.","title":"Masks"},{"location":"Scanner/specifications/tasks/reconstruction_tasks/#voxels","text":"This task class is used to compute a volume (ref?) from back-projection of the binary (Masks task) or labelled (Segmentation2D task) masks. By default, it is downstream the Masks & Colmap tasks. The following methods are available to compute back-projection: carving averaging Clarification required! what is a volume ?! the difference with \"point-cloud\" is not too clear... * document back-projection algorithms! It produces a 3D array saved as NPZ (compressed numpy array). This array may be binary if downstream of the Masks task, or a labelled array if downstream the Segmentation2D task.","title":"Voxels"},{"location":"Scanner/specifications/tasks/reconstruction_tasks/#pointcloud","text":"This task class is used to transform the binary volumetric data, from the Voxels tasks into an open3d 3D point-cloud. By default, it is downstream the Voxels task. It uses an Exact Euclidean distance transform method (ref?). It produces a PLY file.","title":"PointCloud"},{"location":"Scanner/specifications/tasks/reconstruction_tasks/#trianglemesh","text":"This task class is used to transform a 3D point-cloud into an open3d 3D triangulated mesh. By default, it is downstream the PointCloud task. It uses the poisson_mesh method from the CGAL library described here . It produces a PLY file.","title":"TriangleMesh"},{"location":"Scanner/specifications/tasks/reconstruction_tasks/#curveskeleton","text":"This task class is used to compute a skeleton (ref?) from a 3D triangulated mesh. By default, it is downstream the PointCloud task. It uses the skeletonize_mesh method from the CGAL library described here . It produces a PLY file.","title":"CurveSkeleton"},{"location":"Scanner/specifications/tasks/reconstruction_tasks/#treegraph","text":"This task class is used to generate a tree graph structure (ref?) from a skeleton . By default, it is downstream the CurveSkeleton task. It uses networkx Python package to compute a minimum spanning tree with ??? ( TODO ) It produces a JSON file ??? ( TODO )","title":"TreeGraph"},{"location":"Scanner/specifications/tasks/reconstruction_tasks/#anglesandinternodes","text":"This task class is used to compute angles and internodes between successive organs along the main stem. By default, it is downstream the TreeGraph task. It produces a JSON file ??? ( TODO )","title":"AnglesAndInternodes"},{"location":"Scanner/specifications/tasks/reconstruction_tasks/#segmentation2d","text":"This task class is used to","title":"Segmentation2D"},{"location":"Scanner/specifications/tasks/reconstruction_tasks/#segmentedpointcloud","text":"This task class is used to transform the multiclass volumetric data, from the Segmentation2D tasks into an open3d labelled 3D point-cloud. By default, it is downstream the Segmentation2D task. It produces a PLY file.","title":"SegmentedPointCloud"},{"location":"Scanner/specifications/tasks/reconstruction_tasks/#clusteredmesh","text":"This task class is used to transform a labelled 3D point-cloud into an open3d 3D triangulated mesh. By default, it is downstream the SegmentedPointCloud task. It produces a PLY file.","title":"ClusteredMesh"},{"location":"Scanner/tutorials/","text":"","title":"Home"},{"location":"Scanner/tutorials/basics/","text":"How to use the ROMI scanner software? Link We here assume you have followed the \"installation instructions\" available here . Getting started Link There are some requirements to use the different algorithms in the pipeline. Most of them are installed automatically from the requirements file when using pip. The most important part is Colmap (v3.6). The two requirements that are not shipped with pip are: Colmap (v3.6) for the structure from motion algorithms Blender (>= 2.81) for the virtual scanner Preferably, create a virtual environment for python 3.7 or python 3.8 using virtualenv or a conda environment specific to the 3D Scanner. Warning If using python 3.8, Open3D binaries are not yet available on pip, therefore you have to build Open3D from sources! Basic usage Link Every task on the scanner is launched through the romi_run_task command provided in the plant3dvision module. It is a wrapper for luigi , with preloaded tasks from the plant3dvision module. The general usage is as follows: romi_run_task [ -h ] [ --config CONFIG ] [ --luigicmd LUIGICMD ] [ --module MODULE ] [ --local-scheduler ] [ --log-level LOG_LEVEL ] task scan CONFIG is either a file or a folder. If a file, it must be json or toml and contains the configuration of the task to run. If a folder, it will read all configuration files in json or toml format from the folder. LUIGICMD is an optional parameter specifying an alternative command for luigi . MODULE is an optional parameter for running task from external modules (see TODO). LOG_LEVEL is the level of logging. Defaults to INFO , but can be set to DEBUG to increase verbosity. task is the name of the class to run (see TODO) scan is the location of the target scan on which to process the task. It is of the form DB_LOCATION/SCAN_ID , where DB_LOCATION is a path containing the plantdb marker. Configuration files Link The configuration is in the form of a dictionary, in which each key is the ID of a given task. In toml format, it reads as follows: [FirstTask] parameter1 = value1 parameter2 = value2 [SecondTask] parameter1 = value1 parameter2 = value2 Pipelines Link This is a sample configuration for the full reconstruction pipeline : [Colmap] matcher = \"exhaustive\" compute_dense = false [Colmap.cli_args.feature_extractor] \"--ImageReader.single_camera\" = \"1\" \"--SiftExtraction.use_gpu\" = \"1\" [Colmap.cli_args.exhaustive_matcher] \"--SiftMatching.use_gpu\" = \"1\" [Colmap.cli_args.model_aligner] \"--robust_alignment_max_error\" = \"10\" [Masks] type = \"excess_green\" dilation = 5 binarize = true threshold = 0.0 [Voxels] voxel_size = 1.0 type = \"carving\" [PointCloud] level_set_value = 1.0 [Visualization] max_image_size = 1500 max_pcd_size = 10000 thumbnail_size = 150 pcd_source = \"vox2pcd\" mesh_source = \"delaunay\" To run the full reconstruction pipeline use this configuration file with romi_run_task : romi_run_task --config scanner.json AnglesAndInternodes /path/to/db/scan_id/ --local-scheduler This will process all tasks up to the AnglesAndInternodes task. Every task produces a Fileset , a subdirectory in the scan directory whose name starts the same as the task name. The characters following are a hash of the configuration of the task, so that the outputs of the same task with different parameters can coexist in the same scan. Any change in the parameters will make the needed task to be recomputed with subsequent calls of romi_run_task . Already computed tasks will be left untouched. To recompute a task, just delete the corresponding folder in the scan directory and rerun romi_run_task . Default task reference Link default_modules = { # Scanning modules: \"Scan\" : \"plantimager.tasks.scan\" , \"VirtualPlant\" : \"plantimager.tasks.lpy\" , \"VirtualScan\" : \"plantimager.tasks.scan\" , \"CalibrationScan\" : \"plantimager.tasks.scan\" , # Geometric reconstruction modules: \"Colmap\" : \"plant3dvision.tasks.colmap\" , \"Undistorted\" : \"plant3dvision.tasks.proc2d\" , \"Masks\" : \"plant3dvision.tasks.proc2d\" , \"Voxels\" : \"plant3dvision.tasks.cl\" , \"PointCloud\" : \"plant3dvision.tasks.proc3d\" , \"TriangleMesh\" : \"plant3dvision.tasks.proc3d\" , \"CurveSkeleton\" : \"plant3dvision.tasks.proc3d\" , # Machine learning reconstruction modules: \"Segmentation2D\" : \"plant3dvision.tasks.proc2d\" , \"SegmentedPointCloud\" : \"plant3dvision.tasks.proc3d\" , \"ClusteredMesh\" : \"plant3dvision.tasks.proc3d\" , \"OrganSegmentation\" : \"plant3dvision.tasks.proc3d\" , # Quantification modules: \"TreeGraph\" : \"plant3dvision.tasks.arabidopsis\" , \"AnglesAndInternodes\" : \"plant3dvision.tasks.arabidopsis\" , # Evaluation modules: \"VoxelsGroundTruth\" : \"plant3dvision.tasks.evaluation\" , \"VoxelsEvaluation\" : \"plant3dvision.tasks.evaluation\" , \"PointCloudGroundTruth\" : \"plant3dvision.tasks.evaluation\" , \"PointCloudEvaluation\" : \"plant3dvision.tasks.evaluation\" , \"ClusteredMeshGroundTruth\" : \"plant3dvision.tasks.evaluation\" , \"PointCloudSegmentationEvaluation\" : \"plant3dvision.tasks.evaluation\" , \"Segmentation2DEvaluation\" : \"plant3dvision.tasks.evaluation\" , \"AnglesAndInternodesEvaluation\" : \"plant3dvision.tasks.evaluation\" , # Visu modules: \"Visualization\" : \"plant3dvision.tasks.visualization\" , # Database modules: \"Clean\" : \"romitask.task\" } Warning This is for reference only, please update the changes in the code. This will be later replaced by a reference doc generated from the code! Class name: Scan Module: plantimager.tasks.scan Description: A task for running a scan, real or virtual. Default upstream tasks: None Parameters: - metadata (DictParameter) : metadata for the scan - scanner (DictParameter) : scanner hardware configuration (TODO: see hardware documentation) - path (DictParameter) : scanner path configuration (TODO: see hardware documentation) Class name: CalibrationScan Module: plantimager.tasks.scan Description: A task for running a scan, real or virtual, with a calibration path. It is used to calibrate Colmap poses for subsequent scans. (TODO: see calibration documentation) Default upstream tasks: None Parameters: - metadata (DictParameter) : metadata for the scan - scanner (DictParameter) : scanner hardware configuration (TODO: see hardware documentation) - path (DictParameter) : scanner path configuration (TODO: see hardware documentation) - n_line : number of shots taken on the orthogonal calibration lines Class name: Clean Module: plantimager.tasks.scan Description: Cleanup a scan, keeping only the \"images\" fileset and removing all computed pipelines. Default upstream tasks: None Parameters: - no_confirm (BoolParameter, default=False) : do not ask for confirmation in the command prompt. Class name: Colmap Module: plant3dvision.tasks.colmap Description: Runs colmap on a given scan. Default upstream tasks: Scan Upstream task format: Fileset with image files Output fileset format: images.json, cameras.json, points3D.json, sparse.ply [, dense.ply] Parameters: - matcher (Parameter, default=\"exhaustive\") : either \"exhaustive\" or \"sequential\" (TODO: see Colmap documentation) - compute_dense (BoolParameter) : whether to run the dense Colmap to obtain a dense point cloud - cli_args (DictParameter) : parameters for Colmap command line prompts (TODO: see Colmap documentation) - align_pcd (BoolParameter, default=True) : align point cloud on calibrated or metadata poses ? - calibration_scan_id (Parameter, default=\"\") : ID of the calibration scan. Class name: Undistorted Module: plant3dvision.tasks.proc2d Description: Undistorts images using computed intrinsic camera parameters Default upstream tasks: Scan, Colmap Upstream task format: Fileset with image files Output fileset format: Fileset with image files Class name: Masks Module: plant3dvision.tasks.proc2d Description: compute masks using several functions Default upstream tasks: Undistorted Upstream task format: Fileset with image files Output fileset format: Fileset with grayscale or binary image files Parameters: - type (Parameter) : \"linear\", \"excess_green\", \"vesselness\", \"invert\" (TODO: see segmentation documentation) - parameters (ListParameter) : list of scalar parameters, depends on type - dilation (IntParameter) : by how much to dilate masks if binary - binarize (BoolParameter, default=True) : binarize the masks - threshold (FloatParameter, default=0.0) : threshold for binarization - Class name: Segmentation2D Module: plant3dvision.tasks.proc2d Description: compute masks using trained deep learning models Default upstream tasks: Undistorted Upstream task format: Fileset with image files Output fileset format: Fileset with grayscale image files, each corresponding to a given input image and class Parameters: - query (DictParameter) : query to pass to upstream fileset. It filters file by metadata, e.g {\"channel\": \"rgb\"} will process only input files such that \"channel\" metadata is equal to \"rgb\". - labels (Parameter) : string of the form \"a,b,c\" such that a, b, c are the identifiers of the labels produced by the neural network - Sx, Sy (IntParameter) : size of the input of the neural network. Input pictures are cropped in the center to this size. - model_segmentation_name : name of \".pt\" file that can be found at `https://db.romi-project.eu/models` - Class name: Voxels Module: plant3dvision.tasks.cl Description: Computes a volume from back-projection of 2D segmented images Default upstream tasks: - upstream_mask: Masks - upstream_colmap: Colmap Upstream task format: - upstream_mask: Fileset with grayscale images - upstream_colmap: Output of Colmap task Output fileset format: npz file with as many arrays as classes Parameters: - use_colmap_poses (BoolParameter, default=True): Either use precomputed camera poses or output from the Colmap task - voxel_size (FloatParameter): size of one side of voxels - type (Parameter): \"carving\" or \"averaging\" (TODO: See 3D documentation) - multiclass (BoolParameter, default=False): whether input data is single class or multiclass (e.g as an output of Segmentation2D) - log (BoolParameter, default=True), in the case of \"averaging\" type, whether to apply log when averaging values. Class name: PointCloud Module: plant3dvision.tasks.scan Description: Computes a point cloud from volumetric voxel data (either single or multiclass) Default upstream tasks: Voxels Upstream task format: npz file with as many 3D array as classes Output task format: single point cloud in ply. Metadata may include label name if multiclass. Class name: TriangleMesh Module: plant3dvision.tasks.scan Description: Triangulates input point cloud. Currently ignores class data and needs only one connected component. Default upstream tasks: PointCloud Upstream task format: ply file Output task format: ply triangle mesh file Class name: CurveSkeleton Module: plant3dvision.tasks.scan Description: Creates a 3D curve skeleton Default upstream tasks: TriangleMesh Upstream task format: ply triangle mesh Output task format: json with two entries \"points\" and \"lines\" (TODO: precise) Class name: TreeGraph Module: plant3dvision.tasks.arabidopsis Description: Creates a tree graph of the plant Default upstream tasks: CurveSkeleton Upstream task format: json Output task format: json (TODO: precise) Class name; AnglesAndInternodes Module: plant3dvision.tasks.arabidopsis Description: Computes angles and internode Default upstream tasks: TreeGraph Upstream task format: json Output task format: json (TODO: precise) Scanner API reference Link Objects Link /objects (GET): retrieve the list of obj files in the data folder that can be loaded. /load_object/<object_id> (GET) load the given object in the scene. Takes a translation vector as URL parameters ( dx , dy , dz ) Classes Link /classes (GET): retrieve the list of classes. Backgrounds Link /backgrounds (GET): retrieve the list of hdr files in the hdri folder that can be loaded. /load_background/<background_id> (GET) load the given background in the scene. Camera Link /camera_intrinsics (POST): set camera intrinsics. Keys: width , height , focal /camera_pose (POST): set camera pose. Keys: tx , ty , tz , rx , ry , rz Rendering Link /render (GET): gets the rendering of the scene /render_class/<class_id> (GET) renders the scene, with everything transparent except the given class TODO: missing endpoints httpie # Setup camera http -f post http://localhost:5000/camera_intrinsics width=1920 height=1080 focal=35 # Load arabidopsis_0 http get 'http://localhost:5000/load_object/arabidopsis_0.obj?dx=10&dy=20&dz=1' # Load \"old tree in the park\" background http get http://127.0.0.1:5000/load_background/old_tree_in_city_park_8k.hdr # Move camera http -f post http://localhost:5000/camera_pose tx=-60 ty=0 tz=50 rx=60 ry=0 rz=-90 # Render scene and download image http --download get http://localhost:5000/render # Render only leaves http --download get http://localhost:5000/render_class/Color_7","title":"Basics"},{"location":"Scanner/tutorials/basics/#how-to-use-the-romi-scanner-software","text":"We here assume you have followed the \"installation instructions\" available here .","title":"How to use the ROMI scanner software?"},{"location":"Scanner/tutorials/basics/#getting-started","text":"There are some requirements to use the different algorithms in the pipeline. Most of them are installed automatically from the requirements file when using pip. The most important part is Colmap (v3.6). The two requirements that are not shipped with pip are: Colmap (v3.6) for the structure from motion algorithms Blender (>= 2.81) for the virtual scanner Preferably, create a virtual environment for python 3.7 or python 3.8 using virtualenv or a conda environment specific to the 3D Scanner. Warning If using python 3.8, Open3D binaries are not yet available on pip, therefore you have to build Open3D from sources!","title":"Getting started"},{"location":"Scanner/tutorials/basics/#basic-usage","text":"Every task on the scanner is launched through the romi_run_task command provided in the plant3dvision module. It is a wrapper for luigi , with preloaded tasks from the plant3dvision module. The general usage is as follows: romi_run_task [ -h ] [ --config CONFIG ] [ --luigicmd LUIGICMD ] [ --module MODULE ] [ --local-scheduler ] [ --log-level LOG_LEVEL ] task scan CONFIG is either a file or a folder. If a file, it must be json or toml and contains the configuration of the task to run. If a folder, it will read all configuration files in json or toml format from the folder. LUIGICMD is an optional parameter specifying an alternative command for luigi . MODULE is an optional parameter for running task from external modules (see TODO). LOG_LEVEL is the level of logging. Defaults to INFO , but can be set to DEBUG to increase verbosity. task is the name of the class to run (see TODO) scan is the location of the target scan on which to process the task. It is of the form DB_LOCATION/SCAN_ID , where DB_LOCATION is a path containing the plantdb marker.","title":"Basic usage"},{"location":"Scanner/tutorials/basics/#configuration-files","text":"The configuration is in the form of a dictionary, in which each key is the ID of a given task. In toml format, it reads as follows: [FirstTask] parameter1 = value1 parameter2 = value2 [SecondTask] parameter1 = value1 parameter2 = value2","title":"Configuration files"},{"location":"Scanner/tutorials/basics/#pipelines","text":"This is a sample configuration for the full reconstruction pipeline : [Colmap] matcher = \"exhaustive\" compute_dense = false [Colmap.cli_args.feature_extractor] \"--ImageReader.single_camera\" = \"1\" \"--SiftExtraction.use_gpu\" = \"1\" [Colmap.cli_args.exhaustive_matcher] \"--SiftMatching.use_gpu\" = \"1\" [Colmap.cli_args.model_aligner] \"--robust_alignment_max_error\" = \"10\" [Masks] type = \"excess_green\" dilation = 5 binarize = true threshold = 0.0 [Voxels] voxel_size = 1.0 type = \"carving\" [PointCloud] level_set_value = 1.0 [Visualization] max_image_size = 1500 max_pcd_size = 10000 thumbnail_size = 150 pcd_source = \"vox2pcd\" mesh_source = \"delaunay\" To run the full reconstruction pipeline use this configuration file with romi_run_task : romi_run_task --config scanner.json AnglesAndInternodes /path/to/db/scan_id/ --local-scheduler This will process all tasks up to the AnglesAndInternodes task. Every task produces a Fileset , a subdirectory in the scan directory whose name starts the same as the task name. The characters following are a hash of the configuration of the task, so that the outputs of the same task with different parameters can coexist in the same scan. Any change in the parameters will make the needed task to be recomputed with subsequent calls of romi_run_task . Already computed tasks will be left untouched. To recompute a task, just delete the corresponding folder in the scan directory and rerun romi_run_task .","title":"Pipelines"},{"location":"Scanner/tutorials/basics/#default-task-reference","text":"default_modules = { # Scanning modules: \"Scan\" : \"plantimager.tasks.scan\" , \"VirtualPlant\" : \"plantimager.tasks.lpy\" , \"VirtualScan\" : \"plantimager.tasks.scan\" , \"CalibrationScan\" : \"plantimager.tasks.scan\" , # Geometric reconstruction modules: \"Colmap\" : \"plant3dvision.tasks.colmap\" , \"Undistorted\" : \"plant3dvision.tasks.proc2d\" , \"Masks\" : \"plant3dvision.tasks.proc2d\" , \"Voxels\" : \"plant3dvision.tasks.cl\" , \"PointCloud\" : \"plant3dvision.tasks.proc3d\" , \"TriangleMesh\" : \"plant3dvision.tasks.proc3d\" , \"CurveSkeleton\" : \"plant3dvision.tasks.proc3d\" , # Machine learning reconstruction modules: \"Segmentation2D\" : \"plant3dvision.tasks.proc2d\" , \"SegmentedPointCloud\" : \"plant3dvision.tasks.proc3d\" , \"ClusteredMesh\" : \"plant3dvision.tasks.proc3d\" , \"OrganSegmentation\" : \"plant3dvision.tasks.proc3d\" , # Quantification modules: \"TreeGraph\" : \"plant3dvision.tasks.arabidopsis\" , \"AnglesAndInternodes\" : \"plant3dvision.tasks.arabidopsis\" , # Evaluation modules: \"VoxelsGroundTruth\" : \"plant3dvision.tasks.evaluation\" , \"VoxelsEvaluation\" : \"plant3dvision.tasks.evaluation\" , \"PointCloudGroundTruth\" : \"plant3dvision.tasks.evaluation\" , \"PointCloudEvaluation\" : \"plant3dvision.tasks.evaluation\" , \"ClusteredMeshGroundTruth\" : \"plant3dvision.tasks.evaluation\" , \"PointCloudSegmentationEvaluation\" : \"plant3dvision.tasks.evaluation\" , \"Segmentation2DEvaluation\" : \"plant3dvision.tasks.evaluation\" , \"AnglesAndInternodesEvaluation\" : \"plant3dvision.tasks.evaluation\" , # Visu modules: \"Visualization\" : \"plant3dvision.tasks.visualization\" , # Database modules: \"Clean\" : \"romitask.task\" } Warning This is for reference only, please update the changes in the code. This will be later replaced by a reference doc generated from the code! Class name: Scan Module: plantimager.tasks.scan Description: A task for running a scan, real or virtual. Default upstream tasks: None Parameters: - metadata (DictParameter) : metadata for the scan - scanner (DictParameter) : scanner hardware configuration (TODO: see hardware documentation) - path (DictParameter) : scanner path configuration (TODO: see hardware documentation) Class name: CalibrationScan Module: plantimager.tasks.scan Description: A task for running a scan, real or virtual, with a calibration path. It is used to calibrate Colmap poses for subsequent scans. (TODO: see calibration documentation) Default upstream tasks: None Parameters: - metadata (DictParameter) : metadata for the scan - scanner (DictParameter) : scanner hardware configuration (TODO: see hardware documentation) - path (DictParameter) : scanner path configuration (TODO: see hardware documentation) - n_line : number of shots taken on the orthogonal calibration lines Class name: Clean Module: plantimager.tasks.scan Description: Cleanup a scan, keeping only the \"images\" fileset and removing all computed pipelines. Default upstream tasks: None Parameters: - no_confirm (BoolParameter, default=False) : do not ask for confirmation in the command prompt. Class name: Colmap Module: plant3dvision.tasks.colmap Description: Runs colmap on a given scan. Default upstream tasks: Scan Upstream task format: Fileset with image files Output fileset format: images.json, cameras.json, points3D.json, sparse.ply [, dense.ply] Parameters: - matcher (Parameter, default=\"exhaustive\") : either \"exhaustive\" or \"sequential\" (TODO: see Colmap documentation) - compute_dense (BoolParameter) : whether to run the dense Colmap to obtain a dense point cloud - cli_args (DictParameter) : parameters for Colmap command line prompts (TODO: see Colmap documentation) - align_pcd (BoolParameter, default=True) : align point cloud on calibrated or metadata poses ? - calibration_scan_id (Parameter, default=\"\") : ID of the calibration scan. Class name: Undistorted Module: plant3dvision.tasks.proc2d Description: Undistorts images using computed intrinsic camera parameters Default upstream tasks: Scan, Colmap Upstream task format: Fileset with image files Output fileset format: Fileset with image files Class name: Masks Module: plant3dvision.tasks.proc2d Description: compute masks using several functions Default upstream tasks: Undistorted Upstream task format: Fileset with image files Output fileset format: Fileset with grayscale or binary image files Parameters: - type (Parameter) : \"linear\", \"excess_green\", \"vesselness\", \"invert\" (TODO: see segmentation documentation) - parameters (ListParameter) : list of scalar parameters, depends on type - dilation (IntParameter) : by how much to dilate masks if binary - binarize (BoolParameter, default=True) : binarize the masks - threshold (FloatParameter, default=0.0) : threshold for binarization - Class name: Segmentation2D Module: plant3dvision.tasks.proc2d Description: compute masks using trained deep learning models Default upstream tasks: Undistorted Upstream task format: Fileset with image files Output fileset format: Fileset with grayscale image files, each corresponding to a given input image and class Parameters: - query (DictParameter) : query to pass to upstream fileset. It filters file by metadata, e.g {\"channel\": \"rgb\"} will process only input files such that \"channel\" metadata is equal to \"rgb\". - labels (Parameter) : string of the form \"a,b,c\" such that a, b, c are the identifiers of the labels produced by the neural network - Sx, Sy (IntParameter) : size of the input of the neural network. Input pictures are cropped in the center to this size. - model_segmentation_name : name of \".pt\" file that can be found at `https://db.romi-project.eu/models` - Class name: Voxels Module: plant3dvision.tasks.cl Description: Computes a volume from back-projection of 2D segmented images Default upstream tasks: - upstream_mask: Masks - upstream_colmap: Colmap Upstream task format: - upstream_mask: Fileset with grayscale images - upstream_colmap: Output of Colmap task Output fileset format: npz file with as many arrays as classes Parameters: - use_colmap_poses (BoolParameter, default=True): Either use precomputed camera poses or output from the Colmap task - voxel_size (FloatParameter): size of one side of voxels - type (Parameter): \"carving\" or \"averaging\" (TODO: See 3D documentation) - multiclass (BoolParameter, default=False): whether input data is single class or multiclass (e.g as an output of Segmentation2D) - log (BoolParameter, default=True), in the case of \"averaging\" type, whether to apply log when averaging values. Class name: PointCloud Module: plant3dvision.tasks.scan Description: Computes a point cloud from volumetric voxel data (either single or multiclass) Default upstream tasks: Voxels Upstream task format: npz file with as many 3D array as classes Output task format: single point cloud in ply. Metadata may include label name if multiclass. Class name: TriangleMesh Module: plant3dvision.tasks.scan Description: Triangulates input point cloud. Currently ignores class data and needs only one connected component. Default upstream tasks: PointCloud Upstream task format: ply file Output task format: ply triangle mesh file Class name: CurveSkeleton Module: plant3dvision.tasks.scan Description: Creates a 3D curve skeleton Default upstream tasks: TriangleMesh Upstream task format: ply triangle mesh Output task format: json with two entries \"points\" and \"lines\" (TODO: precise) Class name: TreeGraph Module: plant3dvision.tasks.arabidopsis Description: Creates a tree graph of the plant Default upstream tasks: CurveSkeleton Upstream task format: json Output task format: json (TODO: precise) Class name; AnglesAndInternodes Module: plant3dvision.tasks.arabidopsis Description: Computes angles and internode Default upstream tasks: TreeGraph Upstream task format: json Output task format: json (TODO: precise)","title":"Default task reference"},{"location":"Scanner/tutorials/basics/#scanner-api-reference","text":"","title":"Scanner API reference"},{"location":"Scanner/tutorials/basics/#objects","text":"/objects (GET): retrieve the list of obj files in the data folder that can be loaded. /load_object/<object_id> (GET) load the given object in the scene. Takes a translation vector as URL parameters ( dx , dy , dz )","title":"Objects"},{"location":"Scanner/tutorials/basics/#classes","text":"/classes (GET): retrieve the list of classes.","title":"Classes"},{"location":"Scanner/tutorials/basics/#backgrounds","text":"/backgrounds (GET): retrieve the list of hdr files in the hdri folder that can be loaded. /load_background/<background_id> (GET) load the given background in the scene.","title":"Backgrounds"},{"location":"Scanner/tutorials/basics/#camera","text":"/camera_intrinsics (POST): set camera intrinsics. Keys: width , height , focal /camera_pose (POST): set camera pose. Keys: tx , ty , tz , rx , ry , rz","title":"Camera"},{"location":"Scanner/tutorials/basics/#rendering","text":"/render (GET): gets the rendering of the scene /render_class/<class_id> (GET) renders the scene, with everything transparent except the given class TODO: missing endpoints httpie # Setup camera http -f post http://localhost:5000/camera_intrinsics width=1920 height=1080 focal=35 # Load arabidopsis_0 http get 'http://localhost:5000/load_object/arabidopsis_0.obj?dx=10&dy=20&dz=1' # Load \"old tree in the park\" background http get http://127.0.0.1:5000/load_background/old_tree_in_city_park_8k.hdr # Move camera http -f post http://localhost:5000/camera_pose tx=-60 ty=0 tz=50 rx=60 ry=0 rz=-90 # Render scene and download image http --download get http://localhost:5000/render # Render only leaves http --download get http://localhost:5000/render_class/Color_7","title":"Rendering"},{"location":"Scanner/tutorials/calibration/","text":"Extrinsic Image Calibration Link Objective Link Calibration is giving the right scale to your images and is thus crucial to perform measures from phenotyping imaging. Scale (pixel size) is a priori unknown in a picture. In addition, some aspects of an hardware setup can create artefacts affecting scaling during 3D reconstruction. Hence, we developed a procedure of extrinsic calibration to scale 3D reconstructions to real world unit and correct possible artefacts induced by the configuration of our plant-imager robot. In this tutorial, you will learn how to calibrate an image acquisition for downstream analysis and how to re-use a previously made calibration for an analysis, provided that the set-up is the same. Note Intrinsic calibration corrects possible defects induced by the lens of the camera. These type of defects are not addressed by this procedure. Prerequisite Link Make sure that you installed all the ROMI software to run image acquisitions with the ROMI Plant Imager (explanation here ) install also plant-3d-vision to perform the calibration. We highly recommend the use of dockers to run ROMI software. Details to build and run docker of the ROMI Plant Imager are here Before reading this tutorial, you should first be able to run a basic acquisition without calibration, as explained in this tutorial . set up a database or quickly generate a simple database with the following commands: mkdir path/to/db touch path/to/db/romidb You have now your file-based romi database plantdb Principles of the extrinsic calibration performed here Link The motor positions moving the camera along the acquisition path give a first indication of the scale of the picture, but this motor information is as accurate as the encoder allows them to be. To have the closest scaling from reality, we use Colmap (a structure-from-motion algorithm) at the very beginning of the 3d reconstructions (see here ): this technique allows to refine the positions of the camera given by the robot motors. However, those computed positions are determined up to a scaling and roto-translation of the world and as a result, present a problem for measuring real world unit quantities. In addition, each camera pose is aligned with the corresponding CNC arm position. This can lead to a bias in scaling induced by the offset between the camera optical center and the CNC arm as represented in the following picture: It is particularly true when doing circular path (which is often the case with the phenotyping station). Indeed, because of that offset, the distance between 2 camera poses is bigger than it should be and as a result, the reconstructed the object is bigger than it is in real life (with a relative error of 2d / D). To correct that, a procedure has been developed to perform an extrinsic calibration and apply the results for further image acquisitions using the same hardware settings. Step-by-step tutorial Link 1. Calibration acquisition Link Because the bias is mainly induced by making a circular path of image acquisition, one way to avoid it is to do a calibration acquisition with first a path constituted of two lines (two orthogonal lines in our case) followed by the path that will be used by other acquisitions. To do so, run the task 'CalibrationScan' the same way as for a regular acquisition with the romi_run_task command, including your regular configuration file adapted to your plant imager (hereafter: hardware.toml ). In the command, define a folder inside your romi database (called above plantdb ) that will store the data of this calibration acquisition: romi_run_task --config config/hardware.toml \\ #command and config CalibrationScan \\ # romi task /path/to/plantdb/calibration_scan_id/ #data destination of this calibration scan Note Run this command either in a docker container of the plant-imager or, if you install plant-imager in your system, do not forget to activate the appropriate virtual environment to make romi_run_task command accessible. Colmap performance increases when several \"recognizable\" objects are present in the scene, so that the program easily defines matching reference points between overlapping images. We advise to put such objects in the scene for the calibration acquisition (they could be removed later) 2. Compute circular poses from path lines with Colmap Link Thanks to the linear path added to the circular one, Colmap can now retrieve accurate poses with a proper scaling. Colmap can be easily run with romi software plant-3d-vision . For such a run, a proper configuration file (.toml) is required. A default one is provided with plant-3d-vision , and accessible from your local git-cloned repository or in the repository included inside the docker container. romi_run_task --config path/toconfig/geom_pipe_full.toml \\ #command and path to config file Colmap \\ #the task Colmap /path/to/plantdb/calibration_scan_id/ #data destination folder Note Run this command either in a docker container of plant-3d-vision or, if you install plant-imager in your system, do not forget to activate the appropriate virtual environment to make romi_run_task command accessible. 3. (re)Use the poses extracted from the calibration scan Link Now, the calibrated poses can be used to properly scale 3d reconstruction each time an analysis is performed (full process detailed here ) on other image dataset. To do so, just add in the Colmap section of the configuration .toml file for reconstruction: [Colmap] calibration_scan_id = \"calibration_scan_id\" #name of the folder containing calibration data Important : * Calibration_scan_id and the other dataset to be analyzed must be in the same romi database . * To be valid , calibration data can only be used if the camera position has not changed at all ( tilt , etc ... ) in the robotic arm . For instance , a new calibration acquisition should be performed each time the camera is removed and replaced back on the arm .","title":"Extrinsic calibration procedure"},{"location":"Scanner/tutorials/calibration/#extrinsic-image-calibration","text":"","title":"Extrinsic Image Calibration"},{"location":"Scanner/tutorials/calibration/#objective","text":"Calibration is giving the right scale to your images and is thus crucial to perform measures from phenotyping imaging. Scale (pixel size) is a priori unknown in a picture. In addition, some aspects of an hardware setup can create artefacts affecting scaling during 3D reconstruction. Hence, we developed a procedure of extrinsic calibration to scale 3D reconstructions to real world unit and correct possible artefacts induced by the configuration of our plant-imager robot. In this tutorial, you will learn how to calibrate an image acquisition for downstream analysis and how to re-use a previously made calibration for an analysis, provided that the set-up is the same. Note Intrinsic calibration corrects possible defects induced by the lens of the camera. These type of defects are not addressed by this procedure.","title":"Objective"},{"location":"Scanner/tutorials/calibration/#prerequisite","text":"Make sure that you installed all the ROMI software to run image acquisitions with the ROMI Plant Imager (explanation here ) install also plant-3d-vision to perform the calibration. We highly recommend the use of dockers to run ROMI software. Details to build and run docker of the ROMI Plant Imager are here Before reading this tutorial, you should first be able to run a basic acquisition without calibration, as explained in this tutorial . set up a database or quickly generate a simple database with the following commands: mkdir path/to/db touch path/to/db/romidb You have now your file-based romi database plantdb","title":"Prerequisite"},{"location":"Scanner/tutorials/calibration/#principles-of-the-extrinsic-calibration-performed-here","text":"The motor positions moving the camera along the acquisition path give a first indication of the scale of the picture, but this motor information is as accurate as the encoder allows them to be. To have the closest scaling from reality, we use Colmap (a structure-from-motion algorithm) at the very beginning of the 3d reconstructions (see here ): this technique allows to refine the positions of the camera given by the robot motors. However, those computed positions are determined up to a scaling and roto-translation of the world and as a result, present a problem for measuring real world unit quantities. In addition, each camera pose is aligned with the corresponding CNC arm position. This can lead to a bias in scaling induced by the offset between the camera optical center and the CNC arm as represented in the following picture: It is particularly true when doing circular path (which is often the case with the phenotyping station). Indeed, because of that offset, the distance between 2 camera poses is bigger than it should be and as a result, the reconstructed the object is bigger than it is in real life (with a relative error of 2d / D). To correct that, a procedure has been developed to perform an extrinsic calibration and apply the results for further image acquisitions using the same hardware settings.","title":"Principles of the extrinsic calibration performed here"},{"location":"Scanner/tutorials/calibration/#step-by-step-tutorial","text":"","title":"Step-by-step tutorial"},{"location":"Scanner/tutorials/calibration/#1-calibration-acquisition","text":"Because the bias is mainly induced by making a circular path of image acquisition, one way to avoid it is to do a calibration acquisition with first a path constituted of two lines (two orthogonal lines in our case) followed by the path that will be used by other acquisitions. To do so, run the task 'CalibrationScan' the same way as for a regular acquisition with the romi_run_task command, including your regular configuration file adapted to your plant imager (hereafter: hardware.toml ). In the command, define a folder inside your romi database (called above plantdb ) that will store the data of this calibration acquisition: romi_run_task --config config/hardware.toml \\ #command and config CalibrationScan \\ # romi task /path/to/plantdb/calibration_scan_id/ #data destination of this calibration scan Note Run this command either in a docker container of the plant-imager or, if you install plant-imager in your system, do not forget to activate the appropriate virtual environment to make romi_run_task command accessible. Colmap performance increases when several \"recognizable\" objects are present in the scene, so that the program easily defines matching reference points between overlapping images. We advise to put such objects in the scene for the calibration acquisition (they could be removed later)","title":"1. Calibration acquisition"},{"location":"Scanner/tutorials/calibration/#2-compute-circular-poses-from-path-lines-with-colmap","text":"Thanks to the linear path added to the circular one, Colmap can now retrieve accurate poses with a proper scaling. Colmap can be easily run with romi software plant-3d-vision . For such a run, a proper configuration file (.toml) is required. A default one is provided with plant-3d-vision , and accessible from your local git-cloned repository or in the repository included inside the docker container. romi_run_task --config path/toconfig/geom_pipe_full.toml \\ #command and path to config file Colmap \\ #the task Colmap /path/to/plantdb/calibration_scan_id/ #data destination folder Note Run this command either in a docker container of plant-3d-vision or, if you install plant-imager in your system, do not forget to activate the appropriate virtual environment to make romi_run_task command accessible.","title":"2. Compute circular poses from path lines with Colmap"},{"location":"Scanner/tutorials/calibration/#3-reuse-the-poses-extracted-from-the-calibration-scan","text":"Now, the calibrated poses can be used to properly scale 3d reconstruction each time an analysis is performed (full process detailed here ) on other image dataset. To do so, just add in the Colmap section of the configuration .toml file for reconstruction: [Colmap] calibration_scan_id = \"calibration_scan_id\" #name of the folder containing calibration data Important : * Calibration_scan_id and the other dataset to be analyzed must be in the same romi database . * To be valid , calibration data can only be used if the camera position has not changed at all ( tilt , etc ... ) in the robotic arm . For instance , a new calibration acquisition should be performed each time the camera is removed and replaced back on the arm .","title":"3. (re)Use the poses extracted from the calibration scan"},{"location":"Scanner/tutorials/hardware_scan/","text":"Plant Imager Bot Link Objective Link This tutorial will guide through the steps of acquiring images of a plant using the plant imager robot In order to collect data in the process of plant phenotyping, the plant imager robot takes RGB images of an object following a particular path with precise camera poses. Prerequisite Link To run an acquisition, you should previously have: built the robot following the guidelines here installed the necessary ROMI software here Step-by-step tutorial Link 1. Check that you are well interfaced with the plant imager Link make sure you are in the conda environment or that you run properly the docker for the plantimager repository interface the machine running the ROMI software with the plant imager: check that your device is correctly connected to the Gimbal and CNC both by USB turn on camera and connect it to the device via Wi-Fi set up a DB or quickly generate a simple database with the following commands: mkdir path/to/db touch path/to/db/romidb You have now your file based database plantdb 2. Get the right configuration Link Scan is the basic task for running an acquisition with the robot. To run this task properly with romi_run_task , a configuration file is needed. A default one for the plant imager can be found under plantimager/config/hardware.toml . It regroups specifications on: the acquisition path (ScanPath) needed parameters for connection between hardware components (CNC, Gimbal and camera) and software (Scan.scanner) object metadata (in Scan.metadata.object) hardware metadata (in Scan.metadata.hardware) [ ScanPath ] # Example, circular path with 60 points: class_name = \"Circle\" [ScanPath.kwargs] center_x = 375 center_y = 375 z = 80 tilt = 0 radius = 300 n_points = 60 [ Scan . scanner . cnc ] # module and kwargs linked to the CNC module = \"plantimager.grbl\" [Scan.scanner.cnc.kwargs] homing = true port = \"/dev/ttyACM0\" [ Scan . scanner . gimbal ] # module and kwargs linked to the gimbal module = \"plantimager.blgimbal\" [Scan.scanner.gimbal.kwargs] port = \"/dev/ttyACM1\" has_tilt = false zero_pan = 0 invert_rotation = true [ Scan . scanner . camera ] # camera related parameters module = \"plantimager.sony\" [Scan.scanner.camera.kwargs] device_ip = \"192.168.122.1\" api_port = \"10000\" postview = true use_flashair = false rotation = 270 [ Scan . metadata . object ] # object related metadata species = \"chenopodium album\" seed_stock = \"Col-0\" plant_id = \"3dt_chenoA\" growth_environment = \"Lyon-indoor\" growth_conditions = \"SD+LD\" treatment = \"None\" DAG = 40 sample = \"main_stem\" experiment_id = \"3dt_26-01-2021\" dataset_id = \"3dt\" [ Scan . metadata . hardware ] # hardware related metadata frame = \"30profile v1\" X_motor = \"X-Carve NEMA23\" Y_motor = \"X-Carve NEMA23\" Z_motor = \"X-Carve NEMA23\" pan_motor = \"iPower Motor GM4108H-120T Brushless Gimbal Motor\" tilt_motor = \"None\" sensor = \"RX0\" [ Scan . metadata . workspace ] # A volume containing the target imaged object x = [ 200, 600,] y = [ 200, 600,] z = [ -100, 300,] Warning This is a default configuration file. You will most probably need to create one to fit your hardware setup. Check the configuration documentation for the hardware and the imaged object 3. Run an acquisition with the Scan task Link Assuming you have an active database, you can now run the Scan task using romi_run_task : romi_run_task --config config/hardware.toml Scan /path/to/db/imageset_id/ where: /path/to/db must be an existing FSDB database there is no /path/to/db/imageset_id already existing in the database. This will create the corresponding folder and fill it with images from the imageset . Warning After a rather short time following running the command, you should hear the robot start and when the acquisition is finished, a This progress looks :) should appear. If it's not the case, try to look at the Troubleshooting section at the end of this tutorial 4. Obtain an image set Link Once the acquisition is done, the database is updated, and we now have the following tree structure: db/ \u251c\u2500\u2500 imageset_id/ \u2502 \u251c\u2500\u2500 files.json \u2502 \u251c\u2500\u2500 images/ \u2502 \u251c\u2500\u2500 metadata/ \u2502 \u2502 \u2514\u2500\u2500 images/ \u2502 \u2502 \u2514\u2500\u2500 images.json \u2502 \u2514\u2500\u2500 scan.toml \u2514\u2500\u2500 romidb with: images containing a list of RGB images acquired by the camera moving around the plant metadata/images a folder filled with json files recording the poses (camera coordinates) for each taken image metadata/images.json containing parameters of the acquisition that will be used later in reconstruction (type of format for the images, info on the object and the workspace) files.json detailing the files contained in the imageset_id scan.json , a copy of the acquisition config file You can now reconstruct your plant in 3d ! Troubleshooting Link Serial access denied Link The CNC and Gimbal might be connected to different ports than the ones specified in the configuration file. Please check with the dmesg -w command. Look here if you can not communicate with the scanner using usb. Make sure the device used to run the acquisition is indeed connected to the camera (wifi) Message to Gimbal still transiting : Traceback ( most recent call last ) : File \"/home/romi/miniconda3/envs/scan_0.8/lib/python3.8/site-packages/serial/serialposix.py\" , line 265 , in open self.fd = os.open ( self.portstr, os.O_RDWR | os.O_NOCTTY | os.O_NONBLOCK ) OSError: [ Errno 16 ] Device or resource busy: '/dev/ttyACM0' Try disconnect and reconnect the USB link and rerun an acquisition","title":"Plant imaging"},{"location":"Scanner/tutorials/hardware_scan/#plant-imager-bot","text":"","title":"Plant Imager Bot"},{"location":"Scanner/tutorials/hardware_scan/#objective","text":"This tutorial will guide through the steps of acquiring images of a plant using the plant imager robot In order to collect data in the process of plant phenotyping, the plant imager robot takes RGB images of an object following a particular path with precise camera poses.","title":"Objective"},{"location":"Scanner/tutorials/hardware_scan/#prerequisite","text":"To run an acquisition, you should previously have: built the robot following the guidelines here installed the necessary ROMI software here","title":"Prerequisite"},{"location":"Scanner/tutorials/hardware_scan/#step-by-step-tutorial","text":"","title":"Step-by-step tutorial"},{"location":"Scanner/tutorials/hardware_scan/#1-check-that-you-are-well-interfaced-with-the-plant-imager","text":"make sure you are in the conda environment or that you run properly the docker for the plantimager repository interface the machine running the ROMI software with the plant imager: check that your device is correctly connected to the Gimbal and CNC both by USB turn on camera and connect it to the device via Wi-Fi set up a DB or quickly generate a simple database with the following commands: mkdir path/to/db touch path/to/db/romidb You have now your file based database plantdb","title":"1. Check that you are well interfaced with the plant imager"},{"location":"Scanner/tutorials/hardware_scan/#2-get-the-right-configuration","text":"Scan is the basic task for running an acquisition with the robot. To run this task properly with romi_run_task , a configuration file is needed. A default one for the plant imager can be found under plantimager/config/hardware.toml . It regroups specifications on: the acquisition path (ScanPath) needed parameters for connection between hardware components (CNC, Gimbal and camera) and software (Scan.scanner) object metadata (in Scan.metadata.object) hardware metadata (in Scan.metadata.hardware) [ ScanPath ] # Example, circular path with 60 points: class_name = \"Circle\" [ScanPath.kwargs] center_x = 375 center_y = 375 z = 80 tilt = 0 radius = 300 n_points = 60 [ Scan . scanner . cnc ] # module and kwargs linked to the CNC module = \"plantimager.grbl\" [Scan.scanner.cnc.kwargs] homing = true port = \"/dev/ttyACM0\" [ Scan . scanner . gimbal ] # module and kwargs linked to the gimbal module = \"plantimager.blgimbal\" [Scan.scanner.gimbal.kwargs] port = \"/dev/ttyACM1\" has_tilt = false zero_pan = 0 invert_rotation = true [ Scan . scanner . camera ] # camera related parameters module = \"plantimager.sony\" [Scan.scanner.camera.kwargs] device_ip = \"192.168.122.1\" api_port = \"10000\" postview = true use_flashair = false rotation = 270 [ Scan . metadata . object ] # object related metadata species = \"chenopodium album\" seed_stock = \"Col-0\" plant_id = \"3dt_chenoA\" growth_environment = \"Lyon-indoor\" growth_conditions = \"SD+LD\" treatment = \"None\" DAG = 40 sample = \"main_stem\" experiment_id = \"3dt_26-01-2021\" dataset_id = \"3dt\" [ Scan . metadata . hardware ] # hardware related metadata frame = \"30profile v1\" X_motor = \"X-Carve NEMA23\" Y_motor = \"X-Carve NEMA23\" Z_motor = \"X-Carve NEMA23\" pan_motor = \"iPower Motor GM4108H-120T Brushless Gimbal Motor\" tilt_motor = \"None\" sensor = \"RX0\" [ Scan . metadata . workspace ] # A volume containing the target imaged object x = [ 200, 600,] y = [ 200, 600,] z = [ -100, 300,] Warning This is a default configuration file. You will most probably need to create one to fit your hardware setup. Check the configuration documentation for the hardware and the imaged object","title":"2. Get the right configuration"},{"location":"Scanner/tutorials/hardware_scan/#3-run-an-acquisition-with-the-scan-task","text":"Assuming you have an active database, you can now run the Scan task using romi_run_task : romi_run_task --config config/hardware.toml Scan /path/to/db/imageset_id/ where: /path/to/db must be an existing FSDB database there is no /path/to/db/imageset_id already existing in the database. This will create the corresponding folder and fill it with images from the imageset . Warning After a rather short time following running the command, you should hear the robot start and when the acquisition is finished, a This progress looks :) should appear. If it's not the case, try to look at the Troubleshooting section at the end of this tutorial","title":"3. Run an acquisition with the Scan task"},{"location":"Scanner/tutorials/hardware_scan/#4-obtain-an-image-set","text":"Once the acquisition is done, the database is updated, and we now have the following tree structure: db/ \u251c\u2500\u2500 imageset_id/ \u2502 \u251c\u2500\u2500 files.json \u2502 \u251c\u2500\u2500 images/ \u2502 \u251c\u2500\u2500 metadata/ \u2502 \u2502 \u2514\u2500\u2500 images/ \u2502 \u2502 \u2514\u2500\u2500 images.json \u2502 \u2514\u2500\u2500 scan.toml \u2514\u2500\u2500 romidb with: images containing a list of RGB images acquired by the camera moving around the plant metadata/images a folder filled with json files recording the poses (camera coordinates) for each taken image metadata/images.json containing parameters of the acquisition that will be used later in reconstruction (type of format for the images, info on the object and the workspace) files.json detailing the files contained in the imageset_id scan.json , a copy of the acquisition config file You can now reconstruct your plant in 3d !","title":"4. Obtain an image set"},{"location":"Scanner/tutorials/hardware_scan/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"Scanner/tutorials/hardware_scan/#serial-access-denied","text":"The CNC and Gimbal might be connected to different ports than the ones specified in the configuration file. Please check with the dmesg -w command. Look here if you can not communicate with the scanner using usb. Make sure the device used to run the acquisition is indeed connected to the camera (wifi) Message to Gimbal still transiting : Traceback ( most recent call last ) : File \"/home/romi/miniconda3/envs/scan_0.8/lib/python3.8/site-packages/serial/serialposix.py\" , line 265 , in open self.fd = os.open ( self.portstr, os.O_RDWR | os.O_NOCTTY | os.O_NONBLOCK ) OSError: [ Errno 16 ] Device or resource busy: '/dev/ttyACM0' Try disconnect and reconnect the USB link and rerun an acquisition","title":"Serial access denied"},{"location":"Scanner/tutorials/plant-3d-explorer/","text":"How to see directly the results of your plant phenotyping with the plant-3d-explorer ? Link Objective Link Throughout the whole process of plant phenotyping, viewing data is often needed. This tutorial explains how to use the romi plant-3d-explorer , a web-server tool, to explore, display and interact with most of the diverse data generated during a typical plant phenotyping experiment from 2D images (2D images, 3D objects like meshes or point cloud, quality evaluations, trait measurements). After this tutorial, you should be able to: connect the plant-3d-explorer to a database containing the phenotyping data of one to several plants ; explore the database content with the plant-3d-explorer menu page ; For each plant, display, overlay and inspect in 3d every data generated during analysis Prerequisite Link install romi plant-3d-explorer (from source or using a docker image ) & read install procedure install romi plantdb (from source or using a docker image ) & read install procedure install romi plant-3d-vision (from source or using a docker image ) & read install procedure Create and activate isolated python environment (see the procedure here ) Note for docker users You can avoid installs by using docker only. Read first the docker procedures ( 'docker for plant-3d-vision' and 'docker-compose to run both database and 3d explorer with docker containers' ). In the following tutorial (steps 1, 2 and 3), follow the docker logo to adapt the procedure. Linked documentation Link Manual of the romi plant-3d-explorer Step-by-step tutorial Link Principle: the plant-3d-explorer is a web client that displays in your favorite web browser data exposed by a server (here, romi plantdb ) on a particular url. The process consists in pointing the server to your folder of interest, starting the server and starting the client that points to the served url. Warning the plant-3d-explorer has only been developed and tested on Chrome. 1. Preparing your database for display by the plant-3d-explorer Link Starting point: your database is made of one or several datasets , which all correspond to a single plant phenotyping experiment: each dataset contains at least 2D images (raw acquisitions) and metadata, and possibly several other data generated by subsequent 3D reconstruction, segmentation and analysis. Note Your database must follow the rules of romi databases: please make sure that you comply to requirements . You can also download an example database here . example : let's consider a database called my_experiment containing 3 datasets (named plant1, plant2, plant3) generated by phenotyping three plants. open a terminal and go to your local database directory if romi commands (like romi_run_task ) are not accessible from your terminal, activate the appropriate python environment (e.g. using venv or conda) required for romi commands (or read this procedure ) process all datasets for display by the plant-3d-explorer by running the following code dataset_list =( 'plant1' , 'plant2' , 'plant3' ) for ds in \" ${ dataset_list [@] } \" do romi_run_task Visualization path_to/my_experiment/ \" $ds \" / --config ~/config/ml_pipe_real.toml done Note For more information about using romi_run_task command, the Visualization task and the config file, please read XXXXX. Note for docker users Start a docker container by mounting your database as a volume ( details ) In the container, run the same Visualization Task has above check result : a new folder called Visualization should have been created in each dataset of your database Note for docker users Skip step 2 & 3 and follow instead instructions given by 'docker-compose to run both database and 3d explorer with docker containers' 2. Connect your database to a local server Link Continue in the same shell terminal (if you open a new terminal, do not forget to activate appropriate python environment) set the DB location using the DB_LOCATION environment variable Type the following commands to launch the server: export DB_LOCATION = /path/to/your/db romi_scanner_rest_api #command that starts the server check result : the terminal prints various information given by the server (e.g. number of datasets in the database). Do not stop this terminal as this will shut down the server. 3. Connect the plant-3d-explorer to the server Link Open a new terminal go to your local cloned directory of plant-3d-explorer/ start the frontend visualization server by entering: npm start You should now be able to access the plant-3d-explorer on http://localhost:3000 . Depending on you system preferences, your default web browser may automatically open a window displaying the server content. If not, open your web browser and enter http://localhost:3000 in the url bar. Note You need to add a file .env.local at project's root to set the API URL: REACT_APP_API_URL='{`API URL}' . Without this, the app will use: http://localhost:5000 , which is the default for romi_scanner_rest_api . 4. Explore your database content via the menu page Link Note More description about the function of the menu page: read here The starting page of the plant-3d-explorer lists the datasets of the connected database as a table and looks like this: The top search bar allows you to find particular datasets based on keywords. Data filters: in the header row, click on an icon to activate the filter (datasets that do not contain the data will be filtered out) 3d objects generated from the plant 2D images (icons respectively stand for: mesh, point cloud, segmented point cloud, skeleton and organs) or phyllotaxis data (manual or computed phyllotaxis measurements) Open a dataset with the green 'Open' button at the far right of a row 5. View a single dataset and all related data Link Note This is only a brief description to allow a quick start. More description here By default, the plant-3d-explorer displays in the main panel the skeleton and the organs (if available) and phyllotaxis data (as graphs) in the right panel. Mouse-over most elements provides a brief description. Select the 3D layers to display Link In the top left corner of the main panel, icons allows you to quickly (un)select 3D layers (if available): White icons are active, dark grey are available but not active, light grey are not available for this dataset From left to right, icons represents respectively the mesh, the point-cloud, the segmented point cloud, the skeleton and the organs. In the center of the middle panel are icons for general viewing options: Activate the camera icon displays the camera poses (only works if overlay with 2 images is deactivated) Click the round arrow to reset the view Moving the view in the \"free\" 3D (without 2D overlay) Link Easy movements are accessible with a mouse: scroll to zoom in/out left click rotate right click translate Activate overlay with 2D images Link click on any image of the bottom carousel to activate the display of 2 images in the main panel. On Mouse-over, a single picture is enlarged and proposes to open it in the main panel. Overlay with active 3D layers is automatic. In the carousel, the box around the active displayed 2D image is now permanent. To close the 2D overlay, just click the close button of the boxed picture in the carousel. Moving the view with 2D overlay Link Note that movement control with the mouse slightly changes compared to the \"free\" view without 2D overlay. Notably, the free rotation mode is not possible anymore, since it is constrained by the real movements made by the camera when it took the pictures. Slide right/left the active box picture in the carousel to reproduce the camera movement scroll to zoom in/out left-click to translate The phyllotaxis measure plots Link Plots represent the successive measures of divergence angles (left, in degrees) and internode length (right, in mm) between consecutive pairs of organs (here fruits) along the stem, from the base to the inflorescence tip. Both plots can be closed by clicking the cross at the far right of the plot's title. Closed plot panels can be re-opened by clicking a green \"+\" sign appearing at the right-hand corner when at least one plot is closed. In the plots, a blue curve correspond to \"automated\" measure computed through an analysis pipeline (such as pipelines developed in romi plant-3d-vision). If available in the dataset, a red curve indicates a ground-truth \"manual\" measure. Mouse-over any of the two plots highlights an interval that correspond to a measure between two consecutive organs (fruits) segmented by the analysis. The interval appears synchronously on both plots if opened. This interval and the organs are numbered by their order from the base of the stem, these numbers appear on the (vertical) X-axis of the plots. The exact value of the selected interval is displayed on top of the plot (\"automated\" and \"manual\" values in blue and red respectively, if available). As shown above, when the 'organ' 3d-layer is active in the main panel, mouse-over the plot synchronously select the corresponding pair of organs in the current view (all other organ layers just disappear). Clicking the interval in the plot maintains the selection of the organ pair active despite further mouse movements. To deactivate the selection, click the green cross at the end of the shaded selection rectangle on either of the plots. Organ colors are also synchronized between main panel and plot panels. In the free 3D mode only, clicking a fruit layer display a bubble telling the organ number, colored as the corresponding fruit layer. Bubbles stay on screen if the 2D overlay is activated (as in the picture above). Go back to main page Link In the top left corner of the page, click \"all scans\":","title":"plant-3d-explorer: view your data"},{"location":"Scanner/tutorials/plant-3d-explorer/#how-to-see-directly-the-results-of-your-plant-phenotyping-with-the-plant-3d-explorer","text":"","title":"How to see directly the results of your plant phenotyping with the plant-3d-explorer ?"},{"location":"Scanner/tutorials/plant-3d-explorer/#objective","text":"Throughout the whole process of plant phenotyping, viewing data is often needed. This tutorial explains how to use the romi plant-3d-explorer , a web-server tool, to explore, display and interact with most of the diverse data generated during a typical plant phenotyping experiment from 2D images (2D images, 3D objects like meshes or point cloud, quality evaluations, trait measurements). After this tutorial, you should be able to: connect the plant-3d-explorer to a database containing the phenotyping data of one to several plants ; explore the database content with the plant-3d-explorer menu page ; For each plant, display, overlay and inspect in 3d every data generated during analysis","title":"Objective"},{"location":"Scanner/tutorials/plant-3d-explorer/#prerequisite","text":"install romi plant-3d-explorer (from source or using a docker image ) & read install procedure install romi plantdb (from source or using a docker image ) & read install procedure install romi plant-3d-vision (from source or using a docker image ) & read install procedure Create and activate isolated python environment (see the procedure here ) Note for docker users You can avoid installs by using docker only. Read first the docker procedures ( 'docker for plant-3d-vision' and 'docker-compose to run both database and 3d explorer with docker containers' ). In the following tutorial (steps 1, 2 and 3), follow the docker logo to adapt the procedure.","title":"Prerequisite"},{"location":"Scanner/tutorials/plant-3d-explorer/#linked-documentation","text":"Manual of the romi plant-3d-explorer","title":"Linked documentation"},{"location":"Scanner/tutorials/plant-3d-explorer/#step-by-step-tutorial","text":"Principle: the plant-3d-explorer is a web client that displays in your favorite web browser data exposed by a server (here, romi plantdb ) on a particular url. The process consists in pointing the server to your folder of interest, starting the server and starting the client that points to the served url. Warning the plant-3d-explorer has only been developed and tested on Chrome.","title":"Step-by-step tutorial"},{"location":"Scanner/tutorials/plant-3d-explorer/#1-preparing-your-database-for-display-by-the-plant-3d-explorer","text":"Starting point: your database is made of one or several datasets , which all correspond to a single plant phenotyping experiment: each dataset contains at least 2D images (raw acquisitions) and metadata, and possibly several other data generated by subsequent 3D reconstruction, segmentation and analysis. Note Your database must follow the rules of romi databases: please make sure that you comply to requirements . You can also download an example database here . example : let's consider a database called my_experiment containing 3 datasets (named plant1, plant2, plant3) generated by phenotyping three plants. open a terminal and go to your local database directory if romi commands (like romi_run_task ) are not accessible from your terminal, activate the appropriate python environment (e.g. using venv or conda) required for romi commands (or read this procedure ) process all datasets for display by the plant-3d-explorer by running the following code dataset_list =( 'plant1' , 'plant2' , 'plant3' ) for ds in \" ${ dataset_list [@] } \" do romi_run_task Visualization path_to/my_experiment/ \" $ds \" / --config ~/config/ml_pipe_real.toml done Note For more information about using romi_run_task command, the Visualization task and the config file, please read XXXXX. Note for docker users Start a docker container by mounting your database as a volume ( details ) In the container, run the same Visualization Task has above check result : a new folder called Visualization should have been created in each dataset of your database Note for docker users Skip step 2 & 3 and follow instead instructions given by 'docker-compose to run both database and 3d explorer with docker containers'","title":"1. Preparing your database for display by the plant-3d-explorer"},{"location":"Scanner/tutorials/plant-3d-explorer/#2-connect-your-database-to-a-local-server","text":"Continue in the same shell terminal (if you open a new terminal, do not forget to activate appropriate python environment) set the DB location using the DB_LOCATION environment variable Type the following commands to launch the server: export DB_LOCATION = /path/to/your/db romi_scanner_rest_api #command that starts the server check result : the terminal prints various information given by the server (e.g. number of datasets in the database). Do not stop this terminal as this will shut down the server.","title":"2. Connect your database to a local server"},{"location":"Scanner/tutorials/plant-3d-explorer/#3-connect-the-plant-3d-explorer-to-the-server","text":"Open a new terminal go to your local cloned directory of plant-3d-explorer/ start the frontend visualization server by entering: npm start You should now be able to access the plant-3d-explorer on http://localhost:3000 . Depending on you system preferences, your default web browser may automatically open a window displaying the server content. If not, open your web browser and enter http://localhost:3000 in the url bar. Note You need to add a file .env.local at project's root to set the API URL: REACT_APP_API_URL='{`API URL}' . Without this, the app will use: http://localhost:5000 , which is the default for romi_scanner_rest_api .","title":"3. Connect the plant-3d-explorer to the server"},{"location":"Scanner/tutorials/plant-3d-explorer/#4-explore-your-database-content-via-the-menu-page","text":"Note More description about the function of the menu page: read here The starting page of the plant-3d-explorer lists the datasets of the connected database as a table and looks like this: The top search bar allows you to find particular datasets based on keywords. Data filters: in the header row, click on an icon to activate the filter (datasets that do not contain the data will be filtered out) 3d objects generated from the plant 2D images (icons respectively stand for: mesh, point cloud, segmented point cloud, skeleton and organs) or phyllotaxis data (manual or computed phyllotaxis measurements) Open a dataset with the green 'Open' button at the far right of a row","title":"4. Explore your database content via the menu page"},{"location":"Scanner/tutorials/plant-3d-explorer/#5-view-a-single-dataset-and-all-related-data","text":"Note This is only a brief description to allow a quick start. More description here By default, the plant-3d-explorer displays in the main panel the skeleton and the organs (if available) and phyllotaxis data (as graphs) in the right panel. Mouse-over most elements provides a brief description.","title":"5. View a single dataset and all related data"},{"location":"Scanner/tutorials/plant-3d-explorer/#select-the-3d-layers-to-display","text":"In the top left corner of the main panel, icons allows you to quickly (un)select 3D layers (if available): White icons are active, dark grey are available but not active, light grey are not available for this dataset From left to right, icons represents respectively the mesh, the point-cloud, the segmented point cloud, the skeleton and the organs. In the center of the middle panel are icons for general viewing options: Activate the camera icon displays the camera poses (only works if overlay with 2 images is deactivated) Click the round arrow to reset the view","title":"Select the 3D layers to display"},{"location":"Scanner/tutorials/plant-3d-explorer/#moving-the-view-in-the-free-3d-without-2d-overlay","text":"Easy movements are accessible with a mouse: scroll to zoom in/out left click rotate right click translate","title":"Moving the view in the \"free\" 3D (without 2D overlay)"},{"location":"Scanner/tutorials/plant-3d-explorer/#activate-overlay-with-2d-images","text":"click on any image of the bottom carousel to activate the display of 2 images in the main panel. On Mouse-over, a single picture is enlarged and proposes to open it in the main panel. Overlay with active 3D layers is automatic. In the carousel, the box around the active displayed 2D image is now permanent. To close the 2D overlay, just click the close button of the boxed picture in the carousel.","title":"Activate overlay with 2D images"},{"location":"Scanner/tutorials/plant-3d-explorer/#moving-the-view-with-2d-overlay","text":"Note that movement control with the mouse slightly changes compared to the \"free\" view without 2D overlay. Notably, the free rotation mode is not possible anymore, since it is constrained by the real movements made by the camera when it took the pictures. Slide right/left the active box picture in the carousel to reproduce the camera movement scroll to zoom in/out left-click to translate","title":"Moving the view with 2D overlay"},{"location":"Scanner/tutorials/plant-3d-explorer/#the-phyllotaxis-measure-plots","text":"Plots represent the successive measures of divergence angles (left, in degrees) and internode length (right, in mm) between consecutive pairs of organs (here fruits) along the stem, from the base to the inflorescence tip. Both plots can be closed by clicking the cross at the far right of the plot's title. Closed plot panels can be re-opened by clicking a green \"+\" sign appearing at the right-hand corner when at least one plot is closed. In the plots, a blue curve correspond to \"automated\" measure computed through an analysis pipeline (such as pipelines developed in romi plant-3d-vision). If available in the dataset, a red curve indicates a ground-truth \"manual\" measure. Mouse-over any of the two plots highlights an interval that correspond to a measure between two consecutive organs (fruits) segmented by the analysis. The interval appears synchronously on both plots if opened. This interval and the organs are numbered by their order from the base of the stem, these numbers appear on the (vertical) X-axis of the plots. The exact value of the selected interval is displayed on top of the plot (\"automated\" and \"manual\" values in blue and red respectively, if available). As shown above, when the 'organ' 3d-layer is active in the main panel, mouse-over the plot synchronously select the corresponding pair of organs in the current view (all other organ layers just disappear). Clicking the interval in the plot maintains the selection of the organ pair active despite further mouse movements. To deactivate the selection, click the green cross at the end of the shaded selection rectangle on either of the plots. Organ colors are also synchronized between main panel and plot panels. In the free 3D mode only, clicking a fruit layer display a bubble telling the organ number, colored as the corresponding fruit layer. Bubbles stay on screen if the 2D overlay is activated (as in the picture above).","title":"The phyllotaxis measure plots"},{"location":"Scanner/tutorials/plant-3d-explorer/#go-back-to-main-page","text":"In the top left corner of the page, click \"all scans\":","title":"Go back to main page"},{"location":"Scanner/tutorials/reconstruct_scan/","text":"Plant reconstruction and analysis pipeline Link Getting started Link To follows this guide you should have: installed the necessary ROMI software here or followed the instructions for the docker image here access to a database with a \"plant acquisition\" to reconstruct (or use the provided examples) Reconstruction pipeline Link Cleaning a dataset Link If you made a mess, had a failure or just want to start fresh with your dataset, no need to save a copy on the side, you can use the Clean task: romi_run_task Clean integration_tests/2019-02-01_10-56-33 \\ --config plant3dvision/config/original_pipe_0.toml --local-scheduler Here the config may use the [Clean] section where you can defines the force option: [Clean] force = true If true the Clean task will run silently, else in interactive mode. Geometric pipeline Link Real scan dataset Link The full geometric pipeline , ie. all the way to angles and internodes measurement, can be called on real dataset with: romi_run_task AnglesAndInternodes integration_tests/2019-02-01_10-56-33 \\ --config plant3dvision/config/original_pipe_0.toml --local-scheduler Note This example uses a real scan dataset from the test database. Virtual plant dataset Link The full geometric pipeline , ie. all the way to angles and internodes measurement, can be called on a virtual dataset with: romi_run_task AnglesAndInternodes integration_tests/arabidopsis_26 \\ --config plant3dvision/config/original_pipe_0.toml --local-scheduler Note This example uses a virtual scan dataset from the test database. Warning If you get something like this during the Voxel tasks: Choose platform: [0] <pyopencl.Platform 'NVIDIA CUDA' at 0x55d904d5af50> Choice [0]: that mean you need to specify the environment variable PYOPENCL_CTX='0' Machine Learning pipeline Link Warning This requires the installation of the romiseg libraries (see here for install instructions ) and a trained PyTorch model! Note A trained model, to place under <dataset>/models/models , is accessible here: https://media.romi-project.eu/data/Resnetdataset_gl_png_896_896_epoch50.pt Real scan dataset Link The full geometric pipeline , ie. all the way to angles and internodes measurement, can be called on real dataset with: romi_run_task PointCloud integration_tests/2019-02-01_10-56-33 --config plant3dvision/config/ml_pipe_vplants_3.toml --local-scheduler Note This example uses a real scan dataset from the test database. Virtual plant dataset Link The full geometric pipeline , ie. all the way to angles and internodes measurement, can be called on a virtual dataset with: romi_run_task PointCloud integration_tests/arabidopsis_26 \\ --config plant3dvision/config/ml_pipe_vplants_3.toml --local-scheduler Note This example uses a virtual scan dataset from the test database.","title":"Scan reconstruction"},{"location":"Scanner/tutorials/reconstruct_scan/#plant-reconstruction-and-analysis-pipeline","text":"","title":"Plant reconstruction and analysis pipeline"},{"location":"Scanner/tutorials/reconstruct_scan/#getting-started","text":"To follows this guide you should have: installed the necessary ROMI software here or followed the instructions for the docker image here access to a database with a \"plant acquisition\" to reconstruct (or use the provided examples)","title":"Getting started"},{"location":"Scanner/tutorials/reconstruct_scan/#reconstruction-pipeline","text":"","title":"Reconstruction pipeline"},{"location":"Scanner/tutorials/reconstruct_scan/#cleaning-a-dataset","text":"If you made a mess, had a failure or just want to start fresh with your dataset, no need to save a copy on the side, you can use the Clean task: romi_run_task Clean integration_tests/2019-02-01_10-56-33 \\ --config plant3dvision/config/original_pipe_0.toml --local-scheduler Here the config may use the [Clean] section where you can defines the force option: [Clean] force = true If true the Clean task will run silently, else in interactive mode.","title":"Cleaning a dataset"},{"location":"Scanner/tutorials/reconstruct_scan/#geometric-pipeline","text":"","title":"Geometric pipeline"},{"location":"Scanner/tutorials/reconstruct_scan/#real-scan-dataset","text":"The full geometric pipeline , ie. all the way to angles and internodes measurement, can be called on real dataset with: romi_run_task AnglesAndInternodes integration_tests/2019-02-01_10-56-33 \\ --config plant3dvision/config/original_pipe_0.toml --local-scheduler Note This example uses a real scan dataset from the test database.","title":"Real scan dataset"},{"location":"Scanner/tutorials/reconstruct_scan/#virtual-plant-dataset","text":"The full geometric pipeline , ie. all the way to angles and internodes measurement, can be called on a virtual dataset with: romi_run_task AnglesAndInternodes integration_tests/arabidopsis_26 \\ --config plant3dvision/config/original_pipe_0.toml --local-scheduler Note This example uses a virtual scan dataset from the test database. Warning If you get something like this during the Voxel tasks: Choose platform: [0] <pyopencl.Platform 'NVIDIA CUDA' at 0x55d904d5af50> Choice [0]: that mean you need to specify the environment variable PYOPENCL_CTX='0'","title":"Virtual plant dataset"},{"location":"Scanner/tutorials/reconstruct_scan/#machine-learning-pipeline","text":"Warning This requires the installation of the romiseg libraries (see here for install instructions ) and a trained PyTorch model! Note A trained model, to place under <dataset>/models/models , is accessible here: https://media.romi-project.eu/data/Resnetdataset_gl_png_896_896_epoch50.pt","title":"Machine Learning pipeline"},{"location":"Scanner/tutorials/reconstruct_scan/#real-scan-dataset_1","text":"The full geometric pipeline , ie. all the way to angles and internodes measurement, can be called on real dataset with: romi_run_task PointCloud integration_tests/2019-02-01_10-56-33 --config plant3dvision/config/ml_pipe_vplants_3.toml --local-scheduler Note This example uses a real scan dataset from the test database.","title":"Real scan dataset"},{"location":"Scanner/tutorials/reconstruct_scan/#virtual-plant-dataset_1","text":"The full geometric pipeline , ie. all the way to angles and internodes measurement, can be called on a virtual dataset with: romi_run_task PointCloud integration_tests/arabidopsis_26 \\ --config plant3dvision/config/ml_pipe_vplants_3.toml --local-scheduler Note This example uses a virtual scan dataset from the test database.","title":"Virtual plant dataset"},{"location":"Scanner/tutorials/virtual_plant_imager_large_dataset/","text":"How to use the Virtual Plant Imager to generate a large dataset of virtual plant for machine learning purposes Link Objective Link Working with virtual plants instead of real ones makes data acquisition inexpensive and has the advantage to parametrize the type of data. By design, ground truth data can be easily extracted from virtual datasets for evaluation purposes and building machine learning models. The Virtual Plant Imager is designed two address these two issues. After reading this tutorial, you should be able to generate a single virtual plant dataset in order to evaluate the robustness of plant-3d-vision . Prerequisite Link If it is not already done, you must be able to build and run the docker image by following the instructions . Step-by-step tutorial Link Principle: Technically, the Virtual Plant Imager relies on Blender v2.81a to generate the images of 3d model of the plants. The 3d model can be provided as an input or can be also generated by lpy based on biological rules. An Http server acts as an interface to drive Blender generation scripts. 1. Preparing your scan data Link First, you have to create a working database on your host machine, let's say home/host/path/database_example . You can find an example of this database here . You can obtain sample data for the scanner here, and put it in the data folder. wget https://db.romi-project.eu/models/arabidopsis_data.zip unzip arabidopsis_data.zip -d data To use custom data, it must consist in .obj file, in which each type of organ corresponds to a distinct mesh. This mesh must have a single material whose name is the name of the organ. The data dir must contain the obj and mtl files. Additionally, background HDRI files can be downloaded from hdri haven . Download .hdr files and put them in the hdri folder. 2. Generating a large dataset for machine learning purposes Link After preparing your working database directory. You have to run the docker container with the database mounted. cd plant-imager/docker ./run.sh -db /home/host/path/database_example # This will map to `db` directory located in the the docker's user home To generate a large dataset, you have to run the script generate_dataset.py by passing the config file and the output folder. ( lpyEnv ) user@5c9e389f223d python generate_dataset.py plant-imager/config/vscan_lpy_blender.toml db/learning_set After a while, and if the generation succeeded the learning_set folder will be populated by virtual plants.","title":"How to generate large \"virtual\" training data for machine-learning ?"},{"location":"Scanner/tutorials/virtual_plant_imager_large_dataset/#how-to-use-the-virtual-plant-imager-to-generate-a-large-dataset-of-virtual-plant-for-machine-learning-purposes","text":"","title":"How to use the Virtual Plant Imager to generate a large dataset of virtual plant for machine learning purposes"},{"location":"Scanner/tutorials/virtual_plant_imager_large_dataset/#objective","text":"Working with virtual plants instead of real ones makes data acquisition inexpensive and has the advantage to parametrize the type of data. By design, ground truth data can be easily extracted from virtual datasets for evaluation purposes and building machine learning models. The Virtual Plant Imager is designed two address these two issues. After reading this tutorial, you should be able to generate a single virtual plant dataset in order to evaluate the robustness of plant-3d-vision .","title":"Objective"},{"location":"Scanner/tutorials/virtual_plant_imager_large_dataset/#prerequisite","text":"If it is not already done, you must be able to build and run the docker image by following the instructions .","title":"Prerequisite"},{"location":"Scanner/tutorials/virtual_plant_imager_large_dataset/#step-by-step-tutorial","text":"Principle: Technically, the Virtual Plant Imager relies on Blender v2.81a to generate the images of 3d model of the plants. The 3d model can be provided as an input or can be also generated by lpy based on biological rules. An Http server acts as an interface to drive Blender generation scripts.","title":"Step-by-step tutorial"},{"location":"Scanner/tutorials/virtual_plant_imager_large_dataset/#1-preparing-your-scan-data","text":"First, you have to create a working database on your host machine, let's say home/host/path/database_example . You can find an example of this database here . You can obtain sample data for the scanner here, and put it in the data folder. wget https://db.romi-project.eu/models/arabidopsis_data.zip unzip arabidopsis_data.zip -d data To use custom data, it must consist in .obj file, in which each type of organ corresponds to a distinct mesh. This mesh must have a single material whose name is the name of the organ. The data dir must contain the obj and mtl files. Additionally, background HDRI files can be downloaded from hdri haven . Download .hdr files and put them in the hdri folder.","title":"1. Preparing your scan data"},{"location":"Scanner/tutorials/virtual_plant_imager_large_dataset/#2-generating-a-large-dataset-for-machine-learning-purposes","text":"After preparing your working database directory. You have to run the docker container with the database mounted. cd plant-imager/docker ./run.sh -db /home/host/path/database_example # This will map to `db` directory located in the the docker's user home To generate a large dataset, you have to run the script generate_dataset.py by passing the config file and the output folder. ( lpyEnv ) user@5c9e389f223d python generate_dataset.py plant-imager/config/vscan_lpy_blender.toml db/learning_set After a while, and if the generation succeeded the learning_set folder will be populated by virtual plants.","title":"2. Generating a large dataset for machine learning purposes"},{"location":"Scanner/tutorials/virtual_plant_imager_single_dataset/","text":"How to evaluate a 3D reconstruction and automated measures with a virtual plant as ground truth Link Objective Link Quantitative evaluation of a 3D reconstruction and/or automated measure from a phenotyping experiment is critical, from both developer and end-user perspectives. However, obtaining ground truth reference is often tedious (e.g. manual measurements, it must be anticipated (synchronous measures with image acquisition), and some type of data are just inacessible with available technologies (e.g. having a reference point cloud). Virtual plants makes data acquisition inexpensive and allows to parametrize the type of data. By design, ground truth data can be easily extracted from these virtual datasets. We thus designed the Virtual Plant Imager to take images of any 3D object, as a digital twin of our real plant imager . After reading this tutorial, you should be able to generate a single virtual plant dataset (including several ground truth reference) in order to evaluate the phenotyping results generated through an analysis pipeline made with our plant-3d-vision tool suite. Prerequisite Link We highly recommend the use of dockers to run ROMI software. If it is not already done, you must be able to build and run the docker images of: the (Virtual) Plant Imager by following these instructions . This is required to generate the virtual data (initial plant 3D model, ground truth and RGB images). the plant-3d-vision by following these instructions . This is required to reconstruct a 3D model from the virtual 2D images, as if they were images of real plants. This docker will also allow you to evaluate this reconstruction using the available virtual ground truth data. Step-by-step tutorial Link Principle : You want to evaluate the results generated by an analysis pipeline made with our plant-3d-vision tool suite. Let's say that this pipeline is defined by a typical configuration file, test_pipe.toml . The idea is to generate images of a virtual 3D plant and provide these picture as input to the tested analysis pipeline. Technically, the Virtual Plant Imager relies on Blender v2.81a to generate at set of (2D) RGB images from the plant 3d model, mimicking what a real camera would do on a real plant. Any virtual camera pose can be generated (ie. distance, angle), but virtual poses similar to the real robot ( plant imager ) are preferred. An HTTP server acts as an interface to drive Blender generation scripts. The virtual plant 3D model (with some of its ground truth references) can be imported and given as an input. However, we provide an integrated procedure to generate a virtual 3D plant directly \"on the fly\" with Lpy , using a Lpy model and customizable parameters. Some ground truth references will also be automatically generated. Once this virtual plant has been virtually imaged, there are all data and metadata required to run an analysis with the tested pipeline. The results of this analysis will be compared to the virtual ground truth. Four type of evaluations are currently implemented : evaluation of a 2D segmentation evaluation of a 3D segmentation of the point cloud comparison of point cloud similarity evaluation of phyllotaxis measures (angles and internodes) 1. Prepare data into a proper database Link First, create a working directory on your host machine, let's say home/host/path/my_virtual_db . You can find an example of such a directory here . This working directory is a proper \"romi\" database which contains additionnal data for the virtual plant generation and/or imaging grouped in a so-called `vscan_data folder: Legend : (*) the name is fixed and cannot be changed (!) the folder/file must exist (no tag means that the folder is not required for the program to run) my_virtual_db/ \u2502 romidb (!*) # a (empty) marker file for recognition by the plantdb module \u2514\u2500\u2500\u2500vscan_data/ (!*) \u2502 \u2514\u2500\u2500\u2500hdri/ (*) \u2502 \u2502 hdri_file1.hdr \u2502 \u2502 hdri_file2.hdr \u2502 \u2502 etc... \u2502 \u2514\u2500\u2500\u2500lpy/ (*) \u2502 \u2502 my_plant_species_model.lpy \u2502 \u2514\u2500\u2500\u2500obj/ (*) \u2502 \u2502 VirtualPlant.obj \u2502 \u2502 VirtualPlant_mtl \u2502 \u2514\u2500\u2500\u2500metadata/(!*) \u2502 \u2502 hdri.json \u2502 \u2502 lpy.json \u2502 \u2502 obj.json \u2502 \u2502 palette.json \u2502 \u2502 scenes.json \u2502 \u2514\u2500\u2500\u2500palette/ (*) \u2502 \u2502 my_plant_species_model.png \u2502 \u2514\u2500\u2500\u2500scenes/ (*) \u2502 files.json 1.1 quick ready-to-use example Link Recommended if you are not familiar with the virtual plant imager . You can directly obtain a functional working directory from the repository of the plant-imager you cloned in your host machine So if your working directory is named my_virtual_db , execute in a terminal: cd plant-imager # enter the cloned repository in your host machine cp -r database_example home/host/path/my_virtual_db To skip details and directly run the virtual plant imager , go now to section [2.] (#2-generate-virtual-images-from-a-lpy-virtual-plant-in-a-virtual-scene) 1.2 Customize data of the virtual plant and/or of the virtual images Link Warning For advanced users. If you modify data, you most likely need to modify the configuration .toml file downstream. You can modify and enrich the virtual dataset in several manner (modifying the LPy model and parameters, importing your own model and avoiding Lpy-generation, change background scenes, etc...). For all these options, please refer to the specifications of the virtual plant imager . 2. Generate virtual images from a (Lpy) virtual plant in a virtual scene Link Start the docker container of the plant-imager with your database mounted: cd plant-imager/docker ./run.sh -db /home/host/path/my_virtual_db # This will map your working databse to the `db` directory located in the docker's user home Then, in this docker container, generate the virtual dataset by running the following command: ( lpyEnv ) user@5c9e389f223d romi_run_task --config plant-imager/config/vscan_lpy_blender.toml VirtualScan db/my_virtual_plant # Run VirtualScan by specifying the output folder 'my_virtual_plant' The computation can take a few minutes, depending on your system capacities. if it works, the terminal should display something like that: ===== Luigi Execution Summary ===== Scheduled 4 tasks of which: * 2 complete ones were encountered: - 1 LpyFileset ( scan_id = vscan_data ) - 1 PaletteFileset ( scan_id = vscan_data ) * 2 ran successfully: - 1 VirtualPlant ( ... ) - 1 VirtualScan ( ... ) This progress looks : ) because there were no failed tasks or missing dependencies Results : in your database, a new folder (here called my_virtual_plant) should have been created, that contain data and metadata related to the virtual image acquisition of this virtual plant ! my_virtual_db \u2502 romidb \u2514\u2500\u2500\u2500vscan_data/ \u2514\u2500\u2500\u2500my_virtual_plant/ \u2502 \u251c\u2500\u2500 images/ \u2502 \u251c\u2500\u2500 metadata/ \u2502 \u2502 \u2514\u2500\u2500 images/ \u2502 \u2502 \u2514\u2500\u2500 images.json \u2502 \u2514\u2500\u2500 files.json \u2502 \u2514\u2500\u2500 scan.toml With the default parameters provided with this example (Lpy model and configuration file), there is only one generated plant, which has the following main characteristics It is a model of an Arabidopsis thaliana plant It has only a main stem and no lateral branches (simplified architecture) It is a mature plant, that has grown an elongated inflorescence stem bearing several mature fruit (called a 'silique', the typical pod of the Brassicaceae family) and still has some flowers at the very tip. In the next two sections, we point to simple paramaters of the configuration file used for this task to modify either the virtual plant or the virtual imaging. 2.1 (optional) how to modify the virtual plant with LPy parameters Link Note Detailed description can be found here Below are the main lpy parameters that can be customized to change how the virtual plant looks like (age, size, branching, etc...). [VirtualPlant.lpy_globals] BRANCHON = false MEAN_NB_DAYS = 70 STDEV_NB_DAYS = 5 BETA = 51 INTERNODE_LENGTH = 1.3 STEM_DIAMETER = 0.09 2.2 (optional) how to modify the virtual imaging performed by the virtual imager Link Note Detailed description can be found here Below are the main lpy parameters that can be customized to change how the virtual images are taken (path, background scenes, resolution, etc...) virtual camera path [ScanPath] class_name = \"Circle\" [ScanPath.kwargs] center_x = -2 center_y = 3 z = 65 tilt = 8 radius = 75 n_points = 18 3. Running a reconstruction pipeline on the virtual dataset Link Once you have a virtual dataset of images that all look like a real one, you can analyze it like a real one with romi pipelines from our plant-3d-vision tool suite ! Remember that the pipeline you want to evaluate is defined by the following configuration file: test_pipe.toml . To adapt to the virtual imaging and focus the evaluation to the downstream image analysis and 3D reconstruction, you can adapt the configuration file to include ground truth from virtual imaging to use ground truth poses. Create a new configuration file for the evaluation and modify it as follows: cp test_pipe.toml test_pipe_veval.toml #copy and rename the configuration file of the pipeline you want to test In the newly created test_pipe_veval.toml , deactivate use of colmap poses for the volume carving algorithm ([Voxel] Task of the pipeline). [Voxels] use_colmap_poses = false [Masks] upstream_task = Scan Then the analysis pipeline can be run as usual except that colmap will not be run : romi_run_task \\ #romi pipeline scheduler --config path/to/test_pipe_veval.toml \\ # configuration of the pipeline to evaluate AnglesAndInternodes \\ #Last task to execute /path/to/my_virtual_plant #folder inside the database on which the analysis will be run This run should process all dependencies and generates notably a segmented point cloud and measures of the phyllotaxis (angles and internodes) ! Note any available romi Tasks for image analysis can be runned here. Please refer to the list of Tasks implemented in our romi software suite. Note The command line can be executed in docker container or in a terminal if you have activated the correct virtual environments and proceeded to local installation of the software. Please refer to this tutorial if you encounter problems to run pipeline from our plant-3d-vision tool suite. After execution, the terminal should display luigi execution summary, as in this example: ===== Luigi Execution Summary ===== Scheduled 8 tasks of which: * 2 complete ones were encountered: - 1 ImagesFilesetExists ( scan_id = , fileset_id = images ) - 1 ModelFileset ( scan_id = models ) * 6 ran successfully: - 1 AnglesAndInternodes ( ... ) - 1 OrganSegmentation ( scan_id = , upstream_task = SegmentedPointCloud, eps = 2 .0, min_points = 5 ) - 1 PointCloud ( ... ) - 1 Segmentation2D ( ... ) - 1 SegmentedPointCloud ( scan_id = , upstream_task = PointCloud, upstream_segmentation = Segmentation2D, use_colmap_poses = False ) ... This progress looks : ) because there were no failed tasks or missing dependencies ===== Luigi Execution Summary ===== Results : new subfolders and metadata resulting from the analysis should have been created in the folder of the analyzed plant datset (called my_virtual_plant in this example). The particular data generated depends on the pipeline you called. We provide an example here with a pipeline involving machine-learning based segmentation of 2D images and proceeding up to phyllotaxis measures (angles & internodes.) legend : * (.) indicates data that were already present before the run of the pipeline (but the data content may have been modified) * folder names generated by the analysis generally start with the corresponding Task name end with a hashcode to keep track of task execution by the Luigi scheduler (e.g. _1_0_2_0_0_1_5f7aad388e). Such code is replaced by '_hashcode' suffix in the example below my_virtual_db \u2502 romidb (.) \u2514\u2500\u2500\u2500vscan_data/ (.) \u2514\u2500\u2500\u2500my_virtual_plant/ (.) \u2502 \u251c\u2500\u2500 images/ (.) \u2502 \u251c\u2500\u2500 metadata/ (.) \u2502 \u251c\u2500\u2500 AnglesAndInternodes_hashcode/ \u2502 \u251c\u2500\u2500 OrganSegmentation_hashcode/ \u2502 \u251c\u2500\u2500 PointCloud_hashcode/ \u2502 \u251c\u2500\u2500 Segmentation2D_hashcode/ \u2502 \u251c\u2500\u2500 PointCloudGroundTruth_100000__VirtualPlantObj_hashcode/ \u2502 \u251c\u2500\u2500 SegmentedPointCloud__Segmentation2D_PointCloud_3a1e8e0010/ \u2502 \u251c\u2500\u2500 VirtualPlant/ \u2502 \u251c\u2500\u2500 VirtualPlant_arabidopsis_note___BRANCHON___fal___angles____inte_hashcode/ \u2502 \u251c\u2500\u2500 Voxels_False___background_____False_hashcode/ \u2502 \u2514\u2500\u2500 files.json (.) \u2502 \u2514\u2500\u2500 scan.toml (.) \u2502 \u2514\u2500\u2500 pipeline.toml 4. Evaluate the quality of the construction by comparing to the virtual ground truth data Link (work in progress) Once the analysis results are generated, you can now compare this results to the expected ground truth reference of the virtual plant. Several Evaluation Tasks have been developped by romi: check the list to know which results is evaluating each of them. In the below example, we would like to evaluate the pointcloud reconstruction, so we run: romi_run_task \\ #romi pipeline scheduler --config path/to/test_pipe_veval.toml \\ # configuration of the pipeline to evaluate PointCloudEvaluation \\ #evaluation task of to run /path/to/my_virtual_plant #analyzed data folder of the database that you want to evaluate Note Please refer to this [tutorial] if you encounter problems to run pipeline from our plant-3d-vision tool suite. 5. View and scrutinize in 3D all data generated (images, reconstruction and evalutation) Link (work in progress) Use of the plant-3d-explorer","title":"How to evaluate an analysis in a virtual world (both virtual plant and imager) ? "},{"location":"Scanner/tutorials/virtual_plant_imager_single_dataset/#how-to-evaluate-a-3d-reconstruction-and-automated-measures-with-a-virtual-plant-as-ground-truth","text":"","title":"How to evaluate a 3D reconstruction and automated measures with a virtual plant as ground truth"},{"location":"Scanner/tutorials/virtual_plant_imager_single_dataset/#objective","text":"Quantitative evaluation of a 3D reconstruction and/or automated measure from a phenotyping experiment is critical, from both developer and end-user perspectives. However, obtaining ground truth reference is often tedious (e.g. manual measurements, it must be anticipated (synchronous measures with image acquisition), and some type of data are just inacessible with available technologies (e.g. having a reference point cloud). Virtual plants makes data acquisition inexpensive and allows to parametrize the type of data. By design, ground truth data can be easily extracted from these virtual datasets. We thus designed the Virtual Plant Imager to take images of any 3D object, as a digital twin of our real plant imager . After reading this tutorial, you should be able to generate a single virtual plant dataset (including several ground truth reference) in order to evaluate the phenotyping results generated through an analysis pipeline made with our plant-3d-vision tool suite.","title":"Objective"},{"location":"Scanner/tutorials/virtual_plant_imager_single_dataset/#prerequisite","text":"We highly recommend the use of dockers to run ROMI software. If it is not already done, you must be able to build and run the docker images of: the (Virtual) Plant Imager by following these instructions . This is required to generate the virtual data (initial plant 3D model, ground truth and RGB images). the plant-3d-vision by following these instructions . This is required to reconstruct a 3D model from the virtual 2D images, as if they were images of real plants. This docker will also allow you to evaluate this reconstruction using the available virtual ground truth data.","title":"Prerequisite"},{"location":"Scanner/tutorials/virtual_plant_imager_single_dataset/#step-by-step-tutorial","text":"Principle : You want to evaluate the results generated by an analysis pipeline made with our plant-3d-vision tool suite. Let's say that this pipeline is defined by a typical configuration file, test_pipe.toml . The idea is to generate images of a virtual 3D plant and provide these picture as input to the tested analysis pipeline. Technically, the Virtual Plant Imager relies on Blender v2.81a to generate at set of (2D) RGB images from the plant 3d model, mimicking what a real camera would do on a real plant. Any virtual camera pose can be generated (ie. distance, angle), but virtual poses similar to the real robot ( plant imager ) are preferred. An HTTP server acts as an interface to drive Blender generation scripts. The virtual plant 3D model (with some of its ground truth references) can be imported and given as an input. However, we provide an integrated procedure to generate a virtual 3D plant directly \"on the fly\" with Lpy , using a Lpy model and customizable parameters. Some ground truth references will also be automatically generated. Once this virtual plant has been virtually imaged, there are all data and metadata required to run an analysis with the tested pipeline. The results of this analysis will be compared to the virtual ground truth. Four type of evaluations are currently implemented : evaluation of a 2D segmentation evaluation of a 3D segmentation of the point cloud comparison of point cloud similarity evaluation of phyllotaxis measures (angles and internodes)","title":"Step-by-step tutorial"},{"location":"Scanner/tutorials/virtual_plant_imager_single_dataset/#1-prepare-data-into-a-proper-database","text":"First, create a working directory on your host machine, let's say home/host/path/my_virtual_db . You can find an example of such a directory here . This working directory is a proper \"romi\" database which contains additionnal data for the virtual plant generation and/or imaging grouped in a so-called `vscan_data folder: Legend : (*) the name is fixed and cannot be changed (!) the folder/file must exist (no tag means that the folder is not required for the program to run) my_virtual_db/ \u2502 romidb (!*) # a (empty) marker file for recognition by the plantdb module \u2514\u2500\u2500\u2500vscan_data/ (!*) \u2502 \u2514\u2500\u2500\u2500hdri/ (*) \u2502 \u2502 hdri_file1.hdr \u2502 \u2502 hdri_file2.hdr \u2502 \u2502 etc... \u2502 \u2514\u2500\u2500\u2500lpy/ (*) \u2502 \u2502 my_plant_species_model.lpy \u2502 \u2514\u2500\u2500\u2500obj/ (*) \u2502 \u2502 VirtualPlant.obj \u2502 \u2502 VirtualPlant_mtl \u2502 \u2514\u2500\u2500\u2500metadata/(!*) \u2502 \u2502 hdri.json \u2502 \u2502 lpy.json \u2502 \u2502 obj.json \u2502 \u2502 palette.json \u2502 \u2502 scenes.json \u2502 \u2514\u2500\u2500\u2500palette/ (*) \u2502 \u2502 my_plant_species_model.png \u2502 \u2514\u2500\u2500\u2500scenes/ (*) \u2502 files.json","title":"1. Prepare data into a proper database"},{"location":"Scanner/tutorials/virtual_plant_imager_single_dataset/#11-quick-ready-to-use-example","text":"Recommended if you are not familiar with the virtual plant imager . You can directly obtain a functional working directory from the repository of the plant-imager you cloned in your host machine So if your working directory is named my_virtual_db , execute in a terminal: cd plant-imager # enter the cloned repository in your host machine cp -r database_example home/host/path/my_virtual_db To skip details and directly run the virtual plant imager , go now to section [2.] (#2-generate-virtual-images-from-a-lpy-virtual-plant-in-a-virtual-scene)","title":"1.1 quick ready-to-use example"},{"location":"Scanner/tutorials/virtual_plant_imager_single_dataset/#12-customize-data-of-the-virtual-plant-andor-of-the-virtual-images","text":"Warning For advanced users. If you modify data, you most likely need to modify the configuration .toml file downstream. You can modify and enrich the virtual dataset in several manner (modifying the LPy model and parameters, importing your own model and avoiding Lpy-generation, change background scenes, etc...). For all these options, please refer to the specifications of the virtual plant imager .","title":"1.2 Customize data of the virtual plant and/or of the virtual images"},{"location":"Scanner/tutorials/virtual_plant_imager_single_dataset/#2-generate-virtual-images-from-a-lpy-virtual-plant-in-a-virtual-scene","text":"Start the docker container of the plant-imager with your database mounted: cd plant-imager/docker ./run.sh -db /home/host/path/my_virtual_db # This will map your working databse to the `db` directory located in the docker's user home Then, in this docker container, generate the virtual dataset by running the following command: ( lpyEnv ) user@5c9e389f223d romi_run_task --config plant-imager/config/vscan_lpy_blender.toml VirtualScan db/my_virtual_plant # Run VirtualScan by specifying the output folder 'my_virtual_plant' The computation can take a few minutes, depending on your system capacities. if it works, the terminal should display something like that: ===== Luigi Execution Summary ===== Scheduled 4 tasks of which: * 2 complete ones were encountered: - 1 LpyFileset ( scan_id = vscan_data ) - 1 PaletteFileset ( scan_id = vscan_data ) * 2 ran successfully: - 1 VirtualPlant ( ... ) - 1 VirtualScan ( ... ) This progress looks : ) because there were no failed tasks or missing dependencies Results : in your database, a new folder (here called my_virtual_plant) should have been created, that contain data and metadata related to the virtual image acquisition of this virtual plant ! my_virtual_db \u2502 romidb \u2514\u2500\u2500\u2500vscan_data/ \u2514\u2500\u2500\u2500my_virtual_plant/ \u2502 \u251c\u2500\u2500 images/ \u2502 \u251c\u2500\u2500 metadata/ \u2502 \u2502 \u2514\u2500\u2500 images/ \u2502 \u2502 \u2514\u2500\u2500 images.json \u2502 \u2514\u2500\u2500 files.json \u2502 \u2514\u2500\u2500 scan.toml With the default parameters provided with this example (Lpy model and configuration file), there is only one generated plant, which has the following main characteristics It is a model of an Arabidopsis thaliana plant It has only a main stem and no lateral branches (simplified architecture) It is a mature plant, that has grown an elongated inflorescence stem bearing several mature fruit (called a 'silique', the typical pod of the Brassicaceae family) and still has some flowers at the very tip. In the next two sections, we point to simple paramaters of the configuration file used for this task to modify either the virtual plant or the virtual imaging.","title":"2. Generate virtual images from a (Lpy) virtual plant in a virtual scene"},{"location":"Scanner/tutorials/virtual_plant_imager_single_dataset/#21-optional-how-to-modify-the-virtual-plant-with-lpy-parameters","text":"Note Detailed description can be found here Below are the main lpy parameters that can be customized to change how the virtual plant looks like (age, size, branching, etc...). [VirtualPlant.lpy_globals] BRANCHON = false MEAN_NB_DAYS = 70 STDEV_NB_DAYS = 5 BETA = 51 INTERNODE_LENGTH = 1.3 STEM_DIAMETER = 0.09","title":"2.1 (optional) how to modify the virtual plant with LPy parameters"},{"location":"Scanner/tutorials/virtual_plant_imager_single_dataset/#22-optional-how-to-modify-the-virtual-imaging-performed-by-the-virtual-imager","text":"Note Detailed description can be found here Below are the main lpy parameters that can be customized to change how the virtual images are taken (path, background scenes, resolution, etc...) virtual camera path [ScanPath] class_name = \"Circle\" [ScanPath.kwargs] center_x = -2 center_y = 3 z = 65 tilt = 8 radius = 75 n_points = 18","title":"2.2 (optional) how to modify the virtual imaging performed by the virtual imager"},{"location":"Scanner/tutorials/virtual_plant_imager_single_dataset/#3-running-a-reconstruction-pipeline-on-the-virtual-dataset","text":"Once you have a virtual dataset of images that all look like a real one, you can analyze it like a real one with romi pipelines from our plant-3d-vision tool suite ! Remember that the pipeline you want to evaluate is defined by the following configuration file: test_pipe.toml . To adapt to the virtual imaging and focus the evaluation to the downstream image analysis and 3D reconstruction, you can adapt the configuration file to include ground truth from virtual imaging to use ground truth poses. Create a new configuration file for the evaluation and modify it as follows: cp test_pipe.toml test_pipe_veval.toml #copy and rename the configuration file of the pipeline you want to test In the newly created test_pipe_veval.toml , deactivate use of colmap poses for the volume carving algorithm ([Voxel] Task of the pipeline). [Voxels] use_colmap_poses = false [Masks] upstream_task = Scan Then the analysis pipeline can be run as usual except that colmap will not be run : romi_run_task \\ #romi pipeline scheduler --config path/to/test_pipe_veval.toml \\ # configuration of the pipeline to evaluate AnglesAndInternodes \\ #Last task to execute /path/to/my_virtual_plant #folder inside the database on which the analysis will be run This run should process all dependencies and generates notably a segmented point cloud and measures of the phyllotaxis (angles and internodes) ! Note any available romi Tasks for image analysis can be runned here. Please refer to the list of Tasks implemented in our romi software suite. Note The command line can be executed in docker container or in a terminal if you have activated the correct virtual environments and proceeded to local installation of the software. Please refer to this tutorial if you encounter problems to run pipeline from our plant-3d-vision tool suite. After execution, the terminal should display luigi execution summary, as in this example: ===== Luigi Execution Summary ===== Scheduled 8 tasks of which: * 2 complete ones were encountered: - 1 ImagesFilesetExists ( scan_id = , fileset_id = images ) - 1 ModelFileset ( scan_id = models ) * 6 ran successfully: - 1 AnglesAndInternodes ( ... ) - 1 OrganSegmentation ( scan_id = , upstream_task = SegmentedPointCloud, eps = 2 .0, min_points = 5 ) - 1 PointCloud ( ... ) - 1 Segmentation2D ( ... ) - 1 SegmentedPointCloud ( scan_id = , upstream_task = PointCloud, upstream_segmentation = Segmentation2D, use_colmap_poses = False ) ... This progress looks : ) because there were no failed tasks or missing dependencies ===== Luigi Execution Summary ===== Results : new subfolders and metadata resulting from the analysis should have been created in the folder of the analyzed plant datset (called my_virtual_plant in this example). The particular data generated depends on the pipeline you called. We provide an example here with a pipeline involving machine-learning based segmentation of 2D images and proceeding up to phyllotaxis measures (angles & internodes.) legend : * (.) indicates data that were already present before the run of the pipeline (but the data content may have been modified) * folder names generated by the analysis generally start with the corresponding Task name end with a hashcode to keep track of task execution by the Luigi scheduler (e.g. _1_0_2_0_0_1_5f7aad388e). Such code is replaced by '_hashcode' suffix in the example below my_virtual_db \u2502 romidb (.) \u2514\u2500\u2500\u2500vscan_data/ (.) \u2514\u2500\u2500\u2500my_virtual_plant/ (.) \u2502 \u251c\u2500\u2500 images/ (.) \u2502 \u251c\u2500\u2500 metadata/ (.) \u2502 \u251c\u2500\u2500 AnglesAndInternodes_hashcode/ \u2502 \u251c\u2500\u2500 OrganSegmentation_hashcode/ \u2502 \u251c\u2500\u2500 PointCloud_hashcode/ \u2502 \u251c\u2500\u2500 Segmentation2D_hashcode/ \u2502 \u251c\u2500\u2500 PointCloudGroundTruth_100000__VirtualPlantObj_hashcode/ \u2502 \u251c\u2500\u2500 SegmentedPointCloud__Segmentation2D_PointCloud_3a1e8e0010/ \u2502 \u251c\u2500\u2500 VirtualPlant/ \u2502 \u251c\u2500\u2500 VirtualPlant_arabidopsis_note___BRANCHON___fal___angles____inte_hashcode/ \u2502 \u251c\u2500\u2500 Voxels_False___background_____False_hashcode/ \u2502 \u2514\u2500\u2500 files.json (.) \u2502 \u2514\u2500\u2500 scan.toml (.) \u2502 \u2514\u2500\u2500 pipeline.toml","title":"3. Running a reconstruction pipeline on the virtual dataset"},{"location":"Scanner/tutorials/virtual_plant_imager_single_dataset/#4-evaluate-the-quality-of-the-construction-by-comparing-to-the-virtual-ground-truth-data","text":"(work in progress) Once the analysis results are generated, you can now compare this results to the expected ground truth reference of the virtual plant. Several Evaluation Tasks have been developped by romi: check the list to know which results is evaluating each of them. In the below example, we would like to evaluate the pointcloud reconstruction, so we run: romi_run_task \\ #romi pipeline scheduler --config path/to/test_pipe_veval.toml \\ # configuration of the pipeline to evaluate PointCloudEvaluation \\ #evaluation task of to run /path/to/my_virtual_plant #analyzed data folder of the database that you want to evaluate Note Please refer to this [tutorial] if you encounter problems to run pipeline from our plant-3d-vision tool suite.","title":"4. Evaluate the quality of the construction by comparing to the virtual ground truth data"},{"location":"Scanner/tutorials/virtual_plant_imager_single_dataset/#5-view-and-scrutinize-in-3d-all-data-generated-images-reconstruction-and-evalutation","text":"(work in progress) Use of the plant-3d-explorer","title":"5. View and scrutinize in 3D all data generated  (images, reconstruction and evalutation)"},{"location":"plant-3d-explorer/","text":"This page will describe how to set up and start the plant visualizer. Dependencies Link This project is build using Node JS. As such, you need to install Node JS and npm (which should come with node). !!! important To make sure everything works as intended, check that your version of Node is at least 10, and your version of npm is at lease 6. Installing packages and setting up the environment Link After making sure you have the right versions of Node and npm, you will need to clone the repository of the project. Simply run git clone https://github.com/romi/plant-3d-explorer The next step is to install everything the app needs with the following command: npm install If you want the app to use the remote server to fetch data, run the following command: echo \"REACT_APP_API_URL='https://db.romi-project.eu'\" > .env.local If you don't, the app will use a local server at address localhost:5000 . In order to do this, you need to get plantdb running. Starting the app Link To start a development server (used to develop, or simply test the app), run: npm start The app will then run at address localhost:3000 . !!! warning Using Google Chrome (or Chromium Browser) is recommended as some problems have been encountered on Firefox due to some libraries we used. More commands Link To see a more detailed list of available commands, visit the GitHub repository .","title":"Home"},{"location":"plant-3d-explorer/#dependencies","text":"This project is build using Node JS. As such, you need to install Node JS and npm (which should come with node). !!! important To make sure everything works as intended, check that your version of Node is at least 10, and your version of npm is at lease 6.","title":"Dependencies"},{"location":"plant-3d-explorer/#installing-packages-and-setting-up-the-environment","text":"After making sure you have the right versions of Node and npm, you will need to clone the repository of the project. Simply run git clone https://github.com/romi/plant-3d-explorer The next step is to install everything the app needs with the following command: npm install If you want the app to use the remote server to fetch data, run the following command: echo \"REACT_APP_API_URL='https://db.romi-project.eu'\" > .env.local If you don't, the app will use a local server at address localhost:5000 . In order to do this, you need to get plantdb running.","title":"Installing packages and setting up the environment"},{"location":"plant-3d-explorer/#starting-the-app","text":"To start a development server (used to develop, or simply test the app), run: npm start The app will then run at address localhost:3000 . !!! warning Using Google Chrome (or Chromium Browser) is recommended as some problems have been encountered on Firefox due to some libraries we used.","title":"Starting the app"},{"location":"plant-3d-explorer/#more-commands","text":"To see a more detailed list of available commands, visit the GitHub repository .","title":"More commands"},{"location":"plant-3d-explorer/guide/","text":"The scan list page Link When first opening the app, you will be greeted with a home page displaying every available scans. A scan is defined as a folder containing acquisition of plant data (e.g. 2D RGB images, manual measures) and a set of 3D reconstructions and analysis (e.g. point cloud, mesh, automated measures,...) From this page, you can search for specific keywords, order the scans by their name, date, etc. In addition, scans can be filtered according to what data is actually available for each scan. Note If you're a developer, you can download the metadata and archives associated with each scan as well. The viewer Link Clicking the Open green button next to a scan (far right of the row corresponding to a scan) to open the actual viewer and explore available data. The 3D reconstruction and the 3D-view panel Link The largest panel on the left displays the 3D view, allowing to observe the reconstructed plant and navigate around with basic mouse commands (right/left click and scroll). Check the question mark (?) help icon in the bottom right corner of this panel for more description of mouse control. Several icons appear on the top band of the 3D-view panel: they can simply be activated by clicking, hovering over provide information about their function. Feel free to experiment with the different features and tools of the 3D view. The graphs Link The right side consists of a measurement panels, typically displaying graphs of data related to the plant (either measurements provided as metadata or computed during the analysis). Help tooltips explain what those graphs correspond to. By default, phyllotaxis graphs (see below) are displayed. The green \"plus\" (+) icon allow uploading more measurement panels if available. On each graph, the top right corner cross button closes the panel. Phyllotaxis graphs Link Phyllotaxis graphs are sequences of either divergence angles (in degree) or internode length (in mm) measured between two consecutive lateral organs of the main stem, starting from the base to the shoot tip. Hovering a graph highlights a particular interval and display the order index of the two organs lateral organs bounding this interval. Interval hovering is synchronized between the two phyllotaxis graphs (divergence angles and internodes) if they are active. Synchronization also happen with the 3D-view panel if 'organ highlight' option is active: only the corresponding pair of organ remain visible. Changing organ colors for this pair in the 3D-view panel will also synchronize the colors in the graphs. Clicking on an interval in the graph allow to maintain the selection active. Download buttons at the top of each graph allow to get the data in CSV or TSV format. The photo carousel and camera mode Link The entire bottom of the page is a line called the 'carousel': it contains images contained in the scan folder. Click on any of the images in the carousel, and the 3D-view panel will switch into the camera mode: the available 3D reconstructions will be superposed on the image, allowing to check the accuracy of the 3D reconstruction. The image of the carousel currently displayed in the 3D-View panel is boxed. Dragging this box left/rightwards changes the active image, and the 3D-view is synchronized immediately, allowing to dynamically navigate along the original camera path of acquisition. If the scan folder contain several images that have been made available for the visualizer, the source of images can be changed. The carousel will be populated by this new source and images can be displayed into the 3D viewer by activating the camera mode. Reporting bugs Link If you encounter some kind of unwanted behavior, or have a feature suggestion, head over to the GitHub repository and write an issue!","title":"Guide"},{"location":"plant-3d-explorer/guide/#the-scan-list-page","text":"When first opening the app, you will be greeted with a home page displaying every available scans. A scan is defined as a folder containing acquisition of plant data (e.g. 2D RGB images, manual measures) and a set of 3D reconstructions and analysis (e.g. point cloud, mesh, automated measures,...) From this page, you can search for specific keywords, order the scans by their name, date, etc. In addition, scans can be filtered according to what data is actually available for each scan. Note If you're a developer, you can download the metadata and archives associated with each scan as well.","title":"The scan list page"},{"location":"plant-3d-explorer/guide/#the-viewer","text":"Clicking the Open green button next to a scan (far right of the row corresponding to a scan) to open the actual viewer and explore available data.","title":"The viewer"},{"location":"plant-3d-explorer/guide/#the-3d-reconstruction-and-the-3d-view-panel","text":"The largest panel on the left displays the 3D view, allowing to observe the reconstructed plant and navigate around with basic mouse commands (right/left click and scroll). Check the question mark (?) help icon in the bottom right corner of this panel for more description of mouse control. Several icons appear on the top band of the 3D-view panel: they can simply be activated by clicking, hovering over provide information about their function. Feel free to experiment with the different features and tools of the 3D view.","title":"The 3D reconstruction and the 3D-view panel"},{"location":"plant-3d-explorer/guide/#the-graphs","text":"The right side consists of a measurement panels, typically displaying graphs of data related to the plant (either measurements provided as metadata or computed during the analysis). Help tooltips explain what those graphs correspond to. By default, phyllotaxis graphs (see below) are displayed. The green \"plus\" (+) icon allow uploading more measurement panels if available. On each graph, the top right corner cross button closes the panel.","title":"The graphs"},{"location":"plant-3d-explorer/guide/#phyllotaxis-graphs","text":"Phyllotaxis graphs are sequences of either divergence angles (in degree) or internode length (in mm) measured between two consecutive lateral organs of the main stem, starting from the base to the shoot tip. Hovering a graph highlights a particular interval and display the order index of the two organs lateral organs bounding this interval. Interval hovering is synchronized between the two phyllotaxis graphs (divergence angles and internodes) if they are active. Synchronization also happen with the 3D-view panel if 'organ highlight' option is active: only the corresponding pair of organ remain visible. Changing organ colors for this pair in the 3D-view panel will also synchronize the colors in the graphs. Clicking on an interval in the graph allow to maintain the selection active. Download buttons at the top of each graph allow to get the data in CSV or TSV format.","title":"Phyllotaxis graphs"},{"location":"plant-3d-explorer/guide/#the-photo-carousel-and-camera-mode","text":"The entire bottom of the page is a line called the 'carousel': it contains images contained in the scan folder. Click on any of the images in the carousel, and the 3D-view panel will switch into the camera mode: the available 3D reconstructions will be superposed on the image, allowing to check the accuracy of the 3D reconstruction. The image of the carousel currently displayed in the 3D-View panel is boxed. Dragging this box left/rightwards changes the active image, and the 3D-view is synchronized immediately, allowing to dynamically navigate along the original camera path of acquisition. If the scan folder contain several images that have been made available for the visualizer, the source of images can be changed. The carousel will be populated by this new source and images can be displayed into the 3D viewer by activating the camera mode.","title":"The photo carousel and camera mode"},{"location":"plant-3d-explorer/guide/#reporting-bugs","text":"If you encounter some kind of unwanted behavior, or have a feature suggestion, head over to the GitHub repository and write an issue!","title":"Reporting bugs"}]}
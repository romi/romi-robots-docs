{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Robotics for micro-farms Link ROMI is a four-year Europe-funded research project committed to promote a sustainable, local, and human-scale agriculture. The goal is to develop an open-source, affordable, multipurpose platform adapted to support organic and poly-culture market-garden farms. For more information on the ROMI project, please visit the main website at https://romi-project.eu/ . This website is dedicated to the documentation of the three devices and accompanying software developed by the ROMI project: Plant Phenotyping Farmer's Dashboard Rover Complete module list Link Type Source code link Hardware Plant Imager Cable Bot Rover Storage PlantDB Viewers Plant 3D Explorer Farmers Dashboard Algorithms Virtual Plant Imager Plant 3D Vision romiseg DTW Third-party & wrapping romicgal","title":"Robotics for micro-farms"},{"location":"#robotics-for-micro-farms","text":"ROMI is a four-year Europe-funded research project committed to promote a sustainable, local, and human-scale agriculture. The goal is to develop an open-source, affordable, multipurpose platform adapted to support organic and poly-culture market-garden farms. For more information on the ROMI project, please visit the main website at https://romi-project.eu/ . This website is dedicated to the documentation of the three devices and accompanying software developed by the ROMI project: Plant Phenotyping Farmer's Dashboard Rover","title":"Robotics for micro-farms"},{"location":"#complete-module-list","text":"Type Source code link Hardware Plant Imager Cable Bot Rover Storage PlantDB Viewers Plant 3D Explorer Farmers Dashboard Algorithms Virtual Plant Imager Plant 3D Vision romiseg DTW Third-party & wrapping romicgal","title":"Complete module list"},{"location":"about/","text":"About the ROMI project Link Project funding Link This project has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement No 773875. Research teams Link IAAC develops an aerial robot that can be used by farmers. Iaac also performs real-world tests in the experimental gardens at the Valldaura Self-Sufficient Labs and imagine end-user scenarios. They help deliver the robotics platform to new markets, managing the communication and user communities. Sony CSL is responsible for the development of the LettuceThink robot. They also contribute to the development of the computer vision and machine learning algorithms, in particular, on the 3D plant scanning and the coupling between the formal plant models and the convolutional neural networks. The Virtual Plants team brings its strong expertise in the area of 3D plant architecture reconstruction and modelling. Notably, the team develops computer pipelines to reconstruct plant architecture from 3D data, to assess their reconstruction, and to segment the architecture in its constituent organs. The Adaptive Systems Group expertise lies in models for closed-loop learning and prediction of sensorimotor data, as well as behaviour recognition and generation. The tasks planned will focus on the learning and adaptive techniques for the interaction between robots and plants. The RDP team has a deep understanding of the development and evolution of plant reproductive systems. RDP leads the advanced sensing and analysis of crops, and brings its expertise on the developmental dynamics and modelling of plant architecture. Chatelain P\u00e9pini\u00e8res runs a commercial market farm near Paris. They perform field studies to test the efficiency of the weeding robot and the usefulness of the crop monitoring applications in real-world situations. FEI provides assistance and training for projects partly funded by the European Commission, as coordinator or as partner. FEI intervenes close to them in the administrative, financial coordination and management of their projects. Official Website Link This is the documentation website of the ROMI project, to access the public project presentation , follow this link: https://romi-project.eu/ GitHub sources Link For now these sources are private. Presentation videos Link Preliminary videos to learn more about the project tools!","title":"About the ROMI project"},{"location":"about/#about-the-romi-project","text":"","title":"About the ROMI project"},{"location":"about/#project-funding","text":"This project has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement No 773875.","title":"Project funding"},{"location":"about/#research-teams","text":"IAAC develops an aerial robot that can be used by farmers. Iaac also performs real-world tests in the experimental gardens at the Valldaura Self-Sufficient Labs and imagine end-user scenarios. They help deliver the robotics platform to new markets, managing the communication and user communities. Sony CSL is responsible for the development of the LettuceThink robot. They also contribute to the development of the computer vision and machine learning algorithms, in particular, on the 3D plant scanning and the coupling between the formal plant models and the convolutional neural networks. The Virtual Plants team brings its strong expertise in the area of 3D plant architecture reconstruction and modelling. Notably, the team develops computer pipelines to reconstruct plant architecture from 3D data, to assess their reconstruction, and to segment the architecture in its constituent organs. The Adaptive Systems Group expertise lies in models for closed-loop learning and prediction of sensorimotor data, as well as behaviour recognition and generation. The tasks planned will focus on the learning and adaptive techniques for the interaction between robots and plants. The RDP team has a deep understanding of the development and evolution of plant reproductive systems. RDP leads the advanced sensing and analysis of crops, and brings its expertise on the developmental dynamics and modelling of plant architecture. Chatelain P\u00e9pini\u00e8res runs a commercial market farm near Paris. They perform field studies to test the efficiency of the weeding robot and the usefulness of the crop monitoring applications in real-world situations. FEI provides assistance and training for projects partly funded by the European Commission, as coordinator or as partner. FEI intervenes close to them in the administrative, financial coordination and management of their projects.","title":"Research teams"},{"location":"about/#official-website","text":"This is the documentation website of the ROMI project, to access the public project presentation , follow this link: https://romi-project.eu/","title":"Official Website"},{"location":"about/#github-sources","text":"For now these sources are private.","title":"GitHub sources"},{"location":"about/#presentation-videos","text":"Preliminary videos to learn more about the project tools!","title":"Presentation videos"},{"location":"data/","text":"","title":"Data"},{"location":"documentation/","text":"Robotics for micro-farms Link ROMI is a four-year Europe-funded research project committed to promote a sustainable, local, and human-scale agriculture. The goal is to develop an open-source, affordable, multipurpose platform adapted to support organic and poly-culture market-garden farms. Plant Phenotyping Crop Monitoring Rover Complete module list Link Type Source code link Hardware Plant Imager Cable Bot Rover Storage PlantDB Viewers Plant 3D Explorer Farmers Dashboard Algorithms Virtual Plant Imager Plant 3D Vision romiseg DTW Third-party & wrapping romicgal","title":"Robotics for micro-farms"},{"location":"documentation/#robotics-for-micro-farms","text":"ROMI is a four-year Europe-funded research project committed to promote a sustainable, local, and human-scale agriculture. The goal is to develop an open-source, affordable, multipurpose platform adapted to support organic and poly-culture market-garden farms. Plant Phenotyping Crop Monitoring Rover","title":"Robotics for micro-farms"},{"location":"documentation/#complete-module-list","text":"Type Source code link Hardware Plant Imager Cable Bot Rover Storage PlantDB Viewers Plant 3D Explorer Farmers Dashboard Algorithms Virtual Plant Imager Plant 3D Vision romiseg DTW Third-party & wrapping romicgal","title":"Complete module list"},{"location":"glossary/","text":"Glossary Link We hereafter defines the semantic, names and abbreviations to use in the projects documentations and communications. ROMI Software : the whole set of software developed by ROMI; ROMI Hardware : the three types of robots developed by ROMI, namely the \"cable bot\", the \"rover\" and the \"plant imager\"; Database related Link database : the database itself; scan : a set of images, and the pipelines results; fileset : a set of files ( e.g. a set of RGB images of a plant); file : a file ( e.g. an RGB image of a plant); plant metadata : set of FAIR metadata attached to the plant ( e.g. species, age, growth conditions...); acquisition metadata : set of metadata attached to the acquisition procedure & hardware configuration ( e.g. version of the CNC controller, camera settings, ...); Danger \"scans\" could be renamed \"dataset\" or !","title":"Glossary"},{"location":"glossary/#glossary","text":"We hereafter defines the semantic, names and abbreviations to use in the projects documentations and communications. ROMI Software : the whole set of software developed by ROMI; ROMI Hardware : the three types of robots developed by ROMI, namely the \"cable bot\", the \"rover\" and the \"plant imager\";","title":"Glossary"},{"location":"glossary/#database-related","text":"database : the database itself; scan : a set of images, and the pipelines results; fileset : a set of files ( e.g. a set of RGB images of a plant); file : a file ( e.g. an RGB image of a plant); plant metadata : set of FAIR metadata attached to the plant ( e.g. species, age, growth conditions...); acquisition metadata : set of metadata attached to the acquisition procedure & hardware configuration ( e.g. version of the CNC controller, camera settings, ...); Danger \"scans\" could be renamed \"dataset\" or !","title":"Database related"},{"location":"research/","text":"Research & communications Link Journal papers Link INRIA: Chaudhury A., Godin C. Skeletonization of Plant Point Cloud Data Using Stochastic Optimization Framework . Front Plant Sci. 2020;11:773. Published 2020 Jun 16. doi:10.3389/fpls.2020.00773 UBER, Sony : Schillaci G., Pico Villalpando A., Hafner V. V., Hanappe P., Colliaux D. and Wintz, T. Intrinsic motivation and episodic memories for robot exploration of high-dimensional sensory spaces . Adaptive Behavior , June 2020, doi:10.1177/1059712320922916. ( arXiv:2001.01982 ) UBER : Hafner, V. V., Loviken, P., Pico Villalpando, A., Schillaci, G. (2020). Prerequisites for an Artificial Self . Frontiers in Neurorobotics . Vol. 14, p.5. doi:10.3389/fnbot.2020.00005 . ISSN 1662-5218. Book chapters Link INRIA: A. Chaudhury and C. Godin, Geometry Reconstruction of Plants , in Intelligent Image Analysis for Plant Phenotyping , CRC Press/Taylor and Francis, 2020 (to appear). doi: doi:10.1201/9781315177304 Participation in conferences Link UBER: Pico, A., Schillaci, G., Hafner, V.V. and Lara, B. (2019). Ego-Noise Predictions for Echolocation in Wheeled Robots , Alife 2019 - The 2019 Conference on Artificial Life . pp. 567-573. MIT Press. doi:10.1162/isal_a_00222 Participation in workshops Link INRIA: Chaudhury A., Boudon, F., and Godin C. 3D Plant Phenotyping: All You Need is Labelled Point Cloud Data . Workshop on Computer Vision Problems for Plant Phenotyping, 2020. IAAC, Sony : Sollazzo A., Colliaux D., Garivani S., Minchin J., Garlanda L. and Hanappe, P. Automated vegetable growth analysis from outdoor images acquired with a cablebot . Workshop on Computer Vision Problems for Plant Phenotyping, 2020. (PDF) INRIA: Florian Ingels, contributed talk at the annual meeting of the French Statistical Society Journ\u00e9es de Statistique 2019 (Romain Aza\u00efs and Florian Ingels), 2019/06/03-07. Sony: Wintz T., Colliaux D., Hanappe P. Automated extraction of phyllotactic traits from Arabidopsis thaliana . Workshop on Computer Vision Problems in Plant Phenotyping (CVPPP), 2018 (PDF, last visited 30/10/2020). Sony: Colliaux D., Lahlou A., Wintz T., Boucher E., Garlanda L., Mac\u00e9 A., Hanappe P. Robots for data collection in biology and agriculture . Poster presentation, International Crop Modelling Symposium (iCROPM2020), 3-5 February 2020, Montpellier, France. Sony: Colliaux D., Lahlou A., Wintz T., Boucher E., Garlanda L., Mac\u00e9 A., Hanappe P. Developing low-cost robots for micro-farms: the benefits of computer vision . Poster at the Plant People Planet Symposium, London, 4\u20135 September 2019 (Abstracts book)","title":"Research & communications"},{"location":"research/#research-communications","text":"","title":"Research &amp; communications"},{"location":"research/#journal-papers","text":"INRIA: Chaudhury A., Godin C. Skeletonization of Plant Point Cloud Data Using Stochastic Optimization Framework . Front Plant Sci. 2020;11:773. Published 2020 Jun 16. doi:10.3389/fpls.2020.00773 UBER, Sony : Schillaci G., Pico Villalpando A., Hafner V. V., Hanappe P., Colliaux D. and Wintz, T. Intrinsic motivation and episodic memories for robot exploration of high-dimensional sensory spaces . Adaptive Behavior , June 2020, doi:10.1177/1059712320922916. ( arXiv:2001.01982 ) UBER : Hafner, V. V., Loviken, P., Pico Villalpando, A., Schillaci, G. (2020). Prerequisites for an Artificial Self . Frontiers in Neurorobotics . Vol. 14, p.5. doi:10.3389/fnbot.2020.00005 . ISSN 1662-5218.","title":"Journal papers"},{"location":"research/#book-chapters","text":"INRIA: A. Chaudhury and C. Godin, Geometry Reconstruction of Plants , in Intelligent Image Analysis for Plant Phenotyping , CRC Press/Taylor and Francis, 2020 (to appear). doi: doi:10.1201/9781315177304","title":"Book chapters"},{"location":"research/#participation-in-conferences","text":"UBER: Pico, A., Schillaci, G., Hafner, V.V. and Lara, B. (2019). Ego-Noise Predictions for Echolocation in Wheeled Robots , Alife 2019 - The 2019 Conference on Artificial Life . pp. 567-573. MIT Press. doi:10.1162/isal_a_00222","title":"Participation in conferences"},{"location":"research/#participation-in-workshops","text":"INRIA: Chaudhury A., Boudon, F., and Godin C. 3D Plant Phenotyping: All You Need is Labelled Point Cloud Data . Workshop on Computer Vision Problems for Plant Phenotyping, 2020. IAAC, Sony : Sollazzo A., Colliaux D., Garivani S., Minchin J., Garlanda L. and Hanappe, P. Automated vegetable growth analysis from outdoor images acquired with a cablebot . Workshop on Computer Vision Problems for Plant Phenotyping, 2020. (PDF) INRIA: Florian Ingels, contributed talk at the annual meeting of the French Statistical Society Journ\u00e9es de Statistique 2019 (Romain Aza\u00efs and Florian Ingels), 2019/06/03-07. Sony: Wintz T., Colliaux D., Hanappe P. Automated extraction of phyllotactic traits from Arabidopsis thaliana . Workshop on Computer Vision Problems in Plant Phenotyping (CVPPP), 2018 (PDF, last visited 30/10/2020). Sony: Colliaux D., Lahlou A., Wintz T., Boucher E., Garlanda L., Mac\u00e9 A., Hanappe P. Robots for data collection in biology and agriculture . Poster presentation, International Crop Modelling Symposium (iCROPM2020), 3-5 February 2020, Montpellier, France. Sony: Colliaux D., Lahlou A., Wintz T., Boucher E., Garlanda L., Mac\u00e9 A., Hanappe P. Developing low-cost robots for micro-farms: the benefits of computer vision . Poster at the Plant People Planet Symposium, London, 4\u20135 September 2019 (Abstracts book)","title":"Participation in workshops"},{"location":"Farmers%20Dashboard/","text":".embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; } The Farmers Dashboard is a farming tool that provides daily automated insights about your crop. It helps with mapping of crop beds, the location and identification of individual plants, and the extraction of their growth curves from the collected data. The dashboard benefits polycrop farmers and researchers. It opens up technology and practices common to industrial scale agriculture, making them accessible and useful to ecological and sustainable farmers. It relies on an automated system, data acquisition, a set of tools for image analytics , and finally, an online platform for spatial management and data visualization. The data can be provided by different types of devices a Cablebot, a drone or a Rover according to the configuration of each farm. Multiple iterations were needed to achieve a powerful yet robust and low cost solution. The Cablebot can be fixed above a crop bed using a tension cable, we can use the manual remote to correctly position the camera in order to capture all of the crops. Once set up, the cable bot will move multiple times a day across the crop bed, taking high definition images and sending them to the Romi server. The images are assembled into a unique portrait of your crop bed. After the plants are detected, a catalog of individual plants is created. By comparing them with historical data, we can obtain plant growth curves. All of the information is then combined into a weed map, which is made available on the Farmers Dashboard website. The current system is a fully automated imaging device, able to collect data on a high variety of crops. Because of the legal restrictions on the use of drones and because of the rapid evolution of the drone market, the ROMI project has decided to direct its effort to a hardware solution that complements the existing tools (commercial drones, the Rover): the Cablebot. The Cablebot is adapted for use in greenhouses and polytunnels. These installations take up more than 10% of microfarms and are ill-suited for the use of drones. This reorientation increases the impact of ROMI since we can handle a wider variety of contexts than planned. The use of drones is still an option. Existing drones can still be used in combination with the Farmer\u2019s Dashboard. Nature-based solutions are becoming more and more relevant to increasing cities\u2019 resilience and climate change adaptation. In particular green infrastructure is an emergent trend followed by new buildings worldwide. With it, there\u2019s an emergent market of solutions to support the growth of plants in facades, rooftops and other architectural sites. One relevant task is the monitoring of those gardens, in particular in large buildings and areas with difficult access, like facades. However, the regulations affecting urban areas limit drones and other flying equipment. For that reason, the Cable Bot solution can offer a robust and autonomous solution for permanent or temporary monitoring of rooftops or even vertical gardens. Moreover, the simplicity of the installation allows the system to be set up at lower costs and even for a temporary purpose, like monitoring the plats consolidation phase for new gardens. Finally, remote monitoring of the results via the Farmers Dashboard simplifies the overall operation by offering precise insights and remote control capabilities. To maximize the reuse of hardware components, the Cablebot consists of the following modules: A mobile carrier that can move along the cable. A camera module thats easily attached on the mobile carrier. A fixed charging station . Info For convenience, when we use the term cablebot, we sometimes refer to the mobile carrier mentioned above, and sometimes to the three modules used as a whole. In general, the meaning should be clear from the context.","title":"Introduction"},{"location":"Farmers%20Dashboard/app/","text":"","title":"Farmers Dashboard App"},{"location":"Farmers%20Dashboard/assembly/","text":"Assembly guide Link The cablebot assembly requires simple tools: Pliers, screwdrivers, allen keys and a plastic or wooden tool to apply pressure without damaging the pieces. For electronics assembly some extra tools are needed: solder station, cutting pliers, shrink tube and hot air gun. Carrier module (CARM) Link The carrier module is the most complex component and it has a lot of different parts, to make the assembly process simpler this module is divided into different groups of parts that can be constructed independently and latter joint together. Head Body Arms (left and right) Hands (Left and right) Hands (left and right) Link There are two hands on the CARM module, left and right. The assembly process for both is the same so we only show it once. This pieces hold the pulleys that are in direct contact with the cable on both extremes. They also hold the two end stops switches that detect obstacles along the path. 3d printed parts Link 1x Hand end stop trigger left 1x Hand base left 2x Pulley 608zz 1x Hand base right 1x Hand end stop trigger right Milled Alucobond parts Link 1x Hand left out small 1x Hand left middle small 1x Hand front left 1x Hand right out small 1x Hand right middle small 1x Hand front right 1x Hand left out big 1x Hand left middle big 1x Hand right out big 1x Hand right middle big Just one set is needed either small or big , depending on cable tension (big is better for high tension). Hardware parts Link 2 608zz bearing 2 D2F-01L-D3 End stop 1 Cable 28 AWG - Black - 450mm (left hand) 1 Cable 28 AWG - Green - 450mm (left hand) 1 Cable 28 AWG - Black - 110mm (right hand) 1 Cable 28 AWG - Green - 110mm (right hand) 10 Washer M8x1.5 2 Screw M8x25 2 Nut M8 4 Screw M3x32 4 Nut M8 2 Screw M2x10 2 Nut M2 Assembly Link All the needed pieces, to build the right hand. First install the end-stop (with the cables already soldered to it) inserting first the M2 nuts in the hexagonal holes. Pass the cables through the hole to the back of the printed piece. And through the front hole in the alucobond piece. Align the two holes on both pieces. Pass the cable back to the front throungh the hole and align it with the pocket on the alucobond piece. Align and put toghether the two aluminum parts making sure not to pinch the cables. After joining them check if the cable can move freely. Insert the M8 screw. Place the 5 washers on the screw. Before this step you need to pressure fit the 608zz bearing in to the 3d printed pulley, it can be done easily on a bench press, be sure to slide it until the end. We recomend applying a couple of cyanoacrylate glue drops between the two pieces. Insert the bearing and the printed pulley on the M8 screw and fix it with the nut, be sure to apply enough pressure. Hold the end-stop trigger printed piece in place. Align the aluminum cap and insert the screw. While you push the screw be sure to keep the trigger aligned. Fix the nut on the back side. While keeping aligned the printed piece insert the other screw. Tighten both nuts. Your done with the right hand assembly! Follow the same procedure with the left hand so you can start with the arms. Left Arm Link 3d printed parts Link 1x Left arm cap out 1x Pogo pin clip 1x Left arm cap middle Milled Alucobond Link 1x Arm left Right Arm Link 3d printed parts Link 1x Control board holder 1x Right arm cap middle 1x Right arm cap out Milled Alucobond Link 1x Right arm Head Link The head part of the module holds the brushless motor and the needed electronics to control it. This part slides depending on thee cable tension. 3d printed parts Link 1x Encoder cap 1x Head separator front 1x Head separator up 1x Spring holder up 1x Head separator back 1x Encoder holder Milled Alucobond Link 1x Head front 1x Head middle front 1x Head middle back 1x Head back Hardware parts Link Assembly Link Body Link Battery module (BATM) Link Camera module (CAMM) Link Charging station module (CHAM) Link","title":"Assembly instructions"},{"location":"Farmers%20Dashboard/assembly/#assembly-guide","text":"The cablebot assembly requires simple tools: Pliers, screwdrivers, allen keys and a plastic or wooden tool to apply pressure without damaging the pieces. For electronics assembly some extra tools are needed: solder station, cutting pliers, shrink tube and hot air gun.","title":"Assembly guide"},{"location":"Farmers%20Dashboard/assembly/#carrier-module-carm","text":"The carrier module is the most complex component and it has a lot of different parts, to make the assembly process simpler this module is divided into different groups of parts that can be constructed independently and latter joint together. Head Body Arms (left and right) Hands (Left and right)","title":"Carrier module (CARM)"},{"location":"Farmers%20Dashboard/assembly/#hands-left-and-right","text":"There are two hands on the CARM module, left and right. The assembly process for both is the same so we only show it once. This pieces hold the pulleys that are in direct contact with the cable on both extremes. They also hold the two end stops switches that detect obstacles along the path.","title":"Hands (left and right)"},{"location":"Farmers%20Dashboard/assembly/#3d-printed-parts","text":"1x Hand end stop trigger left 1x Hand base left 2x Pulley 608zz 1x Hand base right 1x Hand end stop trigger right","title":"3d printed parts"},{"location":"Farmers%20Dashboard/assembly/#milled-alucobond-parts","text":"1x Hand left out small 1x Hand left middle small 1x Hand front left 1x Hand right out small 1x Hand right middle small 1x Hand front right 1x Hand left out big 1x Hand left middle big 1x Hand right out big 1x Hand right middle big Just one set is needed either small or big , depending on cable tension (big is better for high tension).","title":"Milled Alucobond parts"},{"location":"Farmers%20Dashboard/assembly/#hardware-parts","text":"2 608zz bearing 2 D2F-01L-D3 End stop 1 Cable 28 AWG - Black - 450mm (left hand) 1 Cable 28 AWG - Green - 450mm (left hand) 1 Cable 28 AWG - Black - 110mm (right hand) 1 Cable 28 AWG - Green - 110mm (right hand) 10 Washer M8x1.5 2 Screw M8x25 2 Nut M8 4 Screw M3x32 4 Nut M8 2 Screw M2x10 2 Nut M2","title":"Hardware parts"},{"location":"Farmers%20Dashboard/assembly/#assembly","text":"All the needed pieces, to build the right hand. First install the end-stop (with the cables already soldered to it) inserting first the M2 nuts in the hexagonal holes. Pass the cables through the hole to the back of the printed piece. And through the front hole in the alucobond piece. Align the two holes on both pieces. Pass the cable back to the front throungh the hole and align it with the pocket on the alucobond piece. Align and put toghether the two aluminum parts making sure not to pinch the cables. After joining them check if the cable can move freely. Insert the M8 screw. Place the 5 washers on the screw. Before this step you need to pressure fit the 608zz bearing in to the 3d printed pulley, it can be done easily on a bench press, be sure to slide it until the end. We recomend applying a couple of cyanoacrylate glue drops between the two pieces. Insert the bearing and the printed pulley on the M8 screw and fix it with the nut, be sure to apply enough pressure. Hold the end-stop trigger printed piece in place. Align the aluminum cap and insert the screw. While you push the screw be sure to keep the trigger aligned. Fix the nut on the back side. While keeping aligned the printed piece insert the other screw. Tighten both nuts. Your done with the right hand assembly! Follow the same procedure with the left hand so you can start with the arms.","title":"Assembly"},{"location":"Farmers%20Dashboard/assembly/#left-arm","text":"","title":"Left Arm"},{"location":"Farmers%20Dashboard/assembly/#3d-printed-parts_1","text":"1x Left arm cap out 1x Pogo pin clip 1x Left arm cap middle","title":"3d printed parts"},{"location":"Farmers%20Dashboard/assembly/#milled-alucobond","text":"1x Arm left","title":"Milled Alucobond"},{"location":"Farmers%20Dashboard/assembly/#right-arm","text":"","title":"Right Arm"},{"location":"Farmers%20Dashboard/assembly/#3d-printed-parts_2","text":"1x Control board holder 1x Right arm cap middle 1x Right arm cap out","title":"3d printed parts"},{"location":"Farmers%20Dashboard/assembly/#milled-alucobond_1","text":"1x Right arm","title":"Milled Alucobond"},{"location":"Farmers%20Dashboard/assembly/#head","text":"The head part of the module holds the brushless motor and the needed electronics to control it. This part slides depending on thee cable tension.","title":"Head"},{"location":"Farmers%20Dashboard/assembly/#3d-printed-parts_3","text":"1x Encoder cap 1x Head separator front 1x Head separator up 1x Spring holder up 1x Head separator back 1x Encoder holder","title":"3d printed parts"},{"location":"Farmers%20Dashboard/assembly/#milled-alucobond_2","text":"1x Head front 1x Head middle front 1x Head middle back 1x Head back","title":"Milled Alucobond"},{"location":"Farmers%20Dashboard/assembly/#hardware-parts_1","text":"","title":"Hardware parts"},{"location":"Farmers%20Dashboard/assembly/#assembly_1","text":"","title":"Assembly"},{"location":"Farmers%20Dashboard/assembly/#body","text":"","title":"Body"},{"location":"Farmers%20Dashboard/assembly/#battery-module-batm","text":"","title":"Battery module (BATM)"},{"location":"Farmers%20Dashboard/assembly/#camera-module-camm","text":"","title":"Camera module (CAMM)"},{"location":"Farmers%20Dashboard/assembly/#charging-station-module-cham","text":"","title":"Charging station module (CHAM)"},{"location":"Farmers%20Dashboard/bot/","text":"The Carrier module (CARM) Link Size: 388mm x 216mm x 90 Weight: 1400gr Max speed: 2m/s (software limited) Max payload: 2500gr Power consumption: 1.5Wh on rest, 45Wh normal operation. The mobile carrier is an autonomous motion platform capable of travelling suspended on a single tensioned cable. It can be attached to the cable in a few seconds and controlled manually vie RF remote control. It integrates the Romi Camera Module as image capture device to allow remote operation and autonomous scanning and image upload. Includes Battery module that recharges automatically in the Charging station module while not scanning. The primary communication is achieved via Wi-Fi to interact with the farmer phone or laptop, the local farm server or to a remote instance through the internet. Wi-Fi ensures enough bandwidth is available to perform the image uploads as well as over-the-air software updates, and it eliminates the need for customized gateways. When a remote connection is required, and the farm does not have one, a Wi-Fi to 4G (or 5G) gateway is located in the recharging station at the cable end.","title":"Introduction"},{"location":"Farmers%20Dashboard/bot/#the-carrier-module-carm","text":"Size: 388mm x 216mm x 90 Weight: 1400gr Max speed: 2m/s (software limited) Max payload: 2500gr Power consumption: 1.5Wh on rest, 45Wh normal operation. The mobile carrier is an autonomous motion platform capable of travelling suspended on a single tensioned cable. It can be attached to the cable in a few seconds and controlled manually vie RF remote control. It integrates the Romi Camera Module as image capture device to allow remote operation and autonomous scanning and image upload. Includes Battery module that recharges automatically in the Charging station module while not scanning. The primary communication is achieved via Wi-Fi to interact with the farmer phone or laptop, the local farm server or to a remote instance through the internet. Wi-Fi ensures enough bandwidth is available to perform the image uploads as well as over-the-air software updates, and it eliminates the need for customized gateways. When a remote connection is required, and the farm does not have one, a Wi-Fi to 4G (or 5G) gateway is located in the recharging station at the cable end.","title":"The Carrier module (CARM)"},{"location":"Farmers%20Dashboard/building/","text":"Get the pieces Link 3d print Link Milling Link Aluminum composite Link Some cablebot parts are milled in a composite panel consisting of two aluminium cover sheets and a polymer core, this material can also be bended if cutted at specific depth with a 90 degrees V end mill. In the cablebot repository you can find all the parts as DXF files. Gcode has to be generated with specific settings for the used CNC machine. Feeds and speeds for V - Curve milling bit to bend alucobond In our case we are using a high revolution spindle that works in between 18k-24k RPM. After some tests, we have found that this feeds and speeds work us well for cutting 3mm alucobond composite: Tool RPM Feed Feed plunge Stepdown V carve - 90\u00ba 24,000 rpm 5,000 mm/min 3,000 mm/min 0.22 mm/step 3 mm flat tool 24,000 rpm 5,000 mm/min 3,000 mm/min 1.2 mm/step 6 mm flat tool 18,000 rpm 5,000 mm/min 3,000 mm/min 1.7 mm/step Gcode generation Link In the Romi Cablebot github repository you can find all the drawings (in DXF format) to generate the needed gcode. In the files you will find different layers depending on the operation and the depth, in this example you can see the layer called vcut for the folding marks, pocket-0.95mm indicates a pocketing operation with a depth of 0.95 mm and profiling to cut the piece. Keep in mind that you will need to add bridges on your piece, so it doesn't move during machining, this process is different depending on the CAM software you use. Generating Gcode in Blender Link Blendercam is a free/libre addon that allows gcode generation inside blender, in this way we avoid the use of extra software and model exporting. Don't forget to check their documentation . To use it you need to clone the addon repository to some place in your computer: git clone https://github.com/vilemduha/blendercam Some python dependencies sould also be installed, you can do it with pip from the command line: $ ./pip3 install shapely $ ./pip3 install vtk $ ./pip3 install Equation OpenCamLib is an optional dependency, but based on our tests we recommend its installation. To activate the addon, in Blender, open the Preferences window ( edit \u2192 preferences ). Clik on File Paths button and enter the path where you cloned the blender CAM repository in the Scripts field. Save perferences and restart blender. Now enable it in Add-ons section (preferences window). Adding a post processor Link After installing the addon you will need a postprocessor script that works with your specific CNC machine. If none of the included ones works for you, you can easily create your own: 1. Modify scripts/addons/cam/__init__.py and add a new item on the machineSettings class (around line 125): 2. Create a new file in scripts/addons/cam/nc/ directory with your post processor name. (ej. raptor.py ). You can copy an existing postprocessor and modify it to fit your needs. 3. Modify scripts/addons/cam/gcodepath.py , search for the exportGcodePath() function and add a condition for your post processor where you specify the extension of the file and the name of the module you just created on step 2. There is example commit on what's needed to add a postprocessor here . It is a little outdated (use gcodepath.py instead of utils.py ) but can be used as a general guide. Steps to get folding traces Link As an example on how to get the proper traces for alucobond milling with folding parts. 1. With the object cutting side pointing up, duplicate it and rotate 90\u00ba with the bottom corner as rotation point. 2. Displace the duplicated part 2mm towards the center . That's one millimeter per side of the folding axis. 3. Repeat both steps on the other side 4. Create a line at 1mm from the part border (centered between both pieces) 5. Repeat the process for the other side, now you have the V cutt milling traces 6. Join the two parts, remove the vertices outside the bottom layer and create a bridge to join both parts. This paths should be -2.2mm from the surface of the material (leaving a thikness of 0.8mm after cutting) Now you can process the part with blendercam to get the gcode. HDPE Link For the tension adjusting slider some HDPE pieces need to be milled, the center piece is the more complicated since the milling has to be on both sides. Feeds and speeds for HDPE: Tool RPM Feed Cut Feed Plunge Stepdown 3 mm flat tool 18,000 rpm 2,000 mm/min 2,000 mm/min 1.4 mm/step 6 mm flat tool 18,000 rpm 6,000 mm/min 5,000 mm/min 1.4 mm/step","title":"Get the pieces"},{"location":"Farmers%20Dashboard/building/#get-the-pieces","text":"","title":"Get the pieces"},{"location":"Farmers%20Dashboard/building/#3d-print","text":"","title":"3d print"},{"location":"Farmers%20Dashboard/building/#milling","text":"","title":"Milling"},{"location":"Farmers%20Dashboard/building/#aluminum-composite","text":"Some cablebot parts are milled in a composite panel consisting of two aluminium cover sheets and a polymer core, this material can also be bended if cutted at specific depth with a 90 degrees V end mill. In the cablebot repository you can find all the parts as DXF files. Gcode has to be generated with specific settings for the used CNC machine. Feeds and speeds for V - Curve milling bit to bend alucobond In our case we are using a high revolution spindle that works in between 18k-24k RPM. After some tests, we have found that this feeds and speeds work us well for cutting 3mm alucobond composite: Tool RPM Feed Feed plunge Stepdown V carve - 90\u00ba 24,000 rpm 5,000 mm/min 3,000 mm/min 0.22 mm/step 3 mm flat tool 24,000 rpm 5,000 mm/min 3,000 mm/min 1.2 mm/step 6 mm flat tool 18,000 rpm 5,000 mm/min 3,000 mm/min 1.7 mm/step","title":"Aluminum composite"},{"location":"Farmers%20Dashboard/building/#gcode-generation","text":"In the Romi Cablebot github repository you can find all the drawings (in DXF format) to generate the needed gcode. In the files you will find different layers depending on the operation and the depth, in this example you can see the layer called vcut for the folding marks, pocket-0.95mm indicates a pocketing operation with a depth of 0.95 mm and profiling to cut the piece. Keep in mind that you will need to add bridges on your piece, so it doesn't move during machining, this process is different depending on the CAM software you use.","title":"Gcode generation"},{"location":"Farmers%20Dashboard/building/#generating-gcode-in-blender","text":"Blendercam is a free/libre addon that allows gcode generation inside blender, in this way we avoid the use of extra software and model exporting. Don't forget to check their documentation . To use it you need to clone the addon repository to some place in your computer: git clone https://github.com/vilemduha/blendercam Some python dependencies sould also be installed, you can do it with pip from the command line: $ ./pip3 install shapely $ ./pip3 install vtk $ ./pip3 install Equation OpenCamLib is an optional dependency, but based on our tests we recommend its installation. To activate the addon, in Blender, open the Preferences window ( edit \u2192 preferences ). Clik on File Paths button and enter the path where you cloned the blender CAM repository in the Scripts field. Save perferences and restart blender. Now enable it in Add-ons section (preferences window).","title":"Generating Gcode in Blender"},{"location":"Farmers%20Dashboard/building/#adding-a-post-processor","text":"After installing the addon you will need a postprocessor script that works with your specific CNC machine. If none of the included ones works for you, you can easily create your own: 1. Modify scripts/addons/cam/__init__.py and add a new item on the machineSettings class (around line 125): 2. Create a new file in scripts/addons/cam/nc/ directory with your post processor name. (ej. raptor.py ). You can copy an existing postprocessor and modify it to fit your needs. 3. Modify scripts/addons/cam/gcodepath.py , search for the exportGcodePath() function and add a condition for your post processor where you specify the extension of the file and the name of the module you just created on step 2. There is example commit on what's needed to add a postprocessor here . It is a little outdated (use gcodepath.py instead of utils.py ) but can be used as a general guide.","title":"Adding a post processor"},{"location":"Farmers%20Dashboard/building/#steps-to-get-folding-traces","text":"As an example on how to get the proper traces for alucobond milling with folding parts. 1. With the object cutting side pointing up, duplicate it and rotate 90\u00ba with the bottom corner as rotation point. 2. Displace the duplicated part 2mm towards the center . That's one millimeter per side of the folding axis. 3. Repeat both steps on the other side 4. Create a line at 1mm from the part border (centered between both pieces) 5. Repeat the process for the other side, now you have the V cutt milling traces 6. Join the two parts, remove the vertices outside the bottom layer and create a bridge to join both parts. This paths should be -2.2mm from the surface of the material (leaving a thikness of 0.8mm after cutting) Now you can process the part with blendercam to get the gcode.","title":"Steps to get folding traces"},{"location":"Farmers%20Dashboard/building/#hdpe","text":"For the tension adjusting slider some HDPE pieces need to be milled, the center piece is the more complicated since the milling has to be on both sides. Feeds and speeds for HDPE: Tool RPM Feed Cut Feed Plunge Stepdown 3 mm flat tool 18,000 rpm 2,000 mm/min 2,000 mm/min 1.4 mm/step 6 mm flat tool 18,000 rpm 6,000 mm/min 5,000 mm/min 1.4 mm/step","title":"HDPE"},{"location":"Farmers%20Dashboard/camera/","text":"Camera module Link Size: 148mm x 143mm x 63mm Weight: 560gr CMOS sensor: 12.3 mp, Sony IMX477 sensor. Optics: Support for C- and CS-mount lenses. Networking: Wi-Fi 2.4GHz 802.11 b/g/n Interfaces: UART Serial, I2C, SWD. Both the Cablebot and the Scanner require a reliable camera module, although the usage in both cases is slightly different For the Scanner , the camera is positioned at a given angle. The movement from angle position to the next is relativement infrequent and slow. For the Cablebot , the camera must adjust in real-time for swinging movements of the system. Controller board Link We decided to use a brushless motor, as is the custom in camera mount systems. We designed a controller board that exploits the functions offered by the TI DRV8313 chip. The DRV8313 requires as an input three Pulse Width Modulation signals (PWM) that encode the phase of each of the three voltages applied to the solenoids of the brushless motor. The control software is integrated into the code for the Romi Rover: https://github.com/romi/romi-rover-build-and-test/tree/ci_dev/romi-rover/gimbal_bldc The design files can be found at https://github.com/romi/bldc_featherwing Microcontroller Link The PWMs signals should be of a high frequency, as to avoid any ripples in the signal, and should be closely synchronised. We therefore opted for a Cortex M0 microcontroller instead of the more common AVR microcontrollers found on the Arduino Uno, for example. Concretely, we are using the Adafruit Feather M0 Basic, but the code should run on any Arduino -compatible SAMD21 microcontroller board. Motor and encoder Link To estimate the angular position of the motor we use a HAL-based encoder. We are using standard components that are sold for the drone market. In particular, we are using the iPower Motor GM4108H-120T Gimbal Motor with AS5048A Encoder and slip ring from iFlight-rc.com. Camera Link We use the recent Raspberry Pi High Quality Camera Module. The module is connected to the Raspberry Pi Zero W single-board computer. The camera module has a CS mount that allows us to change the lens. Wiring Link We had recurring problems with the cabling of the cameras in our previous solutions. The micro USB connectors were not reliable enough and often lost contact. The transmission of the power and the communication over a long USB cable often failed (power drop, broken serial link). We therefore choose to bring a 12V cable to the camera, connect it over a sturdy plug, and include a DC-DC converter inside the camera. Also, we use the WiFi functionality offered by the Raspberry Pi Zero W to send commands and download the images. Housing Link A housing was designed that follows the same production principles used in other coblebot components.","title":"Camera Module"},{"location":"Farmers%20Dashboard/camera/#camera-module","text":"Size: 148mm x 143mm x 63mm Weight: 560gr CMOS sensor: 12.3 mp, Sony IMX477 sensor. Optics: Support for C- and CS-mount lenses. Networking: Wi-Fi 2.4GHz 802.11 b/g/n Interfaces: UART Serial, I2C, SWD. Both the Cablebot and the Scanner require a reliable camera module, although the usage in both cases is slightly different For the Scanner , the camera is positioned at a given angle. The movement from angle position to the next is relativement infrequent and slow. For the Cablebot , the camera must adjust in real-time for swinging movements of the system.","title":"Camera module"},{"location":"Farmers%20Dashboard/camera/#controller-board","text":"We decided to use a brushless motor, as is the custom in camera mount systems. We designed a controller board that exploits the functions offered by the TI DRV8313 chip. The DRV8313 requires as an input three Pulse Width Modulation signals (PWM) that encode the phase of each of the three voltages applied to the solenoids of the brushless motor. The control software is integrated into the code for the Romi Rover: https://github.com/romi/romi-rover-build-and-test/tree/ci_dev/romi-rover/gimbal_bldc The design files can be found at https://github.com/romi/bldc_featherwing","title":"Controller board"},{"location":"Farmers%20Dashboard/camera/#microcontroller","text":"The PWMs signals should be of a high frequency, as to avoid any ripples in the signal, and should be closely synchronised. We therefore opted for a Cortex M0 microcontroller instead of the more common AVR microcontrollers found on the Arduino Uno, for example. Concretely, we are using the Adafruit Feather M0 Basic, but the code should run on any Arduino -compatible SAMD21 microcontroller board.","title":"Microcontroller"},{"location":"Farmers%20Dashboard/camera/#motor-and-encoder","text":"To estimate the angular position of the motor we use a HAL-based encoder. We are using standard components that are sold for the drone market. In particular, we are using the iPower Motor GM4108H-120T Gimbal Motor with AS5048A Encoder and slip ring from iFlight-rc.com.","title":"Motor and encoder"},{"location":"Farmers%20Dashboard/camera/#camera","text":"We use the recent Raspberry Pi High Quality Camera Module. The module is connected to the Raspberry Pi Zero W single-board computer. The camera module has a CS mount that allows us to change the lens.","title":"Camera"},{"location":"Farmers%20Dashboard/camera/#wiring","text":"We had recurring problems with the cabling of the cameras in our previous solutions. The micro USB connectors were not reliable enough and often lost contact. The transmission of the power and the communication over a long USB cable often failed (power drop, broken serial link). We therefore choose to bring a 12V cable to the camera, connect it over a sturdy plug, and include a DC-DC converter inside the camera. Also, we use the WiFi functionality offered by the Raspberry Pi Zero W to send commands and download the images.","title":"Wiring"},{"location":"Farmers%20Dashboard/camera/#housing","text":"A housing was designed that follows the same production principles used in other coblebot components.","title":"Housing"},{"location":"Farmers%20Dashboard/electronics/","text":"Control electronics Link The entire motion system is controlled by a low-cost and low-power microcontroller (Microchip SAMD21) that interfaces with the camera module. The much powerful computer in the camera module runs the main logics and communication subsystem based on the software and hardware stack used in the Rover, ensuring modularity and scalability. As both the camera module and the Rover run the Raspberry PI ARM based Linux architecture our software stack is portable across each one of the robots. Those ensuring the Rover and the carrier use the same remote management interfaces. Following that approach the carrier can be managed using the same standard RC remote controller for on-site maintenance operations. The Carrier module electronics is compossed by three main PCB's: the control PCB that holds the man microcontroller on charge of the navigation, the Odrvie motor driver and the power ditribution PCB. Control PCB Link The navigation control is managed by any Arduino SAMD21 compatible board. This board will receive direct instructions via RC control or commands through the Serial port sent by the Raspberry pi in the Camera Module. Inputs Link Two endstops (2 interrupts) Position encoder (SPI) Motor encoder (1 interrupt) Battery voltage (from voltage dividerconnected to batt) RF speed channel (1 interrupts) we have a second channel with no use for now. USB or TX/RX only Serial port (level converter?). IMU (I2C) Charger connected input (1 interrupt) User button (1 interrupt) Outputs Link Motor control (PWM) User led (Addressable RGB) Feather Pinout Link Feather M0 Pin Function Int 0 - RX (Serial1) (yellow) PA11 Odrive Serial GPIO1 \u2192 TX SERCOM0.3 1 - TX (Serial1) (green) PA10 Odrive Serial GPIO2 \u2192 RX SERCOM0.2 5 PA15 Addressable Led \u2192 DIN 6 PA20 Endstop Front or right (was Back \u00b9) \ud83d\udfe2 9 PA07 Endstop Back or left (was Front \u00b9) \ud83d\udfe2 10 - TX0 (green) PA18 Camera Module \u2192 RX SERCOM1.2 11 PA16 RC \u2192 STR (Optional Gimbal control) \ud83d\udfe2 12 - RX0 (yellow) PA19 Camera Module \u2192 TX SERCOM1.3 13 PA17 RC Throttle \u2192 THR \ud83d\udfe2 15 - A1 PB08 Odrive Reset nRST (in J2) 16 - A2 PB09 User Button \ud83d\udfe2 17 - A3 PA04 Charging station home int \ud83d\udfe2 18 - A4 PA05 ADNS \u2192 MOT 19 - A5 PB02 ADNS \u2192 SS 20 - SDA PA22 I2C \u2192 SDA SERCOM3.0 21 - SCL PA23 I2C \u2192 SCL SERCOM3.1 22 - MISO PA12 ADNS \u2192 MI SERCOM_ALT4.0 23 - MOSI PB10 ADNS \u2192 MO SERCOM_ALT4.2 24 - SCK PB11 ADNS \u2192 SC SERCOM_ALT4.3 RST Reset Button Mouse laser motion sensor Link As a way to get real closed loop navigation the ADNS-9800 Laser Motion Sensor is being used to track the movement over the cable. This sensor is still to be integrated in the latest prototype, tests are being made to warranty that the cable is always visible to the sensor independently of the tension level. Optimal position of the laser sensor. IMU Link An Inertial measurement unit is used to give feedback to the microcontroller about balance and vibrations, acceleration and speed algorithms to take advantage of this data are still under development. The ISM330 Adafruit QWIIC breakout board is being used through I2C bus. Compatible Adafruit library Arduino library Endstops Link To detect collisions OMRON D3V-013-1C23 miniature switches are used as end stops on both sides of the cablebot. A 3d printed cover protects the electronic parts and trigers the switch when an obstacle is found. Cabling is routed through the structure to avoid any damage on the lines. The cabling is routed trough machined channels between the aluminum sandwich sheets keeping it protected and organized. Remote Control Link Any radio frequency remote control with at least one channel can be used with the CARM. We use one PWM channel to control the speed along the cable. A second PWM channel is already wired to be used in the future, for example to control camera orientation. We have used HK-GT2B model with good results. 2.4GHz AFHDS signal operation 3CH operation 3.7v 800mAh Rechargeable li-ion transmitter battery RF Power: 20dBm (100mW) max Modulation: GFSK Sensitivity: 1024 Transmitter Weight: 270g Receiver power: 4.5~6.5 VDC Depending on the RF hardware sometimes adjusting the signal top/down limits with the remote potentiometers is not enough, this values can be adjusted on firmware changing the RC_CALIBRATION constant values for Min, Middle and Max values here . To find out the values of your remote print to the console the value of the rcSpeed variable, some where in your loop() function and check the serial output while the trigger is at rest, top and bottom positions. To minimize vibrations of the carrier module while operated with the RF remote control, the noise on the signal is cleaned, applying exponential smoothing to it.","title":"Electronics"},{"location":"Farmers%20Dashboard/electronics/#control-electronics","text":"The entire motion system is controlled by a low-cost and low-power microcontroller (Microchip SAMD21) that interfaces with the camera module. The much powerful computer in the camera module runs the main logics and communication subsystem based on the software and hardware stack used in the Rover, ensuring modularity and scalability. As both the camera module and the Rover run the Raspberry PI ARM based Linux architecture our software stack is portable across each one of the robots. Those ensuring the Rover and the carrier use the same remote management interfaces. Following that approach the carrier can be managed using the same standard RC remote controller for on-site maintenance operations. The Carrier module electronics is compossed by three main PCB's: the control PCB that holds the man microcontroller on charge of the navigation, the Odrvie motor driver and the power ditribution PCB.","title":"Control electronics"},{"location":"Farmers%20Dashboard/electronics/#control-pcb","text":"The navigation control is managed by any Arduino SAMD21 compatible board. This board will receive direct instructions via RC control or commands through the Serial port sent by the Raspberry pi in the Camera Module.","title":"Control PCB"},{"location":"Farmers%20Dashboard/electronics/#inputs","text":"Two endstops (2 interrupts) Position encoder (SPI) Motor encoder (1 interrupt) Battery voltage (from voltage dividerconnected to batt) RF speed channel (1 interrupts) we have a second channel with no use for now. USB or TX/RX only Serial port (level converter?). IMU (I2C) Charger connected input (1 interrupt) User button (1 interrupt)","title":"Inputs"},{"location":"Farmers%20Dashboard/electronics/#outputs","text":"Motor control (PWM) User led (Addressable RGB)","title":"Outputs"},{"location":"Farmers%20Dashboard/electronics/#feather-pinout","text":"Feather M0 Pin Function Int 0 - RX (Serial1) (yellow) PA11 Odrive Serial GPIO1 \u2192 TX SERCOM0.3 1 - TX (Serial1) (green) PA10 Odrive Serial GPIO2 \u2192 RX SERCOM0.2 5 PA15 Addressable Led \u2192 DIN 6 PA20 Endstop Front or right (was Back \u00b9) \ud83d\udfe2 9 PA07 Endstop Back or left (was Front \u00b9) \ud83d\udfe2 10 - TX0 (green) PA18 Camera Module \u2192 RX SERCOM1.2 11 PA16 RC \u2192 STR (Optional Gimbal control) \ud83d\udfe2 12 - RX0 (yellow) PA19 Camera Module \u2192 TX SERCOM1.3 13 PA17 RC Throttle \u2192 THR \ud83d\udfe2 15 - A1 PB08 Odrive Reset nRST (in J2) 16 - A2 PB09 User Button \ud83d\udfe2 17 - A3 PA04 Charging station home int \ud83d\udfe2 18 - A4 PA05 ADNS \u2192 MOT 19 - A5 PB02 ADNS \u2192 SS 20 - SDA PA22 I2C \u2192 SDA SERCOM3.0 21 - SCL PA23 I2C \u2192 SCL SERCOM3.1 22 - MISO PA12 ADNS \u2192 MI SERCOM_ALT4.0 23 - MOSI PB10 ADNS \u2192 MO SERCOM_ALT4.2 24 - SCK PB11 ADNS \u2192 SC SERCOM_ALT4.3 RST Reset Button","title":"Feather Pinout"},{"location":"Farmers%20Dashboard/electronics/#mouse-laser-motion-sensor","text":"As a way to get real closed loop navigation the ADNS-9800 Laser Motion Sensor is being used to track the movement over the cable. This sensor is still to be integrated in the latest prototype, tests are being made to warranty that the cable is always visible to the sensor independently of the tension level. Optimal position of the laser sensor.","title":"Mouse laser motion sensor"},{"location":"Farmers%20Dashboard/electronics/#imu","text":"An Inertial measurement unit is used to give feedback to the microcontroller about balance and vibrations, acceleration and speed algorithms to take advantage of this data are still under development. The ISM330 Adafruit QWIIC breakout board is being used through I2C bus. Compatible Adafruit library Arduino library","title":"IMU"},{"location":"Farmers%20Dashboard/electronics/#endstops","text":"To detect collisions OMRON D3V-013-1C23 miniature switches are used as end stops on both sides of the cablebot. A 3d printed cover protects the electronic parts and trigers the switch when an obstacle is found. Cabling is routed through the structure to avoid any damage on the lines. The cabling is routed trough machined channels between the aluminum sandwich sheets keeping it protected and organized.","title":"Endstops"},{"location":"Farmers%20Dashboard/electronics/#remote-control","text":"Any radio frequency remote control with at least one channel can be used with the CARM. We use one PWM channel to control the speed along the cable. A second PWM channel is already wired to be used in the future, for example to control camera orientation. We have used HK-GT2B model with good results. 2.4GHz AFHDS signal operation 3CH operation 3.7v 800mAh Rechargeable li-ion transmitter battery RF Power: 20dBm (100mW) max Modulation: GFSK Sensitivity: 1024 Transmitter Weight: 270g Receiver power: 4.5~6.5 VDC Depending on the RF hardware sometimes adjusting the signal top/down limits with the remote potentiometers is not enough, this values can be adjusted on firmware changing the RC_CALIBRATION constant values for Min, Middle and Max values here . To find out the values of your remote print to the console the value of the rcSpeed variable, some where in your loop() function and check the serial output while the trigger is at rest, top and bottom positions. To minimize vibrations of the carrier module while operated with the RF remote control, the noise on the signal is cleaned, applying exponential smoothing to it.","title":"Remote Control"},{"location":"Farmers%20Dashboard/install/","text":"Raspberry Pi Link Basic software setup Link Download Raspberry Pi OS (32-bit) Lite and burn the image to a sdcard of at least 8GB. To avid connecting the Pi to a monitor you can follow the instructions to setup WiFi and ssh (scroll down to number 3) for a headless setup. Once your Pi is connected to the network and your'e logged via ssh as pi user there are some simple tasks to do: For security reasons it is recommended that you change the pasword of default user pi pi@raspberry:~ $ passwd Update packages. sudo apt update sudo apt upgrade Create user romi, set password and add it to needed groups. sudo useradd romi sudo usermod -a -G sudo,adm,dialout,video,netdev,plugdev,gpio romi Run raspi-config command and change: Network Options \u2192 Hostname to cablebot If you need you can change keyboard, WLAN country and timezone settings inside Localization Options . Enable the cammera, SSH server if you haven't yet and I2C interface in Interfacing Options . You can also update raspi-config tool via the Update menu. Select Finish and reboot your Pi ( sudo reboot ). Install platformio sudo apt-get install python3-distutils curl -fsSL https://raw.githubusercontent.com/platformio/platformio-core-installer/master/get-platformio.py -o get-platformio.py python3 get-platformio.py Edit your bash profile config fiel (usually .bashrc ) and add the line export PATH = $PATH :~/.platformio/penv/bin OverlayFS Link To avoid file system corruption on Raspberry Pi's that can have their power interrupted suddenly, having a read-only file system it's a good option. In computing, OverlayFS is a union mount filesystem implementation for Linux. It combines multiple different underlying mount points into one, resulting in single directory structure that contains underlying files and sub-directories from all sources. Common applications overlay a read/write partition over a read-only partition, such as with LiveCDs and IoT devices with limited flash memory write cycles. Wikipedia When using OverlayFS no filesystem change will survive reboot, that means no bash history! Raspbian Link In raspbian you can run raspi-config and under Advanced Options you will find Overlay FS option: Just enable it and set boot partition to read only (raspi-config will ask you for this) and reboot. You can revert this changes with the same procedure. Archlinux Link Install Raspi-Overlayroot Link git clone https://github.com/nils-werner/raspi-overlayroot cd raspi-overlayroot makepkg -si Then try rebooting, it should boot as normal. Enable overlayroot hook Link Then in /etc/mkinitcpio.conf add overlay to your MODULES array add overlayroot to your HOOKS array and rebuild the initramfs by running mkinitcpio -P and reboot. It should boot as normal. Enable overlayroot in commandline Link With the initramfs in place, you can now enable overlayroot by adding overlayroot to the end of the Kernel commandline, editing /boot/cmdline.txt root=/dev/mmcblk0p2 rw rootwait console=ttyAMA0,115200 console=tty1 selinux=0 plymouth.enable=0 smsc95xx.turbo_mode=N dwc_otg.lpm_enable=0 kgdboc=ttyAMA0,115200 elevator=noop overlayroot and reboot. You should see a warning during login that any changes you make to your filesystem will be non-persistent after this point. Set filesystems readonly Link You can now also set the entire root filesystem as readonly by changing rw to ro in the Kernel commandline root=/dev/mmcblk0p2 ro rootwait console=ttyAMA0,115200 console=tty1 selinux=0 plymouth.enable=0 smsc95xx.turbo_mode=N dwc_otg.lpm_enable=0 kgdboc=ttyAMA0,115200 elevator=noop overlayroot and adding ro to /etc/fstab # # /etc/fstab: static file system information # # <file system> <dir> <type> <options> <dump> <pass> /dev/mmcblk0p1 /boot vfat defaults,ro 0 0 Editing the root filesystem Link You can run rwrootfs to remount all file systems as read-write and change into an interactive shell in your SD card file system. After exiting that shell, the fileystems will remain read-write until next reboot. Alternatively you can undo all changes from Enable overlayroot in commandline and Set filesystems readonly and reboot. This is the recommended way of system upgrades. Info Resources * https://github.com/nils-werner/raspi-overlayroot * https://wiki.archlinux.org/title/Overlay_filesystem Romi autossh Link Locating a Romi device (ej. cablebot) on a network can be a dificult task depending on the network topology. Services like Dataplicity can solve this problem but have some disatvantages as not being free software, cost, vendor lock, etc. Having a server accesible via a public IP address is enough to make this work. Maintaining a persistent SSH reverse tunnel with autoSSH between the Romi device and the server will allow us to access the device from anywhere without knowing his IP address. Client side (Romi device) Link Generate a rsa key (withouth passphrase) Link ssh-keygen Copy your key to the server to allow passwordless access. Link The user must exist already on the server, we recomend creating a specific user for this task. ssh-copy-id user@server Test a reverse SSH tunnel Link Select a specific port for each of the devices to use on the port-s:localhost:22 part of the command, so that port number ( port-s ) of the server will be linked to port 22 on the device. ssh -R port-s:localhost:22 [-p srv-ssh-port] user@server-with-public-ip Now try to log in from another computer with: ssh -J user@server -p port-s user@localhost If that worked we can now setup the autossh to make the conection on boot and keep it alive. Install autossh Link Depending on your linux package manager, ej. sudo pacman -S autossh or sudo apt install autossh Test autossh reverse tunneling Link autossh -M 0 -f -o \"ServerAliveInterval 45\" -o \"ServerAliveCountMax 2\" -N -R port-s:localhost:22 [-p srv-ssh-port] user@server-with-public-ip Options: -M Specifies monitor port. May be overridden by environment variable AUTOSSH_PORT. 0 turns monitoring loop off. -f Run in background (autossh handles this, and does not pass it to ssh.) -N Do not execute a remote command. This is useful for just forwarding ports. -R [bind_address:]port:host:hostport Specifies that connections to the given TCP port or Unix socket on the remote (server) host are to be forwarded to the local side. From the autossh manual page: Setting the monitor port to 0 turns the monitoring function off, and autossh will only restart ssh upon ssh's exit. For example, if you are using a recent version of OpenSSH, you may wish to explore using the ServerAliveInterval and ServerAliveCountMax options to have the SSH client exit if it finds itself no longer connected to the server. In many ways this may be a better solution than the monitoring port. So we are using ServerAliveInterval : Sets a timeout interval in seconds after which if no data has been received from the server, ssh(1) will send a message through the encrypted channel to request a response from the server. and ServerAliveCountMax : Sets the number of server alive messages (see below) which may be sent without ssh(1) receiving any messages back from the server. If this threshold is reached while server alive messages are being sent, ssh will disconnect from the server, terminating the session. that means that after 45 seconds ssh by itself will try to communicate with the server, if it fails will try again in 45 more seconds and after two failures it will terminate the session, in wich case autossh will restart the connection. Starting autossh on boot with systemd Link Create a new file /etc/systemd/system/autossh.service and add this to it, remember to set your port ( port-s ): [Unit] Description=AutoSSH service for port port-s After=network.target [Service] User=user Environment=\"AUTOSSH_GATETIME=0\" ExecStart=/usr/bin/autossh -M 0 -o ControlMaster=no -o \"ServerAliveInterval 45\" -o \"ServerAliveCountMax 2\" -N -i ~/.ssh/id_rsa -R port-s:localhost:22 [-p srv-ssh-port] user@server-with-public-ip [Install] WantedBy=multi-user.target Now enable and start the service: sudo systemctl enable autossh sudo systemctl start autossh From now autossh will be started at boot time and will keep the tunnel alive. Warning Only tested on Arch Linux. Server side Link The simplest way is using the same user in the server and the clients, so if you haven't already created the user, do it. No Password logins (optional) Link For extra security you can disable password login, only allowing logins with keys: sudo vi /etc/ssh/sshd_config To do it system wide change PasswordAuthentication yes to PasswordAuthentication no . Or if you want to restrict password logins only for a specific user (eg. romi) comment the previous line: #PasswordAuthentication no and add this content to the file: Match User user(romi) PasswordAuthentication no The inconvenience of this approach is that every time you want to add a new key (give rights to a new device) you will need to change this temporarily to allow password logins. Remember that after changing the sshd_config file you need to restart the service with systemctl restart sshd . Restrict client commands Link To avoid security risks in case a key on a romi-device has been compromised, we are going to restrict the commands that the client is able to execute to the minimal. This can be achieved by editing ~/.ssh/authorized_keys, by prefixing the desired key, e.g. as follows: command=\"\" ssh-rsa \u2026 This will allow any login with this specific key only to execute the command specified between the quotes, none in this example. Client side (user computer) Link Generate and copy RSA key Link If you haven't generated your ssh key do it with the ssh-keygen command and copy this key to the server and the romi device. Setup connection with ssh config file Link Add a new entry in your .ssh/config with the following content: Host tunnel Hostname server-with-public-ip [port srv-ssh-port] user user Host romi-device Hostname localhost port port-s user user ProxyJump tunnel This will allow you to simply do ssh romi-device to log in your device.","title":"Install the software"},{"location":"Farmers%20Dashboard/install/#raspberry-pi","text":"","title":"Raspberry Pi"},{"location":"Farmers%20Dashboard/install/#basic-software-setup","text":"Download Raspberry Pi OS (32-bit) Lite and burn the image to a sdcard of at least 8GB. To avid connecting the Pi to a monitor you can follow the instructions to setup WiFi and ssh (scroll down to number 3) for a headless setup. Once your Pi is connected to the network and your'e logged via ssh as pi user there are some simple tasks to do: For security reasons it is recommended that you change the pasword of default user pi pi@raspberry:~ $ passwd Update packages. sudo apt update sudo apt upgrade Create user romi, set password and add it to needed groups. sudo useradd romi sudo usermod -a -G sudo,adm,dialout,video,netdev,plugdev,gpio romi Run raspi-config command and change: Network Options \u2192 Hostname to cablebot If you need you can change keyboard, WLAN country and timezone settings inside Localization Options . Enable the cammera, SSH server if you haven't yet and I2C interface in Interfacing Options . You can also update raspi-config tool via the Update menu. Select Finish and reboot your Pi ( sudo reboot ). Install platformio sudo apt-get install python3-distutils curl -fsSL https://raw.githubusercontent.com/platformio/platformio-core-installer/master/get-platformio.py -o get-platformio.py python3 get-platformio.py Edit your bash profile config fiel (usually .bashrc ) and add the line export PATH = $PATH :~/.platformio/penv/bin","title":"Basic software setup"},{"location":"Farmers%20Dashboard/install/#overlayfs","text":"To avoid file system corruption on Raspberry Pi's that can have their power interrupted suddenly, having a read-only file system it's a good option. In computing, OverlayFS is a union mount filesystem implementation for Linux. It combines multiple different underlying mount points into one, resulting in single directory structure that contains underlying files and sub-directories from all sources. Common applications overlay a read/write partition over a read-only partition, such as with LiveCDs and IoT devices with limited flash memory write cycles. Wikipedia When using OverlayFS no filesystem change will survive reboot, that means no bash history!","title":"OverlayFS"},{"location":"Farmers%20Dashboard/install/#raspbian","text":"In raspbian you can run raspi-config and under Advanced Options you will find Overlay FS option: Just enable it and set boot partition to read only (raspi-config will ask you for this) and reboot. You can revert this changes with the same procedure.","title":"Raspbian"},{"location":"Farmers%20Dashboard/install/#archlinux","text":"","title":"Archlinux"},{"location":"Farmers%20Dashboard/install/#install-raspi-overlayroot","text":"git clone https://github.com/nils-werner/raspi-overlayroot cd raspi-overlayroot makepkg -si Then try rebooting, it should boot as normal.","title":"Install Raspi-Overlayroot"},{"location":"Farmers%20Dashboard/install/#enable-overlayroot-hook","text":"Then in /etc/mkinitcpio.conf add overlay to your MODULES array add overlayroot to your HOOKS array and rebuild the initramfs by running mkinitcpio -P and reboot. It should boot as normal.","title":"Enable overlayroot hook"},{"location":"Farmers%20Dashboard/install/#enable-overlayroot-in-commandline","text":"With the initramfs in place, you can now enable overlayroot by adding overlayroot to the end of the Kernel commandline, editing /boot/cmdline.txt root=/dev/mmcblk0p2 rw rootwait console=ttyAMA0,115200 console=tty1 selinux=0 plymouth.enable=0 smsc95xx.turbo_mode=N dwc_otg.lpm_enable=0 kgdboc=ttyAMA0,115200 elevator=noop overlayroot and reboot. You should see a warning during login that any changes you make to your filesystem will be non-persistent after this point.","title":"Enable overlayroot in commandline"},{"location":"Farmers%20Dashboard/install/#set-filesystems-readonly","text":"You can now also set the entire root filesystem as readonly by changing rw to ro in the Kernel commandline root=/dev/mmcblk0p2 ro rootwait console=ttyAMA0,115200 console=tty1 selinux=0 plymouth.enable=0 smsc95xx.turbo_mode=N dwc_otg.lpm_enable=0 kgdboc=ttyAMA0,115200 elevator=noop overlayroot and adding ro to /etc/fstab # # /etc/fstab: static file system information # # <file system> <dir> <type> <options> <dump> <pass> /dev/mmcblk0p1 /boot vfat defaults,ro 0 0","title":"Set filesystems readonly"},{"location":"Farmers%20Dashboard/install/#editing-the-root-filesystem","text":"You can run rwrootfs to remount all file systems as read-write and change into an interactive shell in your SD card file system. After exiting that shell, the fileystems will remain read-write until next reboot. Alternatively you can undo all changes from Enable overlayroot in commandline and Set filesystems readonly and reboot. This is the recommended way of system upgrades. Info Resources * https://github.com/nils-werner/raspi-overlayroot * https://wiki.archlinux.org/title/Overlay_filesystem","title":"Editing the root filesystem"},{"location":"Farmers%20Dashboard/install/#romi-autossh","text":"Locating a Romi device (ej. cablebot) on a network can be a dificult task depending on the network topology. Services like Dataplicity can solve this problem but have some disatvantages as not being free software, cost, vendor lock, etc. Having a server accesible via a public IP address is enough to make this work. Maintaining a persistent SSH reverse tunnel with autoSSH between the Romi device and the server will allow us to access the device from anywhere without knowing his IP address.","title":"Romi autossh"},{"location":"Farmers%20Dashboard/install/#client-side-romi-device","text":"","title":"Client side (Romi device)"},{"location":"Farmers%20Dashboard/install/#generate-a-rsa-key-withouth-passphrase","text":"ssh-keygen","title":"Generate a rsa key (withouth passphrase)"},{"location":"Farmers%20Dashboard/install/#copy-your-key-to-the-server-to-allow-passwordless-access","text":"The user must exist already on the server, we recomend creating a specific user for this task. ssh-copy-id user@server","title":"Copy your key to the server to allow passwordless access."},{"location":"Farmers%20Dashboard/install/#test-a-reverse-ssh-tunnel","text":"Select a specific port for each of the devices to use on the port-s:localhost:22 part of the command, so that port number ( port-s ) of the server will be linked to port 22 on the device. ssh -R port-s:localhost:22 [-p srv-ssh-port] user@server-with-public-ip Now try to log in from another computer with: ssh -J user@server -p port-s user@localhost If that worked we can now setup the autossh to make the conection on boot and keep it alive.","title":"Test a reverse SSH tunnel"},{"location":"Farmers%20Dashboard/install/#install-autossh","text":"Depending on your linux package manager, ej. sudo pacman -S autossh or sudo apt install autossh","title":"Install autossh"},{"location":"Farmers%20Dashboard/install/#test-autossh-reverse-tunneling","text":"autossh -M 0 -f -o \"ServerAliveInterval 45\" -o \"ServerAliveCountMax 2\" -N -R port-s:localhost:22 [-p srv-ssh-port] user@server-with-public-ip Options: -M Specifies monitor port. May be overridden by environment variable AUTOSSH_PORT. 0 turns monitoring loop off. -f Run in background (autossh handles this, and does not pass it to ssh.) -N Do not execute a remote command. This is useful for just forwarding ports. -R [bind_address:]port:host:hostport Specifies that connections to the given TCP port or Unix socket on the remote (server) host are to be forwarded to the local side. From the autossh manual page: Setting the monitor port to 0 turns the monitoring function off, and autossh will only restart ssh upon ssh's exit. For example, if you are using a recent version of OpenSSH, you may wish to explore using the ServerAliveInterval and ServerAliveCountMax options to have the SSH client exit if it finds itself no longer connected to the server. In many ways this may be a better solution than the monitoring port. So we are using ServerAliveInterval : Sets a timeout interval in seconds after which if no data has been received from the server, ssh(1) will send a message through the encrypted channel to request a response from the server. and ServerAliveCountMax : Sets the number of server alive messages (see below) which may be sent without ssh(1) receiving any messages back from the server. If this threshold is reached while server alive messages are being sent, ssh will disconnect from the server, terminating the session. that means that after 45 seconds ssh by itself will try to communicate with the server, if it fails will try again in 45 more seconds and after two failures it will terminate the session, in wich case autossh will restart the connection.","title":"Test autossh reverse tunneling"},{"location":"Farmers%20Dashboard/install/#starting-autossh-on-boot-with-systemd","text":"Create a new file /etc/systemd/system/autossh.service and add this to it, remember to set your port ( port-s ): [Unit] Description=AutoSSH service for port port-s After=network.target [Service] User=user Environment=\"AUTOSSH_GATETIME=0\" ExecStart=/usr/bin/autossh -M 0 -o ControlMaster=no -o \"ServerAliveInterval 45\" -o \"ServerAliveCountMax 2\" -N -i ~/.ssh/id_rsa -R port-s:localhost:22 [-p srv-ssh-port] user@server-with-public-ip [Install] WantedBy=multi-user.target Now enable and start the service: sudo systemctl enable autossh sudo systemctl start autossh From now autossh will be started at boot time and will keep the tunnel alive. Warning Only tested on Arch Linux.","title":"Starting autossh on boot with systemd"},{"location":"Farmers%20Dashboard/install/#server-side","text":"The simplest way is using the same user in the server and the clients, so if you haven't already created the user, do it.","title":"Server side"},{"location":"Farmers%20Dashboard/install/#no-password-logins-optional","text":"For extra security you can disable password login, only allowing logins with keys: sudo vi /etc/ssh/sshd_config To do it system wide change PasswordAuthentication yes to PasswordAuthentication no . Or if you want to restrict password logins only for a specific user (eg. romi) comment the previous line: #PasswordAuthentication no and add this content to the file: Match User user(romi) PasswordAuthentication no The inconvenience of this approach is that every time you want to add a new key (give rights to a new device) you will need to change this temporarily to allow password logins. Remember that after changing the sshd_config file you need to restart the service with systemctl restart sshd .","title":"No Password logins (optional)"},{"location":"Farmers%20Dashboard/install/#restrict-client-commands","text":"To avoid security risks in case a key on a romi-device has been compromised, we are going to restrict the commands that the client is able to execute to the minimal. This can be achieved by editing ~/.ssh/authorized_keys, by prefixing the desired key, e.g. as follows: command=\"\" ssh-rsa \u2026 This will allow any login with this specific key only to execute the command specified between the quotes, none in this example.","title":"Restrict client commands"},{"location":"Farmers%20Dashboard/install/#client-side-user-computer","text":"","title":"Client side (user computer)"},{"location":"Farmers%20Dashboard/install/#generate-and-copy-rsa-key","text":"If you haven't generated your ssh key do it with the ssh-keygen command and copy this key to the server and the romi device.","title":"Generate and copy RSA key"},{"location":"Farmers%20Dashboard/install/#setup-connection-with-ssh-config-file","text":"Add a new entry in your .ssh/config with the following content: Host tunnel Hostname server-with-public-ip [port srv-ssh-port] user user Host romi-device Hostname localhost port port-s user user ProxyJump tunnel This will allow you to simply do ssh romi-device to log in your device.","title":"Setup connection with ssh config file"},{"location":"Farmers%20Dashboard/legacy/","text":"Wirebot Link Main Board and Eletronics Link Raspberry Pi - packages, Dependencies and Configurations Link Download the image Robotics Ubuntu+ROS Raspberry Pi Image (3B+ Support) that comes with Ubuntu 16.04 (LXDE), and ROS Kinetic. Copy the image to the SD card. Instructions here. Resizes the file system to fill the SD card before booting following this instructions. Acces to the raspi-config utility: $computer :~ $sudo raspi-config Choose \"Expand root partition to fill SD card\" option: The Ubiquityrobotics images come up as a Wifi acces point. The SSID is ubiquityrobotXXXX where XXXX is part of the MAC address. Connect to the wifi hostopost and use folowing wifi password: robotseverywhere Once connected, it is possible to log into the Pi with ssh ubuntu@10.42.0.1 with the following password of: ubuntu Desable the default robots and node runing on the pi. $ ubuntu@ubiquityrobot.local: $sudo systemctl disable magni-base Raspberry Pi - Setting up the WIREDBOT to the Network Link Open a new terminal window, and log in to the robot with ssh: ATENTION : The HOSTNAME for firts time is \u201cubiquityrobot.local\u201d. $ computer:~ $ssh ubuntu@ubiquityrobot.local ATENTION : The password for firts time is \u201cubuntu\u201d. Change the hostname using pifi. Type the following command: $ ubuntu@ubiquityrobot.local:~ $sudo pifi set-hostname wiredbot Reboot the Pi. $ ubuntu@ubiquityrobot.local:~ $sudo reboot Log in to the robot with the new hostname \"wiredbot\": $ computer:~ $ssh ubuntu@wiredbot.local Use pifi to list the nearby networks: $ ubuntu@wiredbot:~ $pifi list seen ATENTION : Search for the network where the robots are connected. Swich to to the desire network by using the following command. $ ubuntu@NEWHOSTNAME:~ $sudo pifi add localNetwork password ATTENTION : The keyword \"localNetwork\" on this documentation refert to the network the robot need to be connected. The keyword \"pass\" on this documentation refer to the password of the network. Reboot the Pi. $ ubuntu@wiredbot:~ $sudo reboot Test the connectivity with the Pi. Open a new terminal window on a external on a diferent computer: $ computer:~ $ping wirebot.local TIP : Press control-c to stop the pinging ADVERTENCE : If something goes wrong, the PI will come back up as access point mode. Search on the network for the name ubiquityrobot, reboot and start over. Log into the PI by using: $ computer:~ $ssh ubuntu@wirebot.local the output will be: The authenticity of host \u201810.0.0.113 ( 10 .0.0.113 ) \u2019 can\u2019t be established. ECDSA key fingerprint is SHA256:sDDeGZzL8FPY3kMmvhwjPC9wH+mGsAxJL/dNXpoYnsc. Are you sure you want to continue connecting ( yes/no ) ? continue by wrinting: $ computer:~ $yes the password is still. ubuntu Update and updagrade de Pi. $ ubuntu@wiredbot:~ $sudo apt-get update $ ubuntu@wiredbot:~ $sudo apt-get upgrade ROS - Setting up the ROS NODES and Arduino Firmware. Link Make sure you have installed the resent updates and updagrades. $ ubuntu@wiredbot:~ $sudo apt-get update $ ubuntu@wiredbot:~ $sudo apt-get upgrade Point to the workspace folder for ros packages Clone the repository on the Pi, the romi/grlbl_serial into the /src folder of your catkin workspace and rebuild your workspace: $ ubuntu@wiredbot:~ $cd ~/catkin_ws/src/ $ ubuntu@wiredbot:~ $git clone git@github.com:romi/grlbl_serial.git $ ubuntu@wiredbot:~ $catkin_make Clone the repository on the Pi, the romi/i2c_pca9685_driver into the /src folder of your catkin workspace and rebuild your workspace: $ ubuntu@wiredbot:~ $cd ~/catkin_ws/src/ $ ubuntu@wiredbot:~ $git clone git@github.com:romi/i2c_pca9685_driver.git $ ubuntu@wiredbot:~ $catkin_make Wiring diagram. Link Schematics: List Part Item Description Quantity 0 Raspberry pi model 3b+ 1 1 Raspberry Pi Camera Module v2 1 2 16-Channel 12-bit - I2C interface - PCA9685 1 3 Arduino UNO 1 4 Arduino CNC Shield V3 1 5 A4988 Stepper Motor Driver 4 6 Nema 23 Unipolar 1.8deg 1 7 Survey3W Camera - Orange+Cyan+NIR (OCN, NDVI) 1 8 Survey3W HDMI PWM Trigger Cable 1 9 Survey3 Advanced GPS Receiver 1 10 12V Power Supply 1 11 Wires and general hardware - Hardware Setup. Link 1.Drawings * Assebly drawing - Top view * Assebly drawing - Botton View List Part Item Description Quantity 0 Aluminium Profile 20\u00d720 T-Slot 5 4 1 Idler Pulley Plate 6 2 Join Plat T 4 3 Corner connector 90 degree (V-Slot) 2 4 Gantry Plate V-Slot 20-80 2 5 3M Drop in Tee Nuts \u2013 Insert nuts 50 6 3M Allen Low Profile Screws 50 7 M8 Allen Screw - 45mm Long 6 8 Motor Mount Plate NEMA 23 1 9 Nylon Pulley And Wheel - 40 mm Diameter - 8 mm Bearing 6 10 Nema 23 stepper motor 1 11 P65 Weatherproof Enclosure/electrical enclosure box 2 12 5mm Shock Cord - Marine Grade Polyester Coated Rubber Rope - Running ROS node - Path Planning Link ROS Nodes Overview. ROS Master - Run ROS Nodes over the raspberry PI. Log into the raspberry PI by using: $ computer:~ $ssh ubuntu@wirebot.local (OPTIONAL) Edit the path planning according to the dimensions of the field to scan and the desired length and amount of waypoints. $ ubuntu@ubiquityrobot.local:~ $sudo nano ~/catkin_ws/src/grlbl_serial/src/path_planning_action_client.py * Edit the path_planning_action_client.py by changing the variable movement_goal.xyz_position that is under the function def path_planning_client() . Here is an example of a Path planning that takes pictures of every 500mm in a distance of 10mts: movement_goal.xyz_position = [\"{'x':0, 'y':0, 'z':500, 'delay':20}\", \"{'x':0, 'y':0, 'z':1000, 'delay':20}\", \"{'x':0, 'y':0, 'z':1500, 'delay':20}\", \"{'x':0, 'y':0, 'z':2000, 'delay':20}\", \"{'x':0, 'y':0, 'z':2500, 'delay':20}\", \"{'x':0, 'y':0, 'z':3000, 'delay':20}\", \"{'x':0, 'y':0, 'z':3500, 'delay':20}\", \"{'x':0, 'y':0, 'z':4000, 'delay':20}\", \"{'x':0, 'y':0, 'z':4500, 'delay':20}\", \"{'x':0, 'y':0, 'z':5000, 'delay':20}\", \"{'x':0, 'y':0, 'z':5500, 'delay':20}\", \"{'x':0, 'y':0, 'z':6000, 'delay':20}\", \"{'x':0, 'y':0, 'z':6500, 'delay':20}\", \"{'x':0, 'y':0, 'z':7000, 'delay':20}\", \"{'x':0, 'y':0, 'z':7500, 'delay':20}\", \"{'x':0, 'y':0, 'z':8000, 'delay':20}\", \"{'x':0, 'y':0, 'z':8500, 'delay':20}\", \"{'x':0, 'y':0, 'z':9000, 'delay':20}\", \"{'x':0, 'y':0, 'z':9500, 'delay':20}\"] Start up the nodes and the ROS master by launching the path_planning_action_server_node node under the raspberry PI: $ ubuntu@ubiquityrobot.local:~ $roslaunch grlbl_serial path_planning_action_server_node.launch (ADVERTENCE) If the ROS package is not under the autocomplete method of the terminal. The problem will be solve by sourcing the devel/setup.bash. $ ubuntu@ubiquityrobot.local:~ $source ~/catkin_ws/src/devel/setup.bash 3. ROS Slave - Run ROS Nodes over the Remote Computer. Start up the nodes by launching the path_planning_action_client_node node under the remote computer: $ ubuntu@ubiquityrobot.local:~ $roslaunch grlbl_serial path_planning_action_client_node.launch (OPTIONAL) This node as well can by launch over the raspberry PI. This can be done by lauching the node over a new terminal. By launching the previous ROS node on the WIREDBOT. The starting process of collecting photos from the Mapir camera and the Raspi Cam will be launch automatically according to the path planning instructions save on the path_planning_action_client.py file. Saving the data from the WIREDBOT. (UNDER-DEVELOPMENT) Link Kepp running or re start the node and the ROS master by launching the path_planning_action_server_node node under the raspberry PI: $ ubuntu@ubiquityrobot.local:~ $roslaunch grlbl_serial path_planning_action_server_node.launch Publish a 1500us to the /i2c_pca9685_driver wiredbot_PWMValues/wiredbot_PWMValues mapir_control_pwm: $ ubuntu@ubiquityrobot.local:~ $rostopic pub -1 /i2c_pca9685_driver wiredbot_PWMValues/wiredbot_PWMValues int16 mapir_control_pwm 1500 int16 motor_A_pwm int16 motor_B_pwm Once ros is publishing the message mapir_control_pwm 1500us under the topic \\i2c_pca9685_driver\\wiredbot_PWMValues. The camera is ready to mount. On the raspberry PI. Mount the camera by using the following commands. $ ubuntu@ubiquityrobot.local:~ $mkdir /mapir $ ubuntu@ubiquityrobot.local:~ $mkdir sudo mount -t vfat /dev/sdb2 /mapir $ ubuntu@ubiquityrobot.local:~ $cd /mapir/DCMI/Photos * Image Gallery - Valldaura: Link Lettuce Think and Wirebot: Wirebot on the field: Wirebot on the field: WIREBOT 3D Scans: Link","title":"Legacy"},{"location":"Farmers%20Dashboard/legacy/#wirebot","text":"","title":"Wirebot"},{"location":"Farmers%20Dashboard/legacy/#main-board-and-eletronics","text":"","title":"Main Board and Eletronics"},{"location":"Farmers%20Dashboard/legacy/#raspberry-pi-packages-dependencies-and-configurations","text":"Download the image Robotics Ubuntu+ROS Raspberry Pi Image (3B+ Support) that comes with Ubuntu 16.04 (LXDE), and ROS Kinetic. Copy the image to the SD card. Instructions here. Resizes the file system to fill the SD card before booting following this instructions. Acces to the raspi-config utility: $computer :~ $sudo raspi-config Choose \"Expand root partition to fill SD card\" option: The Ubiquityrobotics images come up as a Wifi acces point. The SSID is ubiquityrobotXXXX where XXXX is part of the MAC address. Connect to the wifi hostopost and use folowing wifi password: robotseverywhere Once connected, it is possible to log into the Pi with ssh ubuntu@10.42.0.1 with the following password of: ubuntu Desable the default robots and node runing on the pi. $ ubuntu@ubiquityrobot.local: $sudo systemctl disable magni-base","title":"Raspberry Pi - packages, Dependencies and Configurations"},{"location":"Farmers%20Dashboard/legacy/#raspberry-pi-setting-up-the-wiredbot-to-the-network","text":"Open a new terminal window, and log in to the robot with ssh: ATENTION : The HOSTNAME for firts time is \u201cubiquityrobot.local\u201d. $ computer:~ $ssh ubuntu@ubiquityrobot.local ATENTION : The password for firts time is \u201cubuntu\u201d. Change the hostname using pifi. Type the following command: $ ubuntu@ubiquityrobot.local:~ $sudo pifi set-hostname wiredbot Reboot the Pi. $ ubuntu@ubiquityrobot.local:~ $sudo reboot Log in to the robot with the new hostname \"wiredbot\": $ computer:~ $ssh ubuntu@wiredbot.local Use pifi to list the nearby networks: $ ubuntu@wiredbot:~ $pifi list seen ATENTION : Search for the network where the robots are connected. Swich to to the desire network by using the following command. $ ubuntu@NEWHOSTNAME:~ $sudo pifi add localNetwork password ATTENTION : The keyword \"localNetwork\" on this documentation refert to the network the robot need to be connected. The keyword \"pass\" on this documentation refer to the password of the network. Reboot the Pi. $ ubuntu@wiredbot:~ $sudo reboot Test the connectivity with the Pi. Open a new terminal window on a external on a diferent computer: $ computer:~ $ping wirebot.local TIP : Press control-c to stop the pinging ADVERTENCE : If something goes wrong, the PI will come back up as access point mode. Search on the network for the name ubiquityrobot, reboot and start over. Log into the PI by using: $ computer:~ $ssh ubuntu@wirebot.local the output will be: The authenticity of host \u201810.0.0.113 ( 10 .0.0.113 ) \u2019 can\u2019t be established. ECDSA key fingerprint is SHA256:sDDeGZzL8FPY3kMmvhwjPC9wH+mGsAxJL/dNXpoYnsc. Are you sure you want to continue connecting ( yes/no ) ? continue by wrinting: $ computer:~ $yes the password is still. ubuntu Update and updagrade de Pi. $ ubuntu@wiredbot:~ $sudo apt-get update $ ubuntu@wiredbot:~ $sudo apt-get upgrade","title":"Raspberry Pi - Setting up the WIREDBOT to the Network"},{"location":"Farmers%20Dashboard/legacy/#ros-setting-up-the-ros-nodes-and-arduino-firmware","text":"Make sure you have installed the resent updates and updagrades. $ ubuntu@wiredbot:~ $sudo apt-get update $ ubuntu@wiredbot:~ $sudo apt-get upgrade Point to the workspace folder for ros packages Clone the repository on the Pi, the romi/grlbl_serial into the /src folder of your catkin workspace and rebuild your workspace: $ ubuntu@wiredbot:~ $cd ~/catkin_ws/src/ $ ubuntu@wiredbot:~ $git clone git@github.com:romi/grlbl_serial.git $ ubuntu@wiredbot:~ $catkin_make Clone the repository on the Pi, the romi/i2c_pca9685_driver into the /src folder of your catkin workspace and rebuild your workspace: $ ubuntu@wiredbot:~ $cd ~/catkin_ws/src/ $ ubuntu@wiredbot:~ $git clone git@github.com:romi/i2c_pca9685_driver.git $ ubuntu@wiredbot:~ $catkin_make","title":"ROS - Setting up the ROS NODES and Arduino Firmware."},{"location":"Farmers%20Dashboard/legacy/#wiring-diagram","text":"Schematics: List Part Item Description Quantity 0 Raspberry pi model 3b+ 1 1 Raspberry Pi Camera Module v2 1 2 16-Channel 12-bit - I2C interface - PCA9685 1 3 Arduino UNO 1 4 Arduino CNC Shield V3 1 5 A4988 Stepper Motor Driver 4 6 Nema 23 Unipolar 1.8deg 1 7 Survey3W Camera - Orange+Cyan+NIR (OCN, NDVI) 1 8 Survey3W HDMI PWM Trigger Cable 1 9 Survey3 Advanced GPS Receiver 1 10 12V Power Supply 1 11 Wires and general hardware -","title":"Wiring diagram."},{"location":"Farmers%20Dashboard/legacy/#hardware-setup","text":"1.Drawings * Assebly drawing - Top view * Assebly drawing - Botton View List Part Item Description Quantity 0 Aluminium Profile 20\u00d720 T-Slot 5 4 1 Idler Pulley Plate 6 2 Join Plat T 4 3 Corner connector 90 degree (V-Slot) 2 4 Gantry Plate V-Slot 20-80 2 5 3M Drop in Tee Nuts \u2013 Insert nuts 50 6 3M Allen Low Profile Screws 50 7 M8 Allen Screw - 45mm Long 6 8 Motor Mount Plate NEMA 23 1 9 Nylon Pulley And Wheel - 40 mm Diameter - 8 mm Bearing 6 10 Nema 23 stepper motor 1 11 P65 Weatherproof Enclosure/electrical enclosure box 2 12 5mm Shock Cord - Marine Grade Polyester Coated Rubber Rope -","title":"Hardware Setup."},{"location":"Farmers%20Dashboard/legacy/#running-ros-node-path-planning","text":"ROS Nodes Overview. ROS Master - Run ROS Nodes over the raspberry PI. Log into the raspberry PI by using: $ computer:~ $ssh ubuntu@wirebot.local (OPTIONAL) Edit the path planning according to the dimensions of the field to scan and the desired length and amount of waypoints. $ ubuntu@ubiquityrobot.local:~ $sudo nano ~/catkin_ws/src/grlbl_serial/src/path_planning_action_client.py * Edit the path_planning_action_client.py by changing the variable movement_goal.xyz_position that is under the function def path_planning_client() . Here is an example of a Path planning that takes pictures of every 500mm in a distance of 10mts: movement_goal.xyz_position = [\"{'x':0, 'y':0, 'z':500, 'delay':20}\", \"{'x':0, 'y':0, 'z':1000, 'delay':20}\", \"{'x':0, 'y':0, 'z':1500, 'delay':20}\", \"{'x':0, 'y':0, 'z':2000, 'delay':20}\", \"{'x':0, 'y':0, 'z':2500, 'delay':20}\", \"{'x':0, 'y':0, 'z':3000, 'delay':20}\", \"{'x':0, 'y':0, 'z':3500, 'delay':20}\", \"{'x':0, 'y':0, 'z':4000, 'delay':20}\", \"{'x':0, 'y':0, 'z':4500, 'delay':20}\", \"{'x':0, 'y':0, 'z':5000, 'delay':20}\", \"{'x':0, 'y':0, 'z':5500, 'delay':20}\", \"{'x':0, 'y':0, 'z':6000, 'delay':20}\", \"{'x':0, 'y':0, 'z':6500, 'delay':20}\", \"{'x':0, 'y':0, 'z':7000, 'delay':20}\", \"{'x':0, 'y':0, 'z':7500, 'delay':20}\", \"{'x':0, 'y':0, 'z':8000, 'delay':20}\", \"{'x':0, 'y':0, 'z':8500, 'delay':20}\", \"{'x':0, 'y':0, 'z':9000, 'delay':20}\", \"{'x':0, 'y':0, 'z':9500, 'delay':20}\"] Start up the nodes and the ROS master by launching the path_planning_action_server_node node under the raspberry PI: $ ubuntu@ubiquityrobot.local:~ $roslaunch grlbl_serial path_planning_action_server_node.launch (ADVERTENCE) If the ROS package is not under the autocomplete method of the terminal. The problem will be solve by sourcing the devel/setup.bash. $ ubuntu@ubiquityrobot.local:~ $source ~/catkin_ws/src/devel/setup.bash 3. ROS Slave - Run ROS Nodes over the Remote Computer. Start up the nodes by launching the path_planning_action_client_node node under the remote computer: $ ubuntu@ubiquityrobot.local:~ $roslaunch grlbl_serial path_planning_action_client_node.launch (OPTIONAL) This node as well can by launch over the raspberry PI. This can be done by lauching the node over a new terminal. By launching the previous ROS node on the WIREDBOT. The starting process of collecting photos from the Mapir camera and the Raspi Cam will be launch automatically according to the path planning instructions save on the path_planning_action_client.py file.","title":"Running ROS node - Path Planning"},{"location":"Farmers%20Dashboard/legacy/#saving-the-data-from-the-wiredbot-under-development","text":"Kepp running or re start the node and the ROS master by launching the path_planning_action_server_node node under the raspberry PI: $ ubuntu@ubiquityrobot.local:~ $roslaunch grlbl_serial path_planning_action_server_node.launch Publish a 1500us to the /i2c_pca9685_driver wiredbot_PWMValues/wiredbot_PWMValues mapir_control_pwm: $ ubuntu@ubiquityrobot.local:~ $rostopic pub -1 /i2c_pca9685_driver wiredbot_PWMValues/wiredbot_PWMValues int16 mapir_control_pwm 1500 int16 motor_A_pwm int16 motor_B_pwm Once ros is publishing the message mapir_control_pwm 1500us under the topic \\i2c_pca9685_driver\\wiredbot_PWMValues. The camera is ready to mount. On the raspberry PI. Mount the camera by using the following commands. $ ubuntu@ubiquityrobot.local:~ $mkdir /mapir $ ubuntu@ubiquityrobot.local:~ $mkdir sudo mount -t vfat /dev/sdb2 /mapir $ ubuntu@ubiquityrobot.local:~ $cd /mapir/DCMI/Photos *","title":"Saving the data from the WIREDBOT. (UNDER-DEVELOPMENT)"},{"location":"Farmers%20Dashboard/legacy/#image-gallery-valldaura","text":"Lettuce Think and Wirebot: Wirebot on the field: Wirebot on the field:","title":"Image Gallery - Valldaura:"},{"location":"Farmers%20Dashboard/legacy/#wirebot-3d-scans","text":"","title":"WIREBOT 3D Scans:"},{"location":"Farmers%20Dashboard/motion/","text":"Motion system Link The platform is battery powered (12V LiPo 2Ah) and uses a brushless servo motor (DFROBOT FIT0441) combined with an optical cable tracker (ADNS-9800) to precisely move using a close loop system. It also contains built-in end stops switches as well as an inertial measurement unit to ensure it can safely operate autonomously.","title":"Motion System"},{"location":"Farmers%20Dashboard/motion/#motion-system","text":"The platform is battery powered (12V LiPo 2Ah) and uses a brushless servo motor (DFROBOT FIT0441) combined with an optical cable tracker (ADNS-9800) to precisely move using a close loop system. It also contains built-in end stops switches as well as an inertial measurement unit to ensure it can safely operate autonomously.","title":"Motion system"},{"location":"Farmers%20Dashboard/pipeline/","text":"Generating Orthomosaic Image Link For the scope of this application, a rigorous system of collecting photographs of the fields was set, representing a first set of data to process; also to test different strategies of automatic navigation paths, by sending the same waypoints to the drones and to cable-bot. This set of instructions are basically following the rules of sweeping the entire area to then transform the data into an orthomosaic image which covers the whole crop. It\u2019s important that the images have around 50% to 70% vertical overlap and around 30% overlap on sides. Each two pairs of images which have overlaps together are processed to find the same key points available between both (see fig 2) and following this pairing the images are geometrically corrected (orthorectified) such that the scale is uniform in all of them. OpenDroneMap software/API was used to generate the orthomosaic image, the final outcome of this process is an aerial image of the whole crop which has a uniform scale. (see fig 3) Frame Alignment Generated orthomosaic images could contain some extra parts such as extra borders, as well as random orientation. To be able to compare and analyze the maps generated from each scan overtime, we need to orient and crop them with similar boundaries and orientation. For this we include two processes of rotating the image if needed to straighten it, and as well, cropping the unnecessary parts in the borders to optimize the computation parts related to segmentation and future steps. The process of straightening the image starts by applying principal component analysis (PCA) to extract the axis line of the image, this axis line is then used to straighten the image. (see fig 4) After aligning and cropping the orthomosaic images, they are stored in our database for the image segmentation task. Plant Detection Link The collected orthomosaic images are generated with the pipeline explained above. In this section, we describe how the lettuces are detected and masked using Mask-RCNN, which is an instance segmentation algorithm. The generated output images show individual masks for each lettuce that is detected. The Mask-RCNN method has proven to be accurate (see the discussion of the segmentation methods in Section 1.3.6 - T6.3) . This method is able to separate lettuces from different kinds of plants or weeds, as well works well with different light and shadow conditions. The orthomosaic images generated in step 4 are high quality large scale images. Training and Detection algorithms on these large scale images are computationally heavy therefore to apply the detection on orthomosaic images, first we divide the images to a smaller grid. The division and cell size of this grid is dependent on the crop scenario and can be defined manually by the user. The detection algorithm is applied on each cell (see Fig. 3.7). The results are merged back together to generate the overall detected image (see Fig. 3.8). In addition to the masked images, a json file containing the position and area (in the scale of image) of each detected plant is stored as well. In the next stages the masked images and the json files are used to catalog individual plants and monitor them through different scans. Tracking Individual Plants Link Once we have segmented the scan image and obtained the list of individual plants, we have to identify each plant and track each individual throughout all the scans. We tested two methods to track individual plants. In the first method, we detect each plant in each image and extract the center point of each lettuce, the centers points from each scan are then compared together with Iterative Closest Point (ICP) algorithm to track the same lettuce in different scans. In the second method, we register all maps to a common frame using feature matching and detected plants at initial growth stage. The area around each plant is then considered in all images for measuring each plant size as approximated by the projected leaf area. Iterative closest point algorithm on the centers of lettuces This process uses the center points of lettuces in consecutive scans for the ICP method to find the same lettuces over different scans. However this method has two major issues. Undetected lettuces in some images create unmatched points Changes in appearance of lettuces move their center. Registration Through Feature Matching In this process we use the position of the detected lettuces as well as the orthomosaic image generated in step 4, to track individual plants over different scans. One of the main challenges in registering and tracking individual plants in this process is to have the same frame and coordinate system for all the images, This challenge is due to two main reasons. First, the orthomosaic images have different resolutions, as well the main frame for orthomosaic images is not exactly the same for all the images. To create the same coordinate system for all the images, we use the first scan as our reference coordinate system and find the transformations between each two consecutive images using SURF feature matching methods available in OpenCV library which is based on the RANSAC algorithm. (Fig. 3.10) For a series of images, the transformations between each two consecutive images gets combined with all the previous transformations to calculate the transformation to our reference image (first scan). Then all the images as well as the coordinates of plants from the json file are transformed to match the same coordinate system. (see Fig. 3.11) Creating a Catalog of Individual Plants Link Registration of images as well as plant\u2019s coordinates on the same coordinate system, allows for tracking of the same plant within different scans. This happens through clustering the plants\u2019 coordinates that are within a certain distance from each other (see Fig. 3.12). Next step is to Index the detected plant with the same ID over different scans. As well as creating a catalog of individual plants. Leaf Area Monitoring and plant growth curves Link The process of registering different scans over a common reference model results in having all the coordinates and detection images in the same scale. By knowing the scale of the reference image we can scale all the images to real size world coordinates. This is crucial for extracting the leaf area from 2D images. In order to extract the leaf area we calculate the amount of detected pixels (black) over the total amount of pixels in the image which is then multiplied by the scale factor of the image. (see Fig. 3.14) The individual leaf area extracted in each scan, is stored together with the plant\u2019s index in a database. Creation of weed maps Link The results of the semantic segmentation can be used to map the weeds as well as the crops (Fig. 3.16). The resulting weed map provides an indicator on the areas where the pressure of the weeds is highest. These maps can be used in combination with the weeding Rover to prioritise the weeding activities.","title":"Crops Insights Pipeline"},{"location":"Farmers%20Dashboard/pipeline/#generating-orthomosaic-image","text":"For the scope of this application, a rigorous system of collecting photographs of the fields was set, representing a first set of data to process; also to test different strategies of automatic navigation paths, by sending the same waypoints to the drones and to cable-bot. This set of instructions are basically following the rules of sweeping the entire area to then transform the data into an orthomosaic image which covers the whole crop. It\u2019s important that the images have around 50% to 70% vertical overlap and around 30% overlap on sides. Each two pairs of images which have overlaps together are processed to find the same key points available between both (see fig 2) and following this pairing the images are geometrically corrected (orthorectified) such that the scale is uniform in all of them. OpenDroneMap software/API was used to generate the orthomosaic image, the final outcome of this process is an aerial image of the whole crop which has a uniform scale. (see fig 3) Frame Alignment Generated orthomosaic images could contain some extra parts such as extra borders, as well as random orientation. To be able to compare and analyze the maps generated from each scan overtime, we need to orient and crop them with similar boundaries and orientation. For this we include two processes of rotating the image if needed to straighten it, and as well, cropping the unnecessary parts in the borders to optimize the computation parts related to segmentation and future steps. The process of straightening the image starts by applying principal component analysis (PCA) to extract the axis line of the image, this axis line is then used to straighten the image. (see fig 4) After aligning and cropping the orthomosaic images, they are stored in our database for the image segmentation task.","title":"Generating Orthomosaic Image"},{"location":"Farmers%20Dashboard/pipeline/#plant-detection","text":"The collected orthomosaic images are generated with the pipeline explained above. In this section, we describe how the lettuces are detected and masked using Mask-RCNN, which is an instance segmentation algorithm. The generated output images show individual masks for each lettuce that is detected. The Mask-RCNN method has proven to be accurate (see the discussion of the segmentation methods in Section 1.3.6 - T6.3) . This method is able to separate lettuces from different kinds of plants or weeds, as well works well with different light and shadow conditions. The orthomosaic images generated in step 4 are high quality large scale images. Training and Detection algorithms on these large scale images are computationally heavy therefore to apply the detection on orthomosaic images, first we divide the images to a smaller grid. The division and cell size of this grid is dependent on the crop scenario and can be defined manually by the user. The detection algorithm is applied on each cell (see Fig. 3.7). The results are merged back together to generate the overall detected image (see Fig. 3.8). In addition to the masked images, a json file containing the position and area (in the scale of image) of each detected plant is stored as well. In the next stages the masked images and the json files are used to catalog individual plants and monitor them through different scans.","title":"Plant Detection"},{"location":"Farmers%20Dashboard/pipeline/#tracking-individual-plants","text":"Once we have segmented the scan image and obtained the list of individual plants, we have to identify each plant and track each individual throughout all the scans. We tested two methods to track individual plants. In the first method, we detect each plant in each image and extract the center point of each lettuce, the centers points from each scan are then compared together with Iterative Closest Point (ICP) algorithm to track the same lettuce in different scans. In the second method, we register all maps to a common frame using feature matching and detected plants at initial growth stage. The area around each plant is then considered in all images for measuring each plant size as approximated by the projected leaf area. Iterative closest point algorithm on the centers of lettuces This process uses the center points of lettuces in consecutive scans for the ICP method to find the same lettuces over different scans. However this method has two major issues. Undetected lettuces in some images create unmatched points Changes in appearance of lettuces move their center. Registration Through Feature Matching In this process we use the position of the detected lettuces as well as the orthomosaic image generated in step 4, to track individual plants over different scans. One of the main challenges in registering and tracking individual plants in this process is to have the same frame and coordinate system for all the images, This challenge is due to two main reasons. First, the orthomosaic images have different resolutions, as well the main frame for orthomosaic images is not exactly the same for all the images. To create the same coordinate system for all the images, we use the first scan as our reference coordinate system and find the transformations between each two consecutive images using SURF feature matching methods available in OpenCV library which is based on the RANSAC algorithm. (Fig. 3.10) For a series of images, the transformations between each two consecutive images gets combined with all the previous transformations to calculate the transformation to our reference image (first scan). Then all the images as well as the coordinates of plants from the json file are transformed to match the same coordinate system. (see Fig. 3.11)","title":"Tracking Individual Plants"},{"location":"Farmers%20Dashboard/pipeline/#creating-a-catalog-of-individual-plants","text":"Registration of images as well as plant\u2019s coordinates on the same coordinate system, allows for tracking of the same plant within different scans. This happens through clustering the plants\u2019 coordinates that are within a certain distance from each other (see Fig. 3.12). Next step is to Index the detected plant with the same ID over different scans. As well as creating a catalog of individual plants.","title":"Creating a Catalog of Individual Plants"},{"location":"Farmers%20Dashboard/pipeline/#leaf-area-monitoring-and-plant-growth-curves","text":"The process of registering different scans over a common reference model results in having all the coordinates and detection images in the same scale. By knowing the scale of the reference image we can scale all the images to real size world coordinates. This is crucial for extracting the leaf area from 2D images. In order to extract the leaf area we calculate the amount of detected pixels (black) over the total amount of pixels in the image which is then multiplied by the scale factor of the image. (see Fig. 3.14) The individual leaf area extracted in each scan, is stored together with the plant\u2019s index in a database.","title":"Leaf Area Monitoring and plant growth curves"},{"location":"Farmers%20Dashboard/pipeline/#creation-of-weed-maps","text":"The results of the semantic segmentation can be used to map the weeds as well as the crops (Fig. 3.16). The resulting weed map provides an indicator on the areas where the pressure of the weeds is highest. These maps can be used in combination with the weeding Rover to prioritise the weeding activities.","title":"Creation of weed maps"},{"location":"Farmers%20Dashboard/power/","text":"Power Link Power Link The power circuit has a main switch that avoids battery discharging during transportation or storing. Battery voltage is feeded directly, a DC/DC voltage converter provides low voltage to the logic electronics subsystem. The battery voltage is feeded camera module allowing high voltage motors. Both battery terminals are directly exposed to the charger station allowing the carge process during rest periods. Power distribution PCB Link DC/DC switching regulator Link To provide low voltage (5v) the OKI-78SR-5/1.5-W36E-C DC/DC switching regulator is used. It can provide 1.5A @ 5v and can stand inputs as high as 36v input. With this regulator we feed all the electronics contained in the carrier unit the camera module electronics use the same component to provide low voltage to it's own electronics. Battery module Link","title":"Power"},{"location":"Farmers%20Dashboard/power/#power","text":"","title":"Power"},{"location":"Farmers%20Dashboard/power/#power_1","text":"The power circuit has a main switch that avoids battery discharging during transportation or storing. Battery voltage is feeded directly, a DC/DC voltage converter provides low voltage to the logic electronics subsystem. The battery voltage is feeded camera module allowing high voltage motors. Both battery terminals are directly exposed to the charger station allowing the carge process during rest periods.","title":"Power"},{"location":"Farmers%20Dashboard/power/#power-distribution-pcb","text":"","title":"Power distribution PCB"},{"location":"Farmers%20Dashboard/power/#dcdc-switching-regulator","text":"To provide low voltage (5v) the OKI-78SR-5/1.5-W36E-C DC/DC switching regulator is used. It can provide 1.5A @ 5v and can stand inputs as high as 36v input. With this regulator we feed all the electronics contained in the carrier unit the camera module electronics use the same component to provide low voltage to it's own electronics.","title":"DC/DC switching regulator"},{"location":"Farmers%20Dashboard/power/#battery-module","text":"","title":"Battery module"},{"location":"Farmers%20Dashboard/station/","text":"Size: 532mm x 240 x 805mm Weight: 3900gr (Without Access Point) Power Input - 120-240V AC, Output - 17V DC The Charging Station module (CHAM) provides several features to the cablebot system: Rest position with minimal power consumption (motor can be disabled). Homming zero position with known precise geo position. Shelter from enviromental hazzards. Automatic charging via pogo pins contact connector. Optional WiFi access point. It allows the Carrier Module to park in a position that offers shelter against weather elements in addition to providing the necessary voltage for battery charging. A mechanical/magnetic clip system was designed to hold the CARM in a fixed rest position, once parked, the motor can be disabled to save power and a pogo pin contact connector enables charging. Alignment guides allow parking even under windy conditions. On homming the CARM obtains a georeferenced fixed point from which precise position can be calculated for geotagging captured images. The CHAM holds an AC/DC power supply and an optional Wi-Fi access point. Installation ca be done directly on the cable with a 4 tensioning system that guarantees stability and allows correct alignment. A vertical fixed post or flat surface such as the wall of a building can also be used as holding structures. The carrier has three pogo pins that, via an interrupt signal, allows it to know when it is on home position and start the charging process. :q","title":"Charging Station"},{"location":"Farmers%20Dashboard/structure/","text":"Structure Link The whole design uses a CNC folded aluminium-plastic sandwich panel designed to provide adequate outdoor resistance while being light-weight. The moving parts and other fixtures use custom 3D printed plastic (PLA/ABS). All the cabling and electronics integrate into the internal structure to minimize damage caused by environmental factors. The tension mechanism design allows it to move it from one cable to another as well to adapt to different cable tensions dynamically, and it uses a self-lubricated polymer to avoid maintenance. All the parts can be made on a Fab Lab or other rapid prototyping facility with no customized tooling required. To facilitate documentation and assembly the Carrier Module is conceptually grouped in different parts: The hands hold the lateral pulleys supporting the module on the cable; the arms are the main simetrical structure, joined by the central part of the sliding dynamic tension mechanism; the head is the moving part that holds the motor ant ist electronics; and finally the body that holds everything togheter and works as a bridge for the cabling between the different components. Hands Link The miniature switches used as end stops are held by a 3d printed piece that integrates a plastic hinge as trigger element. Integrated in the same housing are the 3d printed lateral pulleys held by a 608zz bearing. Dinamyc Tension Link The Farmers Dashboard carrier has a system the allows dinamyc adaptation to different cable tensions, allowing a light motor torque to move even on high tension cables neede to cover long distances and avoiding slips on low tension cables. Three pieces of machined HDPE in sandwich form are used to allow smooth movement of the central part of the robot maintaining correct alignment. To allow a bigger range of tensions pre-compresion level of the springs can be adjusted by turning the three screws under the base piece. By turning the front screws the level of friction between the HDPE slides can also be regulated.","title":"Structure"},{"location":"Farmers%20Dashboard/structure/#structure","text":"The whole design uses a CNC folded aluminium-plastic sandwich panel designed to provide adequate outdoor resistance while being light-weight. The moving parts and other fixtures use custom 3D printed plastic (PLA/ABS). All the cabling and electronics integrate into the internal structure to minimize damage caused by environmental factors. The tension mechanism design allows it to move it from one cable to another as well to adapt to different cable tensions dynamically, and it uses a self-lubricated polymer to avoid maintenance. All the parts can be made on a Fab Lab or other rapid prototyping facility with no customized tooling required. To facilitate documentation and assembly the Carrier Module is conceptually grouped in different parts: The hands hold the lateral pulleys supporting the module on the cable; the arms are the main simetrical structure, joined by the central part of the sliding dynamic tension mechanism; the head is the moving part that holds the motor ant ist electronics; and finally the body that holds everything togheter and works as a bridge for the cabling between the different components.","title":"Structure"},{"location":"Farmers%20Dashboard/structure/#hands","text":"The miniature switches used as end stops are held by a 3d printed piece that integrates a plastic hinge as trigger element. Integrated in the same housing are the 3d printed lateral pulleys held by a 608zz bearing.","title":"Hands"},{"location":"Farmers%20Dashboard/structure/#dinamyc-tension","text":"The Farmers Dashboard carrier has a system the allows dinamyc adaptation to different cable tensions, allowing a light motor torque to move even on high tension cables neede to cover long distances and avoiding slips on low tension cables. Three pieces of machined HDPE in sandwich form are used to allow smooth movement of the central part of the robot maintaining correct alignment. To allow a bigger range of tensions pre-compresion level of the springs can be adjusted by turning the three screws under the base piece. By turning the front screws the level of friction between the HDPE slides can also be regulated.","title":"Dinamyc Tension"},{"location":"Farmers%20Dashboard/tension/","text":"Dinamyc Tension Link The Farmers Dashboard carrier has a system the allows dinamyc adaptation to different cable tensions, allowing a light motor torque to move even on high tension cables neede to cover long distances and avoiding slips on low tension cables. Three pieces of machined HDPE in sandwich form are used to allow smooth movement of the central part of the robot maintaining correct alignment. To allow a bigger range of tensions pre-compresion level of the springs can be adjusted by turning the three screws under the base piece. By turning the front screws the level of friction between the HDPE slides can also be regulated. Still testing Test for finding the right combination of spring strength and movement range for common cable setups on poly tunnels is still on.","title":"Dinamyc Tension"},{"location":"Farmers%20Dashboard/tension/#dinamyc-tension","text":"The Farmers Dashboard carrier has a system the allows dinamyc adaptation to different cable tensions, allowing a light motor torque to move even on high tension cables neede to cover long distances and avoiding slips on low tension cables. Three pieces of machined HDPE in sandwich form are used to allow smooth movement of the central part of the robot maintaining correct alignment. To allow a bigger range of tensions pre-compresion level of the springs can be adjusted by turning the three screws under the base piece. By turning the front screws the level of friction between the HDPE slides can also be regulated. Still testing Test for finding the right combination of spring strength and movement range for common cable setups on poly tunnels is still on.","title":"Dinamyc Tension"},{"location":"Farmers%20Dashboard/use/","text":"How to use the cablebot Link On/off Hanging on cable RC operation Scheduling tasks Charging Troubleshooting guide","title":"How to use the cablebot"},{"location":"Farmers%20Dashboard/use/#how-to-use-the-cablebot","text":"On/off Hanging on cable RC operation Scheduling tasks Charging Troubleshooting guide","title":"How to use the cablebot"},{"location":"Rover/","text":"The Romi Rover documentation Link The documentation for the Romi Rover is split over the following files: File Description Manual The user manual gives an overview of how to use the rover and what it can do. Configuration The doc details the configuration file and the scripts file of the rover. Hardware In the hardware section you can find (1) the drawings for the rover's frame (prototype V3), (2) the 3D designs of the printed components, and (3) the schema and components of the electronics. Software The software section gives an overview of the software of the rover. Librcom This document provides additional details of the rcom communication library that is used by the rover and that may be useful for other projects also. Happy hacking!","title":"Overview"},{"location":"Rover/#the-romi-rover-documentation","text":"The documentation for the Romi Rover is split over the following files: File Description Manual The user manual gives an overview of how to use the rover and what it can do. Configuration The doc details the configuration file and the scripts file of the rover. Hardware In the hardware section you can find (1) the drawings for the rover's frame (prototype V3), (2) the 3D designs of the printed components, and (3) the schema and components of the electronics. Software The software section gives an overview of the software of the rover. Librcom This document provides additional details of the rcom communication library that is used by the rover and that may be useful for other projects also. Happy hacking!","title":"The Romi Rover documentation"},{"location":"Rover/api-nodes/","text":"API of the nodes Link The control software of the Romi Rover consists of number of nodes that communicate among each other. A number of topics are defined. These topics are a bit like an API. Besides the topics, the nodes also use different types of communication patterns. In most cases, the interaction with a node is similar to a remote procedure call. The client sends a JSON request over a messagelink (a websocket) to the node. The node replies with a JSON response message. In some cases, a simple HTTP request and response is used instead of a messagelink. In other cases, the node only braodcasts out JSON formatted events over a websocket. The types of communications are listed below. Type Description service Uses the HTML request-response pattern controller Uses RPC over WebSocket streamer Transmits a continuous data flow over HTML messagehub Communicates over a WebSocket datahub Broadcasts messages over UDP Different nodes can implement the same topic. For example, the nodes video4linux , picamera , fake_camera all implement the camera topic. However, only one of these nodes should be active at one time. A nodes can also implement several topics. For example, the motor controller implements both the motorcontroller and encoders topics. This document does not go into the details of messagelinks or how to do an HTTP request. Here we simply document all the available topics, the type of communication, and their messages they expect. In rcom, a communication end-point is identified by the combination of topic and type. So a streamer with the topic camera is distinct from the service with the same topic. The following table lists all the topics and the types that have been defined by Romi Rover. In the sections below you will find more details on each. API Description Type camera Provides RGB images service and streamer camera_recorder Records a sequence of images to disk controller cnc Controls a XYZ motion device controller configuration Exports the configuration file service control_panel Controls the display and power relays controller encoders Broadcasts the encoder values of the wheels datahub fsdb Broadcasts events about newly created files messagehub gimbal Controls the camera mount controller motorcontroller Controls the wheel motors controller navigation Controls the displacement of the rover controller pose Broadcasts the position and orientation of the rover datahub proxy A web proxy to all the nodes and a web server for static pages service & messagehub script_engine Executes scripts service tool_carrier Handles the mechanical weeding tool carrier controller camera Link The existing implementations are: video4linux , realsense , picamera , and fake_camera . The camera combines two communication end-points interfaces. It has a service that provides single JPEG images. It also has a streamer interface that broadcasts a continuous stream of JPEG images encoded as a multipart response (the corresponding mimetype is \"multipart/x-mixed-replace\"). The two handled URIs are: /camera.jpg : Use this URI to retrieve the latest RGB image from the camera service. /stream.html : This URI to get a continuous, video-over-html stream from the camera streamer. camera_recorder Link The camera_recorder is a controller that connects to a camera. Upon request, it will start recording the images of the camera to disk. It accepts to commands: start and stop. start Link Start recording the images to disk. Example: {\"command\": \"start\"} stop Link Stop the recording. Example: {\"command\": \"stop\"} configuration Link The configuration service allows nodes to obtain the settings of the rover. A simple HTML request with the name of the settings will return its associated value as a JSON-formatted object. Suppose the configuration file contains the following: { \"menu\" : { \"starters\" : [ \"velout\u00e9 de champignons\" , \"tomato & mozarella\" ], \"mains\" : [ \"eggplant lasagna\" , \"meatloaf with mashed potatoes\" ], \"deserts\" : [ \"chocolate mousse\" , \"panna cotta\" , \"cr\u00e8me brul\u00e9e\" ], } } Then an HTTP request to the URI /menu/starters will return [\"velout\u00e9 de champignons\", \"tomato & mozarella\"] in the body of the response. Implementation note: You can use the function client_get in the rcom API to obtain the value directly as a json_object_t : json_object_t list = client_get ( \"configuration\" , \"menu/starters\" ); control_panel Link The control_panel is the controller that interfaces with the physical control panel of the rover. It currently handles to commands: shutdown and display. shutdown Link Goes through the following steps: Asks the control panel to cut the power circuit (cuts the motors but not the controllers), then initiates the shutdown of the on-board computer, and requests the control panel to cut the power of the logical circuit (with a 5 seconds delay). Example: {\"command\": \"shutdown\"} display Link Asks the control panel to display a message. The length of the message is currently limited to 16 characters. The message may not be displayed immediately, or may be skipped, if too many messages are sent. Example: {\"command\": \"display\", \"message\": \"No network\"} cnc Link Existing implementations: grbl , oquam , and fake_cnc . The CNC has a JSON-over-WebSocket controller interface that accepts the commands documented below. moveto Link Move to a specified absolute position in meters. Example: {\"command\": \"moveto\", \"x\": 0.4, \"y\": 0.4} {\"command\": \"moveto\", \"z\": -0.1} homing Link Move to the home position, if the CNC has limit switches. Example: {\"command\": \"homing\"} travel Link The travel command serves to make the CNC travel a given path, defined by a list of points in absolute coordinates in meter. Example: {\"command\": \"travel\", \"path\": [[0,0,0], [0.5,0,0], [0.5,0.5,0], [0,0.5,0], [0,0,0]]} spindle Link Starts or stops the spindle. A speed can be specified with a value between 0 (stopped) and 1 (maximum speed). Example: {\"command\": \"spindle\", \"speed\": 1} encoders Link fsdb Link gimbal Link motor_controller Link navigation Link pose Link proxy Link script_engine Link tool_carrier Link","title":"Api nodes"},{"location":"Rover/api-nodes/#api-of-the-nodes","text":"The control software of the Romi Rover consists of number of nodes that communicate among each other. A number of topics are defined. These topics are a bit like an API. Besides the topics, the nodes also use different types of communication patterns. In most cases, the interaction with a node is similar to a remote procedure call. The client sends a JSON request over a messagelink (a websocket) to the node. The node replies with a JSON response message. In some cases, a simple HTTP request and response is used instead of a messagelink. In other cases, the node only braodcasts out JSON formatted events over a websocket. The types of communications are listed below. Type Description service Uses the HTML request-response pattern controller Uses RPC over WebSocket streamer Transmits a continuous data flow over HTML messagehub Communicates over a WebSocket datahub Broadcasts messages over UDP Different nodes can implement the same topic. For example, the nodes video4linux , picamera , fake_camera all implement the camera topic. However, only one of these nodes should be active at one time. A nodes can also implement several topics. For example, the motor controller implements both the motorcontroller and encoders topics. This document does not go into the details of messagelinks or how to do an HTTP request. Here we simply document all the available topics, the type of communication, and their messages they expect. In rcom, a communication end-point is identified by the combination of topic and type. So a streamer with the topic camera is distinct from the service with the same topic. The following table lists all the topics and the types that have been defined by Romi Rover. In the sections below you will find more details on each. API Description Type camera Provides RGB images service and streamer camera_recorder Records a sequence of images to disk controller cnc Controls a XYZ motion device controller configuration Exports the configuration file service control_panel Controls the display and power relays controller encoders Broadcasts the encoder values of the wheels datahub fsdb Broadcasts events about newly created files messagehub gimbal Controls the camera mount controller motorcontroller Controls the wheel motors controller navigation Controls the displacement of the rover controller pose Broadcasts the position and orientation of the rover datahub proxy A web proxy to all the nodes and a web server for static pages service & messagehub script_engine Executes scripts service tool_carrier Handles the mechanical weeding tool carrier controller","title":"API of the nodes"},{"location":"Rover/api-nodes/#camera","text":"The existing implementations are: video4linux , realsense , picamera , and fake_camera . The camera combines two communication end-points interfaces. It has a service that provides single JPEG images. It also has a streamer interface that broadcasts a continuous stream of JPEG images encoded as a multipart response (the corresponding mimetype is \"multipart/x-mixed-replace\"). The two handled URIs are: /camera.jpg : Use this URI to retrieve the latest RGB image from the camera service. /stream.html : This URI to get a continuous, video-over-html stream from the camera streamer.","title":"camera"},{"location":"Rover/api-nodes/#camera_recorder","text":"The camera_recorder is a controller that connects to a camera. Upon request, it will start recording the images of the camera to disk. It accepts to commands: start and stop.","title":"camera_recorder"},{"location":"Rover/api-nodes/#start","text":"Start recording the images to disk. Example: {\"command\": \"start\"}","title":"start"},{"location":"Rover/api-nodes/#stop","text":"Stop the recording. Example: {\"command\": \"stop\"}","title":"stop"},{"location":"Rover/api-nodes/#configuration","text":"The configuration service allows nodes to obtain the settings of the rover. A simple HTML request with the name of the settings will return its associated value as a JSON-formatted object. Suppose the configuration file contains the following: { \"menu\" : { \"starters\" : [ \"velout\u00e9 de champignons\" , \"tomato & mozarella\" ], \"mains\" : [ \"eggplant lasagna\" , \"meatloaf with mashed potatoes\" ], \"deserts\" : [ \"chocolate mousse\" , \"panna cotta\" , \"cr\u00e8me brul\u00e9e\" ], } } Then an HTTP request to the URI /menu/starters will return [\"velout\u00e9 de champignons\", \"tomato & mozarella\"] in the body of the response. Implementation note: You can use the function client_get in the rcom API to obtain the value directly as a json_object_t : json_object_t list = client_get ( \"configuration\" , \"menu/starters\" );","title":"configuration"},{"location":"Rover/api-nodes/#control_panel","text":"The control_panel is the controller that interfaces with the physical control panel of the rover. It currently handles to commands: shutdown and display.","title":"control_panel"},{"location":"Rover/api-nodes/#shutdown","text":"Goes through the following steps: Asks the control panel to cut the power circuit (cuts the motors but not the controllers), then initiates the shutdown of the on-board computer, and requests the control panel to cut the power of the logical circuit (with a 5 seconds delay). Example: {\"command\": \"shutdown\"}","title":"shutdown"},{"location":"Rover/api-nodes/#display","text":"Asks the control panel to display a message. The length of the message is currently limited to 16 characters. The message may not be displayed immediately, or may be skipped, if too many messages are sent. Example: {\"command\": \"display\", \"message\": \"No network\"}","title":"display"},{"location":"Rover/api-nodes/#cnc","text":"Existing implementations: grbl , oquam , and fake_cnc . The CNC has a JSON-over-WebSocket controller interface that accepts the commands documented below.","title":"cnc"},{"location":"Rover/api-nodes/#moveto","text":"Move to a specified absolute position in meters. Example: {\"command\": \"moveto\", \"x\": 0.4, \"y\": 0.4} {\"command\": \"moveto\", \"z\": -0.1}","title":"moveto"},{"location":"Rover/api-nodes/#homing","text":"Move to the home position, if the CNC has limit switches. Example: {\"command\": \"homing\"}","title":"homing"},{"location":"Rover/api-nodes/#travel","text":"The travel command serves to make the CNC travel a given path, defined by a list of points in absolute coordinates in meter. Example: {\"command\": \"travel\", \"path\": [[0,0,0], [0.5,0,0], [0.5,0.5,0], [0,0.5,0], [0,0,0]]}","title":"travel"},{"location":"Rover/api-nodes/#spindle","text":"Starts or stops the spindle. A speed can be specified with a value between 0 (stopped) and 1 (maximum speed). Example: {\"command\": \"spindle\", \"speed\": 1}","title":"spindle"},{"location":"Rover/api-nodes/#encoders","text":"","title":"encoders"},{"location":"Rover/api-nodes/#fsdb","text":"","title":"fsdb"},{"location":"Rover/api-nodes/#gimbal","text":"","title":"gimbal"},{"location":"Rover/api-nodes/#motor_controller","text":"","title":"motor_controller"},{"location":"Rover/api-nodes/#navigation","text":"","title":"navigation"},{"location":"Rover/api-nodes/#pose","text":"","title":"pose"},{"location":"Rover/api-nodes/#proxy","text":"","title":"proxy"},{"location":"Rover/api-nodes/#script_engine","text":"","title":"script_engine"},{"location":"Rover/api-nodes/#tool_carrier","text":"","title":"tool_carrier"},{"location":"Rover/configuration/","text":"Romi Rover configuration Link The Romi Rover control software uses the following input files (see also the Software documentation: The configuration file: Most of the settings of the rover can be modified in this file. The script file: This file lists all the commands of the rover that are actionnable by the user, and what these commands should do. The configuration and script files are discussed in detail below. Configuration file Link The rover control software is starting using the romi-rover command. This command takes the path to the main configuration as an argument. Example configutation files can be found in the romi-rover/config directory. The configuration data is formated in JSON and is a dictionnary of top-level section names and the values for that section. It's overall structure is: { \"imager\" : \"...\" , \"navigation\" : { }, \"oquam\" : { }, \"ports\" : { }, \"user-interface\" : { }, \"weeder\" : { } } The navigation section contains the parameters related to the navigation, from the low-level settings of the motor drivers to the settings for the autonomous navigation. The oquam section contains all the parameters related to the CNC. The ports section lists all of the serial ports and their usage. The user-interface is used to configure such things as the controller to rover speed mapping. Finally, in the weeder section you can change the settings related to the weeding algorithm. We will discuss each of these sections below. Dimensions Link As much as possible, the configuration and script files uses meter for distances, seconds for time, m/s for speeds, m/s\u00b2 for acceleration. We deviate from the scientific standard for angles, for which we use degrees instead of radians. The navigation section Link An example of a complete navigation section is shown below: { \"navigation\" : { \"rover\" : { \"wheel-diameter\" : 0.47 , \"wheelbase\" : 1.45 , \"wheeltrack\" : 1.35 , \"maximum-speed\" : 3 , \"maximum-acceleration\" : 0.5 }, \"brush-motor-driver\" : { \"encoder-directions\" : { \"left\" : -1 , \"right\" : 1 }, \"encoder-steps\" : 16000 , \"maximum-signal-amplitude\" : 500 , \"pid\" : { \"ki\" : [ 3 , 100 ], \"kp\" : [ 100 , 100 ] } }, \"track-follower\" : \"python\" } } The rover section sets some of the physical parameters of the rover: Name Description wheel-diameter The diameter of the motorized wheel, in meter (m) wheelbase The distance between the axes of the front wheel and the back wheel (m) wheeltrack The distance between the back wheels (front wheels), measured from the middle of the left wheel to the middle of the right wheel (m) maximum-speed The maximum allowed speed (m/s) maximum-acceleration The maximum allowed acceleration (m/s\u00b2) The brush-motor-driver object has a number of settings for the firmware that controls the motors:. Name Description encoder-directions This value is an array of two values, the first concerns the left wheel, the second concerns the right wheel. The values are either 1 or -1. In case the value is 1, the encoder values decrement when the wheel is turning forwards. In case the value is 1, the encoder values increment when the wheel is turning forwards. encoder-steps The number of encoder steps for a full turn of the wheel. This should take into account the number of steps of the encoder itself, plus the number of turns of the gearbox. maximum-signal-amplitude The maximum signal that the driver can send to the motor driver. Should be a value > 0 and <= 500; pid These are the values for the PI controller - a PID Controller without the 'D' - used by the driver. Two values are needed: the constants Kp and Ki. They are not specified as floating-point values but as a couple [numerator, denominator]. So [3, 100] means 3/100 th . Finally, the track-follower settings defines which algorithm should be used for the track following used for autonomous navigation. Three options are available at the time of writing: Name Description odometry Use the encoders on the wheels. manual Expect manual input from the controller. python This is the line following algorithm currently being tested that follows a line of crops. The name will likely change when the algorithms is integrated in the main software. The Oquam section Link { \"oquam\" : { \"cnc-range\" : [[ 0 , 1.0 ], [ 0 , 0.75 ], [ -0.4 , 0 ]], \"path-maximum-deviation\" : 0.01 , \"path-slice-duration\" : 0.02 , \"stepper-settings\" : { \"steps-per-revolution\" : [ 200 , 200 , 200 ], \"microsteps\" : [ 8 , 8 , 1 ], \"gears-ratio\" : [ 1 , 1 , 1 ], \"displacement-per-revolution\" : [ 0.04 , 0.04 , -0.0015 ], \"maximum-rpm\" : [ 300 , 300 , 300 ], \"maximum-acceleration\" : [ 0.3 , 0.3 , 0.03 ] } } } Name Description cnc-range The dimensions of the three axes of the CNC. The values must be provided as an array of three couples, for the x, y, and z axis. Each couple specifies the minimum and maximum position in meters. path-maximum-deviation This value, in meters, sets the maximum allowed deviation from the ideal path when the CNC is requested to trace a polygonal path. By allowing a small deviation, the CNC can maintain a given speed while assuring that the maximum accelerations (and thus forces) are respected. It allows for smoother path travelings. path-slice-duration A long path is sliced into small segments of constant speed. This variable sets the default duration of these segments. The stepper-settings provide the information on the stepper motors that are used. The following six pieces of information are required: Name Description steps-per-revolution The number os steps per revolution of the stepper motors, for the x, y, and z axis. microsteps If micro-stepping was enabled, the number of micro-steps for the x, y, and z axis (a value of 1 indicates no micro-stepping). gears-ratio If the stepper motors use gears, provide the gear ratio. 1 means no gearbox is used. A value of N means that N revolutions of the stepper motor are required for 1 revolution of the output axis, or that the driver has to send N times more steps to the stepper motor for the output axis to complete a revolution. displacement-per-revolution This value specifies by how much the CNC moves, in meter, for one revolution of the output axis of the motor + gearbox combination. This value is related to the size of the pulley that pulls on the belt. maximum-rpm The maximum speed of the stepper motors. The datasheets of the stepper motor generaly indicate this value in revolutions per minute (rpm). maximum-acceleration The maximum allowed acceleration, in m/s\u00b2, for each of the axes. The ports section Link This dictionnary lists which firmware drivers are available on what system ports. The rover uses mostly serial connections. The joystick uses a different device, however. The rcdiscover utility can be used to generate this list: $ ./bin/rcdiscover path/to/config.json The list has entries as follows: { \"ports\" : { \"oquam\" : { \"port\" : \"/dev/ttyACM5\" , \"type\" : \"serial\" } } } First comes the name of the firmware that is accessible through this port. It tells the type of the port ('serial' or 'input-device'), and the device's path in the 'port' field). The user-interface section Link The user interface config contains the settings for the speed controller. There are two drive modes: fast and accurate. Each have their own settings: { \"user-interface\" : { \"speed-controller\" : { \"accurate\" : { \"speed-multiplier\" : 0.2 , \"direction-multiplier\" : 0.05 , \"use-speed-curve\" : true , \"speed-curve-exponent\" : 1 , \"use-direction-curve\" : true , \"direction-curve-exponent\" : 1 }, \"fast\" : { \"speed-multiplier\" : 0.3 , \"direction-multiplier\" : 0.15 , \"use-speed-curve\" : true , \"speed-curve-exponent\" : 1 , \"use-direction-curve\" : true , \"direction-curve-exponent\" : 1 } } } } The speed controller sets the the speed of the left and right wheel and thus the radius of the turn that rover will follow. The two input values from the controller that set the speed and direction are mapped onto the range [-1, 1]. These normalized values are then used to determine the relative speed of the left and right wheel. The absolute speed of the rover is the product of the relative speed and the absolute speed given in the rover settings discussed above. The relative speed of the left wheel is: speed_left = ( speed_multiplier * speed + direction_multiplier * direction ); where speed and direction are the normalized inputs from the controller. Name Description speed-multiplier This number, with a value between 0 and 1, determines the maximum relative speed when the the speed trigger is completely pressed by the user. direction-multiplier The number defines the maximum speed difference, and thus the maximum turning angle, between the left and right wheel. In addition, it is possible to use an exponential curve to map the speed and direction values. This may give a more smooth behaviour when the rover starts from stand-still to higher speeds. A speed of zero still maps to zero, and a speed of one still maps to one, but in between the mapping follows a curve instead of a straight line. The curve is as follows: x = (( exp ( exponent * x ) - 1.0 ) / ( exp ( exponent ) - 1.0 )); Name Description use-speed-curve true of false: should the speed controller use the exponential curve to map the speed value? speed-curve-exponent The exponent of the speed curve. use-direction-curve true of false: should the speed controller use the exponential curve to map the direction value? direction-curve-exponent The exponent of the direction curve. The weeder section Link The weeder sections contains the settings for the weeder tool. These are as follows: { \"weeder\" : { \"diameter-tool\" : 0.04 , \"imagecropper\" : { \"workspace\" : [ 306 , 210 , 985 , 745 ] }, \"speed\" : 0.8 , \"z0\" : -0.22 } } Name Description diameter-tool The size of the weeding tool head, in meter. imagecropper.workspace The crop size and position the camera image to obtain the image of the workspace only. speed The relative speed of the weeding tool, between 0 and 1.0. This speed is multiplied by the maximum speed of the CNC to obtain the final speed of the weeding tool. z0 The position of the weeding tool when weeding. Script file Link The script file consists of a JSON array of script objects: [ { \"id\" : \"script1\" , \"title\" : \"The First Script\" , \"script\" : [ //... ] }, //... ] The script objects have the following fields: Name Value Required Description id String Required The internal name of the script title String Required The name to be shown to end-users script Array Required The list of actions to be taken The script itself is an array that lists all the actions to be taken, one after the other. For example: [ { \"id\" : \"script1\" , \"title\" : \"The First Script\" , \"script\" : [ { \"action\" : \"start_recording\" }, { \"action\" : \"move\" , \"distance\" : 8 , \"speed\" : 400 }, { \"action\" : \"stop_recording\" }, { \"action\" : \"move\" , \"distance\" : -8 , \"speed\" : -400 } ] }, //... ] Each action object has an action field. The list of available actions are: Name Description start_recording Start recording the images of the top camera stop_recording Stop recording the images of the top camera move Move a given distance (at a given speed) hoe Start weeding homing Go to the home position (for rovers with a limit switch only) The move action takes the following parameters: Name Value Required Description distance Number (in meters) Required The distance to be travelled speed Number (-1000 < speed < 1000) Optional The speed The moveat action takes a speed value, as in the move command above.","title":"Configuration"},{"location":"Rover/configuration/#romi-rover-configuration","text":"The Romi Rover control software uses the following input files (see also the Software documentation: The configuration file: Most of the settings of the rover can be modified in this file. The script file: This file lists all the commands of the rover that are actionnable by the user, and what these commands should do. The configuration and script files are discussed in detail below.","title":"Romi Rover configuration"},{"location":"Rover/configuration/#configuration-file","text":"The rover control software is starting using the romi-rover command. This command takes the path to the main configuration as an argument. Example configutation files can be found in the romi-rover/config directory. The configuration data is formated in JSON and is a dictionnary of top-level section names and the values for that section. It's overall structure is: { \"imager\" : \"...\" , \"navigation\" : { }, \"oquam\" : { }, \"ports\" : { }, \"user-interface\" : { }, \"weeder\" : { } } The navigation section contains the parameters related to the navigation, from the low-level settings of the motor drivers to the settings for the autonomous navigation. The oquam section contains all the parameters related to the CNC. The ports section lists all of the serial ports and their usage. The user-interface is used to configure such things as the controller to rover speed mapping. Finally, in the weeder section you can change the settings related to the weeding algorithm. We will discuss each of these sections below.","title":"Configuration file"},{"location":"Rover/configuration/#dimensions","text":"As much as possible, the configuration and script files uses meter for distances, seconds for time, m/s for speeds, m/s\u00b2 for acceleration. We deviate from the scientific standard for angles, for which we use degrees instead of radians.","title":"Dimensions"},{"location":"Rover/configuration/#the-navigation-section","text":"An example of a complete navigation section is shown below: { \"navigation\" : { \"rover\" : { \"wheel-diameter\" : 0.47 , \"wheelbase\" : 1.45 , \"wheeltrack\" : 1.35 , \"maximum-speed\" : 3 , \"maximum-acceleration\" : 0.5 }, \"brush-motor-driver\" : { \"encoder-directions\" : { \"left\" : -1 , \"right\" : 1 }, \"encoder-steps\" : 16000 , \"maximum-signal-amplitude\" : 500 , \"pid\" : { \"ki\" : [ 3 , 100 ], \"kp\" : [ 100 , 100 ] } }, \"track-follower\" : \"python\" } } The rover section sets some of the physical parameters of the rover: Name Description wheel-diameter The diameter of the motorized wheel, in meter (m) wheelbase The distance between the axes of the front wheel and the back wheel (m) wheeltrack The distance between the back wheels (front wheels), measured from the middle of the left wheel to the middle of the right wheel (m) maximum-speed The maximum allowed speed (m/s) maximum-acceleration The maximum allowed acceleration (m/s\u00b2) The brush-motor-driver object has a number of settings for the firmware that controls the motors:. Name Description encoder-directions This value is an array of two values, the first concerns the left wheel, the second concerns the right wheel. The values are either 1 or -1. In case the value is 1, the encoder values decrement when the wheel is turning forwards. In case the value is 1, the encoder values increment when the wheel is turning forwards. encoder-steps The number of encoder steps for a full turn of the wheel. This should take into account the number of steps of the encoder itself, plus the number of turns of the gearbox. maximum-signal-amplitude The maximum signal that the driver can send to the motor driver. Should be a value > 0 and <= 500; pid These are the values for the PI controller - a PID Controller without the 'D' - used by the driver. Two values are needed: the constants Kp and Ki. They are not specified as floating-point values but as a couple [numerator, denominator]. So [3, 100] means 3/100 th . Finally, the track-follower settings defines which algorithm should be used for the track following used for autonomous navigation. Three options are available at the time of writing: Name Description odometry Use the encoders on the wheels. manual Expect manual input from the controller. python This is the line following algorithm currently being tested that follows a line of crops. The name will likely change when the algorithms is integrated in the main software.","title":"The navigation section"},{"location":"Rover/configuration/#the-oquam-section","text":"{ \"oquam\" : { \"cnc-range\" : [[ 0 , 1.0 ], [ 0 , 0.75 ], [ -0.4 , 0 ]], \"path-maximum-deviation\" : 0.01 , \"path-slice-duration\" : 0.02 , \"stepper-settings\" : { \"steps-per-revolution\" : [ 200 , 200 , 200 ], \"microsteps\" : [ 8 , 8 , 1 ], \"gears-ratio\" : [ 1 , 1 , 1 ], \"displacement-per-revolution\" : [ 0.04 , 0.04 , -0.0015 ], \"maximum-rpm\" : [ 300 , 300 , 300 ], \"maximum-acceleration\" : [ 0.3 , 0.3 , 0.03 ] } } } Name Description cnc-range The dimensions of the three axes of the CNC. The values must be provided as an array of three couples, for the x, y, and z axis. Each couple specifies the minimum and maximum position in meters. path-maximum-deviation This value, in meters, sets the maximum allowed deviation from the ideal path when the CNC is requested to trace a polygonal path. By allowing a small deviation, the CNC can maintain a given speed while assuring that the maximum accelerations (and thus forces) are respected. It allows for smoother path travelings. path-slice-duration A long path is sliced into small segments of constant speed. This variable sets the default duration of these segments. The stepper-settings provide the information on the stepper motors that are used. The following six pieces of information are required: Name Description steps-per-revolution The number os steps per revolution of the stepper motors, for the x, y, and z axis. microsteps If micro-stepping was enabled, the number of micro-steps for the x, y, and z axis (a value of 1 indicates no micro-stepping). gears-ratio If the stepper motors use gears, provide the gear ratio. 1 means no gearbox is used. A value of N means that N revolutions of the stepper motor are required for 1 revolution of the output axis, or that the driver has to send N times more steps to the stepper motor for the output axis to complete a revolution. displacement-per-revolution This value specifies by how much the CNC moves, in meter, for one revolution of the output axis of the motor + gearbox combination. This value is related to the size of the pulley that pulls on the belt. maximum-rpm The maximum speed of the stepper motors. The datasheets of the stepper motor generaly indicate this value in revolutions per minute (rpm). maximum-acceleration The maximum allowed acceleration, in m/s\u00b2, for each of the axes.","title":"The Oquam section"},{"location":"Rover/configuration/#the-ports-section","text":"This dictionnary lists which firmware drivers are available on what system ports. The rover uses mostly serial connections. The joystick uses a different device, however. The rcdiscover utility can be used to generate this list: $ ./bin/rcdiscover path/to/config.json The list has entries as follows: { \"ports\" : { \"oquam\" : { \"port\" : \"/dev/ttyACM5\" , \"type\" : \"serial\" } } } First comes the name of the firmware that is accessible through this port. It tells the type of the port ('serial' or 'input-device'), and the device's path in the 'port' field).","title":"The ports section"},{"location":"Rover/configuration/#the-user-interface-section","text":"The user interface config contains the settings for the speed controller. There are two drive modes: fast and accurate. Each have their own settings: { \"user-interface\" : { \"speed-controller\" : { \"accurate\" : { \"speed-multiplier\" : 0.2 , \"direction-multiplier\" : 0.05 , \"use-speed-curve\" : true , \"speed-curve-exponent\" : 1 , \"use-direction-curve\" : true , \"direction-curve-exponent\" : 1 }, \"fast\" : { \"speed-multiplier\" : 0.3 , \"direction-multiplier\" : 0.15 , \"use-speed-curve\" : true , \"speed-curve-exponent\" : 1 , \"use-direction-curve\" : true , \"direction-curve-exponent\" : 1 } } } } The speed controller sets the the speed of the left and right wheel and thus the radius of the turn that rover will follow. The two input values from the controller that set the speed and direction are mapped onto the range [-1, 1]. These normalized values are then used to determine the relative speed of the left and right wheel. The absolute speed of the rover is the product of the relative speed and the absolute speed given in the rover settings discussed above. The relative speed of the left wheel is: speed_left = ( speed_multiplier * speed + direction_multiplier * direction ); where speed and direction are the normalized inputs from the controller. Name Description speed-multiplier This number, with a value between 0 and 1, determines the maximum relative speed when the the speed trigger is completely pressed by the user. direction-multiplier The number defines the maximum speed difference, and thus the maximum turning angle, between the left and right wheel. In addition, it is possible to use an exponential curve to map the speed and direction values. This may give a more smooth behaviour when the rover starts from stand-still to higher speeds. A speed of zero still maps to zero, and a speed of one still maps to one, but in between the mapping follows a curve instead of a straight line. The curve is as follows: x = (( exp ( exponent * x ) - 1.0 ) / ( exp ( exponent ) - 1.0 )); Name Description use-speed-curve true of false: should the speed controller use the exponential curve to map the speed value? speed-curve-exponent The exponent of the speed curve. use-direction-curve true of false: should the speed controller use the exponential curve to map the direction value? direction-curve-exponent The exponent of the direction curve.","title":"The user-interface section"},{"location":"Rover/configuration/#the-weeder-section","text":"The weeder sections contains the settings for the weeder tool. These are as follows: { \"weeder\" : { \"diameter-tool\" : 0.04 , \"imagecropper\" : { \"workspace\" : [ 306 , 210 , 985 , 745 ] }, \"speed\" : 0.8 , \"z0\" : -0.22 } } Name Description diameter-tool The size of the weeding tool head, in meter. imagecropper.workspace The crop size and position the camera image to obtain the image of the workspace only. speed The relative speed of the weeding tool, between 0 and 1.0. This speed is multiplied by the maximum speed of the CNC to obtain the final speed of the weeding tool. z0 The position of the weeding tool when weeding.","title":"The weeder section"},{"location":"Rover/configuration/#script-file","text":"The script file consists of a JSON array of script objects: [ { \"id\" : \"script1\" , \"title\" : \"The First Script\" , \"script\" : [ //... ] }, //... ] The script objects have the following fields: Name Value Required Description id String Required The internal name of the script title String Required The name to be shown to end-users script Array Required The list of actions to be taken The script itself is an array that lists all the actions to be taken, one after the other. For example: [ { \"id\" : \"script1\" , \"title\" : \"The First Script\" , \"script\" : [ { \"action\" : \"start_recording\" }, { \"action\" : \"move\" , \"distance\" : 8 , \"speed\" : 400 }, { \"action\" : \"stop_recording\" }, { \"action\" : \"move\" , \"distance\" : -8 , \"speed\" : -400 } ] }, //... ] Each action object has an action field. The list of available actions are: Name Description start_recording Start recording the images of the top camera stop_recording Stop recording the images of the top camera move Move a given distance (at a given speed) hoe Start weeding homing Go to the home position (for rovers with a limit switch only) The move action takes the following parameters: Name Value Required Description distance Number (in meters) Required The distance to be travelled speed Number (-1000 < speed < 1000) Optional The speed The moveat action takes a speed value, as in the move command above.","title":"Script file"},{"location":"Rover/developer/","text":"Developer Documentation Link To download and compile the code, plase refer to the latest doc at the romi-rover-build-and-test code repository .","title":"Developer"},{"location":"Rover/developer/#developer-documentation","text":"To download and compile the code, plase refer to the latest doc at the romi-rover-build-and-test code repository .","title":"Developer Documentation"},{"location":"Rover/hardware/","text":"Hardware Documentation Link This document describes the hardware, both the mechanical parts and the electronics.. The main structure Link The figure below gives an overview of the main components. The mechanical components Link The frame Link The wheels Link The boxes Link The CNC Link We use currently use the X-Carve. Please follow X-Carve's documentation at . The Z-axis Link The cover Link The frame Link The drawings of the frame are available as PDF files. The current version is prototype V3: File Description Overview of the components The main frame. (fr: Ch\u00e2ssis) The left wheel module (fr: Roue motrice gauche) The right wheel module. It is a mirror of the left wheel module. (fr: Roue motrice droite) The arches for the rain and light cover. (fr: Toit) The caster wheel. (fr: Roue folle) In it's latest version, the caster wheel now has a straight fork. This drawing updates and replaces the fork in the previous drawing. The electronics Link The basic architecture of the control modules Link NOTE : The current rovers don't implements the schema above, yet: The CNC has no encoders. The rover in Valldaur doesn't have a control panel. The control panel Link NOTE : The control panel is still being developed at the time of writing. The control panel is used to start and stop the rover and to display status information on the character display. Component Specifications Example Controller Arduino Uno or equivalent Proto shield The shield allows you to solder the wires securely Adafruit Sparkfun Amazon Relay (2x) TODO Sparkfun Push button with LEDS (2x) One green and one red Adafruit LCD Character Display Comptable with XXX Farnell The power circuit Link NOTE : This is the new power circuit that will be used with the control panel. It's currently not implemented in either rover. There are three separate power circuits: Always-on circuit : This circuits powers the control panel. Logic circuit : This circuits powers the embedded PC and other control circuits. Power circuit : This circuit drives all the motors. This is the circuit that is cut when the security switch (the big red button) is pressed. The control panel actuates two relays (Relay 1 & 2) according to the two start-up phases (the PC and the logic circuits start up first, then the motors are powered up). The third relay is designed to handle strong currents. It has a protection against sparks and back-currents. Most of the logic runs on 5V. (TODO: Add a Meanwell power converter. Q: One converter for the the control panel + one of the logic circuit? Or one single converter?) The figure does not show the power line for the weeding hoe. The hoe is turned on/off using a fourth relay that is connected to the gShield board of the CNC. It using the CNC's spindle on/off functionality. You can find more information on this in the section on the CNC below. Component Specifications Example Relay Non-Latching, protection against sparks and back-current (TODO: be more precise) RS Online Security switch Farnell: button and housing The navigation module Link The navigation uses a differential wheel drive, with two motorized wheels in the back and two swivel caster in the front. This makes the control fairly straight-forward and the components are easy to source. The navigation module can also receive input from a remote control to steer the rover. The main components are shown below: Components Link Component Specifications Example Motors Brushed DC motors , 24 V, minimum 200 W We are using wheel chair motors for now. We bought a set at Superdroid Robots . Encoders Incremental encoders US Digital E2 (Available from Superdroid Robotics ). Controller Arduino Uno or equivalent Proto shield The shield allows you to solder the wires securely Adafruit Sparkfun Amazon Motor driver Preferably one board that can drive two motors. Two drivers, one for each motor, is possible, too. Power input: 24V, Maximum output current: > 15 A per motor, Control signals: two PWM signals (similar to RC input) for the left and right motor. The driver implements a standard H-bridge to control to power supplied to the motors (both forward and backward rotation). Sabertooth 2x60A Sabertooth 2x32A RoboClaw 2x60A RC controller and receiver A standard RC receiver that outputs a PWM signal. Powered by 5V. We've succesfully used the remote controllers for Spektrum , this one for example or similar Wheels You will need those, too. We are using the wheel provided by Superdroid Robotics for now Wiring diagram Link The tool positioning Link A CNC is adapted for use in the rover. We replaced the spindle that is normally used to carve wooden pieces, with a rotating weeding hoe. We are using to larger, 1000 mm sized version of the X-Carve . The newer X-Carve uses a custom design board for the control. However, we prefer using the older solution that combines an Arduino Uno with a gShield because it is smaller and more generic. For the time being, we use the grbl language to send commands from the embedded PC to the CNC controller. Therefore, any solution that accepts grbl commands should be drop-in solution for the Uno+gShield combo. Component Specifications Example CNC Minimum work area: 0.7 m x 0.7m X-Carve Optional: Controller board Must run grbl interpreter Arduino Uno Optional: Stepper drivers (3 steppers) The driver must use the STEP/DIR control signals gShield [Arduino CNC Shield] eBay and RepRap You can still have a look at XCarve's older documentation on how to wire the controller boards: http://x-carve-instructions.inventables.com/xcarve2015/step10/ http://x-carve-instructions.inventables.com/xcarve2015/step14/ Notable, the following two diagrams are of interest: The yellow wire marked \"spinle\" in the image above is used to turn the weeding hoe on or off, as shown in the figure below (see also the figure in the section on the power circuit).","title":"Hardware"},{"location":"Rover/hardware/#hardware-documentation","text":"This document describes the hardware, both the mechanical parts and the electronics..","title":"Hardware Documentation"},{"location":"Rover/hardware/#the-main-structure","text":"The figure below gives an overview of the main components.","title":"The main structure"},{"location":"Rover/hardware/#the-mechanical-components","text":"","title":"The mechanical components"},{"location":"Rover/hardware/#the-frame","text":"","title":"The frame"},{"location":"Rover/hardware/#the-wheels","text":"","title":"The wheels"},{"location":"Rover/hardware/#the-boxes","text":"","title":"The boxes"},{"location":"Rover/hardware/#the-cnc","text":"We use currently use the X-Carve. Please follow X-Carve's documentation at .","title":"The CNC"},{"location":"Rover/hardware/#the-z-axis","text":"","title":"The Z-axis"},{"location":"Rover/hardware/#the-cover","text":"","title":"The cover"},{"location":"Rover/hardware/#the-frame_1","text":"The drawings of the frame are available as PDF files. The current version is prototype V3: File Description Overview of the components The main frame. (fr: Ch\u00e2ssis) The left wheel module (fr: Roue motrice gauche) The right wheel module. It is a mirror of the left wheel module. (fr: Roue motrice droite) The arches for the rain and light cover. (fr: Toit) The caster wheel. (fr: Roue folle) In it's latest version, the caster wheel now has a straight fork. This drawing updates and replaces the fork in the previous drawing.","title":"The frame"},{"location":"Rover/hardware/#the-electronics","text":"","title":"The electronics"},{"location":"Rover/hardware/#the-basic-architecture-of-the-control-modules","text":"NOTE : The current rovers don't implements the schema above, yet: The CNC has no encoders. The rover in Valldaur doesn't have a control panel.","title":"The basic architecture of the control modules"},{"location":"Rover/hardware/#the-control-panel","text":"NOTE : The control panel is still being developed at the time of writing. The control panel is used to start and stop the rover and to display status information on the character display. Component Specifications Example Controller Arduino Uno or equivalent Proto shield The shield allows you to solder the wires securely Adafruit Sparkfun Amazon Relay (2x) TODO Sparkfun Push button with LEDS (2x) One green and one red Adafruit LCD Character Display Comptable with XXX Farnell","title":"The control panel"},{"location":"Rover/hardware/#the-power-circuit","text":"NOTE : This is the new power circuit that will be used with the control panel. It's currently not implemented in either rover. There are three separate power circuits: Always-on circuit : This circuits powers the control panel. Logic circuit : This circuits powers the embedded PC and other control circuits. Power circuit : This circuit drives all the motors. This is the circuit that is cut when the security switch (the big red button) is pressed. The control panel actuates two relays (Relay 1 & 2) according to the two start-up phases (the PC and the logic circuits start up first, then the motors are powered up). The third relay is designed to handle strong currents. It has a protection against sparks and back-currents. Most of the logic runs on 5V. (TODO: Add a Meanwell power converter. Q: One converter for the the control panel + one of the logic circuit? Or one single converter?) The figure does not show the power line for the weeding hoe. The hoe is turned on/off using a fourth relay that is connected to the gShield board of the CNC. It using the CNC's spindle on/off functionality. You can find more information on this in the section on the CNC below. Component Specifications Example Relay Non-Latching, protection against sparks and back-current (TODO: be more precise) RS Online Security switch Farnell: button and housing","title":"The power circuit"},{"location":"Rover/hardware/#the-navigation-module","text":"The navigation uses a differential wheel drive, with two motorized wheels in the back and two swivel caster in the front. This makes the control fairly straight-forward and the components are easy to source. The navigation module can also receive input from a remote control to steer the rover. The main components are shown below:","title":"The navigation module"},{"location":"Rover/hardware/#components","text":"Component Specifications Example Motors Brushed DC motors , 24 V, minimum 200 W We are using wheel chair motors for now. We bought a set at Superdroid Robots . Encoders Incremental encoders US Digital E2 (Available from Superdroid Robotics ). Controller Arduino Uno or equivalent Proto shield The shield allows you to solder the wires securely Adafruit Sparkfun Amazon Motor driver Preferably one board that can drive two motors. Two drivers, one for each motor, is possible, too. Power input: 24V, Maximum output current: > 15 A per motor, Control signals: two PWM signals (similar to RC input) for the left and right motor. The driver implements a standard H-bridge to control to power supplied to the motors (both forward and backward rotation). Sabertooth 2x60A Sabertooth 2x32A RoboClaw 2x60A RC controller and receiver A standard RC receiver that outputs a PWM signal. Powered by 5V. We've succesfully used the remote controllers for Spektrum , this one for example or similar Wheels You will need those, too. We are using the wheel provided by Superdroid Robotics for now","title":"Components"},{"location":"Rover/hardware/#wiring-diagram","text":"","title":"Wiring diagram"},{"location":"Rover/hardware/#the-tool-positioning","text":"A CNC is adapted for use in the rover. We replaced the spindle that is normally used to carve wooden pieces, with a rotating weeding hoe. We are using to larger, 1000 mm sized version of the X-Carve . The newer X-Carve uses a custom design board for the control. However, we prefer using the older solution that combines an Arduino Uno with a gShield because it is smaller and more generic. For the time being, we use the grbl language to send commands from the embedded PC to the CNC controller. Therefore, any solution that accepts grbl commands should be drop-in solution for the Uno+gShield combo. Component Specifications Example CNC Minimum work area: 0.7 m x 0.7m X-Carve Optional: Controller board Must run grbl interpreter Arduino Uno Optional: Stepper drivers (3 steppers) The driver must use the STEP/DIR control signals gShield [Arduino CNC Shield] eBay and RepRap You can still have a look at XCarve's older documentation on how to wire the controller boards: http://x-carve-instructions.inventables.com/xcarve2015/step10/ http://x-carve-instructions.inventables.com/xcarve2015/step14/ Notable, the following two diagrams are of interest: The yellow wire marked \"spinle\" in the image above is used to turn the weeding hoe on or off, as shown in the figure below (see also the figure in the section on the power circuit).","title":"The tool positioning"},{"location":"Rover/librcom/","text":"rcom Link rcom is light-weight C++ libary for inter-node communication. All data is sent over websockets and rcom provides an implementation of both server-side and client-side websockets. rcom offers a low-level API that can be used to build several communication patterns, such the publisher-subscriber pattern (pub-sub), or a message bus. rcom also offers a higher-level API that provides the remote procedure call pattern (RPC). We will discuss this API in more detail first. After that we will present the generic API. Installation Link The installation process follows the classical clone/cmake/make pattern: $ git clone -b ci_dev https://github.com/romi/librcom.git $ cd librcom/ $ mkdir build $ cd build $ cmake .. $ make Then run the tests to make sure all is well: $ ctest -V To check the code coverage run: $ make librcom_unit_tests_coverage $ firefox librcom/librcom_unit_tests_coverage/index.html Using rcom for remote procedure calls Link We will document how to use rcom through C++ API. However, it is possible to combine rcom with code writen in Python or Javascript, among other. We will provide some examples further below. Using C++ Link Suppose that you are writing an application called Madness that controls a bunch of happy monsters on the local network (whatever...). You design an interface called IMonster , as follows: #include <string> #include <iostream> class IMonster { public : virtual ~ IMonster () = default ; virtual void jump_around () = 0 ; virtual void gently_scare_someone ( const std :: string & person_id ) = 0 ; virtual double get_energy_level () = 0 ; }; All the monsters of your application will derive from this interface, such as the HappyMonster below. class HappyMonster : public IMonster { protected : std :: string name_ ; double energy_ ; public : HappyMonster ( const std :: string name ); ~ HappyMonster () override = default ; void jump_around () override ; void gently_scare_someone ( const std :: string & person_id ) override ; double get_energy_level () override ; }; HappyMonster :: HappyMonster ( const std :: string name ) : name_ ( name ), energy_ ( 1.0 ) { } void HappyMonster :: jump_around () { std :: cout << \"Jump around!\" << std :: endl ; } void HappyMonster :: gently_scare_someone ( const std :: string & person_id ) { std :: cout << \"Hey \" << person_id << \", don't watch that. Watch this. \" << \"This is the happy happy monster show.\" << std :: endl ; } double HappyMonster :: get_energy_level () { return energy_ ; } You can now write a small application, create a monster, and have it do things. int main ( int argc , char ** argv ) { HappyMonster monster ( \"Elmo\" ); monster . gently_scare_someone ( \"you\" ); return 0 ; } The full code of this example is split over the following files: monster_simple.cpp , IMonster.h , and HappyMonster.h The client-side application Link In the next step we will write a monster that lives in a remote application, either on the same machine but in a different process, or on a remote machine on the local network. We will write a new type of monster, called RemoteMonster . #include \"rcom/RemoteStub.h\" #include \"rcom/RcomClient.h\" class RemoteMonster : public IMonster , public rcom :: RemoteStub { public : RemoteMonster ( std :: unique_ptr < rcom :: IRPCClient >& client ); ~ RemoteMonster () override = default ; void jump_around () override ; void gently_scare_someone ( const std :: string & person_id ) override ; double get_energy_level () override ; }; The new class inherits both from IMonster and RemoteStub . The latter is part of rcom. You can also see that the RemoteMonster constructor takes an instance of IRPCClient as an argument. This class represents the connection between the local application and the remote process. As you can see below, this pointer is passed to the constructor of RemoteStub who will use it to send and receive messages. Normally, you should not have to add arguments to the constructor or create additional member variables in the RemoteMonster class because it is just a stub that will forward all requests to the real implementation that lives in a remote process. RemoteMonster :: RemoteMonster ( std :: unique_ptr < rcom :: IRPCClient >& client ) : RemoteStub ( client ) { } We still have to implement the methods of our example class. They are shown below. void RemoteMonster::jump_around () { bool success = execute_simple_request ( \"jump-around\" ); if ( ! success ) { std :: cout << \"jump_around failed\" << std :: endl ; } } void RemoteMonster::gently_scare_someone ( const std :: string & person_id ) { nlohmann :: json params ; params [ \"person-in\" ] = person_id ; bool success = execute_with_params ( \"gently-scare-someone\" , params ); if ( ! success ) { std :: cout << \"gently_scare_someone failed\" << std :: endl ; } } double RemoteMonster::get_energy_level () { double energy_level = -1.0 ; nlohmann :: json result ; bool success = execute_with_result ( \"get-energy-level\" , result ); if ( success ) { energy_level = result [ \"energy-level\" ]; } else { std :: cout << \"get_energy_level failed\" << std :: endl ; } return energy_level ; } The implementation mostly calls upon the methods provided by RemoteStub : Use execute_simple_request for methods that don't take any arguments and return no values. Use execute_with_params when the caller has to send arguments, but no return value is expected. Use execute_with_result when there are no arguments but a value is returned. Finally, the generic method execute takes arguments for the remote method and returns a value. Both the parameters and the return value are sent using the JSON format. The RemoteStub takes care of the encoding the data to a JSON string representation and parsing the incoming string to a C++ JSON data structure. For this rcom uses the JSON library by Niels Lohmann. Check out its documentation to get to know all its features. The various execute methods return true when the remote method was executed successfully and false when an error occured. They do not throw an exception. This leaves the choice up to you whether to throw an exception in response to a failed invokation or not. When an error occured, the RemoteStub will write a message with to the rcom logger. See more on the log system below. NOTE: The other functions, such as RcomClient::create below do throw exceptions. Here is the main function, again, rewriten for the use of the remote monster: int main () { try { auto client = rcom :: RcomClient :: create ( \"elmo\" , 10.0 ); RemoteMonster monster ( client ); monster . gently_scare_someone ( \"you\" ); } catch ( std :: exception & e ) { log_error ( \"main: '%s'\" , e . what ()); } return 0 ; } The function rcom::RcomClient::create establishes the connection to a remote object on the local network (or local machine) identified by \"elmo\". The second argument is a timeout for the connection. If \"elmo\" doesn't show up within 10 seconds, the application calls it quits. If the connection is established, it is passed to the RemoteMonster object. The application can then call the IMonster methods as if the remote monster was a normal, local object. The full code of the new version can be found in monster_client.cpp . The registry Link If you run the example application above, it will quit with the following error message: ERROR: Socket::connect: failed to bind the socket ERROR: Socket::Socket: Failed to connect to address 192 .168.1.100:10101 ERROR: main: 'Socket: Failed to connect' In order for the example above to find the \"elmo\" object, rcom uses another service called the rcom-registry . It is basically a directory service the maps identifiers to IP addresses. You will have to start the service separately: $ ./bin/rcom-registry INFO: Registry server running at 192 .168.1.100:10101. If you run the example application again, it will still quit. This time, after 10 seconds, it will show the error message below: WARNING: MessageLink::connect: Failed to obtain address for topic 'elmo' ERROR: MessageLink: Failed to connect: elmo ERROR: main: 'MessageLink: Failed to connect' This is normal: we didn't implement and start the remote process, yet. We will look into that in the next session. The server-side application Link The remote side - or server side - will receive requests coming from the application that was introduced above. These requests are sent as JSON strings. They have to be parsed and mapped to the methods of the actual C++ object that the remote client wants to address. For this, we will use an adaptor, as follows: int main () { try { std :: string name = \"elmo\" ; HappyMonster monster ( name ); MonsterAdaptor adaptor ( monster ); auto monster_server = rcom :: RcomServer :: create ( name , adaptor ); while ( true ) { monster_server -> handle_events (); usleep ( 1000 ); } } catch ( std :: exception & e ) { log_error ( \"main: '%s'\" , e . what ()); } return 0 ; } The MonsterAdaptor instance sits in between the generic RcomServer object and the HappyMonster object. The server will handle incoming JSON requests and call the adapter. The adaptor must map the request to the Monster object. Any return values will be converted to JSON by the server and sent back. The key here is the adapter class. It looks as follows: class MonsterAdaptor : public rcom :: IRPCHandler { protected : IMonster & monster_ ; public : MonsterAdaptor ( IMonster & monster ); ~ MonsterAdaptor () override = default ; void execute ( const std :: string & method , nlohmann :: json & params , nlohmann :: json & result , rcom :: RPCError & status ) override ; void execute ( const std :: string & method , nlohmann :: json & params , rcom :: MemBuffer & result , rcom :: RPCError & status ) override ; }; The two execute methods will be called by the server instance. The first one is for JSON text messages. The second one is for methods returning large binary data. The use of binary data will be discussed later. In our example, the execute method checks the value of the method argument and then dispatches the call to the appropriate methods on the \"real\" C++ object: void MonsterAdaptor::execute ( const std :: string & method , nlohmann :: json & params , nlohmann :: json & result , rcom :: RPCError & error ) { error . code = 0 ; if ( method == \"jump-around\" ) { monster_ . jump_around (); } else if ( method == \"gently-scare-someone\" ) { std :: string id = params [ \"person-id\" ]; monster_ . gently_scare_someone ( id ); } else if ( method == \"get-energy-level\" ) { result [ \"energy-level\" ] = monster_ . get_energy_level (); } else { error . code = rcom :: RPCError :: kMethodNotFound ; error . message = \"Unknown method\" ; } } That's it! The full code of this section can be found here: monster_server.cpp . Run the example Link To run the example, you must first start the rcom-registry : $ build/bin/rcom-registry INFO: Registry server running at 192 .168.1.100:10101. Then, in another shell, you start the server-side application that runs the remote object: $ build/bin/monster_server The rcom-registry console should display something like the message below. It shows that the remote server successfully registered with the \"elmo\" identifier. INFO: RegistryServer: Received message: {\"request\": \"register\", \"topic\": \"elmo\", \"address\": \"192.168.1.100:45175\"} INFO: RegistryServer: Register topic 'elmo' at 192.168.1.100:45175 In a third shell, you can now start the client application: $ build/bin/monster_client This example application will quit almost immediately because it doesn't do anything other than send a simple message. The console of monster_server should show the following, though: Hey you, don't watch that. Watch this. This is the happy happy monster show. Returning binary data Link To send binary data in the textual JSON format, it has to be encoded, for example, using the Base64 encoding. This can be quite a performance hit. For example, when the Raspberry Pi Zero has to transmit images, this encoding becomes a showstopper. So, it is therfore possible to return the data as a binary buffer. This is the reason for the second execute method in the adapter class discussed above. On the client side, you will have to do the following: rcom :: MemBuffer & MyClass::call_method_with_binary_output ( rcom :: MemBuffer & buffer ) { nlohmann :: json params ; RPCError error ; buffer . clear (); client_ -> execute ( \"method-id\" , params , buffer , error ); if ( error . code != 0 ) { // ... } return buffer ; } In the example above, we don't use the execute methods of the stub but directly the execute method of the client connection maintained the stub. Currently, it is only possible to retrive binary data from the server. There is no method, yet, for sending a buffer of binary data to the server. If you have to send binary data, you will have to encode it and sending it as part of the JSON request. The generic API Link rcom provides both server-side and client-side websockets. We'll call them client end-point and server end-points. A separate application, called 'rcom-registry' is a directory server that maintains the list of all server end-points. The rcom-registry application should be launched separately before any other application. The server end-points are identified using a topic, which is a free-form string. The topic should be unique for a given rcom-registry. Client end-points that want to communicate with a server first contact the rcom-registry to obtain the address of the server end-point. The address is simply a combination of IP address and port number. The client can then connect to the server end-point directly. An application can open several server end-points. And a single server end-point can handle many clients. The rcom library does not impose any format on the messages sent back and forth between the client and the server. Since the websocket standard makes a distinction between text-based, so does rcom. But under the hood, rcom is agnostic about the content of the messages. The logger Link By default, the rcom libray logs the internal messages, including error messages, to the console. If you are writing a large application, you probably want to redirect these messages to a file or a GUI window. In that case, you can subclass the rcom::ILog interface and inject it into the API functions discussed so far. For example, in the example discussed previously, we created a client connection to a remote object as follows: int main () { // ... auto client = rcom :: RcomClient :: create ( \"elmo\" , 10.0 ); // ... } This can be adapted as follows: #include \"MyLog.h\" int main () { // ... auto log = std :: make_shared < MyLog > (); auto client = rcom :: RcomClient :: create ( \"elmo\" , 10.0 , log ); // ... } The class MyLog implements the rcom::Ilog interface. It must handle the four types of messages that may be sent by the library as follows: #include <iostream.h> #include <rcom/ILog.h> class MyLog : public rcom :: ILog { public : MyLog () {} ~ MyLog () override = default ; void error ( const std :: string & message ) override { std :: cout << \"MyErr: \" << message << std :: endl ; } void warn ( const std :: string & message ) override { std :: cout << \"MyWarn: \" << message << std :: endl ; } void info ( const std :: string & message ) override { std :: cout << \"MyInfo: \" << message << std :: endl ; } void debug ( const std :: string & message ) override { std :: cout << \"MyDebug: \" << message << std :: endl ; } }; Similarly, for the server-side, you can pass your own the ILog object: #include \"MyLog.h\" int main () { // ... auto monster_server = rcom :: RcomServer :: create ( name , adaptor , log ); // ... } Fixed port Link No registration Link Security Link Specifying the address of the registry Link Behind a web server Link http Link https Link Connecting from Javascript Link TODO: This section is work in progress (as is most of this documentation BTW). Connecting to a remote object from Javascript is a two-step process: Create a websocket to rcom-registry to obtain the address of the requested object. function createRemoteMonster ( name , registry ) { var registrySocket = new WebSocket ( 'ws://' + registry + ':10101' ); registrySocket . onopen = function ( event ) { var request = { 'request' : 'get' , 'topic' : name }; registrySocket . send ( JSON . stringify ( request )); }; registrySocket . onmessage = function ( event ) { console . log ( event . data ); var reply = JSON . parse ( event . data ); if ( reply . success ) { registrySocket . close (); monster = new RemoteMonster ( reply . address ); } } } Create a websocket to the remote object using the obtained address. class RemoteMonster { constructor ( address ) { this . socket = new WebSocket ( 'ws://' + address ); this . socket . onmessage = ( event ) => { this . handleMessage ( event . data ); }; this . socket . onopen = ( event ) => { // ... }; } handleMessage ( buffer ) { var response = JSON . parse ( buffer ); if ( response . error ) { this . handleErrorMessage ( response . error ); } else if ( response . method == 'get-energy-level' ) { console . log ( 'RemoteMonster: Energy level ' + response [ 'energy-level' ] } } handleErrorMessage ( err ) { console . log ( 'RemoteMonster: Method: ' + response . method + ', Error: ' + response . error . message ); } execute ( method , params ) { var request = { 'method' : method , 'params' : params }; var s = JSON . stringify ( request ); this . socket . send ( s ); } jumpAround () { this . execute ( 'jump-around' ); } gentlyScareSomeone ( id ) { this . execute ( 'gently-scare-someone' , { 'person-id' : id }}; } getEnergyLevel () { this . execute ( 'get-energy-level' ); } } Connecting from Python Link The rcom library provides some helper code to exchange data between Python code and rcom objects written in C++. At the current development stage, this Python code has only been used for prototyping during development. The code is not production ready but it may help to get started in your own projects. In the root directory of the rcom repository, you will find a directory called python that contains the Python rcom modules and some examples. You can install the rcom Python code and dependencies as follows: $ cd python $ python3 setup.py install --user A Python client connecting to an C++ rcom server Link To run the example, start the rcom-registry server in a new shell: $ bin/rcom-registry In another shell, start the remote monster server that we discussed above: $ bin/monster_server Finally, run the Python client: $ python3 examples/monster_client.py The Python code looks as follows. First, we define a new class RemoteMonster that subclasses the RcomClient from the rcom.rcom_client module. from rcom.rcom_client import RcomClient class RemoteMonster ( RcomClient ): def __init__ ( self , name , registry ): super () . __init__ ( name , registry ) def jump_around ( self ): self . execute ( 'jump-around' ) def gently_scare_someone ( self , person_id ): self . execute ( 'gently-scare-someone' , { 'person-id' : person_id }) def get_energy_level ( self ): answer = self . execute ( 'get-energy-level' ) return answer [ 'energy-level' ] TODO: The implementation still requires that you pass the IP address to the registry to the RcomClient instance. You can find the local IP address using this code snippet: import socket def get_local_ip (): s = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) s . connect (( \"8.8.8.8\" , 80 )) ip = s . getsockname ()[ 0 ] s . close () return ip Calling the remote C++ object is now very straightforward: monster = RemoteMonster ( 'elmo' , get_local_ip ()) monster . jump_around () monster . gently_scare_someone ( 'you' ) energy = monster . get_energy_level () print ( f 'energy level is { energy } ' ) Overview of the classes and code Link ILinux , Linux , MockLinux : To facilitate unit testing, the system functions are abstracted in the ILinux interface. The Linux class provides the default implementation, and MockLinux the implementation used for testing. The interface ISocket defines a standard TCP/IP socket API. The class Socket is the default implementation of the API. Similarly, IServerSocket defines the API for a socket that accepts incoming connection. It's default implementation can be found in the ServerSocket class. Both Socket and ServerSocket actually share a lot of functionality. This functionality is grouped together in the class BaseSocket , which encapsulates the standard BSD socket interface. Both Both Socket and ServerSocket delegate most of the methods to BaseSocket . Websockets have there own API, defined in IWebSocket . This interface basically defines the methods to send or receive a message. The WebSocket class provides the default implementation. It uses an ISocket to send and receive data on the TCP/IP connection and then implements the websocket protocol as defined in RFC 6455 . Most of the code doesn't create WebSockets directly but uses an instance of ISocketFactory to create them. Again, this facilitates the testing of the code by passing in a MockSocketFactory . The WebSocketServer implements a server that waits for incoming websocket connections and creates a new ServerSideWebSocket after a successful handshake. It also maintains the list of all open connections. This allows to send broadcast messages to all client connected to this server. The handle_events method should be called regularly to deal with the incoming connection requests. We distinguish between server-side and client-side websockets: ServerSideWebSocket : The websocket created on the server-side in response to a new incoming connection. ClientSideWebSocket : The websocket created by the client to connect to a WebSocketServer . Both inherit implementation from the WebSocket class. A MessageHub is like a WebSocketServer with the following additional functionality: It has a topic name. It registers the topic and its address to the remote registry. RPC classes Link IRPCHandler IRPCClient IRPCServer IMessageListener RcomClient RemoteStub RcomServer RcomMessageHandler TODO Link Remote access 4G router, set-up at the farm Managing an fleet of rovers 4G router with a solar panel queue management doc format messages describe format message for different actions: move, path, grab, ... c++ -> Python","title":"Librcom"},{"location":"Rover/librcom/#rcom","text":"rcom is light-weight C++ libary for inter-node communication. All data is sent over websockets and rcom provides an implementation of both server-side and client-side websockets. rcom offers a low-level API that can be used to build several communication patterns, such the publisher-subscriber pattern (pub-sub), or a message bus. rcom also offers a higher-level API that provides the remote procedure call pattern (RPC). We will discuss this API in more detail first. After that we will present the generic API.","title":"rcom"},{"location":"Rover/librcom/#installation","text":"The installation process follows the classical clone/cmake/make pattern: $ git clone -b ci_dev https://github.com/romi/librcom.git $ cd librcom/ $ mkdir build $ cd build $ cmake .. $ make Then run the tests to make sure all is well: $ ctest -V To check the code coverage run: $ make librcom_unit_tests_coverage $ firefox librcom/librcom_unit_tests_coverage/index.html","title":"Installation"},{"location":"Rover/librcom/#using-rcom-for-remote-procedure-calls","text":"We will document how to use rcom through C++ API. However, it is possible to combine rcom with code writen in Python or Javascript, among other. We will provide some examples further below.","title":"Using rcom for remote procedure calls"},{"location":"Rover/librcom/#using-c","text":"Suppose that you are writing an application called Madness that controls a bunch of happy monsters on the local network (whatever...). You design an interface called IMonster , as follows: #include <string> #include <iostream> class IMonster { public : virtual ~ IMonster () = default ; virtual void jump_around () = 0 ; virtual void gently_scare_someone ( const std :: string & person_id ) = 0 ; virtual double get_energy_level () = 0 ; }; All the monsters of your application will derive from this interface, such as the HappyMonster below. class HappyMonster : public IMonster { protected : std :: string name_ ; double energy_ ; public : HappyMonster ( const std :: string name ); ~ HappyMonster () override = default ; void jump_around () override ; void gently_scare_someone ( const std :: string & person_id ) override ; double get_energy_level () override ; }; HappyMonster :: HappyMonster ( const std :: string name ) : name_ ( name ), energy_ ( 1.0 ) { } void HappyMonster :: jump_around () { std :: cout << \"Jump around!\" << std :: endl ; } void HappyMonster :: gently_scare_someone ( const std :: string & person_id ) { std :: cout << \"Hey \" << person_id << \", don't watch that. Watch this. \" << \"This is the happy happy monster show.\" << std :: endl ; } double HappyMonster :: get_energy_level () { return energy_ ; } You can now write a small application, create a monster, and have it do things. int main ( int argc , char ** argv ) { HappyMonster monster ( \"Elmo\" ); monster . gently_scare_someone ( \"you\" ); return 0 ; } The full code of this example is split over the following files: monster_simple.cpp , IMonster.h , and HappyMonster.h","title":"Using C++"},{"location":"Rover/librcom/#the-client-side-application","text":"In the next step we will write a monster that lives in a remote application, either on the same machine but in a different process, or on a remote machine on the local network. We will write a new type of monster, called RemoteMonster . #include \"rcom/RemoteStub.h\" #include \"rcom/RcomClient.h\" class RemoteMonster : public IMonster , public rcom :: RemoteStub { public : RemoteMonster ( std :: unique_ptr < rcom :: IRPCClient >& client ); ~ RemoteMonster () override = default ; void jump_around () override ; void gently_scare_someone ( const std :: string & person_id ) override ; double get_energy_level () override ; }; The new class inherits both from IMonster and RemoteStub . The latter is part of rcom. You can also see that the RemoteMonster constructor takes an instance of IRPCClient as an argument. This class represents the connection between the local application and the remote process. As you can see below, this pointer is passed to the constructor of RemoteStub who will use it to send and receive messages. Normally, you should not have to add arguments to the constructor or create additional member variables in the RemoteMonster class because it is just a stub that will forward all requests to the real implementation that lives in a remote process. RemoteMonster :: RemoteMonster ( std :: unique_ptr < rcom :: IRPCClient >& client ) : RemoteStub ( client ) { } We still have to implement the methods of our example class. They are shown below. void RemoteMonster::jump_around () { bool success = execute_simple_request ( \"jump-around\" ); if ( ! success ) { std :: cout << \"jump_around failed\" << std :: endl ; } } void RemoteMonster::gently_scare_someone ( const std :: string & person_id ) { nlohmann :: json params ; params [ \"person-in\" ] = person_id ; bool success = execute_with_params ( \"gently-scare-someone\" , params ); if ( ! success ) { std :: cout << \"gently_scare_someone failed\" << std :: endl ; } } double RemoteMonster::get_energy_level () { double energy_level = -1.0 ; nlohmann :: json result ; bool success = execute_with_result ( \"get-energy-level\" , result ); if ( success ) { energy_level = result [ \"energy-level\" ]; } else { std :: cout << \"get_energy_level failed\" << std :: endl ; } return energy_level ; } The implementation mostly calls upon the methods provided by RemoteStub : Use execute_simple_request for methods that don't take any arguments and return no values. Use execute_with_params when the caller has to send arguments, but no return value is expected. Use execute_with_result when there are no arguments but a value is returned. Finally, the generic method execute takes arguments for the remote method and returns a value. Both the parameters and the return value are sent using the JSON format. The RemoteStub takes care of the encoding the data to a JSON string representation and parsing the incoming string to a C++ JSON data structure. For this rcom uses the JSON library by Niels Lohmann. Check out its documentation to get to know all its features. The various execute methods return true when the remote method was executed successfully and false when an error occured. They do not throw an exception. This leaves the choice up to you whether to throw an exception in response to a failed invokation or not. When an error occured, the RemoteStub will write a message with to the rcom logger. See more on the log system below. NOTE: The other functions, such as RcomClient::create below do throw exceptions. Here is the main function, again, rewriten for the use of the remote monster: int main () { try { auto client = rcom :: RcomClient :: create ( \"elmo\" , 10.0 ); RemoteMonster monster ( client ); monster . gently_scare_someone ( \"you\" ); } catch ( std :: exception & e ) { log_error ( \"main: '%s'\" , e . what ()); } return 0 ; } The function rcom::RcomClient::create establishes the connection to a remote object on the local network (or local machine) identified by \"elmo\". The second argument is a timeout for the connection. If \"elmo\" doesn't show up within 10 seconds, the application calls it quits. If the connection is established, it is passed to the RemoteMonster object. The application can then call the IMonster methods as if the remote monster was a normal, local object. The full code of the new version can be found in monster_client.cpp .","title":"The client-side application"},{"location":"Rover/librcom/#the-registry","text":"If you run the example application above, it will quit with the following error message: ERROR: Socket::connect: failed to bind the socket ERROR: Socket::Socket: Failed to connect to address 192 .168.1.100:10101 ERROR: main: 'Socket: Failed to connect' In order for the example above to find the \"elmo\" object, rcom uses another service called the rcom-registry . It is basically a directory service the maps identifiers to IP addresses. You will have to start the service separately: $ ./bin/rcom-registry INFO: Registry server running at 192 .168.1.100:10101. If you run the example application again, it will still quit. This time, after 10 seconds, it will show the error message below: WARNING: MessageLink::connect: Failed to obtain address for topic 'elmo' ERROR: MessageLink: Failed to connect: elmo ERROR: main: 'MessageLink: Failed to connect' This is normal: we didn't implement and start the remote process, yet. We will look into that in the next session.","title":"The registry"},{"location":"Rover/librcom/#the-server-side-application","text":"The remote side - or server side - will receive requests coming from the application that was introduced above. These requests are sent as JSON strings. They have to be parsed and mapped to the methods of the actual C++ object that the remote client wants to address. For this, we will use an adaptor, as follows: int main () { try { std :: string name = \"elmo\" ; HappyMonster monster ( name ); MonsterAdaptor adaptor ( monster ); auto monster_server = rcom :: RcomServer :: create ( name , adaptor ); while ( true ) { monster_server -> handle_events (); usleep ( 1000 ); } } catch ( std :: exception & e ) { log_error ( \"main: '%s'\" , e . what ()); } return 0 ; } The MonsterAdaptor instance sits in between the generic RcomServer object and the HappyMonster object. The server will handle incoming JSON requests and call the adapter. The adaptor must map the request to the Monster object. Any return values will be converted to JSON by the server and sent back. The key here is the adapter class. It looks as follows: class MonsterAdaptor : public rcom :: IRPCHandler { protected : IMonster & monster_ ; public : MonsterAdaptor ( IMonster & monster ); ~ MonsterAdaptor () override = default ; void execute ( const std :: string & method , nlohmann :: json & params , nlohmann :: json & result , rcom :: RPCError & status ) override ; void execute ( const std :: string & method , nlohmann :: json & params , rcom :: MemBuffer & result , rcom :: RPCError & status ) override ; }; The two execute methods will be called by the server instance. The first one is for JSON text messages. The second one is for methods returning large binary data. The use of binary data will be discussed later. In our example, the execute method checks the value of the method argument and then dispatches the call to the appropriate methods on the \"real\" C++ object: void MonsterAdaptor::execute ( const std :: string & method , nlohmann :: json & params , nlohmann :: json & result , rcom :: RPCError & error ) { error . code = 0 ; if ( method == \"jump-around\" ) { monster_ . jump_around (); } else if ( method == \"gently-scare-someone\" ) { std :: string id = params [ \"person-id\" ]; monster_ . gently_scare_someone ( id ); } else if ( method == \"get-energy-level\" ) { result [ \"energy-level\" ] = monster_ . get_energy_level (); } else { error . code = rcom :: RPCError :: kMethodNotFound ; error . message = \"Unknown method\" ; } } That's it! The full code of this section can be found here: monster_server.cpp .","title":"The server-side application"},{"location":"Rover/librcom/#run-the-example","text":"To run the example, you must first start the rcom-registry : $ build/bin/rcom-registry INFO: Registry server running at 192 .168.1.100:10101. Then, in another shell, you start the server-side application that runs the remote object: $ build/bin/monster_server The rcom-registry console should display something like the message below. It shows that the remote server successfully registered with the \"elmo\" identifier. INFO: RegistryServer: Received message: {\"request\": \"register\", \"topic\": \"elmo\", \"address\": \"192.168.1.100:45175\"} INFO: RegistryServer: Register topic 'elmo' at 192.168.1.100:45175 In a third shell, you can now start the client application: $ build/bin/monster_client This example application will quit almost immediately because it doesn't do anything other than send a simple message. The console of monster_server should show the following, though: Hey you, don't watch that. Watch this. This is the happy happy monster show.","title":"Run the example"},{"location":"Rover/librcom/#returning-binary-data","text":"To send binary data in the textual JSON format, it has to be encoded, for example, using the Base64 encoding. This can be quite a performance hit. For example, when the Raspberry Pi Zero has to transmit images, this encoding becomes a showstopper. So, it is therfore possible to return the data as a binary buffer. This is the reason for the second execute method in the adapter class discussed above. On the client side, you will have to do the following: rcom :: MemBuffer & MyClass::call_method_with_binary_output ( rcom :: MemBuffer & buffer ) { nlohmann :: json params ; RPCError error ; buffer . clear (); client_ -> execute ( \"method-id\" , params , buffer , error ); if ( error . code != 0 ) { // ... } return buffer ; } In the example above, we don't use the execute methods of the stub but directly the execute method of the client connection maintained the stub. Currently, it is only possible to retrive binary data from the server. There is no method, yet, for sending a buffer of binary data to the server. If you have to send binary data, you will have to encode it and sending it as part of the JSON request.","title":"Returning binary data"},{"location":"Rover/librcom/#the-generic-api","text":"rcom provides both server-side and client-side websockets. We'll call them client end-point and server end-points. A separate application, called 'rcom-registry' is a directory server that maintains the list of all server end-points. The rcom-registry application should be launched separately before any other application. The server end-points are identified using a topic, which is a free-form string. The topic should be unique for a given rcom-registry. Client end-points that want to communicate with a server first contact the rcom-registry to obtain the address of the server end-point. The address is simply a combination of IP address and port number. The client can then connect to the server end-point directly. An application can open several server end-points. And a single server end-point can handle many clients. The rcom library does not impose any format on the messages sent back and forth between the client and the server. Since the websocket standard makes a distinction between text-based, so does rcom. But under the hood, rcom is agnostic about the content of the messages.","title":"The generic API"},{"location":"Rover/librcom/#the-logger","text":"By default, the rcom libray logs the internal messages, including error messages, to the console. If you are writing a large application, you probably want to redirect these messages to a file or a GUI window. In that case, you can subclass the rcom::ILog interface and inject it into the API functions discussed so far. For example, in the example discussed previously, we created a client connection to a remote object as follows: int main () { // ... auto client = rcom :: RcomClient :: create ( \"elmo\" , 10.0 ); // ... } This can be adapted as follows: #include \"MyLog.h\" int main () { // ... auto log = std :: make_shared < MyLog > (); auto client = rcom :: RcomClient :: create ( \"elmo\" , 10.0 , log ); // ... } The class MyLog implements the rcom::Ilog interface. It must handle the four types of messages that may be sent by the library as follows: #include <iostream.h> #include <rcom/ILog.h> class MyLog : public rcom :: ILog { public : MyLog () {} ~ MyLog () override = default ; void error ( const std :: string & message ) override { std :: cout << \"MyErr: \" << message << std :: endl ; } void warn ( const std :: string & message ) override { std :: cout << \"MyWarn: \" << message << std :: endl ; } void info ( const std :: string & message ) override { std :: cout << \"MyInfo: \" << message << std :: endl ; } void debug ( const std :: string & message ) override { std :: cout << \"MyDebug: \" << message << std :: endl ; } }; Similarly, for the server-side, you can pass your own the ILog object: #include \"MyLog.h\" int main () { // ... auto monster_server = rcom :: RcomServer :: create ( name , adaptor , log ); // ... }","title":"The logger"},{"location":"Rover/librcom/#fixed-port","text":"","title":"Fixed port"},{"location":"Rover/librcom/#no-registration","text":"","title":"No registration"},{"location":"Rover/librcom/#security","text":"","title":"Security"},{"location":"Rover/librcom/#specifying-the-address-of-the-registry","text":"","title":"Specifying the address of the registry"},{"location":"Rover/librcom/#behind-a-web-server","text":"","title":"Behind a web server"},{"location":"Rover/librcom/#http","text":"","title":"http"},{"location":"Rover/librcom/#https","text":"","title":"https"},{"location":"Rover/librcom/#connecting-from-javascript","text":"TODO: This section is work in progress (as is most of this documentation BTW). Connecting to a remote object from Javascript is a two-step process: Create a websocket to rcom-registry to obtain the address of the requested object. function createRemoteMonster ( name , registry ) { var registrySocket = new WebSocket ( 'ws://' + registry + ':10101' ); registrySocket . onopen = function ( event ) { var request = { 'request' : 'get' , 'topic' : name }; registrySocket . send ( JSON . stringify ( request )); }; registrySocket . onmessage = function ( event ) { console . log ( event . data ); var reply = JSON . parse ( event . data ); if ( reply . success ) { registrySocket . close (); monster = new RemoteMonster ( reply . address ); } } } Create a websocket to the remote object using the obtained address. class RemoteMonster { constructor ( address ) { this . socket = new WebSocket ( 'ws://' + address ); this . socket . onmessage = ( event ) => { this . handleMessage ( event . data ); }; this . socket . onopen = ( event ) => { // ... }; } handleMessage ( buffer ) { var response = JSON . parse ( buffer ); if ( response . error ) { this . handleErrorMessage ( response . error ); } else if ( response . method == 'get-energy-level' ) { console . log ( 'RemoteMonster: Energy level ' + response [ 'energy-level' ] } } handleErrorMessage ( err ) { console . log ( 'RemoteMonster: Method: ' + response . method + ', Error: ' + response . error . message ); } execute ( method , params ) { var request = { 'method' : method , 'params' : params }; var s = JSON . stringify ( request ); this . socket . send ( s ); } jumpAround () { this . execute ( 'jump-around' ); } gentlyScareSomeone ( id ) { this . execute ( 'gently-scare-someone' , { 'person-id' : id }}; } getEnergyLevel () { this . execute ( 'get-energy-level' ); } }","title":"Connecting from Javascript"},{"location":"Rover/librcom/#connecting-from-python","text":"The rcom library provides some helper code to exchange data between Python code and rcom objects written in C++. At the current development stage, this Python code has only been used for prototyping during development. The code is not production ready but it may help to get started in your own projects. In the root directory of the rcom repository, you will find a directory called python that contains the Python rcom modules and some examples. You can install the rcom Python code and dependencies as follows: $ cd python $ python3 setup.py install --user","title":"Connecting from Python"},{"location":"Rover/librcom/#a-python-client-connecting-to-an-c-rcom-server","text":"To run the example, start the rcom-registry server in a new shell: $ bin/rcom-registry In another shell, start the remote monster server that we discussed above: $ bin/monster_server Finally, run the Python client: $ python3 examples/monster_client.py The Python code looks as follows. First, we define a new class RemoteMonster that subclasses the RcomClient from the rcom.rcom_client module. from rcom.rcom_client import RcomClient class RemoteMonster ( RcomClient ): def __init__ ( self , name , registry ): super () . __init__ ( name , registry ) def jump_around ( self ): self . execute ( 'jump-around' ) def gently_scare_someone ( self , person_id ): self . execute ( 'gently-scare-someone' , { 'person-id' : person_id }) def get_energy_level ( self ): answer = self . execute ( 'get-energy-level' ) return answer [ 'energy-level' ] TODO: The implementation still requires that you pass the IP address to the registry to the RcomClient instance. You can find the local IP address using this code snippet: import socket def get_local_ip (): s = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) s . connect (( \"8.8.8.8\" , 80 )) ip = s . getsockname ()[ 0 ] s . close () return ip Calling the remote C++ object is now very straightforward: monster = RemoteMonster ( 'elmo' , get_local_ip ()) monster . jump_around () monster . gently_scare_someone ( 'you' ) energy = monster . get_energy_level () print ( f 'energy level is { energy } ' )","title":"A Python client connecting to an C++ rcom server"},{"location":"Rover/librcom/#overview-of-the-classes-and-code","text":"ILinux , Linux , MockLinux : To facilitate unit testing, the system functions are abstracted in the ILinux interface. The Linux class provides the default implementation, and MockLinux the implementation used for testing. The interface ISocket defines a standard TCP/IP socket API. The class Socket is the default implementation of the API. Similarly, IServerSocket defines the API for a socket that accepts incoming connection. It's default implementation can be found in the ServerSocket class. Both Socket and ServerSocket actually share a lot of functionality. This functionality is grouped together in the class BaseSocket , which encapsulates the standard BSD socket interface. Both Both Socket and ServerSocket delegate most of the methods to BaseSocket . Websockets have there own API, defined in IWebSocket . This interface basically defines the methods to send or receive a message. The WebSocket class provides the default implementation. It uses an ISocket to send and receive data on the TCP/IP connection and then implements the websocket protocol as defined in RFC 6455 . Most of the code doesn't create WebSockets directly but uses an instance of ISocketFactory to create them. Again, this facilitates the testing of the code by passing in a MockSocketFactory . The WebSocketServer implements a server that waits for incoming websocket connections and creates a new ServerSideWebSocket after a successful handshake. It also maintains the list of all open connections. This allows to send broadcast messages to all client connected to this server. The handle_events method should be called regularly to deal with the incoming connection requests. We distinguish between server-side and client-side websockets: ServerSideWebSocket : The websocket created on the server-side in response to a new incoming connection. ClientSideWebSocket : The websocket created by the client to connect to a WebSocketServer . Both inherit implementation from the WebSocket class. A MessageHub is like a WebSocketServer with the following additional functionality: It has a topic name. It registers the topic and its address to the remote registry.","title":"Overview of the classes and code"},{"location":"Rover/librcom/#rpc-classes","text":"IRPCHandler IRPCClient IRPCServer IMessageListener RcomClient RemoteStub RcomServer RcomMessageHandler","title":"RPC classes"},{"location":"Rover/librcom/#todo","text":"Remote access 4G router, set-up at the farm Managing an fleet of rovers 4G router with a solar panel queue management doc format messages describe format message for different actions: move, path, grab, ... c++ -> Python","title":"TODO"},{"location":"Rover/manual/","text":"ROMI Rover User Manual Link Draft v2 - Rover V3 - Summer 2022 This manual describes the usage of a fully assembled and ready-to-use rover. The information on how to build your own rover can be found in a separate document. Visit the ROMI web site at https://docs.romi-project.eu for more information. \u00a9 Sony Computer Science Laboratories - CC BY-SA 4.0 Licence Acknowledgements Link ROMI has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 773875. Short description Link The ROMI Rover is a farming tool that assists vegetable farmers in maintaining vegetable beds free of weeds. It does this by regularly hoeing the surface of the soil and thus preventing small weeds from taking root. It can do this task mostly autonomously and requires only minor changes to the organisation of the farm. It is designed for vegetable beds between 70 cm and 110 cm wide (not including the passage ways) and for crops up to 50 cm high. It currently handles two types of crops, lettuce and carrots. The lettuce can be planted out in any layout, most likely in a quincunx pattern. In this configuration the rover uses a precision rotary hoe to clean the soil both between the rows and the plants. For carrots, the rover uses classical mechanical tools, such as stirrup hoe, to regularly clean the soil in between the rows. In this configuration, the carrots should be sown in line. A weekly passage of the robot should be sufficient to keep the population of weeds under control. In addition to weeding, the embedded camera can be used to collect images of the vegetable beds. The ROMI Rover is targeted at farms that grow small crops, such as lettuce and carrots, on a relatively small surface, between 200 m\u00b2 and 5 ha (Utilised Agricultural Area). Technical specifications Link This section details the technical specifications, operating instructions and configuration of the current version of the Romi rover: V3. Characteristic Value Dimensions Width: Adjustable between 1.42 m and 2 m Length: 1.8 m Height: 1.44 m With tool carrier: add 1 m to length Weight 140 kg Battery life 8 h (approx. dependent on use case) Charging time TODO Weeding speed Precision weeding: 235 m\u00b2/day Classical weeding: 6400 m\u00b2/day Width vegetable beds Between 0.7 and 1.1 m Handled crops Precision weeding: Lettuce Classical weeding: Carrots Turning space at end of bed 2.8 m (3.5 m with tool carrier) Functional specifications and requirements Link The following configuration is required for the use of the ROMI rover. Image Price Features/Function ROMI rover with a remote control, a battery charger, and a protective cover 5000 \u20ac (estimate) - Prevents weed development - Takes image scans of the beds Overview of the components Link The control panel : The control panel provides a means to view status messages, and request the rover to perform a preconfigured action. The on/off switch : The on/off switch can be found on the electronics housing. Operation instructions Link Overview of the rover\u2019s usage Link The basic usage of the rover is to position it on a vegetable bed and let the machine clean the top-soil with a rotating precision hoe. The rover must be taken to the field using the remote control. The robot currently expects the vegetables to be grown in \"beds\" of 0.7 m to 1.1 m wide. The robot is designed for smaller market farms of less than 5 ha but the size of the farm depends on the number of rovers that you will use, and the amount of crop you want to cover. The rover can navigate autonomously along a bed if there is a clear line of crops that it can follow. Once the rover is positioned at the beginning of a bed, it hoes the surface of the soil so that small weeds cannot take root. It can perform this action along the entire length of the bed. Note that the rover cannot remove mature weeds that have already established themselves. It is therefore necessary to start with a vegetable bed that has been cleaned from all weeds. This can be done with various classical techniques to prepare the vegetable beds. Once the beds are clean, the rover can be used to maintain them as such. Two weeding methods are available. First, a precision weeding method in which the top-soil is turned over in between the rows (also called \"inter-row\") and between the plants (or \"intra-row\"). Second, a classical weeding method in which standard weeding tools are dragged behind the rover between rows of vegetables. For the precision weeding method, the rover uses a camera to detect the plants that are underneath the rover. It then moves the precision weeding tool over the surface whilst passing closely around the detected vegetables. Although the rover is autonomous for weeding a single bed, it is important to stay in proximity to the rover. A U-turn must also be manually performed at the end of the bed and the rover repositioned in line with the rails of the next bed. Setting up the vegetable beds Link The use of the rover requires relatively flat beds. Precision weeding works best if the surface of the culture beds is planar. Ideally, the alleys between the beds should also be flat, to facilitate navigation of the rover. Otherwise, there is a risk that the tool will detach from the soil or that it will dig into the soil. There is no precise measure of how flat the beds should be but small holes in the ground should be avoided. The presence of stones should also be avoided. Small stones (approx. 1 cm) should not perturb the rover very much. NOTE: It is more convenient if the width of the vegetable beds are constant so that the width of the rover doesn't have to be adapted beds. Setting up the Wi-Fi access point Link The use of a Wi-Fi access point is optional but strongly recommended. The rover must be connected to a Wi-Fi access point with Internet access for the following functionality: To automatically upload the images taken by the rover to the Farmer\u2019s Dashboard web application. For remote maintenance. Both features are optional and can be left out when the rover is used for weeding only. However, if you decide to use an access point, it is important that the Wi-Fi signal is strong enough in all the zones where the rover will be used. If not, it may be impossible to connect to the rover\u2019s web interface with a phone or tablet to send instructions to the rover. It will still be possible to send instructions to the rover using the control panel (see \"Controlling the rover through the control panel\"). The set-up of the Wi-Fi network is not part of the ROMI Rover package. In case of doubt, you should seek advice from a professional about the best solution for your premises. However, below, we briefly discuss several options. Use an existing Wi-Fi router: If the zone where you wish to use the rover is adjacent to existing infrastructure (home, barn) and you have the possibility to install an Internet connection at the premises (ADSL modem over a phone line or any other solution), the Wi-Fi capabilities of the modem can be used to offer Internet access to the rover. Expand the reach of an existing Wi-Fi network : An existing Wi-Fi can be extended to increase its reach using Wi-Fi range extenders. They pick up and retransmit an existing Wi-Fi signal. Most extenders require a standard power supply although some can be powered using an USB battery. Using an Ethernet cable of up to 100 metres long, it is possible to position a secondary access point. Some of the Wi-Fi access points can be powered directly over the Ethernet cable (PoE, Power over Ethernet) removing the need for a power socket. It is also possible to send the network signal over existing electricity cables using a technology called power-line communication (PLC). Finally, there exist also long-range wireless outdoor WiFi extenders that transmit the network between two antennas designed for transmission over distances from a 100 metres to over a kilometre. Install a GSM Wi-Fi router : If there is a good mobile phone signal strength in the field, a GSM Wi-Fi router is a viable option. A GSM Wi-Fi router connects to the Internet over a mobile data link (4G, 3G, HSDPA\u2026) and provides access to other devices over Wi-Fi. Separate routers with good antennas can be purchased at reasonable prices but generally require a power plug. Smaller, USB-powered routers are available also and can be plugged directly into a USB port inside the rover. A mobile phone configured as a hotspot is an alternative solution (although with a smaller range than a dedicated router with good antennas). The downside of this option is that it requires a SIM card and a subscription with a mobile network operator. Using a USB GSM modem : In contrast to the solution above, a USB GSM modem is not a stand-alone router but, when plugged in, the Raspberry Pi will see the modem as an additional network interface. In this set-up, the rover remains the hotspot for the Wi-Fi network but will route any Internet traffic through the GSM modem. This solution may require additional changes to the network configuration of the Raspberry Pi. Charging the rover Link The rover uses two 12 V Lithium batteries (the internal working voltage is 24 V). Use the supplied Victron Energy Blue Smart IP67 24V 5A Charger to reload the batteries. Plug 230 V side of the charger in a regular power plug. The 24 V side must be plugged into the POWER CHARGER plug on the battery box. The charger has LED indicators to show the status of the charging cycle. It is also possible to follow the status using a mobile phone using a Bluetooth connection. Check the official manual provided by Victron Energy charger for details. Protection cover Link The rover comes with a PVC protection cover. The cover must always be placed on the rover when the precision weeding is used. If the precision weeding is not used, it can be removed if there is no risk of the CNC becoming wet. The CNC, on its own, is not waterproof. If the CNC is removed, it is possible to use the rover without cover in light rain conditions (TODO: IP level?...) Attendance (TODO: regulations?) Link IMPORTANT: The rover must be used only in the presence of an operator. The operator must be within a distance of XXX metres of the rover and must be able to reach the rover quickly in case of an emergency. The operator should carry the remote control with them at all times whilst the rover is on. This is in order to be able to recover the navigation control of the rover in all circumstances (TODO: add emergency button on the remote control or use certified controller for industrial use). The rover should not be used in proximity to people who have not been instructed to use the rover (TODO). IMPORTANT: The rover must be used only during the day in good light conditions. Storage Link The rover should be kept in a covered and dry space when not in use. Emergency button Link The emergency button on the back of the rover can be used to cut the power to the motors and CNC at any time. To cut the power, push the red button. To power up the motors, the button must be reactivated. This can be done by pulling the button out again. CAUTION: Before reactivating the button, make sure that the CNC and wheel motors are not moving. Adjusting the width of the rover Link The wheel-base of the rover can be adjusted to fit the variable width of the rails and beds. To adjust the width of the rover, loosen the four U-brackets that fix the wheel modules to the main frame and slide the wheel modules to the desired position. Ensure that the position of the modules is symmetric relative to the main frame. After a change to the width of the wheel-base, the CNC may have to be recalibrated if the weeding tool should work on a larger or smaller bed size (see \"Calibrating the CNC\") Engaging/disengaging the motor lock levers (freewheeling mode and drive mode) Link The two wheel motors each have a lock lever that allows them to switch between freewheeling mode or motor drive mode. When the lock lever is in the horizontal position the wheels are freewheeling. In freewheeling mode, the robot can be moved simply by pushing it. Turn the lever 90\u00b0 into the vertical position - pointing to the ground - to switch the drive mode. In the drive mode, the wheels are powered by the motors and to move the rover you must use the remote control or the command interface. Mnemonic tip: When the handle points to the ground, the traction goes to the soil. When the handle is parallel to the axis, the traction \"stays\" in the axis. CAUTION: Only switch to the drive mode when the rover is \"off\" to assure that the motors are powered off. Lock lever vertical: Motor drive mode Lock lever horizontal: Freewheeling mode Control panel state message Link The display of the control panel is divided in two lines. The upper line shows current status of the rover: Ready : The rover is ready for use. The on-board computer is running and the motors can be powered up. Navigating : The rover is in navigation mode. Use the controller to steer the rover. System Failure : The rover encountered an error from which it cannot recover. Please restart the rover. Start-up procedure Link If it is the first usage of the rover, you should go to the section \"First time configuration\". Before starting up, the rover should be in the following state: Verify that the emergency button is deactivated (pushed in). Verify that the on/off switch on the electronics housing is off. The start-up can now proceed: Engage the lock levers on the motor to put the motors into drive mode. Turn the on/off button on the electronic housing on. Activate the emergency by pulling it out. The rover should begin the start-up sequence. When the start-up is completed, the display will show \"Ready\". The motors of the wheels and the CNC are now powered up. You can now use the controller of the web interface to navigate the rover or send commands to the rover. Shut-down procedure Link To turn off the rover, take the following steps: Deactivate the emergency button by pushing it in. This removes the power supply to the motor and CNC. Turn off the on/off button on the electronics housing Switching from drive to freewheeling mode Link When the rover is in drive mode (motors powered on, the lock levers on the motor engaged/vertical), it is possible to go to freewheeling mode as follows: Turn off the power of the motors by pressing the red emergency button. Turn the motor lock levers in the horizontal position (disengaged). Once these steps are completed, you can move the rover by pushing it. Switching from freewheeling to drive mode (TODO: this is not safe, yet) Link To switch from freewheeling to drive mode, the rover\u2019s state should be \u201dOn\u201d, the user may then perform the following operations: Turn the motor lock levers in the vertical position (engaged). Pull the red security button to power the motors. CAUTION: Make sure that the speed and direction controllers of the remote control are in the neutral position. Accessing the web interface of the rover Link Adapt the Wi-Fi settings of the phone, tablet, or computer such that it connects to the same Wi-Fi network as the rover. The interface of the rover is accessible through a web browser. On the mobile device, open up your preferred web browser (see \"Supported web browsers\") and in the address field enter the following URL: http://romi-rover.local . To facilitate access to the interface in future uses, you can add the address to your bookmarks. Using the controller Link The rover comes with the Sony DUALSHOCK\u00ae4 Wireless Controller. The remote controller must be recharged using a micro USB cable that is plugged into the front of the controller. The controller is sensitive to the recharger. Make sure to use a quality recharger. Controlling the rover through the control panel Link You can send commands to the rover or activate the navigation mode using the controller as follows. Send a command to start an action Link The rover must be in the Ready state. Then press the Show Menu button. The name of the first task will appear on the bottom line of the display. Use the Next Menu and Previous Menu buttons to navigate in the list of possible tasks. To cancel and return to the main screen, press Cancel Button. Press the Select Button a second time to confirm the action, or press cancel to return to the menu screen. Switching to navigation mode Link XXX Changing the list of rover actions Link The file uses the JSON format to describe the list of scripts and the associated sequences of actions. The general structure is as follows: [ { \"name\" : \"move-forward\" , \"display_name\" : \"Forward\" , \"script\" : [ { \"action\" : \"move\" , \"distance\" : 0.60 } ] }, { \"name\" : \"move-backward\" , \"display_name\" : \"Backward\" , \"script\" : [ { \"action\" : \"move\" , \"distance\" : -0.60 } ] }, { \"name\" : \"scan\" , \"display_name\" : \"Scan\" , \"script\" : [ { \"action\" : \"start_recording\" }, { \"action\" : \"move\" , \"distance\" : 3.6 }, { \"action\" : \"stop_recording\" } ] } ] The file contains a list of scripts. Each script has a name that is used to identify the script, a display_name that is shown in the user interface, and a script field that consists of a list of actions. The list of available actions and their parameters is out of the scope of this manual. Please refer to the online documentation at https://docs.romi-project.eu/Rover/configuration/ for details. If you make modifications to the file, it is very important that the new content is a valid JSON file. If not, the rover will fail to load the file and no buttons will be shown in the user interface.","title":"User Manual"},{"location":"Rover/manual/#romi-rover-user-manual","text":"Draft v2 - Rover V3 - Summer 2022 This manual describes the usage of a fully assembled and ready-to-use rover. The information on how to build your own rover can be found in a separate document. Visit the ROMI web site at https://docs.romi-project.eu for more information. \u00a9 Sony Computer Science Laboratories - CC BY-SA 4.0 Licence","title":"ROMI Rover User Manual"},{"location":"Rover/manual/#acknowledgements","text":"ROMI has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 773875.","title":"Acknowledgements"},{"location":"Rover/manual/#short-description","text":"The ROMI Rover is a farming tool that assists vegetable farmers in maintaining vegetable beds free of weeds. It does this by regularly hoeing the surface of the soil and thus preventing small weeds from taking root. It can do this task mostly autonomously and requires only minor changes to the organisation of the farm. It is designed for vegetable beds between 70 cm and 110 cm wide (not including the passage ways) and for crops up to 50 cm high. It currently handles two types of crops, lettuce and carrots. The lettuce can be planted out in any layout, most likely in a quincunx pattern. In this configuration the rover uses a precision rotary hoe to clean the soil both between the rows and the plants. For carrots, the rover uses classical mechanical tools, such as stirrup hoe, to regularly clean the soil in between the rows. In this configuration, the carrots should be sown in line. A weekly passage of the robot should be sufficient to keep the population of weeds under control. In addition to weeding, the embedded camera can be used to collect images of the vegetable beds. The ROMI Rover is targeted at farms that grow small crops, such as lettuce and carrots, on a relatively small surface, between 200 m\u00b2 and 5 ha (Utilised Agricultural Area).","title":"Short description"},{"location":"Rover/manual/#technical-specifications","text":"This section details the technical specifications, operating instructions and configuration of the current version of the Romi rover: V3. Characteristic Value Dimensions Width: Adjustable between 1.42 m and 2 m Length: 1.8 m Height: 1.44 m With tool carrier: add 1 m to length Weight 140 kg Battery life 8 h (approx. dependent on use case) Charging time TODO Weeding speed Precision weeding: 235 m\u00b2/day Classical weeding: 6400 m\u00b2/day Width vegetable beds Between 0.7 and 1.1 m Handled crops Precision weeding: Lettuce Classical weeding: Carrots Turning space at end of bed 2.8 m (3.5 m with tool carrier)","title":"Technical specifications"},{"location":"Rover/manual/#functional-specifications-and-requirements","text":"The following configuration is required for the use of the ROMI rover. Image Price Features/Function ROMI rover with a remote control, a battery charger, and a protective cover 5000 \u20ac (estimate) - Prevents weed development - Takes image scans of the beds","title":"Functional specifications and requirements"},{"location":"Rover/manual/#overview-of-the-components","text":"The control panel : The control panel provides a means to view status messages, and request the rover to perform a preconfigured action. The on/off switch : The on/off switch can be found on the electronics housing.","title":"Overview of the components"},{"location":"Rover/manual/#operation-instructions","text":"","title":"Operation instructions"},{"location":"Rover/manual/#overview-of-the-rovers-usage","text":"The basic usage of the rover is to position it on a vegetable bed and let the machine clean the top-soil with a rotating precision hoe. The rover must be taken to the field using the remote control. The robot currently expects the vegetables to be grown in \"beds\" of 0.7 m to 1.1 m wide. The robot is designed for smaller market farms of less than 5 ha but the size of the farm depends on the number of rovers that you will use, and the amount of crop you want to cover. The rover can navigate autonomously along a bed if there is a clear line of crops that it can follow. Once the rover is positioned at the beginning of a bed, it hoes the surface of the soil so that small weeds cannot take root. It can perform this action along the entire length of the bed. Note that the rover cannot remove mature weeds that have already established themselves. It is therefore necessary to start with a vegetable bed that has been cleaned from all weeds. This can be done with various classical techniques to prepare the vegetable beds. Once the beds are clean, the rover can be used to maintain them as such. Two weeding methods are available. First, a precision weeding method in which the top-soil is turned over in between the rows (also called \"inter-row\") and between the plants (or \"intra-row\"). Second, a classical weeding method in which standard weeding tools are dragged behind the rover between rows of vegetables. For the precision weeding method, the rover uses a camera to detect the plants that are underneath the rover. It then moves the precision weeding tool over the surface whilst passing closely around the detected vegetables. Although the rover is autonomous for weeding a single bed, it is important to stay in proximity to the rover. A U-turn must also be manually performed at the end of the bed and the rover repositioned in line with the rails of the next bed.","title":"Overview of the rover\u2019s usage"},{"location":"Rover/manual/#setting-up-the-vegetable-beds","text":"The use of the rover requires relatively flat beds. Precision weeding works best if the surface of the culture beds is planar. Ideally, the alleys between the beds should also be flat, to facilitate navigation of the rover. Otherwise, there is a risk that the tool will detach from the soil or that it will dig into the soil. There is no precise measure of how flat the beds should be but small holes in the ground should be avoided. The presence of stones should also be avoided. Small stones (approx. 1 cm) should not perturb the rover very much. NOTE: It is more convenient if the width of the vegetable beds are constant so that the width of the rover doesn't have to be adapted beds.","title":"Setting up the vegetable beds"},{"location":"Rover/manual/#setting-up-the-wi-fi-access-point","text":"The use of a Wi-Fi access point is optional but strongly recommended. The rover must be connected to a Wi-Fi access point with Internet access for the following functionality: To automatically upload the images taken by the rover to the Farmer\u2019s Dashboard web application. For remote maintenance. Both features are optional and can be left out when the rover is used for weeding only. However, if you decide to use an access point, it is important that the Wi-Fi signal is strong enough in all the zones where the rover will be used. If not, it may be impossible to connect to the rover\u2019s web interface with a phone or tablet to send instructions to the rover. It will still be possible to send instructions to the rover using the control panel (see \"Controlling the rover through the control panel\"). The set-up of the Wi-Fi network is not part of the ROMI Rover package. In case of doubt, you should seek advice from a professional about the best solution for your premises. However, below, we briefly discuss several options. Use an existing Wi-Fi router: If the zone where you wish to use the rover is adjacent to existing infrastructure (home, barn) and you have the possibility to install an Internet connection at the premises (ADSL modem over a phone line or any other solution), the Wi-Fi capabilities of the modem can be used to offer Internet access to the rover. Expand the reach of an existing Wi-Fi network : An existing Wi-Fi can be extended to increase its reach using Wi-Fi range extenders. They pick up and retransmit an existing Wi-Fi signal. Most extenders require a standard power supply although some can be powered using an USB battery. Using an Ethernet cable of up to 100 metres long, it is possible to position a secondary access point. Some of the Wi-Fi access points can be powered directly over the Ethernet cable (PoE, Power over Ethernet) removing the need for a power socket. It is also possible to send the network signal over existing electricity cables using a technology called power-line communication (PLC). Finally, there exist also long-range wireless outdoor WiFi extenders that transmit the network between two antennas designed for transmission over distances from a 100 metres to over a kilometre. Install a GSM Wi-Fi router : If there is a good mobile phone signal strength in the field, a GSM Wi-Fi router is a viable option. A GSM Wi-Fi router connects to the Internet over a mobile data link (4G, 3G, HSDPA\u2026) and provides access to other devices over Wi-Fi. Separate routers with good antennas can be purchased at reasonable prices but generally require a power plug. Smaller, USB-powered routers are available also and can be plugged directly into a USB port inside the rover. A mobile phone configured as a hotspot is an alternative solution (although with a smaller range than a dedicated router with good antennas). The downside of this option is that it requires a SIM card and a subscription with a mobile network operator. Using a USB GSM modem : In contrast to the solution above, a USB GSM modem is not a stand-alone router but, when plugged in, the Raspberry Pi will see the modem as an additional network interface. In this set-up, the rover remains the hotspot for the Wi-Fi network but will route any Internet traffic through the GSM modem. This solution may require additional changes to the network configuration of the Raspberry Pi.","title":"Setting up the Wi-Fi access point"},{"location":"Rover/manual/#charging-the-rover","text":"The rover uses two 12 V Lithium batteries (the internal working voltage is 24 V). Use the supplied Victron Energy Blue Smart IP67 24V 5A Charger to reload the batteries. Plug 230 V side of the charger in a regular power plug. The 24 V side must be plugged into the POWER CHARGER plug on the battery box. The charger has LED indicators to show the status of the charging cycle. It is also possible to follow the status using a mobile phone using a Bluetooth connection. Check the official manual provided by Victron Energy charger for details.","title":"Charging the rover"},{"location":"Rover/manual/#protection-cover","text":"The rover comes with a PVC protection cover. The cover must always be placed on the rover when the precision weeding is used. If the precision weeding is not used, it can be removed if there is no risk of the CNC becoming wet. The CNC, on its own, is not waterproof. If the CNC is removed, it is possible to use the rover without cover in light rain conditions (TODO: IP level?...)","title":"Protection cover"},{"location":"Rover/manual/#attendance-todo-regulations","text":"IMPORTANT: The rover must be used only in the presence of an operator. The operator must be within a distance of XXX metres of the rover and must be able to reach the rover quickly in case of an emergency. The operator should carry the remote control with them at all times whilst the rover is on. This is in order to be able to recover the navigation control of the rover in all circumstances (TODO: add emergency button on the remote control or use certified controller for industrial use). The rover should not be used in proximity to people who have not been instructed to use the rover (TODO). IMPORTANT: The rover must be used only during the day in good light conditions.","title":"Attendance (TODO: regulations?)"},{"location":"Rover/manual/#storage","text":"The rover should be kept in a covered and dry space when not in use.","title":"Storage"},{"location":"Rover/manual/#emergency-button","text":"The emergency button on the back of the rover can be used to cut the power to the motors and CNC at any time. To cut the power, push the red button. To power up the motors, the button must be reactivated. This can be done by pulling the button out again. CAUTION: Before reactivating the button, make sure that the CNC and wheel motors are not moving.","title":"Emergency button"},{"location":"Rover/manual/#adjusting-the-width-of-the-rover","text":"The wheel-base of the rover can be adjusted to fit the variable width of the rails and beds. To adjust the width of the rover, loosen the four U-brackets that fix the wheel modules to the main frame and slide the wheel modules to the desired position. Ensure that the position of the modules is symmetric relative to the main frame. After a change to the width of the wheel-base, the CNC may have to be recalibrated if the weeding tool should work on a larger or smaller bed size (see \"Calibrating the CNC\")","title":"Adjusting the width of the rover"},{"location":"Rover/manual/#engagingdisengaging-the-motor-lock-levers-freewheeling-mode-and-drive-mode","text":"The two wheel motors each have a lock lever that allows them to switch between freewheeling mode or motor drive mode. When the lock lever is in the horizontal position the wheels are freewheeling. In freewheeling mode, the robot can be moved simply by pushing it. Turn the lever 90\u00b0 into the vertical position - pointing to the ground - to switch the drive mode. In the drive mode, the wheels are powered by the motors and to move the rover you must use the remote control or the command interface. Mnemonic tip: When the handle points to the ground, the traction goes to the soil. When the handle is parallel to the axis, the traction \"stays\" in the axis. CAUTION: Only switch to the drive mode when the rover is \"off\" to assure that the motors are powered off. Lock lever vertical: Motor drive mode Lock lever horizontal: Freewheeling mode","title":"Engaging/disengaging the motor lock levers (freewheeling mode and drive mode)"},{"location":"Rover/manual/#control-panel-state-message","text":"The display of the control panel is divided in two lines. The upper line shows current status of the rover: Ready : The rover is ready for use. The on-board computer is running and the motors can be powered up. Navigating : The rover is in navigation mode. Use the controller to steer the rover. System Failure : The rover encountered an error from which it cannot recover. Please restart the rover.","title":"Control panel state message"},{"location":"Rover/manual/#start-up-procedure","text":"If it is the first usage of the rover, you should go to the section \"First time configuration\". Before starting up, the rover should be in the following state: Verify that the emergency button is deactivated (pushed in). Verify that the on/off switch on the electronics housing is off. The start-up can now proceed: Engage the lock levers on the motor to put the motors into drive mode. Turn the on/off button on the electronic housing on. Activate the emergency by pulling it out. The rover should begin the start-up sequence. When the start-up is completed, the display will show \"Ready\". The motors of the wheels and the CNC are now powered up. You can now use the controller of the web interface to navigate the rover or send commands to the rover.","title":"Start-up procedure"},{"location":"Rover/manual/#shut-down-procedure","text":"To turn off the rover, take the following steps: Deactivate the emergency button by pushing it in. This removes the power supply to the motor and CNC. Turn off the on/off button on the electronics housing","title":"Shut-down procedure"},{"location":"Rover/manual/#switching-from-drive-to-freewheeling-mode","text":"When the rover is in drive mode (motors powered on, the lock levers on the motor engaged/vertical), it is possible to go to freewheeling mode as follows: Turn off the power of the motors by pressing the red emergency button. Turn the motor lock levers in the horizontal position (disengaged). Once these steps are completed, you can move the rover by pushing it.","title":"Switching from drive to freewheeling mode"},{"location":"Rover/manual/#switching-from-freewheeling-to-drive-mode-todo-this-is-not-safe-yet","text":"To switch from freewheeling to drive mode, the rover\u2019s state should be \u201dOn\u201d, the user may then perform the following operations: Turn the motor lock levers in the vertical position (engaged). Pull the red security button to power the motors. CAUTION: Make sure that the speed and direction controllers of the remote control are in the neutral position.","title":"Switching from freewheeling to drive mode (TODO: this is not safe, yet)"},{"location":"Rover/manual/#accessing-the-web-interface-of-the-rover","text":"Adapt the Wi-Fi settings of the phone, tablet, or computer such that it connects to the same Wi-Fi network as the rover. The interface of the rover is accessible through a web browser. On the mobile device, open up your preferred web browser (see \"Supported web browsers\") and in the address field enter the following URL: http://romi-rover.local . To facilitate access to the interface in future uses, you can add the address to your bookmarks.","title":"Accessing the web interface of the rover"},{"location":"Rover/manual/#using-the-controller","text":"The rover comes with the Sony DUALSHOCK\u00ae4 Wireless Controller. The remote controller must be recharged using a micro USB cable that is plugged into the front of the controller. The controller is sensitive to the recharger. Make sure to use a quality recharger.","title":"Using the controller"},{"location":"Rover/manual/#controlling-the-rover-through-the-control-panel","text":"You can send commands to the rover or activate the navigation mode using the controller as follows.","title":"Controlling the rover through the control panel"},{"location":"Rover/manual/#send-a-command-to-start-an-action","text":"The rover must be in the Ready state. Then press the Show Menu button. The name of the first task will appear on the bottom line of the display. Use the Next Menu and Previous Menu buttons to navigate in the list of possible tasks. To cancel and return to the main screen, press Cancel Button. Press the Select Button a second time to confirm the action, or press cancel to return to the menu screen.","title":"Send a command to start an action"},{"location":"Rover/manual/#switching-to-navigation-mode","text":"XXX","title":"Switching to navigation mode"},{"location":"Rover/manual/#changing-the-list-of-rover-actions","text":"The file uses the JSON format to describe the list of scripts and the associated sequences of actions. The general structure is as follows: [ { \"name\" : \"move-forward\" , \"display_name\" : \"Forward\" , \"script\" : [ { \"action\" : \"move\" , \"distance\" : 0.60 } ] }, { \"name\" : \"move-backward\" , \"display_name\" : \"Backward\" , \"script\" : [ { \"action\" : \"move\" , \"distance\" : -0.60 } ] }, { \"name\" : \"scan\" , \"display_name\" : \"Scan\" , \"script\" : [ { \"action\" : \"start_recording\" }, { \"action\" : \"move\" , \"distance\" : 3.6 }, { \"action\" : \"stop_recording\" } ] } ] The file contains a list of scripts. Each script has a name that is used to identify the script, a display_name that is shown in the user interface, and a script field that consists of a list of actions. The list of available actions and their parameters is out of the scope of this manual. Please refer to the online documentation at https://docs.romi-project.eu/Rover/configuration/ for details. If you make modifications to the file, it is very important that the new content is a valid JSON file. If not, the rover will fail to load the file and no buttons will be shown in the user interface.","title":"Changing the list of rover actions"},{"location":"Rover/software/","text":"Software Installation Link Overview Link This document describes how to run and compile the software for the ROMI Rover. If you are a developer looking for details on the source code then have a look at the separate Developer Documentation . Prerequisites Link The software of the rover runs on Linux. It is not tied to a specific Linux distribution but we have tested it mostly on recent versions of Debian (includin Raspian) and Ubuntu. The software is mostly writen in C and depends on the following libraries: libr : Common code for the rcom and the libromi libraries. It provides some OS abstraction (for example for threads, memory allocation, file system, networking), some core functionality (logging, time), and some base classes (variable-size memory buffers, json parser, lists, serial connections). Code rcom : An inter-process communication framework. It provides real-time communication using UDP messages and high-level communication based on web protocols (HTTP, Websockets). It also includes several utilities to develop and manage rcom applications. Code libromi : Base classes for the romi rover: fsdb (database with filesystem back-end), image loading and manipulations, \u2026) Code romi-brush-motor-controller : The motor controller. Code romi-rover : All of the apps for the Romi rover. Code By default, the rover uses a USB camera. It is possible to use the Intel Realsense camera on the Picamera instead. In that case, you will have to install additional libraries (see XXX). Installing a Raspberry Pi from scratch Link We use the Lite version of Raspbian. You can download it at https://www.raspberrypi.org/downloads/raspbian/ . There are several ways to prepare the disk image for the RPi. Check the page at https://www.raspberrypi.org/documentation/installation/installing-images/ (there\u2019s lots of information available on this topic online) and follow the instructions that suit you best. Once you have the SD card, connect RPi to screen, keyboard and network (ethernet), power up the board and log in (user pi , password raspberry ). The first thing you want to do is change some of the default settings using the raspi-config tool. In the console type: $ sudo raspi-config The list of settings that you may want to look at includes: 1 Change User Password 2 Network Options Hostname WiFi 4 Localisation Options Change locales Change keyboard layout 5 Interfacing Options Enable SSH 8 Update Next, create the user \u2018romi\u2019: $ sudo adduser romi $ sudo adduser romi dialout $ sudo adduser romi video $ sudo adduser romi sudo After that, quit the current session and login again as user \u2018romi\u2019. The nano text editor is installed by default but if you prefer anoher editor, now is a good time to install it: $ sudo apt install emacs-nox ( ... or any editor you like : ) Install the developer tools: $ sudo apt install build-essential cmake git Install the software dependencies: $ sudo apt install libpng-dev libjpeg9-dev That's it. You should be ready. Quick install #!/bin/bash # Install the dependencies sudo apt install build-essential cmake git libpng-dev libjpeg9-dev # Download, compile and install the libraries & apps for id in libr rcom libromi romi-rover ; do echo ---------------------------------------------- echo Compiling $id # Download or update the github repository if [ -d $id ] ; then cd $id git pull else git clone https://github.com/romi/ $id .git cd $id fi # Standard cmake build sequence mkdir -p build cd build cmake .. make sudo make install sudo ldconfig # Get ready for the next component cd ../.. done Installing the romi-rover apps Link You should first install the libr , rcom , and libromi libraries. Check out their the Github pages for the installation instruction and the API documentation. You should also flash the motor controller to the Arduino (instructions are available on Github ) too. Once that is done, the installation of the romi-rover apps is straight-forward. First, check out the code: $ git clone https://github.com/romi/romi-rover.git Then proceed to the compilation and installation: $ cd romi-rover $ mkdir build $ cd build $ cmake .. $ make $ sudo make install Compiling the picamera app Link Although not currently used by the ROMI Rover, we have an Rcom app to access the Picamera. To get it working, you will first have to install the raspicam library: $ git clone https://github.com/cedricve/raspicam.git $ cd raspicam/ $ mkdir build $ cd build/ $ cmake .. $ make $ sudo make install $ sudo ldconfig Once raspicam is installed, you must re-run cmake to enable the compilation of the picamera app: $ cd romi-rover/build/ $ rm CMakeCache.txt $ cmake .. -DWITH_PICAMERA = ON $ make $ sudo make install Compiling the realsense app Link The fonctionality of the Realsense camera app is not complete. You can use it to obtain RGB images and depth images (as BW PNG images). You will first have to install librealsense2 . When librealsense is installed, re-run cmake to enable the compilation of the realsense app: $ cd romi-rover/build/ $ rm CMakeCache.txt $ cmake .. -DWITH_REALSENSE = ON $ make $ sudo make install Configuration Link Configuring the romi-rover apps Link In the directory /home/romi, create the following directories and copy the default configuration and script files: $ cd /home/romi $ mkdir sessions $ mkdir config $ cp <romi-rover>/config/config-romi-rover.json config/ $ mkdir scripts $ cp <romi-rover>/script/config-default.json scripts/ \"html\": \"<romi-rover>/interface/html\", Starting the apps on boot Link Currently we are still using the old rc.local mechanism. The file /etc/rc.local is no longer included in more recent Ubuntu versions. If ls /etc/rc.local returns an error, you will have to create the file as follows: $ sudo nano /etc/rc.local Copy the following contents: #!/bin/sh -e # # rc.local # # This script is executed at the end of each multiuser runlevel. # Make sure that the script will \"exit 0\" on success or any other # value on error. # # In order to enable or disable this script just change the execution # bits. # # By default this script does nothing. exit 0 Finally, make the script executable. $ sudo chmod +x /etc/rc.local To enable the apps on start-up, add the following line in /etc/rc.local, above the exit 0 line: /usr/local/bin/rclaunch /home/romi/config/config-romi-rover.json & Configuring the image uploads Link Annex: the apps and their options Link \"fake_camera\": { \"image\": \"data/camera.jpg\" },","title":"Software"},{"location":"Rover/software/#software-installation","text":"","title":"Software Installation"},{"location":"Rover/software/#overview","text":"This document describes how to run and compile the software for the ROMI Rover. If you are a developer looking for details on the source code then have a look at the separate Developer Documentation .","title":"Overview"},{"location":"Rover/software/#prerequisites","text":"The software of the rover runs on Linux. It is not tied to a specific Linux distribution but we have tested it mostly on recent versions of Debian (includin Raspian) and Ubuntu. The software is mostly writen in C and depends on the following libraries: libr : Common code for the rcom and the libromi libraries. It provides some OS abstraction (for example for threads, memory allocation, file system, networking), some core functionality (logging, time), and some base classes (variable-size memory buffers, json parser, lists, serial connections). Code rcom : An inter-process communication framework. It provides real-time communication using UDP messages and high-level communication based on web protocols (HTTP, Websockets). It also includes several utilities to develop and manage rcom applications. Code libromi : Base classes for the romi rover: fsdb (database with filesystem back-end), image loading and manipulations, \u2026) Code romi-brush-motor-controller : The motor controller. Code romi-rover : All of the apps for the Romi rover. Code By default, the rover uses a USB camera. It is possible to use the Intel Realsense camera on the Picamera instead. In that case, you will have to install additional libraries (see XXX).","title":"Prerequisites"},{"location":"Rover/software/#installing-a-raspberry-pi-from-scratch","text":"We use the Lite version of Raspbian. You can download it at https://www.raspberrypi.org/downloads/raspbian/ . There are several ways to prepare the disk image for the RPi. Check the page at https://www.raspberrypi.org/documentation/installation/installing-images/ (there\u2019s lots of information available on this topic online) and follow the instructions that suit you best. Once you have the SD card, connect RPi to screen, keyboard and network (ethernet), power up the board and log in (user pi , password raspberry ). The first thing you want to do is change some of the default settings using the raspi-config tool. In the console type: $ sudo raspi-config The list of settings that you may want to look at includes: 1 Change User Password 2 Network Options Hostname WiFi 4 Localisation Options Change locales Change keyboard layout 5 Interfacing Options Enable SSH 8 Update Next, create the user \u2018romi\u2019: $ sudo adduser romi $ sudo adduser romi dialout $ sudo adduser romi video $ sudo adduser romi sudo After that, quit the current session and login again as user \u2018romi\u2019. The nano text editor is installed by default but if you prefer anoher editor, now is a good time to install it: $ sudo apt install emacs-nox ( ... or any editor you like : ) Install the developer tools: $ sudo apt install build-essential cmake git Install the software dependencies: $ sudo apt install libpng-dev libjpeg9-dev That's it. You should be ready. Quick install #!/bin/bash # Install the dependencies sudo apt install build-essential cmake git libpng-dev libjpeg9-dev # Download, compile and install the libraries & apps for id in libr rcom libromi romi-rover ; do echo ---------------------------------------------- echo Compiling $id # Download or update the github repository if [ -d $id ] ; then cd $id git pull else git clone https://github.com/romi/ $id .git cd $id fi # Standard cmake build sequence mkdir -p build cd build cmake .. make sudo make install sudo ldconfig # Get ready for the next component cd ../.. done","title":"Installing a Raspberry Pi from scratch"},{"location":"Rover/software/#installing-the-romi-rover-apps","text":"You should first install the libr , rcom , and libromi libraries. Check out their the Github pages for the installation instruction and the API documentation. You should also flash the motor controller to the Arduino (instructions are available on Github ) too. Once that is done, the installation of the romi-rover apps is straight-forward. First, check out the code: $ git clone https://github.com/romi/romi-rover.git Then proceed to the compilation and installation: $ cd romi-rover $ mkdir build $ cd build $ cmake .. $ make $ sudo make install","title":"Installing the romi-rover apps"},{"location":"Rover/software/#compiling-the-picamera-app","text":"Although not currently used by the ROMI Rover, we have an Rcom app to access the Picamera. To get it working, you will first have to install the raspicam library: $ git clone https://github.com/cedricve/raspicam.git $ cd raspicam/ $ mkdir build $ cd build/ $ cmake .. $ make $ sudo make install $ sudo ldconfig Once raspicam is installed, you must re-run cmake to enable the compilation of the picamera app: $ cd romi-rover/build/ $ rm CMakeCache.txt $ cmake .. -DWITH_PICAMERA = ON $ make $ sudo make install","title":"Compiling the picamera app"},{"location":"Rover/software/#compiling-the-realsense-app","text":"The fonctionality of the Realsense camera app is not complete. You can use it to obtain RGB images and depth images (as BW PNG images). You will first have to install librealsense2 . When librealsense is installed, re-run cmake to enable the compilation of the realsense app: $ cd romi-rover/build/ $ rm CMakeCache.txt $ cmake .. -DWITH_REALSENSE = ON $ make $ sudo make install","title":"Compiling the realsense app"},{"location":"Rover/software/#configuration","text":"","title":"Configuration"},{"location":"Rover/software/#configuring-the-romi-rover-apps","text":"In the directory /home/romi, create the following directories and copy the default configuration and script files: $ cd /home/romi $ mkdir sessions $ mkdir config $ cp <romi-rover>/config/config-romi-rover.json config/ $ mkdir scripts $ cp <romi-rover>/script/config-default.json scripts/ \"html\": \"<romi-rover>/interface/html\",","title":"Configuring the romi-rover apps"},{"location":"Rover/software/#starting-the-apps-on-boot","text":"Currently we are still using the old rc.local mechanism. The file /etc/rc.local is no longer included in more recent Ubuntu versions. If ls /etc/rc.local returns an error, you will have to create the file as follows: $ sudo nano /etc/rc.local Copy the following contents: #!/bin/sh -e # # rc.local # # This script is executed at the end of each multiuser runlevel. # Make sure that the script will \"exit 0\" on success or any other # value on error. # # In order to enable or disable this script just change the execution # bits. # # By default this script does nothing. exit 0 Finally, make the script executable. $ sudo chmod +x /etc/rc.local To enable the apps on start-up, add the following line in /etc/rc.local, above the exit 0 line: /usr/local/bin/rclaunch /home/romi/config/config-romi-rover.json &","title":"Starting the apps on boot"},{"location":"Rover/software/#configuring-the-image-uploads","text":"","title":"Configuring the image uploads"},{"location":"Rover/software/#annex-the-apps-and-their-options","text":"\"fake_camera\": { \"image\": \"data/camera.jpg\" },","title":"Annex: the apps and their options"},{"location":"Rover/UserManual/","text":"Romi Rover User Manual Draft - September 2020 Contents Link This manual describes the usage of a fully assembled and ready to use rover. The information on how to build your own rover can be found in a separate document. Visit the ROMI web site at https://docs.romi-project.eu for more information. This manual consists of the following chapters: Copyright Legend Short Description Technical Specifications Functional Specifications and Requirements Operating Instructions","title":"Index"},{"location":"Rover/UserManual/#contents","text":"This manual describes the usage of a fully assembled and ready to use rover. The information on how to build your own rover can be found in a separate document. Visit the ROMI web site at https://docs.romi-project.eu for more information. This manual consists of the following chapters: Copyright Legend Short Description Technical Specifications Functional Specifications and Requirements Operating Instructions","title":"Contents"},{"location":"Rover/UserManual/copyright/","text":"Copyright Link Copyright \u00a9 Sony Computer Science Laboratories License Link The documentation is available under the CC BY-SA 4.0 License Acknowledgements Link ROMI has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 773875. Credits Link TODO Disclaimers Link TODO","title":"Copyright"},{"location":"Rover/UserManual/copyright/#copyright","text":"Copyright \u00a9 Sony Computer Science Laboratories","title":"Copyright"},{"location":"Rover/UserManual/copyright/#license","text":"The documentation is available under the CC BY-SA 4.0 License","title":"License"},{"location":"Rover/UserManual/copyright/#acknowledgements","text":"ROMI has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 773875.","title":"Acknowledgements"},{"location":"Rover/UserManual/copyright/#credits","text":"TODO","title":"Credits"},{"location":"Rover/UserManual/copyright/#disclaimers","text":"TODO","title":"Disclaimers"},{"location":"Rover/UserManual/description/","text":"Short Description Link The ROMI Rover is a farming tool that assists vegetable farmers in maintaining the vegetable beds free from weeds. It does this by regularly hoeing the surface of the soil and thus preventing small weeds from taking roots. It can do this task mostly autonomously and requires only minor changes to the organization of the farm. It is designed for vegetable beds between 70 cm and 120 cm wide (not including the passage ways) and currently handles two types of crops, lettuce and carrots. The lettuce can be planted out in any layout, most likely in a quincunx pattern. In this configuration the rover uses a precision rotary hoe to clean the soil both between the rows and the plants. For the carrots, the rover uses classical mechanical tools, such as stirrup hoe, to regularly clean the soil in between the rows. In this configuration, the carrots should be sown in line. A weekly passage of the robot should be sufficient to keep the population of weeds under control. In addition to the weeding, the rover provides the following useful functions. It can draw quincunx patterns or straight lines in empty vegetable beds to speed up seeding and planting out. The embedded camera can be used to collect images of the vegetable beds. It can also be used as a motorized tray. The ROMI Rover is targeted at farms that grow lettuce and carrots on a relatively small surface, between 200 m\u00b2 and 5 ha (Utilized Agricultural Area).","title":"Description"},{"location":"Rover/UserManual/description/#short-description","text":"The ROMI Rover is a farming tool that assists vegetable farmers in maintaining the vegetable beds free from weeds. It does this by regularly hoeing the surface of the soil and thus preventing small weeds from taking roots. It can do this task mostly autonomously and requires only minor changes to the organization of the farm. It is designed for vegetable beds between 70 cm and 120 cm wide (not including the passage ways) and currently handles two types of crops, lettuce and carrots. The lettuce can be planted out in any layout, most likely in a quincunx pattern. In this configuration the rover uses a precision rotary hoe to clean the soil both between the rows and the plants. For the carrots, the rover uses classical mechanical tools, such as stirrup hoe, to regularly clean the soil in between the rows. In this configuration, the carrots should be sown in line. A weekly passage of the robot should be sufficient to keep the population of weeds under control. In addition to the weeding, the rover provides the following useful functions. It can draw quincunx patterns or straight lines in empty vegetable beds to speed up seeding and planting out. The embedded camera can be used to collect images of the vegetable beds. It can also be used as a motorized tray. The ROMI Rover is targeted at farms that grow lettuce and carrots on a relatively small surface, between 200 m\u00b2 and 5 ha (Utilized Agricultural Area).","title":"Short Description"},{"location":"Rover/UserManual/legend/","text":"Legend Link The draft of the manual uses the following labels to indicate the status of the development: [v2] The feature refers to the second prototype and its implementation will be different in the third prototype. [v3] The feature refers to the third prototype and exists in a different form in the second prototype. [v2++] The feature will be implemented in the second prototype (deadline: December 2020) [v3++] The feature will be implemented in the third prototype (deadline: May 2021).","title":"Legend"},{"location":"Rover/UserManual/legend/#legend","text":"The draft of the manual uses the following labels to indicate the status of the development: [v2] The feature refers to the second prototype and its implementation will be different in the third prototype. [v3] The feature refers to the third prototype and exists in a different form in the second prototype. [v2++] The feature will be implemented in the second prototype (deadline: December 2020) [v3++] The feature will be implemented in the third prototype (deadline: May 2021).","title":"Legend"},{"location":"Rover/UserManual/requirements/","text":"Functional Specifications and Requirements Link The following configuration is required for the use of the ROMI rover. Profile Price Features/Function Bed of vegetables Width of the bed: min 0.7 m max 1.2 m [v3] Handled crops Lettuce : The lettuce can be planted out in any layout, most likely in a quincunx pattern Carrots : The carrots should be sown in line. ROMI rover with a remote control, a battery charger, and a protective cover [v3] 5000 \u20ac (estimate) Prevents weed development Draw patterns for seeding and planting out Takes image scans of the beds Tool carrier Photo 1000 \u20ac (estimate) Used with the rover to carry the mechanical weeding tools (optional but needed in most configurations) Mechanical weeding tools (from Terrateck or other manufacturers) 400-1000\u20ac (estimate) The weeding tools that will be fixed on the tool carrier. Sold by other manufacturers, for example, by [Terrateck](https://www.terrateck.com) ([catalogue](https://www.terrateck.com/en/16-houes-pousse-pousse)) TOTAL cost of an equipped rover 6400 \u2013 7000 \u20ac Guides Photo Stainless steel tubes (45x1.5): 3 \u20ac/m Tuyau irrigation PE HD 50mm : 4.4 \u20ac/m The guides consist of either stainless metal tubes (preferred) or wooden boards. They facilitate the navigation of the rover along the bed so that it can operate accurately and reliably. A mobile phone Existing phone: 0\u20ac Dedicated phone: 200 \u20ac Used to control the rover To browse the archived images of the online service WiFi Rover as hotspot: 0 \u20ac/month Existing WiFi: 0 \u20ac/month GSM WiFi router: 120\u20ac (router) + 10 \u20ac/month (SIM card) WiFi connectivity in the field To connect the mobile phone to the rover (required) To archives the images taken by the rover (optional) To provide remote assistance (optional) Romi's Online Farmer's Dashboard [v3++] 1 \u20ac/month Monthly fee for the use of the Romi remote server (optional): To archive and browse the images taken by the rover To receive remote assistance Training 70 \u20ac/day Two days of training for the rover (optional) Preprogramming according to the configuration of the farm 50 \u20ac Assistance to fine-tune the rover\u2019s configuration to the farm (optional) Maintenance costs 5 \u20ac/month/rover Maintenance contract with the ROMI association (or a local distributor) (optional) Examples for different farm sizes Link For an urban farm of 5 000 m2 (lettuces or carrots) 83 beds of 50 meters (beds of 80 cm wide, pathway of 40 cm between beds, 3320 m\u00b2 cultivated) 4150 meters of guides (12 450 \u20ac\u2026) 1 rover with tool carrier and mechanical weeding tools (7000 \u20ac) Investment costs: 7000 to 7510 + 12450 \u20ac + 16 \u20ac/months (optional: SIM card, remote maintenance, server archiving) For a vegetable farm of 2 ha 124 beds of 100 meters (beds of 1.1 m wide, pathway of 0.50 m between beds, 13640 m\u00b2 cultivated) 12 400 meters of guides (37.2 k\u20ac) Lettuces (180 000 plants, 54 k\u20ac-129 k\u20ac revenu): 4 rovers Investment costs: 20.5 k\u20ac + 37.2 k\u20ac + 10 to 31 \u20ac/months (wifi and maintenance) Alternative solution: Plastic mulching film: 12 x 226,50 \u20ac = 2712 \u20ac/year [ REF ] Carrots (1.24M plants, 60-130 tonnes, 31k\u20ac-130k\u20ac revenue) 1 rover Investment costs: 7000 to 7510 \u20ac + 49.6 k\u20ac + 10 to 16 \u20ac/months (wifi and maintenance) Sur carottes, l\u2019ensemble des op\u00e9rations de d\u00e9sherbage varie de 120 \u00e0 plus de 900 heures/ha. [ http://itab.asso.fr/downloads/fiches-lpc/lpc-carotte.pdf ] Prix [ https://rnm.franceagrimer.fr/prix?CAROTTE&12MOIS ] For a vegetable farm of 5 ha XXX beds XXX meters of guides XXX rovers Investment costs: XXX \u20ac + \u20ac/months (wifi and maintenance) It has to be pointed out that in many countries, there are public financial supports for farmers which want to invest in robotics.","title":"Functional Specifications and Requirements"},{"location":"Rover/UserManual/requirements/#functional-specifications-and-requirements","text":"The following configuration is required for the use of the ROMI rover. Profile Price Features/Function Bed of vegetables Width of the bed: min 0.7 m max 1.2 m [v3] Handled crops Lettuce : The lettuce can be planted out in any layout, most likely in a quincunx pattern Carrots : The carrots should be sown in line. ROMI rover with a remote control, a battery charger, and a protective cover [v3] 5000 \u20ac (estimate) Prevents weed development Draw patterns for seeding and planting out Takes image scans of the beds Tool carrier Photo 1000 \u20ac (estimate) Used with the rover to carry the mechanical weeding tools (optional but needed in most configurations) Mechanical weeding tools (from Terrateck or other manufacturers) 400-1000\u20ac (estimate) The weeding tools that will be fixed on the tool carrier. Sold by other manufacturers, for example, by [Terrateck](https://www.terrateck.com) ([catalogue](https://www.terrateck.com/en/16-houes-pousse-pousse)) TOTAL cost of an equipped rover 6400 \u2013 7000 \u20ac Guides Photo Stainless steel tubes (45x1.5): 3 \u20ac/m Tuyau irrigation PE HD 50mm : 4.4 \u20ac/m The guides consist of either stainless metal tubes (preferred) or wooden boards. They facilitate the navigation of the rover along the bed so that it can operate accurately and reliably. A mobile phone Existing phone: 0\u20ac Dedicated phone: 200 \u20ac Used to control the rover To browse the archived images of the online service WiFi Rover as hotspot: 0 \u20ac/month Existing WiFi: 0 \u20ac/month GSM WiFi router: 120\u20ac (router) + 10 \u20ac/month (SIM card) WiFi connectivity in the field To connect the mobile phone to the rover (required) To archives the images taken by the rover (optional) To provide remote assistance (optional) Romi's Online Farmer's Dashboard [v3++] 1 \u20ac/month Monthly fee for the use of the Romi remote server (optional): To archive and browse the images taken by the rover To receive remote assistance Training 70 \u20ac/day Two days of training for the rover (optional) Preprogramming according to the configuration of the farm 50 \u20ac Assistance to fine-tune the rover\u2019s configuration to the farm (optional) Maintenance costs 5 \u20ac/month/rover Maintenance contract with the ROMI association (or a local distributor) (optional)","title":"Functional Specifications and Requirements"},{"location":"Rover/UserManual/requirements/#examples-for-different-farm-sizes","text":"For an urban farm of 5 000 m2 (lettuces or carrots) 83 beds of 50 meters (beds of 80 cm wide, pathway of 40 cm between beds, 3320 m\u00b2 cultivated) 4150 meters of guides (12 450 \u20ac\u2026) 1 rover with tool carrier and mechanical weeding tools (7000 \u20ac) Investment costs: 7000 to 7510 + 12450 \u20ac + 16 \u20ac/months (optional: SIM card, remote maintenance, server archiving) For a vegetable farm of 2 ha 124 beds of 100 meters (beds of 1.1 m wide, pathway of 0.50 m between beds, 13640 m\u00b2 cultivated) 12 400 meters of guides (37.2 k\u20ac) Lettuces (180 000 plants, 54 k\u20ac-129 k\u20ac revenu): 4 rovers Investment costs: 20.5 k\u20ac + 37.2 k\u20ac + 10 to 31 \u20ac/months (wifi and maintenance) Alternative solution: Plastic mulching film: 12 x 226,50 \u20ac = 2712 \u20ac/year [ REF ] Carrots (1.24M plants, 60-130 tonnes, 31k\u20ac-130k\u20ac revenue) 1 rover Investment costs: 7000 to 7510 \u20ac + 49.6 k\u20ac + 10 to 16 \u20ac/months (wifi and maintenance) Sur carottes, l\u2019ensemble des op\u00e9rations de d\u00e9sherbage varie de 120 \u00e0 plus de 900 heures/ha. [ http://itab.asso.fr/downloads/fiches-lpc/lpc-carotte.pdf ] Prix [ https://rnm.franceagrimer.fr/prix?CAROTTE&12MOIS ] For a vegetable farm of 5 ha XXX beds XXX meters of guides XXX rovers Investment costs: XXX \u20ac + \u20ac/months (wifi and maintenance) It has to be pointed out that in many countries, there are public financial supports for farmers which want to invest in robotics.","title":"Examples for different farm sizes"},{"location":"Rover/UserManual/specifications/","text":"Technical Specifications Link Feature Value Size [v3] (Width x Length x Height) Min.: 1.45 m x 1.60 m x 1.46 m Max.: 1.70 m x 1.60 m x 1.46 m Weight [v2] 80 kg (estimate) Battery life [v3++] 8 h Charging time TODO Weeding speed Precision weeding: 600 m\u00b2/day [v3++] (235 m\u00b2/day [v2]) Classical weeding 7200 m\u00b2/day [v3] (6400 m\u00b2/day [v2]) Width vegetable beds [v3] Min.: 0.70 m Max.: 1.2 m Handled crops Precision weeding: Lettuce Classical weeding: Carrots Turning space at end of bed 2 m (TODO: verify)","title":"Technical Specifications"},{"location":"Rover/UserManual/specifications/#technical-specifications","text":"Feature Value Size [v3] (Width x Length x Height) Min.: 1.45 m x 1.60 m x 1.46 m Max.: 1.70 m x 1.60 m x 1.46 m Weight [v2] 80 kg (estimate) Battery life [v3++] 8 h Charging time TODO Weeding speed Precision weeding: 600 m\u00b2/day [v3++] (235 m\u00b2/day [v2]) Classical weeding 7200 m\u00b2/day [v3] (6400 m\u00b2/day [v2]) Width vegetable beds [v3] Min.: 0.70 m Max.: 1.2 m Handled crops Precision weeding: Lettuce Classical weeding: Carrots Turning space at end of bed 2 m (TODO: verify)","title":"Technical Specifications"},{"location":"plant-3d-explorer/","text":"This page will describe how to set up and start the plant visualizer. Dependencies Link This project is build using Node JS. As such, you need to install Node JS and npm (which should come with node). !!! important To make sure everything works as intended, check that your version of Node is at least 10, and your version of npm is at lease 6. Installing packages and setting up the environment Link After making sure you have the right versions of Node and npm, you will need to clone the repository of the project. Simply run git clone https://github.com/romi/plant-3d-explorer The next step is to install everything the app needs with the following command: npm install If you want the app to use the remote server to fetch data, run the following command: echo \"REACT_APP_API_URL='https://db.romi-project.eu'\" > .env.local If you don't, the app will use a local server at address localhost:5000 . In order to do this, you need to get plantdb running. Starting the app Link To start a development server (used to develop, or simply test the app), run: npm start The app will then run at address localhost:3000 . !!! warning Using Google Chrome (or Chromium Browser) is recommended as some problems have been encountered on Firefox due to some libraries we used. More commands Link To see a more detailed list of available commands, visit the GitHub repository .","title":"Home"},{"location":"plant-3d-explorer/#dependencies","text":"This project is build using Node JS. As such, you need to install Node JS and npm (which should come with node). !!! important To make sure everything works as intended, check that your version of Node is at least 10, and your version of npm is at lease 6.","title":"Dependencies"},{"location":"plant-3d-explorer/#installing-packages-and-setting-up-the-environment","text":"After making sure you have the right versions of Node and npm, you will need to clone the repository of the project. Simply run git clone https://github.com/romi/plant-3d-explorer The next step is to install everything the app needs with the following command: npm install If you want the app to use the remote server to fetch data, run the following command: echo \"REACT_APP_API_URL='https://db.romi-project.eu'\" > .env.local If you don't, the app will use a local server at address localhost:5000 . In order to do this, you need to get plantdb running.","title":"Installing packages and setting up the environment"},{"location":"plant-3d-explorer/#starting-the-app","text":"To start a development server (used to develop, or simply test the app), run: npm start The app will then run at address localhost:3000 . !!! warning Using Google Chrome (or Chromium Browser) is recommended as some problems have been encountered on Firefox due to some libraries we used.","title":"Starting the app"},{"location":"plant-3d-explorer/#more-commands","text":"To see a more detailed list of available commands, visit the GitHub repository .","title":"More commands"},{"location":"plant-3d-explorer/guide/","text":"The scan list page Link When first opening the app, you will be greeted with a home page displaying every available scans. A scan is defined as a folder containing acquisition of plant data (e.g. 2D RGB images, manual measures) and a set of 3D reconstructions and analysis (e.g. point cloud, mesh, automated measures,...) From this page, you can search for specific keywords, order the scans by their name, date, etc. In addition, scans can be filtered according to what data is actually available for each scan. Note If you're a developer, you can download the metadata and archives associated with each scan as well. The viewer Link Clicking the Open green button next to a scan (far right of the row corresponding to a scan) to open the actual viewer and explore available data. The 3D reconstruction and the 3D-view panel Link The largest panel on the left displays the 3D view, allowing to observe the reconstructed plant and navigate around with basic mouse commands (right/left click and scroll). Check the question mark (?) help icon in the bottom right corner of this panel for more description of mouse control. Several icons appear on the top band of the 3D-view panel: they can simply be activated by clicking, hovering over provide information about their function. Feel free to experiment with the different features and tools of the 3D view. The graphs Link The right side consists of a measurement panels, typically displaying graphs of data related to the plant (either measurements provided as metadata or computed during the analysis). Help tooltips explain what those graphs correspond to. By default, phyllotaxis graphs (see below) are displayed. The green \"plus\" (+) icon allow uploading more measurement panels if available. On each graph, the top right corner cross button closes the panel. Phyllotaxis graphs Link Phyllotaxis graphs are sequences of either divergence angles (in degree) or internode length (in mm) measured between two consecutive lateral organs of the main stem, starting from the base to the shoot tip. Hovering a graph highlights a particular interval and display the order index of the two organs lateral organs bounding this interval. Interval hovering is synchronized between the two phyllotaxis graphs (divergence angles and internodes) if they are active. Synchronization also happen with the 3D-view panel if 'organ highlight' option is active: only the corresponding pair of organ remain visible. Changing organ colors for this pair in the 3D-view panel will also synchronize the colors in the graphs. Clicking on an interval in the graph allow to maintain the selection active. Download buttons at the top of each graph allow to get the data in CSV or TSV format. The photo carousel and camera mode Link The entire bottom of the page is a line called the 'carousel': it contains images contained in the scan folder. Click on any of the images in the carousel, and the 3D-view panel will switch into the camera mode: the available 3D reconstructions will be superposed on the image, allowing to check the accuracy of the 3D reconstruction. The image of the carousel currently displayed in the 3D-View panel is boxed. Dragging this box left/rightwards changes the active image, and the 3D-view is synchronized immediately, allowing to dynamically navigate along the original camera path of acquisition. If the scan folder contain several images that have been made available for the visualizer, the source of images can be changed. The carousel will be populated by this new source and images can be displayed into the 3D viewer by activating the camera mode. Reporting bugs Link If you encounter some kind of unwanted behavior, or have a feature suggestion, head over to the GitHub repository and write an issue!","title":"Guide"},{"location":"plant-3d-explorer/guide/#the-scan-list-page","text":"When first opening the app, you will be greeted with a home page displaying every available scans. A scan is defined as a folder containing acquisition of plant data (e.g. 2D RGB images, manual measures) and a set of 3D reconstructions and analysis (e.g. point cloud, mesh, automated measures,...) From this page, you can search for specific keywords, order the scans by their name, date, etc. In addition, scans can be filtered according to what data is actually available for each scan. Note If you're a developer, you can download the metadata and archives associated with each scan as well.","title":"The scan list page"},{"location":"plant-3d-explorer/guide/#the-viewer","text":"Clicking the Open green button next to a scan (far right of the row corresponding to a scan) to open the actual viewer and explore available data.","title":"The viewer"},{"location":"plant-3d-explorer/guide/#the-3d-reconstruction-and-the-3d-view-panel","text":"The largest panel on the left displays the 3D view, allowing to observe the reconstructed plant and navigate around with basic mouse commands (right/left click and scroll). Check the question mark (?) help icon in the bottom right corner of this panel for more description of mouse control. Several icons appear on the top band of the 3D-view panel: they can simply be activated by clicking, hovering over provide information about their function. Feel free to experiment with the different features and tools of the 3D view.","title":"The 3D reconstruction and the 3D-view panel"},{"location":"plant-3d-explorer/guide/#the-graphs","text":"The right side consists of a measurement panels, typically displaying graphs of data related to the plant (either measurements provided as metadata or computed during the analysis). Help tooltips explain what those graphs correspond to. By default, phyllotaxis graphs (see below) are displayed. The green \"plus\" (+) icon allow uploading more measurement panels if available. On each graph, the top right corner cross button closes the panel.","title":"The graphs"},{"location":"plant-3d-explorer/guide/#phyllotaxis-graphs","text":"Phyllotaxis graphs are sequences of either divergence angles (in degree) or internode length (in mm) measured between two consecutive lateral organs of the main stem, starting from the base to the shoot tip. Hovering a graph highlights a particular interval and display the order index of the two organs lateral organs bounding this interval. Interval hovering is synchronized between the two phyllotaxis graphs (divergence angles and internodes) if they are active. Synchronization also happen with the 3D-view panel if 'organ highlight' option is active: only the corresponding pair of organ remain visible. Changing organ colors for this pair in the 3D-view panel will also synchronize the colors in the graphs. Clicking on an interval in the graph allow to maintain the selection active. Download buttons at the top of each graph allow to get the data in CSV or TSV format.","title":"Phyllotaxis graphs"},{"location":"plant-3d-explorer/guide/#the-photo-carousel-and-camera-mode","text":"The entire bottom of the page is a line called the 'carousel': it contains images contained in the scan folder. Click on any of the images in the carousel, and the 3D-view panel will switch into the camera mode: the available 3D reconstructions will be superposed on the image, allowing to check the accuracy of the 3D reconstruction. The image of the carousel currently displayed in the 3D-View panel is boxed. Dragging this box left/rightwards changes the active image, and the 3D-view is synchronized immediately, allowing to dynamically navigate along the original camera path of acquisition. If the scan folder contain several images that have been made available for the visualizer, the source of images can be changed. The carousel will be populated by this new source and images can be displayed into the 3D viewer by activating the camera mode.","title":"The photo carousel and camera mode"},{"location":"plant-3d-explorer/guide/#reporting-bugs","text":"If you encounter some kind of unwanted behavior, or have a feature suggestion, head over to the GitHub repository and write an issue!","title":"Reporting bugs"},{"location":"plant_imager/","text":"Plant Phenotyping Link Within the ROMI project, some work packages are oriented towards the development of a 3D plant phenotyping platform adapted to single potted plants. To achieve this goal, the team developed a suite of affordable open-source tools (hardware & software) presented hereafter. We aim at making our software architecture modular to ensure the required flexibility and adaptability to most of the robotic & research applications from the ROMI project when possible. Todo simplify module interaction overview Modules Link PlantDB Plant 3D Explorer Plant Imager Plant 3D Vision Usage Link Tutorials","title":"Home"},{"location":"plant_imager/#plant-phenotyping","text":"Within the ROMI project, some work packages are oriented towards the development of a 3D plant phenotyping platform adapted to single potted plants. To achieve this goal, the team developed a suite of affordable open-source tools (hardware & software) presented hereafter. We aim at making our software architecture modular to ensure the required flexibility and adaptability to most of the robotic & research applications from the ROMI project when possible. Todo simplify module interaction overview","title":"Plant Phenotyping"},{"location":"plant_imager/#modules","text":"PlantDB Plant 3D Explorer Plant Imager Plant 3D Vision","title":"Modules"},{"location":"plant_imager/#usage","text":"Tutorials","title":"Usage"},{"location":"plant_imager/build_v2/","text":"Assembly instructions for the second version of the plant imager Link This is the documentation for the second iteration of the plant imager hardware. Assembly overview Link You will have to achieve the following steps: assemble the aluminium frame: instructions assemble the X-Carve CNC: instructions attach the CNC the to aluminium frame assemble one of the electronic controller: X-controller: instructions gShield hat : instructions wire the X-Carve CNC motors & endstops to the controller assemble & mount the custom gimbal: instructions choose a camera & build the camera mount PiCamera: instructions Sony RX-0 gPhoto2 compatible camera [OPTIONAL] add the LED bars inside wiring it all. Open Hardware Link We use \"widely available\" materials and provides all the required information, so you can reproduce and modify our work. If you want to modify it, please note the following important points: The width and depth of the aluminium frame is dependent of the CNC frame size, make sure the CNC frame fits inside the aluminium frame! The height of the aluminium frame will determine the maximum observable plant height. Wiring & communication overview Link The plant imager electronics can be divided in two functional groups: the \"CNC group\", responsible for moving the camera around; the \"Gimbal & Camera group\", allowing to rotate the camera around the z-axis to face the object (plant) to acquire. Note that the CNC group has 3 axis movement (X, Y & Z), the gimbal add a forth one. Overview of the plant imager's electronics. Note that the PiCamera act as a wifi hotspot to which the controller computer is registered. The two groups communicate with the main controller by USB. Note that since the gimbal & camera are located at the end of the arm on the z-axis, the USB cable powering & controlling them are going through the cable rails with those dedicated to the axis motors. This may create interference, so you will need a (long) properly shielded USB cable!","title":"Home"},{"location":"plant_imager/build_v2/#assembly-instructions-for-the-second-version-of-the-plant-imager","text":"This is the documentation for the second iteration of the plant imager hardware.","title":"Assembly instructions for the second version of the plant imager"},{"location":"plant_imager/build_v2/#assembly-overview","text":"You will have to achieve the following steps: assemble the aluminium frame: instructions assemble the X-Carve CNC: instructions attach the CNC the to aluminium frame assemble one of the electronic controller: X-controller: instructions gShield hat : instructions wire the X-Carve CNC motors & endstops to the controller assemble & mount the custom gimbal: instructions choose a camera & build the camera mount PiCamera: instructions Sony RX-0 gPhoto2 compatible camera [OPTIONAL] add the LED bars inside wiring it all.","title":"Assembly overview"},{"location":"plant_imager/build_v2/#open-hardware","text":"We use \"widely available\" materials and provides all the required information, so you can reproduce and modify our work. If you want to modify it, please note the following important points: The width and depth of the aluminium frame is dependent of the CNC frame size, make sure the CNC frame fits inside the aluminium frame! The height of the aluminium frame will determine the maximum observable plant height.","title":"Open Hardware"},{"location":"plant_imager/build_v2/#wiring-communication-overview","text":"The plant imager electronics can be divided in two functional groups: the \"CNC group\", responsible for moving the camera around; the \"Gimbal & Camera group\", allowing to rotate the camera around the z-axis to face the object (plant) to acquire. Note that the CNC group has 3 axis movement (X, Y & Z), the gimbal add a forth one. Overview of the plant imager's electronics. Note that the PiCamera act as a wifi hotspot to which the controller computer is registered. The two groups communicate with the main controller by USB. Note that since the gimbal & camera are located at the end of the arm on the z-axis, the USB cable powering & controlling them are going through the cable rails with those dedicated to the axis motors. This may create interference, so you will need a (long) properly shielded USB cable!","title":"Wiring &amp; communication overview"},{"location":"plant_imager/build_v2/alu_frame/","text":"Assembly instructions for the aluminium frame v2 Link Important All units are in millimeters! BOM Link To assemble the aluminium frame you will need: 4x 1800mm 3030 profiles ( A ) 4x 1200mm 3030 profiles ( B ) 4x 1007mm 3030 profiles ( C ) 16x brackets (30x60) for 3030 profiles 64x M6 T-nuts 64x M6x12 screws Note that: Parts A defines the height of the enclosure; Parts B & C depends on the external size of the chosen CNC frame! Assembly instructions Link Start by assembling the smallest sides of the frame using two A parts & two C parts per side. To ease the build, lay them flat on the floor. Prepare all A parts with their brackets at the right position: Positioning the 30x60 brackets on the 1800mm 3030 profiles (A). Add two C parts per side to construct the \"small sides\". Make sure they are horizontal. Then add the two lower B parts & finally the two upper B parts. Assembled aluminium frame schematic & isometric view. Optional Link Add wood corner brackets to rigidify the structure. Assembled aluminium frame with wood corners schematic. Isometric view of assembled aluminium frame with wood corners.","title":"Aluminium frame"},{"location":"plant_imager/build_v2/alu_frame/#assembly-instructions-for-the-aluminium-frame-v2","text":"Important All units are in millimeters!","title":"Assembly instructions for the aluminium frame v2"},{"location":"plant_imager/build_v2/alu_frame/#bom","text":"To assemble the aluminium frame you will need: 4x 1800mm 3030 profiles ( A ) 4x 1200mm 3030 profiles ( B ) 4x 1007mm 3030 profiles ( C ) 16x brackets (30x60) for 3030 profiles 64x M6 T-nuts 64x M6x12 screws Note that: Parts A defines the height of the enclosure; Parts B & C depends on the external size of the chosen CNC frame!","title":"BOM"},{"location":"plant_imager/build_v2/alu_frame/#assembly-instructions","text":"Start by assembling the smallest sides of the frame using two A parts & two C parts per side. To ease the build, lay them flat on the floor. Prepare all A parts with their brackets at the right position: Positioning the 30x60 brackets on the 1800mm 3030 profiles (A). Add two C parts per side to construct the \"small sides\". Make sure they are horizontal. Then add the two lower B parts & finally the two upper B parts. Assembled aluminium frame schematic & isometric view.","title":"Assembly instructions"},{"location":"plant_imager/build_v2/alu_frame/#optional","text":"Add wood corner brackets to rigidify the structure. Assembled aluminium frame with wood corners schematic. Isometric view of assembled aluminium frame with wood corners.","title":"Optional"},{"location":"plant_imager/build_v2/cnc_calibration/","text":"CNC setup & calibration Link After building & wiring the CNC and fitting it into the aluminium frame it is required to calibrate some Grbl software parameters like the homing directions or the acceleration rates . Using Grbl Link Except if you are familiar with Grbl, if you want to know more, have a look at the official Grbl wiki . Connect to the Arduino Link Then you can use picocom to connect to the arduino: picocom /dev/ttyACM0 -b 115200 Note See here how to find the right USB port. Getting help Link You can type $ (and press enter) to get help. You should not see any local echo of the $ and enter. Grbl should respond with: [HLP:$$ $# $G $I $N $x=val $Nx=line $J=line $SLP $C $X $H ~ ! ? ctrl-x] ok Accessing the saved configuration Link To access the saved configuration, type $$ to obtain the parameter values. For example our config is: $0=10 $1=255 $2=0 $3=5 $4=0 $5=0 $6=0 $10=1 $11=0.020 $12=0.002 $13=0 $20=1 $21=0 $22=1 $23=0 $24=25.000 $25=5000.000 $26=250 $27=1.000 $30=12000 $31=0 $32=0 $100=40.000 $101=40.000 $102=188.947 $110=8000.000 $111=8000.000 $112=1000.000 $120=100.000 $121=100.000 $122=50.000 $130=780.000 $131=790.000 $132=150.000 Note See the official Grbl wiki for details & meaning of parameter. Setup & calibration Link Now we will: verify the cnc respond correctly to homing instruction calibrate if needed Homing the X-carve Link Once you are sure that everything is connected properly (especially the limit switches) you can try to \"home\" the X-Carve manually using $H in the previous terminal connected to Grbl. Warning Be ready to use the emergency stop button in case the axes move in the opposite direction of your limit switches! Note If you don't see any response in the terminal when you type the commands, it is perfectly normal! Change homing direction: Link Parameters named $3 control the homing direction. From the official wiki: Quote By default, Grbl assumes that the axes move in a positive direction when the direction pin signal is low, and a negative direction when the pin is high. To configure the homing direction, you simply need to send the value for the axes you want to invert using the table below. Setting Value Mask Invert X Invert Y Invert Z 0 00000000 N N N 1 00000001 Y N N 2 00000010 N Y N 3 00000011 Y Y N 4 00000100 N N Y 5 00000101 Y N Y 6 00000110 N Y Y 7 00000111 Y Y Y For example, if want to invert the Y axis direction only, you'd send $3=2 to Grbl, and the setting should now read $3=2 (dir port invert mask:00000010) Edit the acceleration rates Link Parameters named $120 , $121 & $123 control the axes acceleration in mm/second/second. From the official wiki: Quote Simplistically, a lower value makes Grbl ease slower into motion, while a higher value yields tighter moves and reaches the desired feed rates much quicker. Much like the max rate setting, each axis has its own acceleration value and are independent of each other. This means that a multi-axis motion will only accelerate as quickly as the lowest contributing axis can. Since the Z-axis arm is long and have a \"heavy weight\" (gimbal + camera) at its lower end, it is probably a good to keep low values to avoid blurry image due to shaking! To determine optimal values, the official wiki is clear, you will have to run some tests: Quote Again, like the max rate setting, the simplest way to determine the values for this setting is to individually test each axis with slowly increasing values until the motor stalls. Then finalize your acceleration setting with a value 10-20% below this absolute max value. This should account for wear, friction, and mass inertia. We highly recommend that you dry test some G-code programs with your new settings before committing to them. Sometimes the loading on your machine is different when moving in all axes together. For example, you can send $120=100.0 to set the X axis acceleration rate to 100 mm/second\u00b2. Note Deceleration rates may also create shaking!","title":"CNC calibration"},{"location":"plant_imager/build_v2/cnc_calibration/#cnc-setup-calibration","text":"After building & wiring the CNC and fitting it into the aluminium frame it is required to calibrate some Grbl software parameters like the homing directions or the acceleration rates .","title":"CNC setup &amp; calibration"},{"location":"plant_imager/build_v2/cnc_calibration/#using-grbl","text":"Except if you are familiar with Grbl, if you want to know more, have a look at the official Grbl wiki .","title":"Using Grbl"},{"location":"plant_imager/build_v2/cnc_calibration/#connect-to-the-arduino","text":"Then you can use picocom to connect to the arduino: picocom /dev/ttyACM0 -b 115200 Note See here how to find the right USB port.","title":"Connect to the Arduino"},{"location":"plant_imager/build_v2/cnc_calibration/#getting-help","text":"You can type $ (and press enter) to get help. You should not see any local echo of the $ and enter. Grbl should respond with: [HLP:$$ $# $G $I $N $x=val $Nx=line $J=line $SLP $C $X $H ~ ! ? ctrl-x] ok","title":"Getting help"},{"location":"plant_imager/build_v2/cnc_calibration/#accessing-the-saved-configuration","text":"To access the saved configuration, type $$ to obtain the parameter values. For example our config is: $0=10 $1=255 $2=0 $3=5 $4=0 $5=0 $6=0 $10=1 $11=0.020 $12=0.002 $13=0 $20=1 $21=0 $22=1 $23=0 $24=25.000 $25=5000.000 $26=250 $27=1.000 $30=12000 $31=0 $32=0 $100=40.000 $101=40.000 $102=188.947 $110=8000.000 $111=8000.000 $112=1000.000 $120=100.000 $121=100.000 $122=50.000 $130=780.000 $131=790.000 $132=150.000 Note See the official Grbl wiki for details & meaning of parameter.","title":"Accessing the saved configuration"},{"location":"plant_imager/build_v2/cnc_calibration/#setup-calibration","text":"Now we will: verify the cnc respond correctly to homing instruction calibrate if needed","title":"Setup &amp; calibration"},{"location":"plant_imager/build_v2/cnc_calibration/#homing-the-x-carve","text":"Once you are sure that everything is connected properly (especially the limit switches) you can try to \"home\" the X-Carve manually using $H in the previous terminal connected to Grbl. Warning Be ready to use the emergency stop button in case the axes move in the opposite direction of your limit switches! Note If you don't see any response in the terminal when you type the commands, it is perfectly normal!","title":"Homing the X-carve"},{"location":"plant_imager/build_v2/cnc_calibration/#change-homing-direction","text":"Parameters named $3 control the homing direction. From the official wiki: Quote By default, Grbl assumes that the axes move in a positive direction when the direction pin signal is low, and a negative direction when the pin is high. To configure the homing direction, you simply need to send the value for the axes you want to invert using the table below. Setting Value Mask Invert X Invert Y Invert Z 0 00000000 N N N 1 00000001 Y N N 2 00000010 N Y N 3 00000011 Y Y N 4 00000100 N N Y 5 00000101 Y N Y 6 00000110 N Y Y 7 00000111 Y Y Y For example, if want to invert the Y axis direction only, you'd send $3=2 to Grbl, and the setting should now read $3=2 (dir port invert mask:00000010)","title":"Change homing direction:"},{"location":"plant_imager/build_v2/cnc_calibration/#edit-the-acceleration-rates","text":"Parameters named $120 , $121 & $123 control the axes acceleration in mm/second/second. From the official wiki: Quote Simplistically, a lower value makes Grbl ease slower into motion, while a higher value yields tighter moves and reaches the desired feed rates much quicker. Much like the max rate setting, each axis has its own acceleration value and are independent of each other. This means that a multi-axis motion will only accelerate as quickly as the lowest contributing axis can. Since the Z-axis arm is long and have a \"heavy weight\" (gimbal + camera) at its lower end, it is probably a good to keep low values to avoid blurry image due to shaking! To determine optimal values, the official wiki is clear, you will have to run some tests: Quote Again, like the max rate setting, the simplest way to determine the values for this setting is to individually test each axis with slowly increasing values until the motor stalls. Then finalize your acceleration setting with a value 10-20% below this absolute max value. This should account for wear, friction, and mass inertia. We highly recommend that you dry test some G-code programs with your new settings before committing to them. Sometimes the loading on your machine is different when moving in all axes together. For example, you can send $120=100.0 to set the X axis acceleration rate to 100 mm/second\u00b2. Note Deceleration rates may also create shaking!","title":"Edit the acceleration rates"},{"location":"plant_imager/build_v2/cnc_communication/","text":"Communicating with the CNC Link Connect to the Arduino UNO Link First you need to find which USB port your arduino is connected to. To do so, you can use dmesg : make sure the usb cable from the arduino is unplugged run dmesg -w in a terminal connect the usb and see something like: [ 70480 .940181 ] usb 1 -2: new full-speed USB device number 31 using xhci_hcd [ 70481 .090857 ] usb 1 -2: New USB device found, idVendor = 2a03, idProduct = 0043 , bcdDevice = 0 .01 [ 70481 .090862 ] usb 1 -2: New USB device strings: Mfr = 1 , Product = 2 , SerialNumber = 220 [ 70481 .090865 ] usb 1 -2: Product: Arduino Uno [ 70481 .090868 ] usb 1 -2: Manufacturer: Arduino Srl [ 70481 .090871 ] usb 1 -2: SerialNumber: 554313131383512001F0 [ 70481 .093408 ] cdc_acm 1 -2:1.0: ttyACM0: USB ACM device Important The important info here is ttyACM0 ! Then you can use picocom to connect to the arduino: picocom /dev/ttyACM0 -b 115200 Note -b 115200 is the baud rate of the connection, read the picocom man page for more info. Once connected you should see something like: picocom v2.2 port is : /dev/ttyACM0 flowcontrol : none baudrate is : 115200 parity is : none databits are : 8 stopbits are : 1 escape is : C-a local echo is : no noinit is : no noreset is : no nolock is : no send_cmd is : sz -vv receive_cmd is : rz -vv -E imap is : omap is : emap is : crcrlf,delbs, Type [ C-a ] [ C-h ] to see available commands Terminal ready Grbl 1 .1f [ '$' for help ] [ MSG: '$H' | '$X' to unlock ] This mean you now have access to a Grbl terminal ( Grbl 1.1f ) to communicate, notably send instructions, to the CNC! Troubleshooting Link Serial access denied Link Look here if you can not communicate with the scanner using usb.","title":"CNC communication"},{"location":"plant_imager/build_v2/cnc_communication/#communicating-with-the-cnc","text":"","title":"Communicating with the CNC"},{"location":"plant_imager/build_v2/cnc_communication/#connect-to-the-arduino-uno","text":"First you need to find which USB port your arduino is connected to. To do so, you can use dmesg : make sure the usb cable from the arduino is unplugged run dmesg -w in a terminal connect the usb and see something like: [ 70480 .940181 ] usb 1 -2: new full-speed USB device number 31 using xhci_hcd [ 70481 .090857 ] usb 1 -2: New USB device found, idVendor = 2a03, idProduct = 0043 , bcdDevice = 0 .01 [ 70481 .090862 ] usb 1 -2: New USB device strings: Mfr = 1 , Product = 2 , SerialNumber = 220 [ 70481 .090865 ] usb 1 -2: Product: Arduino Uno [ 70481 .090868 ] usb 1 -2: Manufacturer: Arduino Srl [ 70481 .090871 ] usb 1 -2: SerialNumber: 554313131383512001F0 [ 70481 .093408 ] cdc_acm 1 -2:1.0: ttyACM0: USB ACM device Important The important info here is ttyACM0 ! Then you can use picocom to connect to the arduino: picocom /dev/ttyACM0 -b 115200 Note -b 115200 is the baud rate of the connection, read the picocom man page for more info. Once connected you should see something like: picocom v2.2 port is : /dev/ttyACM0 flowcontrol : none baudrate is : 115200 parity is : none databits are : 8 stopbits are : 1 escape is : C-a local echo is : no noinit is : no noreset is : no nolock is : no send_cmd is : sz -vv receive_cmd is : rz -vv -E imap is : omap is : emap is : crcrlf,delbs, Type [ C-a ] [ C-h ] to see available commands Terminal ready Grbl 1 .1f [ '$' for help ] [ MSG: '$H' | '$X' to unlock ] This mean you now have access to a Grbl terminal ( Grbl 1.1f ) to communicate, notably send instructions, to the CNC!","title":"Connect to the Arduino UNO"},{"location":"plant_imager/build_v2/cnc_communication/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"plant_imager/build_v2/cnc_communication/#serial-access-denied","text":"Look here if you can not communicate with the scanner using usb.","title":"Serial access denied"},{"location":"plant_imager/build_v2/cnc_electronics/","text":"Wiring of the CNC electronics Link X-Controller assembly Link If you ordered the X-Controller, follow the official assembly instructions here . Note This is what we used in our second and third version of the phenotyping station hardware. Warning We replaced the default Grbl firmware by Oquam , our own implementation. See here for the instructions on how to flash this firmware to the X-Controller. gShield assembly Link If you have the recent version, follow the official wiring instructions here . Here is link to the post 2015 version of the \"wiring\" instructions. Here is link to the post 2015 version of the \"electronic assembly\" instructions. Note This is what we used in our first version of the phenotyping station hardware. BOM Link If you are familiar with the Arduino world, the electronic is pretty straightforward: Arduino UNO (official buy here ) Synthetos gShield (buy here ) Power converter 220V ac. - 24V dc. (buy @ Farnell ) Emergency stop button (buy @ Farnell ) Optional: 24V fan 220V power cord Note Before 2017 X-Carve shipped 400W power units, now they use a 320W unit. The link is for a 350W unit. Wiring instructions Link Wire the Stepper Cable to the gShield Link Once you\u2019ve determined which stepper cable belongs to which axis, you can wire them into the gShield. First loosen all the screws on the gShield (they will jump a thread when they are fully loose, but they won\u2019t come out of the terminal blocks.) The gShield is marked \"X,\" \"Y,\" and \"Z\". Wire the stepper cable according to the markings on the shield and order your wires (from left to right) black, green, white, red. Check out this diagram for clarification. Mount the gShield Link Now push the gShield onto the Arduino. There are pins on the gShield that go into the headers of the Arduino. 24V Fan mount (optional) Link If you want, you can add a 24V fan to cool the shield. Uses the 24V pins on the gShield as shown in the picture below: Loosen the screws in the power terminal of the gShield and insert the red twisted pair into Vmot and the black twisted pair into GND . Connect Limit Switches to gShield Link Crimp the white ends of each limit switch wire pair. The order of the white wires from left to right is X, Y, Z. The first, sixth, and eighth slots are left EMPTY. Pin mapping: D9 : x-limit (red) D10 : y-limit (red) D12 : z-limit (red) GND : ground (all 3) Power the gShield Link Loosen the screws in the power terminal of the gShield and insert the red twisted pair into Vmot and the black twisted pair into GND . This is similar to the optional 24V fan. Note This will also power the Arduino.","title":"CNC electronics"},{"location":"plant_imager/build_v2/cnc_electronics/#wiring-of-the-cnc-electronics","text":"","title":"Wiring of the CNC electronics"},{"location":"plant_imager/build_v2/cnc_electronics/#x-controller-assembly","text":"If you ordered the X-Controller, follow the official assembly instructions here . Note This is what we used in our second and third version of the phenotyping station hardware. Warning We replaced the default Grbl firmware by Oquam , our own implementation. See here for the instructions on how to flash this firmware to the X-Controller.","title":"X-Controller assembly"},{"location":"plant_imager/build_v2/cnc_electronics/#gshield-assembly","text":"If you have the recent version, follow the official wiring instructions here . Here is link to the post 2015 version of the \"wiring\" instructions. Here is link to the post 2015 version of the \"electronic assembly\" instructions. Note This is what we used in our first version of the phenotyping station hardware.","title":"gShield assembly"},{"location":"plant_imager/build_v2/cnc_electronics/#bom","text":"If you are familiar with the Arduino world, the electronic is pretty straightforward: Arduino UNO (official buy here ) Synthetos gShield (buy here ) Power converter 220V ac. - 24V dc. (buy @ Farnell ) Emergency stop button (buy @ Farnell ) Optional: 24V fan 220V power cord Note Before 2017 X-Carve shipped 400W power units, now they use a 320W unit. The link is for a 350W unit.","title":"BOM"},{"location":"plant_imager/build_v2/cnc_electronics/#wiring-instructions","text":"","title":"Wiring instructions"},{"location":"plant_imager/build_v2/cnc_electronics/#wire-the-stepper-cable-to-the-gshield","text":"Once you\u2019ve determined which stepper cable belongs to which axis, you can wire them into the gShield. First loosen all the screws on the gShield (they will jump a thread when they are fully loose, but they won\u2019t come out of the terminal blocks.) The gShield is marked \"X,\" \"Y,\" and \"Z\". Wire the stepper cable according to the markings on the shield and order your wires (from left to right) black, green, white, red. Check out this diagram for clarification.","title":"Wire the Stepper Cable to the gShield"},{"location":"plant_imager/build_v2/cnc_electronics/#mount-the-gshield","text":"Now push the gShield onto the Arduino. There are pins on the gShield that go into the headers of the Arduino.","title":"Mount the gShield"},{"location":"plant_imager/build_v2/cnc_electronics/#24v-fan-mount-optional","text":"If you want, you can add a 24V fan to cool the shield. Uses the 24V pins on the gShield as shown in the picture below: Loosen the screws in the power terminal of the gShield and insert the red twisted pair into Vmot and the black twisted pair into GND .","title":"24V Fan mount (optional)"},{"location":"plant_imager/build_v2/cnc_electronics/#connect-limit-switches-to-gshield","text":"Crimp the white ends of each limit switch wire pair. The order of the white wires from left to right is X, Y, Z. The first, sixth, and eighth slots are left EMPTY. Pin mapping: D9 : x-limit (red) D10 : y-limit (red) D12 : z-limit (red) GND : ground (all 3)","title":"Connect Limit Switches to gShield"},{"location":"plant_imager/build_v2/cnc_electronics/#power-the-gshield","text":"Loosen the screws in the power terminal of the gShield and insert the red twisted pair into Vmot and the black twisted pair into GND . This is similar to the optional 24V fan. Note This will also power the Arduino.","title":"Power the gShield"},{"location":"plant_imager/build_v2/cnc_frame/","text":"Assembly instructions for the CNC frame Link We chose the X-Carve 1000 mm as basis for the CNC gantry, feel free to choose another one but beware to check the aluminium frame still fit! X-Carve 1000mm. BOM Link To build the CNC you will need: X-Carve 1000mmm (US buy here ; EU buy here ) Note We chose the X-Carve because initially you could customize your order and for example not order the work area (wood plate). Apparently it is not possible anymore! Instructions Link Follow the official build instructions for the X-Carve here . You should follow these steps: Gantry - Side Plates Gantry - X-Carriage Gantry - Z-Axis Assemble Gantry Rails Belting Note Do NOT follow the steps indicating how to: build the work area fix the spindle fix the gantry and rails over the work area (wood plate) Once you followed the official instructions, attach the CNC to the aluminium frame before continuing with the next section: wiring the CNC electronics!","title":"CNC frame"},{"location":"plant_imager/build_v2/cnc_frame/#assembly-instructions-for-the-cnc-frame","text":"We chose the X-Carve 1000 mm as basis for the CNC gantry, feel free to choose another one but beware to check the aluminium frame still fit! X-Carve 1000mm.","title":"Assembly instructions for the CNC frame"},{"location":"plant_imager/build_v2/cnc_frame/#bom","text":"To build the CNC you will need: X-Carve 1000mmm (US buy here ; EU buy here ) Note We chose the X-Carve because initially you could customize your order and for example not order the work area (wood plate). Apparently it is not possible anymore!","title":"BOM"},{"location":"plant_imager/build_v2/cnc_frame/#instructions","text":"Follow the official build instructions for the X-Carve here . You should follow these steps: Gantry - Side Plates Gantry - X-Carriage Gantry - Z-Axis Assemble Gantry Rails Belting Note Do NOT follow the steps indicating how to: build the work area fix the spindle fix the gantry and rails over the work area (wood plate) Once you followed the official instructions, attach the CNC to the aluminium frame before continuing with the next section: wiring the CNC electronics!","title":"Instructions"},{"location":"plant_imager/build_v2/gimbal_communication/","text":"Communication & control of the custom gimbal Link Connect to the Feather M0 Link First you need to find which USB port your Feather M0 is connected to. To do so, you can use dmesg : make sure the usb cable from the Feather M0 is unplugged run dmesg -w in a terminal connect the usb and see something like: [ 70481 .093408 ] cdc_acm 1 -2:1.0: ttyACM1: USB ACM device Important The important info here is ttyACM1 ! Then you can use picocom to connect to the Feather M0: picocom /dev/ttyACM1 -b 115200 Usage Link Send plain text-formatted commands for the position or speed commands. See the command list below: x : set velocity (unit: RPM) X : set target position (units: degrees) c : start the calibration of the encoder C : print results of the calibration v : print target velocity p : print PWM value Troubleshooting Link Serial access denied Link Look here if you can not communicate with the scanner using usb.","title":"Gimbal communication"},{"location":"plant_imager/build_v2/gimbal_communication/#communication-control-of-the-custom-gimbal","text":"","title":"Communication &amp; control of the custom gimbal"},{"location":"plant_imager/build_v2/gimbal_communication/#connect-to-the-feather-m0","text":"First you need to find which USB port your Feather M0 is connected to. To do so, you can use dmesg : make sure the usb cable from the Feather M0 is unplugged run dmesg -w in a terminal connect the usb and see something like: [ 70481 .093408 ] cdc_acm 1 -2:1.0: ttyACM1: USB ACM device Important The important info here is ttyACM1 ! Then you can use picocom to connect to the Feather M0: picocom /dev/ttyACM1 -b 115200","title":"Connect to the Feather M0"},{"location":"plant_imager/build_v2/gimbal_communication/#usage","text":"Send plain text-formatted commands for the position or speed commands. See the command list below: x : set velocity (unit: RPM) X : set target position (units: degrees) c : start the calibration of the encoder C : print results of the calibration v : print target velocity p : print PWM value","title":"Usage"},{"location":"plant_imager/build_v2/gimbal_communication/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"plant_imager/build_v2/gimbal_communication/#serial-access-denied","text":"Look here if you can not communicate with the scanner using usb.","title":"Serial access denied"},{"location":"plant_imager/build_v2/gimbal_setup/","text":"Assembly instructions for the custom gimbal Link BOM Link The bill of material is short: Adafruit Feather M0 Custom made ROMI gimbal controller hat Sources Link The sources of the custom-made gimbal controller are available here . The source code to compile & upload with Arduino IDE to the Feather M0 is here . The bracket for the picamera that also hold the M0 & the hat should be 3D printed. The design is available here . Resources Link An introduction to the Adafruit Feather M0 Basic is available on Adafruit's website here .","title":"Gimbal assembly"},{"location":"plant_imager/build_v2/gimbal_setup/#assembly-instructions-for-the-custom-gimbal","text":"","title":"Assembly instructions for the custom gimbal"},{"location":"plant_imager/build_v2/gimbal_setup/#bom","text":"The bill of material is short: Adafruit Feather M0 Custom made ROMI gimbal controller hat","title":"BOM"},{"location":"plant_imager/build_v2/gimbal_setup/#sources","text":"The sources of the custom-made gimbal controller are available here . The source code to compile & upload with Arduino IDE to the Feather M0 is here . The bracket for the picamera that also hold the M0 & the hat should be 3D printed. The design is available here .","title":"Sources"},{"location":"plant_imager/build_v2/gimbal_setup/#resources","text":"An introduction to the Adafruit Feather M0 Basic is available on Adafruit's website here .","title":"Resources"},{"location":"plant_imager/build_v2/picamera_setup/","text":"PiCamera setup Link Warning This is a work in progress! Getting started Link To set up the PiCamera you first need to install a fresh OS on your Raspberry Pi Zero W. Burn a new Raspberry Pi OS Lite image Link We recommend the Raspberry Pi Imager tool to burn a new Raspberry Pi OS (32-bit) Lite. You can find it here . Or you can download an official ZIP here . Accessing the PiZero Link A - Enable SSH remote access Link After installing the Raspberry Pi OS, add an empty file ssh in the boot partition to enable SSH access. Warning As we will later use the Wi-Fi from the PiZero to create an Access Point, we recommend to use the ethernet port to SSH in the PiZero. B - Use local access Link Otherwise, use a screen and keyboard to access the PiZero. Raspberry Pi OS setup Link Before installing the software, you have to configure some Raspberry Pi OS settings & change the password. Change the password Link For security reasons, you have to change the pi user password because the default is known by everyone! Use the raspi-config tool to do it: sudo raspi-config Warning The default password is raspberry ! Edit the timezone & locales Link Change the timezone to get the right time from the PiZero clock! You may also change the locales to suits your needs. Create the Wi-Fi Access Point Link We will now create the AP using command lines following the tutorial from: https://learn.sparkfun.com/tutorials/setting-up-a-raspberry-pi-3-as-an-access-point/all . You may also be interested in doing this with a graphical interface, and we would highly recommend raspap-webgui . Install Packages Link To install the required packages, enter the following into the console: sudo apt-get -y install hostapd dnsmasq Set Static IP Address Link Edit the dhcpcd.conf file: sudo nano /etc/dhcpcd.conf At the bottom of the file, add: denyinterfaces wlan0 Save and exit by pressing Ctrl + X and Y when asked. Next, we need to tell the Raspberry Pi to set a static IP address for the Wi-Fi interface. Open the interfaces file with the following command: sudo nano /etc/network/interfaces At the bottom of that file, add the following: auto lo iface lo inet loopback auto eth0 iface eth0 inet dhcp allow-hotplug wlan0 iface wlan0 inet static address 192.168.0.1 netmask 255.255.255.0 network 192.168.0.0 broadcast 192.168.0.255 Configure Hostapd Link We need to set up hostapd to tell it to broadcast a particular SSID and allow Wi-Fi connections on a certain channel. Edit the hostapd.conf file (this will create a new file, as one likely does not exist yet) with this command: sudo nano /etc/hostapd/hostapd.conf Enter the following into that file. Feel fee to change the ssid (Wi-Fi network name) and the wpa_passphrase (password to join the network) to whatever you'd like. You can also change the channel to something in the 1-11 range (if channel 6 is too crowded in your area). interface=wlan0 driver=nl80211 ssid=romi_hotspot hw_mode=g channel=6 ieee80211n=1 wmm_enabled=1 ht_capab=[HT40][SHORT-GI-20][DSSS_CCK-40] macaddr_acl=0 auth_algs=1 ignore_broadcast_ssid=0 wpa=2 wpa_key_mgmt=WPA-PSK wpa_passphrase=raspberry rsn_pairwise=CCMP Save and exit by pressing Ctrl + X and Y when asked. Unfortunately, hostapd does not know where to find this configuration file, so we need to provide its location to the hostapd startup script. Open /etc/default/hostapd : sudo nano /etc/default/hostapd Find the line #DAEMON_CONF=\"\" and replace it with: DAEMON_CONF=\"/etc/hostapd/hostapd.conf\" Save and exit by pressing Ctrl + X and Y when asked. Configure Dnsmasq Link Dnsmasq will help us automatically assign IP addresses as new devices connect to our network as well as work as a translation between network names and IP addresses. The .conf file that comes with Dnsmasq has a lot of good information in it, so it might be worthwhile to save it (as a backup) rather than delete it. After saving it, open a new one for editing: sudo mv /etc/dnsmasq.conf /etc/dnsmasq.conf.bak sudo nano /etc/dnsmasq.conf In the blank file, paste in the text below. interface=wlan0 listen-address=192.168.0.1 bind-interfaces server=8.8.8.8 domain-needed bogus-priv dhcp-range=192.168.0.100,192.168.0.200,24h Test Wi-Fi connection Link Restart the Raspberry Pi using the following command: sudo reboot After your Pi restarts (no need to log in), you should see romi_hotspot appear as a potential wireless network from your computer. Connect to it (the network password is raspberry, unless you changed it in the hostapd.conf file). Then try to SSH it with: ssh pi@192.138.0.1 Install Python3 & pip Link You will need the Python3 interpreter (Python>=3.6) to run the PiCamera server & pip to install the PiCamera package. sudo apt update && sudo apt upgrade -y sudo apt install python3 python3-pip This will update the package manager, upgrade the system libraries & install Python3 & pip. Install the PiCamera package Link Once you have pip you can install the picamera Python3 package: pip3 install picamera Note We do not create an isolated environment in this case since the sole purpose of the PiZero will be to act as a responsive image server. Camera serve Python code Link To capture and serve the images from the PiCamera, we use this Python script: To upload it to your PiZero, from a terminal: wget https://gist.github.com/jlegrand62/c24e454922f0cf203d6f9ed49f95ecc1/raw/c4cda40ff56984188dca928693852f3f7a317fa4/picamera_server.py Now (test) start the server with: python3 picamera_server.py Todo Explain how to execute python3 picamera_server.py command at PiZero boot.","title":"PiCamera setup"},{"location":"plant_imager/build_v2/picamera_setup/#picamera-setup","text":"Warning This is a work in progress!","title":"PiCamera setup"},{"location":"plant_imager/build_v2/picamera_setup/#getting-started","text":"To set up the PiCamera you first need to install a fresh OS on your Raspberry Pi Zero W.","title":"Getting started"},{"location":"plant_imager/build_v2/picamera_setup/#burn-a-new-raspberry-pi-os-lite-image","text":"We recommend the Raspberry Pi Imager tool to burn a new Raspberry Pi OS (32-bit) Lite. You can find it here . Or you can download an official ZIP here .","title":"Burn a new Raspberry Pi OS Lite image"},{"location":"plant_imager/build_v2/picamera_setup/#accessing-the-pizero","text":"","title":"Accessing the PiZero"},{"location":"plant_imager/build_v2/picamera_setup/#a-enable-ssh-remote-access","text":"After installing the Raspberry Pi OS, add an empty file ssh in the boot partition to enable SSH access. Warning As we will later use the Wi-Fi from the PiZero to create an Access Point, we recommend to use the ethernet port to SSH in the PiZero.","title":"A - Enable SSH remote access"},{"location":"plant_imager/build_v2/picamera_setup/#b-use-local-access","text":"Otherwise, use a screen and keyboard to access the PiZero.","title":"B - Use local access"},{"location":"plant_imager/build_v2/picamera_setup/#raspberry-pi-os-setup","text":"Before installing the software, you have to configure some Raspberry Pi OS settings & change the password.","title":"Raspberry Pi OS setup"},{"location":"plant_imager/build_v2/picamera_setup/#change-the-password","text":"For security reasons, you have to change the pi user password because the default is known by everyone! Use the raspi-config tool to do it: sudo raspi-config Warning The default password is raspberry !","title":"Change the password"},{"location":"plant_imager/build_v2/picamera_setup/#edit-the-timezone-locales","text":"Change the timezone to get the right time from the PiZero clock! You may also change the locales to suits your needs.","title":"Edit the timezone &amp; locales"},{"location":"plant_imager/build_v2/picamera_setup/#create-the-wi-fi-access-point","text":"We will now create the AP using command lines following the tutorial from: https://learn.sparkfun.com/tutorials/setting-up-a-raspberry-pi-3-as-an-access-point/all . You may also be interested in doing this with a graphical interface, and we would highly recommend raspap-webgui .","title":"Create the Wi-Fi Access Point"},{"location":"plant_imager/build_v2/picamera_setup/#install-packages","text":"To install the required packages, enter the following into the console: sudo apt-get -y install hostapd dnsmasq","title":"Install Packages"},{"location":"plant_imager/build_v2/picamera_setup/#set-static-ip-address","text":"Edit the dhcpcd.conf file: sudo nano /etc/dhcpcd.conf At the bottom of the file, add: denyinterfaces wlan0 Save and exit by pressing Ctrl + X and Y when asked. Next, we need to tell the Raspberry Pi to set a static IP address for the Wi-Fi interface. Open the interfaces file with the following command: sudo nano /etc/network/interfaces At the bottom of that file, add the following: auto lo iface lo inet loopback auto eth0 iface eth0 inet dhcp allow-hotplug wlan0 iface wlan0 inet static address 192.168.0.1 netmask 255.255.255.0 network 192.168.0.0 broadcast 192.168.0.255","title":"Set Static IP Address"},{"location":"plant_imager/build_v2/picamera_setup/#configure-hostapd","text":"We need to set up hostapd to tell it to broadcast a particular SSID and allow Wi-Fi connections on a certain channel. Edit the hostapd.conf file (this will create a new file, as one likely does not exist yet) with this command: sudo nano /etc/hostapd/hostapd.conf Enter the following into that file. Feel fee to change the ssid (Wi-Fi network name) and the wpa_passphrase (password to join the network) to whatever you'd like. You can also change the channel to something in the 1-11 range (if channel 6 is too crowded in your area). interface=wlan0 driver=nl80211 ssid=romi_hotspot hw_mode=g channel=6 ieee80211n=1 wmm_enabled=1 ht_capab=[HT40][SHORT-GI-20][DSSS_CCK-40] macaddr_acl=0 auth_algs=1 ignore_broadcast_ssid=0 wpa=2 wpa_key_mgmt=WPA-PSK wpa_passphrase=raspberry rsn_pairwise=CCMP Save and exit by pressing Ctrl + X and Y when asked. Unfortunately, hostapd does not know where to find this configuration file, so we need to provide its location to the hostapd startup script. Open /etc/default/hostapd : sudo nano /etc/default/hostapd Find the line #DAEMON_CONF=\"\" and replace it with: DAEMON_CONF=\"/etc/hostapd/hostapd.conf\" Save and exit by pressing Ctrl + X and Y when asked.","title":"Configure Hostapd"},{"location":"plant_imager/build_v2/picamera_setup/#configure-dnsmasq","text":"Dnsmasq will help us automatically assign IP addresses as new devices connect to our network as well as work as a translation between network names and IP addresses. The .conf file that comes with Dnsmasq has a lot of good information in it, so it might be worthwhile to save it (as a backup) rather than delete it. After saving it, open a new one for editing: sudo mv /etc/dnsmasq.conf /etc/dnsmasq.conf.bak sudo nano /etc/dnsmasq.conf In the blank file, paste in the text below. interface=wlan0 listen-address=192.168.0.1 bind-interfaces server=8.8.8.8 domain-needed bogus-priv dhcp-range=192.168.0.100,192.168.0.200,24h","title":"Configure Dnsmasq"},{"location":"plant_imager/build_v2/picamera_setup/#test-wi-fi-connection","text":"Restart the Raspberry Pi using the following command: sudo reboot After your Pi restarts (no need to log in), you should see romi_hotspot appear as a potential wireless network from your computer. Connect to it (the network password is raspberry, unless you changed it in the hostapd.conf file). Then try to SSH it with: ssh pi@192.138.0.1","title":"Test Wi-Fi connection"},{"location":"plant_imager/build_v2/picamera_setup/#install-python3-pip","text":"You will need the Python3 interpreter (Python>=3.6) to run the PiCamera server & pip to install the PiCamera package. sudo apt update && sudo apt upgrade -y sudo apt install python3 python3-pip This will update the package manager, upgrade the system libraries & install Python3 & pip.","title":"Install Python3 &amp; pip"},{"location":"plant_imager/build_v2/picamera_setup/#install-the-picamera-package","text":"Once you have pip you can install the picamera Python3 package: pip3 install picamera Note We do not create an isolated environment in this case since the sole purpose of the PiZero will be to act as a responsive image server.","title":"Install the PiCamera package"},{"location":"plant_imager/build_v2/picamera_setup/#camera-serve-python-code","text":"To capture and serve the images from the PiCamera, we use this Python script: To upload it to your PiZero, from a terminal: wget https://gist.github.com/jlegrand62/c24e454922f0cf203d6f9ed49f95ecc1/raw/c4cda40ff56984188dca928693852f3f7a317fa4/picamera_server.py Now (test) start the server with: python3 picamera_server.py Todo Explain how to execute python3 picamera_server.py command at PiZero boot.","title":"Camera serve Python code"},{"location":"plant_imager/build_v2/troubleshooting/","text":"Troubleshooting Link Serial access denied Link If you get an error about permission access: Check in what group you are with: groups ${ USER } If you are not in dialout : sudo gpasswd --add ${ USER } dialout Then log out and back in to see changes!","title":"Troubleshooting"},{"location":"plant_imager/build_v2/troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"plant_imager/build_v2/troubleshooting/#serial-access-denied","text":"If you get an error about permission access: Check in what group you are with: groups ${ USER } If you are not in dialout : sudo gpasswd --add ${ USER } dialout Then log out and back in to see changes!","title":"Serial access denied"},{"location":"plant_imager/build_v3/","text":"Assembly instructions for the third version of the plant imager Link This is the documentation for the third iteration of the plant imager hardware. It aims at improving the overall quality of the build and to fix some limitations of the second iteration. Rationale Link Notable limitations of the second version of the plant imager hardware: unused z-axis motor in absence of a tilt motor camera powered by batteries (limited to RX-0) unstable control of the gimbal (due to usb cable length) wobbling camera arm and enclosure poor surrounding isolation, even when wrapped with opaque fabric no indication of camera tilt Change list in this third version: conversion of the z-axis to pan-axis control manual z-axis control thanks to vertical ruler on arm and arrowhead on manual gimbal mount manual tilt control thanks to a 360\u00b0 protractor and arrowhead on manual gimbal mount possibility to add a second picamera oquam firmware instead of Grbl complete wooden enclosure of the plant imager Assembly overview Link You will have to achieve the following steps: assemble the aluminium frame: instructions assemble the X-Carve CNC, except Z-axis motor & arm: instructions attach the CNC to the aluminium frame: instructions add the wooden enclosure & the LED bars inside: instructions assemble the X-Carve controller and attach it to the enclosure (left wooden panel at the same height as the X-Carve) flash oquam firmware in place of Grbl on the X-Carve controller: instructions wire the X-Carve CNC motors & endstops to the controller (except z-axis motor and endstop) assemble the arm: instructions attach the arm to the CNC X-Carriage: instructions assemble the manual gimbal: instructions assemble the picamera: instructions attach the picamera to the gimbal then to the pan arm wire the pan encoder as z-endstop: instructions Open Hardware Link We use \"widely available\" materials and provides all the required information, so you can reproduce and modify our work. If you want to modify it, please note the following important points: The width and depth of the aluminium frame is dependent of the CNC frame size, make sure the CNC frame fits inside the aluminium frame! The height of the aluminium frame will determine the maximum observable plant height. Wiring & communication overview Link Todo Update this section!","title":"Home"},{"location":"plant_imager/build_v3/#assembly-instructions-for-the-third-version-of-the-plant-imager","text":"This is the documentation for the third iteration of the plant imager hardware. It aims at improving the overall quality of the build and to fix some limitations of the second iteration.","title":"Assembly instructions for the third version of the plant imager"},{"location":"plant_imager/build_v3/#rationale","text":"Notable limitations of the second version of the plant imager hardware: unused z-axis motor in absence of a tilt motor camera powered by batteries (limited to RX-0) unstable control of the gimbal (due to usb cable length) wobbling camera arm and enclosure poor surrounding isolation, even when wrapped with opaque fabric no indication of camera tilt Change list in this third version: conversion of the z-axis to pan-axis control manual z-axis control thanks to vertical ruler on arm and arrowhead on manual gimbal mount manual tilt control thanks to a 360\u00b0 protractor and arrowhead on manual gimbal mount possibility to add a second picamera oquam firmware instead of Grbl complete wooden enclosure of the plant imager","title":"Rationale"},{"location":"plant_imager/build_v3/#assembly-overview","text":"You will have to achieve the following steps: assemble the aluminium frame: instructions assemble the X-Carve CNC, except Z-axis motor & arm: instructions attach the CNC to the aluminium frame: instructions add the wooden enclosure & the LED bars inside: instructions assemble the X-Carve controller and attach it to the enclosure (left wooden panel at the same height as the X-Carve) flash oquam firmware in place of Grbl on the X-Carve controller: instructions wire the X-Carve CNC motors & endstops to the controller (except z-axis motor and endstop) assemble the arm: instructions attach the arm to the CNC X-Carriage: instructions assemble the manual gimbal: instructions assemble the picamera: instructions attach the picamera to the gimbal then to the pan arm wire the pan encoder as z-endstop: instructions","title":"Assembly overview"},{"location":"plant_imager/build_v3/#open-hardware","text":"We use \"widely available\" materials and provides all the required information, so you can reproduce and modify our work. If you want to modify it, please note the following important points: The width and depth of the aluminium frame is dependent of the CNC frame size, make sure the CNC frame fits inside the aluminium frame! The height of the aluminium frame will determine the maximum observable plant height.","title":"Open Hardware"},{"location":"plant_imager/build_v3/#wiring-communication-overview","text":"Todo Update this section!","title":"Wiring &amp; communication overview"},{"location":"plant_imager/build_v3/alu_frame/","text":"Aluminium frame assembly Link Important All units are in millimeters! BOM Link ID Name Qty Off the shelf / Custom Material Manufactureur Serial number Link 11 Profil 40x40x2000 4 Off the shelf Bosch Rexroth 3842993120 rs-online.com 12 Profil 40x40x1200 5 Off the shelf 13 Profil 40x40x1006 6 Off the shelf 14 Profil 40x40x160 4 Off the shelf 15 Profil 40x40x100 4 Off the shelf 22 HINGED_FOOT 4 Off the shelf Bosch Rexroth 3842352061 rs-online.com 23 DAMPING_RING_D44 4 Off the shelf Bosch Rexroth 3842521817 rs-online.com 25 brackets_ce-sb3680-8 30 Off the shelf Bosch Rexroth 3 842 529 005 faure-technologies.com 26 Ecrou de but\u00e9e pour rainure de 10 mm - Taraudage M8 220 Off the shelf Bosch Rexroth 3 842 345 081 faure-technologies.com 27 Vis \u00e0 t\u00eate rectangulaire pour rainure de 10 mm - M8x25 220 Off the shelf Bosch Rexroth 3 842 528 718 faure-technologies.com Assembly Link Assemble the frame to obtains the following structure: Note The 4 wall mountings, made of parts 15 & 25, are optional.","title":"Aluminium frame"},{"location":"plant_imager/build_v3/alu_frame/#aluminium-frame-assembly","text":"Important All units are in millimeters!","title":"Aluminium frame assembly"},{"location":"plant_imager/build_v3/alu_frame/#bom","text":"ID Name Qty Off the shelf / Custom Material Manufactureur Serial number Link 11 Profil 40x40x2000 4 Off the shelf Bosch Rexroth 3842993120 rs-online.com 12 Profil 40x40x1200 5 Off the shelf 13 Profil 40x40x1006 6 Off the shelf 14 Profil 40x40x160 4 Off the shelf 15 Profil 40x40x100 4 Off the shelf 22 HINGED_FOOT 4 Off the shelf Bosch Rexroth 3842352061 rs-online.com 23 DAMPING_RING_D44 4 Off the shelf Bosch Rexroth 3842521817 rs-online.com 25 brackets_ce-sb3680-8 30 Off the shelf Bosch Rexroth 3 842 529 005 faure-technologies.com 26 Ecrou de but\u00e9e pour rainure de 10 mm - Taraudage M8 220 Off the shelf Bosch Rexroth 3 842 345 081 faure-technologies.com 27 Vis \u00e0 t\u00eate rectangulaire pour rainure de 10 mm - M8x25 220 Off the shelf Bosch Rexroth 3 842 528 718 faure-technologies.com","title":"BOM"},{"location":"plant_imager/build_v3/alu_frame/#assembly","text":"Assemble the frame to obtains the following structure: Note The 4 wall mountings, made of parts 15 & 25, are optional.","title":"Assembly"},{"location":"plant_imager/build_v3/boms/","text":"Bills of materials Link Frame & Enclosure Link To assemble the aluminium frame & enclosure you will need the following materials: ID Name Qty Off the shelf / Custom Material Manufactureur Serial number Link 1 Planche 1 1 Custom Wood pdf 2 Planche 2 1 Custom Wood pdf 3 Planche 3 1 Custom Wood pdf 4 Planche 4 1 Custom Wood pdf 5 Planche 5 1 Custom Wood pdf 6 Magnet bracket 2 Custom PLA stl 7 LED Strip U Channel 4 Off the shelf amazon.fr 8 LED Ribbon 1 Off the shelf amazon.fr 9 LED Dimmer 1 Off the shelf amazon.fr 10 Convertisseur 24-12V 1 Off the shelf amazon.fr 11 Profil 40x40x2000 4 Off the shelf Bosch Rexroth 3842993120 rs-online.com 12 Profil 40x40x1200 5 Off the shelf 13 Profil 40x40x1006 6 Off the shelf 14 Profil 40x40x160 4 Off the shelf 15 Profil 40x40x100 4 Off the shelf 16 Profil 30x30x2000 2 Off the shelf Bosch Rexroth 3842990720/2000 rs-online.com 17 Profil 30x30x1210 3 Off the shelf 18 STEEL_BASE 4 Off the shelf Bosch Rexroth 3 842 542 667 faure-technologies.com 19 HINGE 3 Off the shelf Bosch Rexroth 3 842 543 331 faure-technologies.com 20 MAGNET_CATCH 2 Off the shelf Bosch Rexroth 3 842 516 165 faure-technologies.com 21 STRAP_SHAPED_HANDLE 1 Off the shelf Bosch Rexroth 3 842 525 766 faure-technologies.com 22 HINGED_FOOT 4 Off the shelf Bosch Rexroth 3842352061 rs-online.com 23 DAMPING_RING_D44 4 Off the shelf Bosch Rexroth 3842521817 rs-online.com 24 BRACKET_30X60 8 Off the shelf Bosch Rexroth 3842523541 faure-technologies.com 25 brackets_ce-sb3680-8 30 Off the shelf Bosch Rexroth 3 842 529 005 faure-technologies.com 26 Ecrou de but\u00e9e pour rainure de 10 mm - Taraudage M8 220 Off the shelf Bosch Rexroth 3 842 345 081 faure-technologies.com 27 Vis \u00e0 t\u00eate rectangulaire pour rainure de 10 mm - M8x25 220 Off the shelf Bosch Rexroth 3 842 528 718 faure-technologies.com 28 Vis \u00e0 t\u00eate rectangulaire pour rainure de 8 mm - M6x20 20 Off the shelf Bosch Rexroth 3 842 523 921 faure-technologies.com 29 Ecrou de but\u00e9e pour rainure de 8 mm - Taraudage M6 20 Off the shelf Bosch Rexroth 3 842 523 925 faure-technologies.com 30 M12x20 Hexagonal Head 4 Off the shelf RS PRO 192-5513 rs-online.com You can find the PDF with the BOM and assembly instructions here . Pan arm Link To assemble the pan arm you will need the following materials: ID Name Qty Off the shelf / Custom Material Manufacturer Serial number Link 1 PLI004 1 Custom Aluminium pdf 2 PLI006 1 Custom Steel Ewellix Makers in Motion LJM12X600ESSC2 pdf , rs-online.com 3 PLI007-3 1 Custom Aluminium pdf 4 PLI009 1 Custom Aluminium pdf 5 PLI011 1 Custom Aluminium pdf 6 PLI012 1 Custom Aluminium pdf 7 PLI016 1 Custom Aluminium pdf 8 Cable Zip 3 Custom PLA stl 9 Dozenal_ruler_30cm 1 Custom PMMA RS PRO 434-295 dxf , rs-online.com 10 PROFILE_D_ETAYAGE_20x40x300 1 Off the shelf Bosch Rexroth R987501076 rs-online.com 11 Slip Ring ROB-13063 1 Off the shelf SparkFun ROB-13063 mouser.fr 12 Nema 23 - 45mm 1 Off the shelf Stepperonline 23HS18-2004H omc-stepperonline.com 13 Convertisseur 24-5V 1 Off the shelf amazon.com 14 Huco 047_20 1 Off the shelf Huco 047.20.3535 rs-online.com 15 CUI_DEVICES_AMT132Q-0048-1200 1 Off the shelf CUI Devices AMT132Q-V eu.mouser.com 16 AMT Cable 1 Off the shelf CUI Devices AMT-18C-3-036 eu.mouser.com 17 AMT Viewpoint Programming Cable 1 Off the shelf CUI Devices AMT-PGRM-18C eu.mouser.com 18 SKF_P 12 TF 2 Off the shelf SKF P 12 TF rs-online.com 19 MQCL-12-A 1 Off the shelf Ruland MQCL-12-A rs-online.com 20 M8x16 Socket Head 4 Off the shelf RS PRO 529-703 rs-online.com 21 M5x10 Shoulder Screw 2 Off the shelf RS PRO 292-271 rs-online.com 22 M3x12 Set Screw 1 Off the shelf RS PRO 137-736 rs-online.com 23 M3x20 Socket Head 2 Off the shelf RS PRO 293-319 rs-online.com 24 M4x12 Socket Head 2 Off the shelf RS PRO 281-035 rs-online.com 25 M5x18 Socket Head 4 Off the shelf Vis express 8420501818 vis-express.fr 26 M5x16 Flat Head 4 Off the shelf RS PRO 232-8423 rs-online.com 27 M6x40 Flat Head 2 Off the shelf RS PRO 917-6238 rs-online.com 28 M5x8 Socket Head 2 Off the shelf RS PRO 660-4633 rs-online.com 29 M5x8 Button Head 3 Off the shelf RS PRO 822-9114 rs-online.com 30 Bosch nut 5 Off the shelf Bosch Rexroth 3842523142 rs-online.com 31 M3 Locknut 2 Off the shelf RS PRO 524-281 rs-online.com 32 M4 Locknut 2 Off the shelf RS PRO 524-304 rs-online.com 33 M5 Locknut 10 Off the shelf RS PRO 122-4371 rs-online.com You can find the PDF with the BOM and assembly instructions here . Manual gimbal Link To assemble the manual gimbal you will need the following materials: ID Name Qty Off the shelf / Custom Material Manufacturer Serial number Link 1 MG004 1 Custom Aluminium pdf 2 MG009 1 Custom Aluminium pdf 3 MG010 1 Custom Aluminium pdf 4 MG012 1 Custom Aluminium pdf 5 MG011 1 Custom PMMA RS PRO 434-295 pdf , dxf , rs-online.com 6 MG014 1 Custom PMMA dxf 7 Mounting Plate Rpi Zero Hq Camera 1 Custom PMMA dxf 8 Protractor dial deg 1 Custom PMMA dxf 9 Spacer M5x8 2 Off the shelf Vis express 8470100818 vis-express.fr 10 Spacer M5x20 1 Off the shelf Vis express 8470102018 vis-express.fr 11 Bouton de serrage M5 3 Off the shelf RS PRO 771-673 rs-online.com 12 M5x40 Socket Head 1 Off the shelf RS PRO 293-353 rs-online.com 13 M5x30 Socket Head 2 Off the shelf RS PRO 290-130 rs-online.com 14 M5x12 Socket Head 2 Off the Shelf RS PRO 281-079 rs-online.com 15 M4x16 Flat Head 2 Off the Shelf RS PRO 171-837 rs-online.com 16 M4x12 Socket Head 1 Off the shelf RS PRO 281-035 rs-online.com 17 M3x8 Socket Head 2 Off the Shelf RS PRO 280-997 rs-online.com 18 Bosch nut 2 Off the Shelf Bosch Rexroth 3842523142 rs-online.com 19 Washer M4 L 1 Off the Shelf Vis express 3534000402 vis-express.fr 20 IGUS GTM-0818-020 2 Off the Shelf IGUS GTM-0818-020 amazon.fr 21 RaspberryPiZeroW 1 Off the Shelf kubii.fr 22 Raspberry Pi HQ Camera 1 Off the Shelf kubii.fr 23 6mm CS-mount lens 1 Off the Shelf kubii.fr 24 Ribbon cable 1 Off the Shelf kubii.fr 25 5mm M2.5 nylon screws 8 Off the Shell thepihut.com 26 M2.5 nylon nuts 8 Off the Self You can find the PDF with the BOM and assembly instructions here . X-Carve CNC Link To assemble the X-Carve CNC, you will need to buy one (1000mm): US buy here EU buy here PiCamera Link To assemble a SINGLE PiCamera HQ, you will need the following list of materials: ID Name Qty Off the shelf / Custom Material Manufacturer Serial number Link 7 Mounting Plate Rpi Zero Hq Camera 1 Custom PMMA dxf 21 RaspberryPiZeroW 1 Off the Shelf kubii.fr 22 Raspberry Pi HQ Camera 1 Off the Shelf kubii.fr 23 6mm CS-mount lens 1 Off the Shelf kubii.fr 24 Ribbon cable 1 Off the Shelf kubii.fr 25 5mm M2.5 nylon screws 8 Off the Shell thepihut.com Note This is contained in the \"Manual gimbal\" BOM. Plant table Link To put our plant at camera height, we use a simple adjustable piano stand from RockJam, findable on Amazon","title":"Complete BOMs"},{"location":"plant_imager/build_v3/boms/#bills-of-materials","text":"","title":"Bills of materials"},{"location":"plant_imager/build_v3/boms/#frame-enclosure","text":"To assemble the aluminium frame & enclosure you will need the following materials: ID Name Qty Off the shelf / Custom Material Manufactureur Serial number Link 1 Planche 1 1 Custom Wood pdf 2 Planche 2 1 Custom Wood pdf 3 Planche 3 1 Custom Wood pdf 4 Planche 4 1 Custom Wood pdf 5 Planche 5 1 Custom Wood pdf 6 Magnet bracket 2 Custom PLA stl 7 LED Strip U Channel 4 Off the shelf amazon.fr 8 LED Ribbon 1 Off the shelf amazon.fr 9 LED Dimmer 1 Off the shelf amazon.fr 10 Convertisseur 24-12V 1 Off the shelf amazon.fr 11 Profil 40x40x2000 4 Off the shelf Bosch Rexroth 3842993120 rs-online.com 12 Profil 40x40x1200 5 Off the shelf 13 Profil 40x40x1006 6 Off the shelf 14 Profil 40x40x160 4 Off the shelf 15 Profil 40x40x100 4 Off the shelf 16 Profil 30x30x2000 2 Off the shelf Bosch Rexroth 3842990720/2000 rs-online.com 17 Profil 30x30x1210 3 Off the shelf 18 STEEL_BASE 4 Off the shelf Bosch Rexroth 3 842 542 667 faure-technologies.com 19 HINGE 3 Off the shelf Bosch Rexroth 3 842 543 331 faure-technologies.com 20 MAGNET_CATCH 2 Off the shelf Bosch Rexroth 3 842 516 165 faure-technologies.com 21 STRAP_SHAPED_HANDLE 1 Off the shelf Bosch Rexroth 3 842 525 766 faure-technologies.com 22 HINGED_FOOT 4 Off the shelf Bosch Rexroth 3842352061 rs-online.com 23 DAMPING_RING_D44 4 Off the shelf Bosch Rexroth 3842521817 rs-online.com 24 BRACKET_30X60 8 Off the shelf Bosch Rexroth 3842523541 faure-technologies.com 25 brackets_ce-sb3680-8 30 Off the shelf Bosch Rexroth 3 842 529 005 faure-technologies.com 26 Ecrou de but\u00e9e pour rainure de 10 mm - Taraudage M8 220 Off the shelf Bosch Rexroth 3 842 345 081 faure-technologies.com 27 Vis \u00e0 t\u00eate rectangulaire pour rainure de 10 mm - M8x25 220 Off the shelf Bosch Rexroth 3 842 528 718 faure-technologies.com 28 Vis \u00e0 t\u00eate rectangulaire pour rainure de 8 mm - M6x20 20 Off the shelf Bosch Rexroth 3 842 523 921 faure-technologies.com 29 Ecrou de but\u00e9e pour rainure de 8 mm - Taraudage M6 20 Off the shelf Bosch Rexroth 3 842 523 925 faure-technologies.com 30 M12x20 Hexagonal Head 4 Off the shelf RS PRO 192-5513 rs-online.com You can find the PDF with the BOM and assembly instructions here .","title":"Frame &amp; Enclosure"},{"location":"plant_imager/build_v3/boms/#pan-arm","text":"To assemble the pan arm you will need the following materials: ID Name Qty Off the shelf / Custom Material Manufacturer Serial number Link 1 PLI004 1 Custom Aluminium pdf 2 PLI006 1 Custom Steel Ewellix Makers in Motion LJM12X600ESSC2 pdf , rs-online.com 3 PLI007-3 1 Custom Aluminium pdf 4 PLI009 1 Custom Aluminium pdf 5 PLI011 1 Custom Aluminium pdf 6 PLI012 1 Custom Aluminium pdf 7 PLI016 1 Custom Aluminium pdf 8 Cable Zip 3 Custom PLA stl 9 Dozenal_ruler_30cm 1 Custom PMMA RS PRO 434-295 dxf , rs-online.com 10 PROFILE_D_ETAYAGE_20x40x300 1 Off the shelf Bosch Rexroth R987501076 rs-online.com 11 Slip Ring ROB-13063 1 Off the shelf SparkFun ROB-13063 mouser.fr 12 Nema 23 - 45mm 1 Off the shelf Stepperonline 23HS18-2004H omc-stepperonline.com 13 Convertisseur 24-5V 1 Off the shelf amazon.com 14 Huco 047_20 1 Off the shelf Huco 047.20.3535 rs-online.com 15 CUI_DEVICES_AMT132Q-0048-1200 1 Off the shelf CUI Devices AMT132Q-V eu.mouser.com 16 AMT Cable 1 Off the shelf CUI Devices AMT-18C-3-036 eu.mouser.com 17 AMT Viewpoint Programming Cable 1 Off the shelf CUI Devices AMT-PGRM-18C eu.mouser.com 18 SKF_P 12 TF 2 Off the shelf SKF P 12 TF rs-online.com 19 MQCL-12-A 1 Off the shelf Ruland MQCL-12-A rs-online.com 20 M8x16 Socket Head 4 Off the shelf RS PRO 529-703 rs-online.com 21 M5x10 Shoulder Screw 2 Off the shelf RS PRO 292-271 rs-online.com 22 M3x12 Set Screw 1 Off the shelf RS PRO 137-736 rs-online.com 23 M3x20 Socket Head 2 Off the shelf RS PRO 293-319 rs-online.com 24 M4x12 Socket Head 2 Off the shelf RS PRO 281-035 rs-online.com 25 M5x18 Socket Head 4 Off the shelf Vis express 8420501818 vis-express.fr 26 M5x16 Flat Head 4 Off the shelf RS PRO 232-8423 rs-online.com 27 M6x40 Flat Head 2 Off the shelf RS PRO 917-6238 rs-online.com 28 M5x8 Socket Head 2 Off the shelf RS PRO 660-4633 rs-online.com 29 M5x8 Button Head 3 Off the shelf RS PRO 822-9114 rs-online.com 30 Bosch nut 5 Off the shelf Bosch Rexroth 3842523142 rs-online.com 31 M3 Locknut 2 Off the shelf RS PRO 524-281 rs-online.com 32 M4 Locknut 2 Off the shelf RS PRO 524-304 rs-online.com 33 M5 Locknut 10 Off the shelf RS PRO 122-4371 rs-online.com You can find the PDF with the BOM and assembly instructions here .","title":"Pan arm"},{"location":"plant_imager/build_v3/boms/#manual-gimbal","text":"To assemble the manual gimbal you will need the following materials: ID Name Qty Off the shelf / Custom Material Manufacturer Serial number Link 1 MG004 1 Custom Aluminium pdf 2 MG009 1 Custom Aluminium pdf 3 MG010 1 Custom Aluminium pdf 4 MG012 1 Custom Aluminium pdf 5 MG011 1 Custom PMMA RS PRO 434-295 pdf , dxf , rs-online.com 6 MG014 1 Custom PMMA dxf 7 Mounting Plate Rpi Zero Hq Camera 1 Custom PMMA dxf 8 Protractor dial deg 1 Custom PMMA dxf 9 Spacer M5x8 2 Off the shelf Vis express 8470100818 vis-express.fr 10 Spacer M5x20 1 Off the shelf Vis express 8470102018 vis-express.fr 11 Bouton de serrage M5 3 Off the shelf RS PRO 771-673 rs-online.com 12 M5x40 Socket Head 1 Off the shelf RS PRO 293-353 rs-online.com 13 M5x30 Socket Head 2 Off the shelf RS PRO 290-130 rs-online.com 14 M5x12 Socket Head 2 Off the Shelf RS PRO 281-079 rs-online.com 15 M4x16 Flat Head 2 Off the Shelf RS PRO 171-837 rs-online.com 16 M4x12 Socket Head 1 Off the shelf RS PRO 281-035 rs-online.com 17 M3x8 Socket Head 2 Off the Shelf RS PRO 280-997 rs-online.com 18 Bosch nut 2 Off the Shelf Bosch Rexroth 3842523142 rs-online.com 19 Washer M4 L 1 Off the Shelf Vis express 3534000402 vis-express.fr 20 IGUS GTM-0818-020 2 Off the Shelf IGUS GTM-0818-020 amazon.fr 21 RaspberryPiZeroW 1 Off the Shelf kubii.fr 22 Raspberry Pi HQ Camera 1 Off the Shelf kubii.fr 23 6mm CS-mount lens 1 Off the Shelf kubii.fr 24 Ribbon cable 1 Off the Shelf kubii.fr 25 5mm M2.5 nylon screws 8 Off the Shell thepihut.com 26 M2.5 nylon nuts 8 Off the Self You can find the PDF with the BOM and assembly instructions here .","title":"Manual gimbal"},{"location":"plant_imager/build_v3/boms/#x-carve-cnc","text":"To assemble the X-Carve CNC, you will need to buy one (1000mm): US buy here EU buy here","title":"X-Carve CNC"},{"location":"plant_imager/build_v3/boms/#picamera","text":"To assemble a SINGLE PiCamera HQ, you will need the following list of materials: ID Name Qty Off the shelf / Custom Material Manufacturer Serial number Link 7 Mounting Plate Rpi Zero Hq Camera 1 Custom PMMA dxf 21 RaspberryPiZeroW 1 Off the Shelf kubii.fr 22 Raspberry Pi HQ Camera 1 Off the Shelf kubii.fr 23 6mm CS-mount lens 1 Off the Shelf kubii.fr 24 Ribbon cable 1 Off the Shelf kubii.fr 25 5mm M2.5 nylon screws 8 Off the Shell thepihut.com Note This is contained in the \"Manual gimbal\" BOM.","title":"PiCamera"},{"location":"plant_imager/build_v3/boms/#plant-table","text":"To put our plant at camera height, we use a simple adjustable piano stand from RockJam, findable on Amazon","title":"Plant table"},{"location":"plant_imager/build_v3/cnc_arm/","text":"Mounting the pan arm Link BOM Link 4x M5x16 flat head screw 4x M5 nuts. Assembly Link Mount the pan arm to the X carriage of the X-carve CNC as follows: Note Use the holes ot the x-carriage & arm mounting plate.","title":"CNC & Pan Arm"},{"location":"plant_imager/build_v3/cnc_arm/#mounting-the-pan-arm","text":"","title":"Mounting the pan arm"},{"location":"plant_imager/build_v3/cnc_arm/#bom","text":"4x M5x16 flat head screw 4x M5 nuts.","title":"BOM"},{"location":"plant_imager/build_v3/cnc_arm/#assembly","text":"Mount the pan arm to the X carriage of the X-carve CNC as follows: Note Use the holes ot the x-carriage & arm mounting plate.","title":"Assembly"},{"location":"plant_imager/build_v3/cnc_communication/","text":"Communicating with Oquam Link Finding the USB port Link First you need to find which USB port your X-Controller board is connected to. To do so, you can use dmesg : make sure the USB cable from the X-Controller board is unplugged from your computer run dmesg -w in a terminal connect the usb to your computer and see something like: [ 42063 .157605 ] usb 1 -2: new full-speed USB device number 7 using xhci_hcd [ 42063 .313985 ] usb 1 -2: New USB device found, idVendor = 0403 , idProduct = 6001 , bcdDevice = 6 .00 [ 42063 .313991 ] usb 1 -2: New USB device strings: Mfr = 1 , Product = 2 , SerialNumber = 3 [ 42063 .313995 ] usb 1 -2: Product: X-Controller [ 42063 .313997 ] usb 1 -2: Manufacturer: Inventables [ 42063 .314000 ] usb 1 -2: SerialNumber: XCONTROLLER6CFIRF [ 42063 .317878 ] ftdi_sio 1 -2:1.0: FTDI USB Serial Device converter detected [ 42063 .317954 ] usb 1 -2: Detected FT232RL [ 42063 .318915 ] usb 1 -2: FTDI USB Serial Device converter now attached to ttyUSB0 Important The important info here is ttyUSB0 ! Serial connection Link With picocom Link Then you can use picocom to connect to the board: picocom /dev/ttyUSB0 -b 115200 --omap crcrlf --echo Note -b 115200 is the baud rate of the connection, read the picocom man page for more info. --omap crcrlf is mapping the serial output from CR to CR+LF . --echo allows you to see what you are typing. Once connected you should see something like: picocom v3.1 port is : /dev/ttyUSB0 flowcontrol : none baudrate is : 115200 parity is : none databits are : 8 stopbits are : 1 escape is : C-a local echo is : yes noinit is : no noreset is : no hangup is : no nolock is : no send_cmd is : sz -vv receive_cmd is : rz -vv -E imap is : omap is : crcrlf, emap is : crcrlf,delbs, logfile is : none initstring : none exit_after is : not set exit is : no Type [ C-a ] [ C-h ] to see available commands Terminal ready This mean you now have access to a Oquam terminal to communicate, notably send instructions, to the CNC! With the serial monitor of the Arduino IDE Link Open the Arduino IDE , go to Tools > Serial Monitor or use the keyboard shortcut Ctrl + Shift + M . Do not forget to select Both NL & CR instead of the default Newline . Troubleshooting Link Serial access denied Link If you get an error about permission access: Check in what groups you are with: groups ${ USER } If you are not in dialout : sudo gpasswd --add ${ USER } dialout Then log out and back in to see changes!","title":"CNC communication"},{"location":"plant_imager/build_v3/cnc_communication/#communicating-with-oquam","text":"","title":"Communicating with Oquam"},{"location":"plant_imager/build_v3/cnc_communication/#finding-the-usb-port","text":"First you need to find which USB port your X-Controller board is connected to. To do so, you can use dmesg : make sure the USB cable from the X-Controller board is unplugged from your computer run dmesg -w in a terminal connect the usb to your computer and see something like: [ 42063 .157605 ] usb 1 -2: new full-speed USB device number 7 using xhci_hcd [ 42063 .313985 ] usb 1 -2: New USB device found, idVendor = 0403 , idProduct = 6001 , bcdDevice = 6 .00 [ 42063 .313991 ] usb 1 -2: New USB device strings: Mfr = 1 , Product = 2 , SerialNumber = 3 [ 42063 .313995 ] usb 1 -2: Product: X-Controller [ 42063 .313997 ] usb 1 -2: Manufacturer: Inventables [ 42063 .314000 ] usb 1 -2: SerialNumber: XCONTROLLER6CFIRF [ 42063 .317878 ] ftdi_sio 1 -2:1.0: FTDI USB Serial Device converter detected [ 42063 .317954 ] usb 1 -2: Detected FT232RL [ 42063 .318915 ] usb 1 -2: FTDI USB Serial Device converter now attached to ttyUSB0 Important The important info here is ttyUSB0 !","title":"Finding the USB port"},{"location":"plant_imager/build_v3/cnc_communication/#serial-connection","text":"","title":"Serial connection"},{"location":"plant_imager/build_v3/cnc_communication/#with-picocom","text":"Then you can use picocom to connect to the board: picocom /dev/ttyUSB0 -b 115200 --omap crcrlf --echo Note -b 115200 is the baud rate of the connection, read the picocom man page for more info. --omap crcrlf is mapping the serial output from CR to CR+LF . --echo allows you to see what you are typing. Once connected you should see something like: picocom v3.1 port is : /dev/ttyUSB0 flowcontrol : none baudrate is : 115200 parity is : none databits are : 8 stopbits are : 1 escape is : C-a local echo is : yes noinit is : no noreset is : no hangup is : no nolock is : no send_cmd is : sz -vv receive_cmd is : rz -vv -E imap is : omap is : crcrlf, emap is : crcrlf,delbs, logfile is : none initstring : none exit_after is : not set exit is : no Type [ C-a ] [ C-h ] to see available commands Terminal ready This mean you now have access to a Oquam terminal to communicate, notably send instructions, to the CNC!","title":"With picocom"},{"location":"plant_imager/build_v3/cnc_communication/#with-the-serial-monitor-of-the-arduino-ide","text":"Open the Arduino IDE , go to Tools > Serial Monitor or use the keyboard shortcut Ctrl + Shift + M . Do not forget to select Both NL & CR instead of the default Newline .","title":"With the serial monitor of the Arduino IDE"},{"location":"plant_imager/build_v3/cnc_communication/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"plant_imager/build_v3/cnc_communication/#serial-access-denied","text":"If you get an error about permission access: Check in what groups you are with: groups ${ USER } If you are not in dialout : sudo gpasswd --add ${ USER } dialout Then log out and back in to see changes!","title":"Serial access denied"},{"location":"plant_imager/build_v3/cnc_electronics/","text":"Wiring of the X-Controller Link BOM Link To assemble the X-Controller, you will need to buy one: US buy here EU buy here Assembly Link If you ordered the X-Controller, follow the official assembly instructions here . Note This is what we used in our first and third version of the plant imager hardware. Warning We replaced the default Grbl firmware by Oquam , our own implementation. See here for the instructions on how to flash this firmware to the X-Controller. Troubleshooting Link Parts Link If you burn the X-Controller main board, you can buy a new one here .","title":"CNC electronics"},{"location":"plant_imager/build_v3/cnc_electronics/#wiring-of-the-x-controller","text":"","title":"Wiring of the X-Controller"},{"location":"plant_imager/build_v3/cnc_electronics/#bom","text":"To assemble the X-Controller, you will need to buy one: US buy here EU buy here","title":"BOM"},{"location":"plant_imager/build_v3/cnc_electronics/#assembly","text":"If you ordered the X-Controller, follow the official assembly instructions here . Note This is what we used in our first and third version of the plant imager hardware. Warning We replaced the default Grbl firmware by Oquam , our own implementation. See here for the instructions on how to flash this firmware to the X-Controller.","title":"Assembly"},{"location":"plant_imager/build_v3/cnc_electronics/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"plant_imager/build_v3/cnc_electronics/#parts","text":"If you burn the X-Controller main board, you can buy a new one here .","title":"Parts"},{"location":"plant_imager/build_v3/cnc_frame/","text":"Assembly instructions for the X-Carve CNC frame Link We chose the X-Carve 1000 mm as basis for the CNC gantry, feel free to choose another one but beware to check the aluminium frame still fit! X-Carve 1000mm. BOM Link To build the CNC you will need: X-Carve 1000mmm (US buy here ; EU buy here ) Note We chose the X-Carve because initially you could customize your order and for example not order the work area (wood plate). Apparently it is not possible anymore! Assembly Link Follow the official assembly instructions for the X-Carve here . You should follow these steps: Gantry - Side Plates Gantry - X-Carriage Assemble Gantry Rails Belting Note Do NOT follow the steps indicating how to: mount the Z-axis build the work area fix the spindle fix the gantry and rails over the work area (wood plate)","title":"CNC frame"},{"location":"plant_imager/build_v3/cnc_frame/#assembly-instructions-for-the-x-carve-cnc-frame","text":"We chose the X-Carve 1000 mm as basis for the CNC gantry, feel free to choose another one but beware to check the aluminium frame still fit! X-Carve 1000mm.","title":"Assembly instructions for the X-Carve CNC frame"},{"location":"plant_imager/build_v3/cnc_frame/#bom","text":"To build the CNC you will need: X-Carve 1000mmm (US buy here ; EU buy here ) Note We chose the X-Carve because initially you could customize your order and for example not order the work area (wood plate). Apparently it is not possible anymore!","title":"BOM"},{"location":"plant_imager/build_v3/cnc_frame/#assembly","text":"Follow the official assembly instructions for the X-Carve here . You should follow these steps: Gantry - Side Plates Gantry - X-Carriage Assemble Gantry Rails Belting Note Do NOT follow the steps indicating how to: mount the Z-axis build the work area fix the spindle fix the gantry and rails over the work area (wood plate)","title":"Assembly"},{"location":"plant_imager/build_v3/enclosure/","text":"Enclosure assembly Link You can find the PDF with the BOM and assembly instructions for the enclosure here . Note You may want to change the orientation of the LED bars to horizontal to reduce shading by the CNC gantry.","title":"Enclosure"},{"location":"plant_imager/build_v3/enclosure/#enclosure-assembly","text":"You can find the PDF with the BOM and assembly instructions for the enclosure here . Note You may want to change the orientation of the LED bars to horizontal to reduce shading by the CNC gantry.","title":"Enclosure assembly"},{"location":"plant_imager/build_v3/flashing_oquam/","text":"Flashing Oquam on X-Controller Link The ROMI project developed the Oquam firmware as an alternative to Grbl . To flash it on the X-Controller follow these instructions. You will need the Arduino IDE software. On Ubuntu, with snap , do: snap install arduino For other OS, have a look at the official Arduino software website here . Download the sources Link Start by downloading the libromi sources from the ROMI GitHub repository: git clone https://github.com/romi/libromi.git Currently, the software is developed under the encoder_z branch: git checkout encoder_z Flashing the firmware Link Create a ZIP archive of the Arduino libraries Link You will need to add the RomiSerial as extra library to the Arduino IDE. It is located under libromi/arduino_libraries/RomiSerial . To create a ZIP archive: zip -r RomiSerial.zip RomiSerial Open the Arduino IDE Link Open the libromi/firmware/Oquam/Oquam.ino sketch using File > Open... or with the Ctrl + O shortcut. Then browse and select the INO file. Add the RomiSerial library Link Add the RomiSerial.zip library using the Sketch > Include Library > Add .ZIP Library... menu. Then browse and select the ZIP file. Select the board Link Select the right board to upload to using the Tools > Board menu and select Arduino UNO . Power up the X-Controller Link Connect the power cord and start the X-Controller. Select the USB port Link Plug the USB cable and select the right USB port to upload to using Tools > Port menu. See find the USB port to determine which port the X-Controller is connected to. Flash the firmware Link You may now flash the firmware with the icon below the main menu. Wait for the IDE to indicate Done uploading on the status bar at the bottom right of the IDE window. Then you can unplug safely! Troubleshooting Link Find the USB port Link To find which USB port your arduino board is connected to, you can use dmesg : make sure the usb cable is unplugged run dmesg -w in a terminal connect the usb (from the arduino board to the computer) and you should see something like: [ 14818 .631347 ] usb 1 -4: new full-speed USB device number 7 using xhci_hcd [ 14818 .787048 ] usb 1 -4: New USB device found, idVendor = 0403 , idProduct = 6001 , bcdDevice = 6 .00 [ 14818 .787062 ] usb 1 -4: New USB device strings: Mfr = 1 , Product = 2 , SerialNumber = 3 [ 14818 .787069 ] usb 1 -4: Product: X-Controller [ 14818 .787073 ] usb 1 -4: Manufacturer: Inventables [ 14818 .787077 ] usb 1 -4: SerialNumber: XCONTROLLER6CFIRF [ 14818 .847148 ] usbcore: registered new interface driver usbserial_generic [ 14818 .847155 ] usbserial: USB Serial support registered for generic [ 14818 .851163 ] usbcore: registered new interface driver ftdi_sio [ 14818 .851179 ] usbserial: USB Serial support registered for FTDI USB Serial Device [ 14818 .851367 ] ftdi_sio 1 -4:1.0: FTDI USB Serial Device converter detected [ 14818 .851402 ] usb 1 -4: Detected FT232RL [ 14818 .851860 ] usb 1 -4: FTDI USB Serial Device converter now attached to ttyUSB0 Important The important info here is ttyUSB0 ! Serial access denied Link If you get an error about permission access: Check you selected the right USB port. If yes, proceed to the next steps. To see which groups your $USER belongs to: groups ${ USER } If you see the dialout group there, go back to checking the USB port! If you are not in dialout : sudo gpasswd --add ${ USER } dialout Then log out for your session, and then log back in to see changes (with groups ${USER} ).","title":"Flashing Oquam"},{"location":"plant_imager/build_v3/flashing_oquam/#flashing-oquam-on-x-controller","text":"The ROMI project developed the Oquam firmware as an alternative to Grbl . To flash it on the X-Controller follow these instructions. You will need the Arduino IDE software. On Ubuntu, with snap , do: snap install arduino For other OS, have a look at the official Arduino software website here .","title":"Flashing Oquam on X-Controller"},{"location":"plant_imager/build_v3/flashing_oquam/#download-the-sources","text":"Start by downloading the libromi sources from the ROMI GitHub repository: git clone https://github.com/romi/libromi.git Currently, the software is developed under the encoder_z branch: git checkout encoder_z","title":"Download the sources"},{"location":"plant_imager/build_v3/flashing_oquam/#flashing-the-firmware","text":"","title":"Flashing the firmware"},{"location":"plant_imager/build_v3/flashing_oquam/#create-a-zip-archive-of-the-arduino-libraries","text":"You will need to add the RomiSerial as extra library to the Arduino IDE. It is located under libromi/arduino_libraries/RomiSerial . To create a ZIP archive: zip -r RomiSerial.zip RomiSerial","title":"Create a ZIP archive of the Arduino libraries"},{"location":"plant_imager/build_v3/flashing_oquam/#open-the-arduino-ide","text":"Open the libromi/firmware/Oquam/Oquam.ino sketch using File > Open... or with the Ctrl + O shortcut. Then browse and select the INO file.","title":"Open the Arduino IDE"},{"location":"plant_imager/build_v3/flashing_oquam/#add-the-romiserial-library","text":"Add the RomiSerial.zip library using the Sketch > Include Library > Add .ZIP Library... menu. Then browse and select the ZIP file.","title":"Add the RomiSerial library"},{"location":"plant_imager/build_v3/flashing_oquam/#select-the-board","text":"Select the right board to upload to using the Tools > Board menu and select Arduino UNO .","title":"Select the board"},{"location":"plant_imager/build_v3/flashing_oquam/#power-up-the-x-controller","text":"Connect the power cord and start the X-Controller.","title":"Power up the X-Controller"},{"location":"plant_imager/build_v3/flashing_oquam/#select-the-usb-port","text":"Plug the USB cable and select the right USB port to upload to using Tools > Port menu. See find the USB port to determine which port the X-Controller is connected to.","title":"Select the USB port"},{"location":"plant_imager/build_v3/flashing_oquam/#flash-the-firmware","text":"You may now flash the firmware with the icon below the main menu. Wait for the IDE to indicate Done uploading on the status bar at the bottom right of the IDE window. Then you can unplug safely!","title":"Flash the firmware"},{"location":"plant_imager/build_v3/flashing_oquam/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"plant_imager/build_v3/flashing_oquam/#find-the-usb-port","text":"To find which USB port your arduino board is connected to, you can use dmesg : make sure the usb cable is unplugged run dmesg -w in a terminal connect the usb (from the arduino board to the computer) and you should see something like: [ 14818 .631347 ] usb 1 -4: new full-speed USB device number 7 using xhci_hcd [ 14818 .787048 ] usb 1 -4: New USB device found, idVendor = 0403 , idProduct = 6001 , bcdDevice = 6 .00 [ 14818 .787062 ] usb 1 -4: New USB device strings: Mfr = 1 , Product = 2 , SerialNumber = 3 [ 14818 .787069 ] usb 1 -4: Product: X-Controller [ 14818 .787073 ] usb 1 -4: Manufacturer: Inventables [ 14818 .787077 ] usb 1 -4: SerialNumber: XCONTROLLER6CFIRF [ 14818 .847148 ] usbcore: registered new interface driver usbserial_generic [ 14818 .847155 ] usbserial: USB Serial support registered for generic [ 14818 .851163 ] usbcore: registered new interface driver ftdi_sio [ 14818 .851179 ] usbserial: USB Serial support registered for FTDI USB Serial Device [ 14818 .851367 ] ftdi_sio 1 -4:1.0: FTDI USB Serial Device converter detected [ 14818 .851402 ] usb 1 -4: Detected FT232RL [ 14818 .851860 ] usb 1 -4: FTDI USB Serial Device converter now attached to ttyUSB0 Important The important info here is ttyUSB0 !","title":"Find the USB port"},{"location":"plant_imager/build_v3/flashing_oquam/#serial-access-denied","text":"If you get an error about permission access: Check you selected the right USB port. If yes, proceed to the next steps. To see which groups your $USER belongs to: groups ${ USER } If you see the dialout group there, go back to checking the USB port! If you are not in dialout : sudo gpasswd --add ${ USER } dialout Then log out for your session, and then log back in to see changes (with groups ${USER} ).","title":"Serial access denied"},{"location":"plant_imager/build_v3/frame_cnc/","text":"Enclosure & CNC assembly Link You may now put the X-Carve CNC into the enclosure as follows: Note While the image above shows the CNC mounted in the aluminium frame with the wooden enclosure, it might be more practical to only mount the wooden panels AFTER mounting the CNC.","title":"Frame & CNC"},{"location":"plant_imager/build_v3/frame_cnc/#enclosure-cnc-assembly","text":"You may now put the X-Carve CNC into the enclosure as follows: Note While the image above shows the CNC mounted in the aluminium frame with the wooden enclosure, it might be more practical to only mount the wooden panels AFTER mounting the CNC.","title":"Enclosure &amp; CNC assembly"},{"location":"plant_imager/build_v3/manual_gimbal/","text":"Manual gimbal Link Manual gimbal with 360\u00b0 protractor & PiCamera. BOM Link ID Name Qty Off the shelf / Custom Material Manufacturer Serial number Link 1 MG004 1 Custom Aluminium pdf 2 MG009 1 Custom Aluminium pdf 3 MG010 1 Custom Aluminium pdf 4 MG012 1 Custom Aluminium pdf 5 MG011 1 Custom PMMA RS PRO 434-295 pdf , dxf , rs-online.com 6 MG014 1 Custom PMMA dxf 7 Mounting Plate Rpi Zero Hq Camera 1 Custom PMMA dxf 8 Protractor dial deg 1 Custom PMMA dxf 9 Spacer M5x8 2 Off the shelf Vis express 8470100818 vis-express.fr 10 Spacer M5x20 1 Off the shelf Vis express 8470102018 vis-express.fr 11 Bouton de serrage M5 3 Off the shelf RS PRO 771-673 rs-online.com 12 M5x40 Socket Head 1 Off the shelf RS PRO 293-353 rs-online.com 13 M5x30 Socket Head 2 Off the shelf RS PRO 290-130 rs-online.com 14 M5x12 Socket Head 2 Off the Shelf RS PRO 281-079 rs-online.com 15 M4x16 Flat Head 2 Off the Shelf RS PRO 171-837 rs-online.com 16 M4x12 Socket Head 1 Off the shelf RS PRO 281-035 rs-online.com 17 M3x8 Socket Head 2 Off the Shelf RS PRO 280-997 rs-online.com 18 Bosch nut 2 Off the Shelf Bosch Rexroth 3842523142 rs-online.com 19 Washer M4 L 1 Off the Shelf Vis express 3534000402 vis-express.fr 20 IGUS GTM-0818-020 2 Off the Shelf IGUS GTM-0818-020 amazon.fr 21 RaspberryPiZeroW 1 Off the Shelf kubii.fr 22 Raspberry Pi HQ Camera 1 Off the Shelf kubii.fr 23 6mm CS-mount lens 1 Off the Shelf kubii.fr 24 Ribbon cable 1 Off the Shelf kubii.fr 25 5mm M2.5 nylon screws 8 Off the Shell thepihut.com 26 M2.5 nylon nuts 8 Off the Self Assembly Link You can find the PDF with the BOM and assembly instructions for the manual gimbal here .","title":"Manual Gimball"},{"location":"plant_imager/build_v3/manual_gimbal/#manual-gimbal","text":"Manual gimbal with 360\u00b0 protractor & PiCamera.","title":"Manual gimbal"},{"location":"plant_imager/build_v3/manual_gimbal/#bom","text":"ID Name Qty Off the shelf / Custom Material Manufacturer Serial number Link 1 MG004 1 Custom Aluminium pdf 2 MG009 1 Custom Aluminium pdf 3 MG010 1 Custom Aluminium pdf 4 MG012 1 Custom Aluminium pdf 5 MG011 1 Custom PMMA RS PRO 434-295 pdf , dxf , rs-online.com 6 MG014 1 Custom PMMA dxf 7 Mounting Plate Rpi Zero Hq Camera 1 Custom PMMA dxf 8 Protractor dial deg 1 Custom PMMA dxf 9 Spacer M5x8 2 Off the shelf Vis express 8470100818 vis-express.fr 10 Spacer M5x20 1 Off the shelf Vis express 8470102018 vis-express.fr 11 Bouton de serrage M5 3 Off the shelf RS PRO 771-673 rs-online.com 12 M5x40 Socket Head 1 Off the shelf RS PRO 293-353 rs-online.com 13 M5x30 Socket Head 2 Off the shelf RS PRO 290-130 rs-online.com 14 M5x12 Socket Head 2 Off the Shelf RS PRO 281-079 rs-online.com 15 M4x16 Flat Head 2 Off the Shelf RS PRO 171-837 rs-online.com 16 M4x12 Socket Head 1 Off the shelf RS PRO 281-035 rs-online.com 17 M3x8 Socket Head 2 Off the Shelf RS PRO 280-997 rs-online.com 18 Bosch nut 2 Off the Shelf Bosch Rexroth 3842523142 rs-online.com 19 Washer M4 L 1 Off the Shelf Vis express 3534000402 vis-express.fr 20 IGUS GTM-0818-020 2 Off the Shelf IGUS GTM-0818-020 amazon.fr 21 RaspberryPiZeroW 1 Off the Shelf kubii.fr 22 Raspberry Pi HQ Camera 1 Off the Shelf kubii.fr 23 6mm CS-mount lens 1 Off the Shelf kubii.fr 24 Ribbon cable 1 Off the Shelf kubii.fr 25 5mm M2.5 nylon screws 8 Off the Shell thepihut.com 26 M2.5 nylon nuts 8 Off the Self","title":"BOM"},{"location":"plant_imager/build_v3/manual_gimbal/#assembly","text":"You can find the PDF with the BOM and assembly instructions for the manual gimbal here .","title":"Assembly"},{"location":"plant_imager/build_v3/pan_arm/","text":"Pan arm assembly Link BOM Link ID Name Qty Off the shelf / Custom Material Manufacturer Serial number Link 1 PLI004 1 Custom Aluminium pdf 2 PLI006 1 Custom Steel Ewellix Makers in Motion LJM12X600ESSC2 pdf , rs-online.com 3 PLI007-3 1 Custom Aluminium pdf 4 PLI009 1 Custom Aluminium pdf 5 PLI011 1 Custom Aluminium pdf 6 PLI012 1 Custom Aluminium pdf 7 PLI016 1 Custom Aluminium pdf 8 Cable Zip 3 Custom PLA stl 9 Dozenal_ruler_30cm 1 Custom PMMA RS PRO 434-295 dxf , rs-online.com 10 PROFILE_D_ETAYAGE_20x40x300 1 Off the shelf Bosch Rexroth R987501076 rs-online.com 11 Slip Ring ROB-13063 1 Off the shelf SparkFun ROB-13063 mouser.fr 12 Nema 23 - 45mm 1 Off the shelf Stepperonline 23HS18-2004H omc-stepperonline.com 13 Convertisseur 24-5V 1 Off the shelf amazon.com 14 Huco 047_20 1 Off the shelf Huco 047.20.3535 rs-online.com 15 CUI_DEVICES_AMT132Q-0048-1200 1 Off the shelf CUI Devices AMT132Q-V eu.mouser.com 16 AMT Cable 1 Off the shelf CUI Devices AMT-18C-3-036 eu.mouser.com 17 AMT Viewpoint Programming Cable 1 Off the shelf CUI Devices AMT-PGRM-18C eu.mouser.com 18 SKF_P 12 TF 2 Off the shelf SKF P 12 TF rs-online.com 19 MQCL-12-A 1 Off the shelf Ruland MQCL-12-A rs-online.com 20 M8x16 Socket Head 4 Off the shelf RS PRO 529-703 rs-online.com 21 M5x10 Shoulder Screw 2 Off the shelf RS PRO 292-271 rs-online.com 22 M3x12 Set Screw 1 Off the shelf RS PRO 137-736 rs-online.com 23 M3x20 Socket Head 2 Off the shelf RS PRO 293-319 rs-online.com 24 M4x12 Socket Head 2 Off the shelf RS PRO 281-035 rs-online.com 25 M5x18 Socket Head 4 Off the shelf Vis express 8420501818 vis-express.fr 26 M5x16 Flat Head 4 Off the shelf RS PRO 232-8423 rs-online.com 27 M6x40 Flat Head 2 Off the shelf RS PRO 917-6238 rs-online.com 28 M5x8 Socket Head 2 Off the shelf RS PRO 660-4633 rs-online.com 29 M5x8 Button Head 3 Off the shelf RS PRO 822-9114 rs-online.com 30 Bosch nut 5 Off the shelf Bosch Rexroth 3842523142 rs-online.com 31 M3 Locknut 2 Off the shelf RS PRO 524-281 rs-online.com 32 M4 Locknut 2 Off the shelf RS PRO 524-304 rs-online.com 33 M5 Locknut 10 Off the shelf RS PRO 122-4371 rs-online.com Assembly Link You can find the PDF with the BOM and assembly instructions for the arm here .","title":"Pan Arm"},{"location":"plant_imager/build_v3/pan_arm/#pan-arm-assembly","text":"","title":"Pan arm assembly"},{"location":"plant_imager/build_v3/pan_arm/#bom","text":"ID Name Qty Off the shelf / Custom Material Manufacturer Serial number Link 1 PLI004 1 Custom Aluminium pdf 2 PLI006 1 Custom Steel Ewellix Makers in Motion LJM12X600ESSC2 pdf , rs-online.com 3 PLI007-3 1 Custom Aluminium pdf 4 PLI009 1 Custom Aluminium pdf 5 PLI011 1 Custom Aluminium pdf 6 PLI012 1 Custom Aluminium pdf 7 PLI016 1 Custom Aluminium pdf 8 Cable Zip 3 Custom PLA stl 9 Dozenal_ruler_30cm 1 Custom PMMA RS PRO 434-295 dxf , rs-online.com 10 PROFILE_D_ETAYAGE_20x40x300 1 Off the shelf Bosch Rexroth R987501076 rs-online.com 11 Slip Ring ROB-13063 1 Off the shelf SparkFun ROB-13063 mouser.fr 12 Nema 23 - 45mm 1 Off the shelf Stepperonline 23HS18-2004H omc-stepperonline.com 13 Convertisseur 24-5V 1 Off the shelf amazon.com 14 Huco 047_20 1 Off the shelf Huco 047.20.3535 rs-online.com 15 CUI_DEVICES_AMT132Q-0048-1200 1 Off the shelf CUI Devices AMT132Q-V eu.mouser.com 16 AMT Cable 1 Off the shelf CUI Devices AMT-18C-3-036 eu.mouser.com 17 AMT Viewpoint Programming Cable 1 Off the shelf CUI Devices AMT-PGRM-18C eu.mouser.com 18 SKF_P 12 TF 2 Off the shelf SKF P 12 TF rs-online.com 19 MQCL-12-A 1 Off the shelf Ruland MQCL-12-A rs-online.com 20 M8x16 Socket Head 4 Off the shelf RS PRO 529-703 rs-online.com 21 M5x10 Shoulder Screw 2 Off the shelf RS PRO 292-271 rs-online.com 22 M3x12 Set Screw 1 Off the shelf RS PRO 137-736 rs-online.com 23 M3x20 Socket Head 2 Off the shelf RS PRO 293-319 rs-online.com 24 M4x12 Socket Head 2 Off the shelf RS PRO 281-035 rs-online.com 25 M5x18 Socket Head 4 Off the shelf Vis express 8420501818 vis-express.fr 26 M5x16 Flat Head 4 Off the shelf RS PRO 232-8423 rs-online.com 27 M6x40 Flat Head 2 Off the shelf RS PRO 917-6238 rs-online.com 28 M5x8 Socket Head 2 Off the shelf RS PRO 660-4633 rs-online.com 29 M5x8 Button Head 3 Off the shelf RS PRO 822-9114 rs-online.com 30 Bosch nut 5 Off the shelf Bosch Rexroth 3842523142 rs-online.com 31 M3 Locknut 2 Off the shelf RS PRO 524-281 rs-online.com 32 M4 Locknut 2 Off the shelf RS PRO 524-304 rs-online.com 33 M5 Locknut 10 Off the shelf RS PRO 122-4371 rs-online.com","title":"BOM"},{"location":"plant_imager/build_v3/pan_arm/#assembly","text":"You can find the PDF with the BOM and assembly instructions for the arm here .","title":"Assembly"},{"location":"plant_imager/build_v3/pan_electronics/","text":"Pan arm electronics Link Pan motor Link We replaced the original z-axis stepper motor by an hollow shaft stepper motor from stepperonline 23HS18-2004H . Full specifications can be accessed here . Wiring of 23HS18-2004H : A+ A- B+ B- red red/white green/white green The original motor was KL23H251-28-4AP from Automation Technology. Full specifications can be accessed here . Wiring of KL23H251-28-4AP : Phase A+ A- B+ B- Pin A C B D Color white red green black Thus, the re-wiring looks like this: X-Carve white wire -> HS stepper red wire X-Carve red wire -> HS stepper red/white wire X-Carve green wire -> HS stepper green/white wire X-Carve black wire -> HS stepper green wire Incremental rotary encoder Link The encoder has 4800 steps and this is too fine for the arduino to see. From our tests, it seems very stable at 360. Change the steps resolution Link get the AMT Cable & the AMT Viewpoint Programming Cable (parts 16 & 17 from the BOM list) load the ATM Viewpoint software (Windows only) set the resolution to 360 . make sure you then align the 0 again after you change resolution. If that does not work: Drink a beer Try again Repeat as necessary. Testing with an Arduino Link Wiring Link To test the encoder with an Arduino UNO, wire the encoder a follows: Encoder Pin 12 Z+ (blue) -> Arduino Pin 11 Encoder Pin 13 Z- (blue & white) -> Arduino GND Encoder Pin 04 GND (green & Brown) -> Arduino GND Encoder Pin 06 +5V (red) -> Arduino +5 You can also wire two switches to PIN9 & GND and to PIN10 and GND of the UNO. It is also possible to wire just one switch and swap PINs during the homing procedure, but beware of shortcuts. Testing Link To test the encoder, we recommend to use the serial monitor of the Arduino IDE as you can manually select Both NL & CR as EOL. Pull the Arduino script libromi/firmware/Oquam/Oquam.ino from the encoder_z branch of the libromi repository. Then build / upload, refers to the detailed flashing instructions . Using the serial monitor of the Arduino IDE, deactivate the X & Y axes (disables axes 0 & 1 , and enables 2 ) by sending: #h[-1,-1,2]:xxxx Then test the homing procedure for the theta axis by sending: #H:xxxx While the board is waiting for homing , turn the encoder, once you hit the 0 you will see a \"homing complete\" message. Now, let's try to home all axis. So activate all of them with: #h[0,1,2]:xxxx Then test the homing procedure for all axes by sending: #H:xxxx While the board is waiting for homing , press the X switch, then the Y switch and finally turn the encoder. When you release the switches you will see a \"homing complete\" message. Again, with the encoder, once you hit the 0 you will see a \"homing complete\" message. Important The RomiSerial expect both \"newline\" (NL) AND \"carriage return\" (CR)! So you have to select Both NL & CR instead of the default Newline in the serial monitor of the Arduino IDE. X-Carve controller Link Wiring Link Simply wire the rotary encoder Z+/Z- pins to the red/black Z-limit wires: Encoder Pin 12 Z+ (blue w/ white stripe) -> red Z-limit wire of the X-Carve controller (PIN12, not PIN11 as with grblShield!) Encoder Pin 13 Z- (white w/ blue stripe) -> black Z-limit wire of the X-Carve controller Encoder Pin 04 GND (green w/ red strip) -> black wire (-) of the 24/5V DC converter Encoder Pin 06 +5V (red w/ green stripe) -> red wire (+) of the 24/5V DC converter Test Link With the Oquam firmware flashed on the X-Controller (instructions here ) and the correct wiring: Open the Arduino IDE Plug the USB from the X-Controller to your computer Select the right USB port (assume ttyUSB0 ) Open the serial monitor of the Arduino IDE, select Both NL & CR , send: #h[0,1,2]:xxxx #H:xxxx The X-Carve should home X & Y axes, then you turn the encoder and when it hits home it should stop the homing process.","title":"Pan electronics"},{"location":"plant_imager/build_v3/pan_electronics/#pan-arm-electronics","text":"","title":"Pan arm electronics"},{"location":"plant_imager/build_v3/pan_electronics/#pan-motor","text":"We replaced the original z-axis stepper motor by an hollow shaft stepper motor from stepperonline 23HS18-2004H . Full specifications can be accessed here . Wiring of 23HS18-2004H : A+ A- B+ B- red red/white green/white green The original motor was KL23H251-28-4AP from Automation Technology. Full specifications can be accessed here . Wiring of KL23H251-28-4AP : Phase A+ A- B+ B- Pin A C B D Color white red green black Thus, the re-wiring looks like this: X-Carve white wire -> HS stepper red wire X-Carve red wire -> HS stepper red/white wire X-Carve green wire -> HS stepper green/white wire X-Carve black wire -> HS stepper green wire","title":"Pan motor"},{"location":"plant_imager/build_v3/pan_electronics/#incremental-rotary-encoder","text":"The encoder has 4800 steps and this is too fine for the arduino to see. From our tests, it seems very stable at 360.","title":"Incremental rotary encoder"},{"location":"plant_imager/build_v3/pan_electronics/#change-the-steps-resolution","text":"get the AMT Cable & the AMT Viewpoint Programming Cable (parts 16 & 17 from the BOM list) load the ATM Viewpoint software (Windows only) set the resolution to 360 . make sure you then align the 0 again after you change resolution. If that does not work: Drink a beer Try again Repeat as necessary.","title":"Change the steps resolution"},{"location":"plant_imager/build_v3/pan_electronics/#testing-with-an-arduino","text":"","title":"Testing with an Arduino"},{"location":"plant_imager/build_v3/pan_electronics/#wiring","text":"To test the encoder with an Arduino UNO, wire the encoder a follows: Encoder Pin 12 Z+ (blue) -> Arduino Pin 11 Encoder Pin 13 Z- (blue & white) -> Arduino GND Encoder Pin 04 GND (green & Brown) -> Arduino GND Encoder Pin 06 +5V (red) -> Arduino +5 You can also wire two switches to PIN9 & GND and to PIN10 and GND of the UNO. It is also possible to wire just one switch and swap PINs during the homing procedure, but beware of shortcuts.","title":"Wiring"},{"location":"plant_imager/build_v3/pan_electronics/#testing","text":"To test the encoder, we recommend to use the serial monitor of the Arduino IDE as you can manually select Both NL & CR as EOL. Pull the Arduino script libromi/firmware/Oquam/Oquam.ino from the encoder_z branch of the libromi repository. Then build / upload, refers to the detailed flashing instructions . Using the serial monitor of the Arduino IDE, deactivate the X & Y axes (disables axes 0 & 1 , and enables 2 ) by sending: #h[-1,-1,2]:xxxx Then test the homing procedure for the theta axis by sending: #H:xxxx While the board is waiting for homing , turn the encoder, once you hit the 0 you will see a \"homing complete\" message. Now, let's try to home all axis. So activate all of them with: #h[0,1,2]:xxxx Then test the homing procedure for all axes by sending: #H:xxxx While the board is waiting for homing , press the X switch, then the Y switch and finally turn the encoder. When you release the switches you will see a \"homing complete\" message. Again, with the encoder, once you hit the 0 you will see a \"homing complete\" message. Important The RomiSerial expect both \"newline\" (NL) AND \"carriage return\" (CR)! So you have to select Both NL & CR instead of the default Newline in the serial monitor of the Arduino IDE.","title":"Testing"},{"location":"plant_imager/build_v3/pan_electronics/#x-carve-controller","text":"","title":"X-Carve controller"},{"location":"plant_imager/build_v3/pan_electronics/#wiring_1","text":"Simply wire the rotary encoder Z+/Z- pins to the red/black Z-limit wires: Encoder Pin 12 Z+ (blue w/ white stripe) -> red Z-limit wire of the X-Carve controller (PIN12, not PIN11 as with grblShield!) Encoder Pin 13 Z- (white w/ blue stripe) -> black Z-limit wire of the X-Carve controller Encoder Pin 04 GND (green w/ red strip) -> black wire (-) of the 24/5V DC converter Encoder Pin 06 +5V (red w/ green stripe) -> red wire (+) of the 24/5V DC converter","title":"Wiring"},{"location":"plant_imager/build_v3/pan_electronics/#test","text":"With the Oquam firmware flashed on the X-Controller (instructions here ) and the correct wiring: Open the Arduino IDE Plug the USB from the X-Controller to your computer Select the right USB port (assume ttyUSB0 ) Open the serial monitor of the Arduino IDE, select Both NL & CR , send: #h[0,1,2]:xxxx #H:xxxx The X-Carve should home X & Y axes, then you turn the encoder and when it hits home it should stop the homing process.","title":"Test"},{"location":"plant_imager/build_v3/picamera/","text":"PiCamera HQ Link BOM Link To assemble a SINGLE PiCamera HQ, you will need the following list of materials: ID Name Qty Off the shelf / Custom Material Manufacturer Serial number Link 7 Mounting Plate Rpi Zero Hq Camera 1 Custom PMMA dxf 21 RaspberryPiZeroW 1 Off the Shelf kubii.fr 22 Raspberry Pi HQ Camera 1 Off the Shelf kubii.fr 23 6mm CS-mount lens 1 Off the Shelf kubii.fr 24 Ribbon cable 1 Off the Shelf kubii.fr 25 5mm M2.5 nylon screws 8 Off the Shell thepihut.com Note This is contained in the \"Manual gimbal\" BOM. Assembly Link Todo Write assembly instructions! Software Link Todo Write software instructions!","title":"PiCamera"},{"location":"plant_imager/build_v3/picamera/#picamera-hq","text":"","title":"PiCamera HQ"},{"location":"plant_imager/build_v3/picamera/#bom","text":"To assemble a SINGLE PiCamera HQ, you will need the following list of materials: ID Name Qty Off the shelf / Custom Material Manufacturer Serial number Link 7 Mounting Plate Rpi Zero Hq Camera 1 Custom PMMA dxf 21 RaspberryPiZeroW 1 Off the Shelf kubii.fr 22 Raspberry Pi HQ Camera 1 Off the Shelf kubii.fr 23 6mm CS-mount lens 1 Off the Shelf kubii.fr 24 Ribbon cable 1 Off the Shelf kubii.fr 25 5mm M2.5 nylon screws 8 Off the Shell thepihut.com Note This is contained in the \"Manual gimbal\" BOM.","title":"BOM"},{"location":"plant_imager/build_v3/picamera/#assembly","text":"Todo Write assembly instructions!","title":"Assembly"},{"location":"plant_imager/build_v3/picamera/#software","text":"Todo Write software instructions!","title":"Software"},{"location":"plant_imager/build_v3/resources/","text":"Resources Link File formats Link a dfx file is a data file saved in a format developed by Autodesk and used for CAD (computer-aided design) vector image files, such as AutoCAD documents. DXF files are similar to .DWG files but are more compatible with other programs since they are ASCII (text) based fileinfo.com . an stl file is a 3D model saved in the stereolithography file format developed by 3D Systems. It contains plain text or binary data that describes a set of triangular facets, which comprise a model. STL files are quite common, and they can be opened in many CAD and 3D modeling programs fileinfo.com . a step file is a 3D model file formatted in STEP (Standard for the Exchange of Product Data), an ISO standard exchange format. It contains three-dimensional model data saved in a text format recognized by multiple computer-assisted design (CAD) programs wikipedia . an f3z file is a Zip-compressed archive used by Autodesk Fusion 360, a 3D CAD industrial and mechanical design tool fileinfo.com . an SLDASM file is a 3D assembly created by SolidWorks CAD software. SLDASM files are comprised of SolidWorks part (.SLDPRT) files, which users combine to form assemblies fileinfo.com . an smg file is a 3D CAD model saved in the SolidWorks Composer file format. It contains a 3D assembly, which may include one or more parts or sub-assemblies. SMG files also store document properties, which include information about the coordinate system and textures of the assembly. fileinfo.com . CAD & Models Link Pan arm Link f3z file SLDASM file smg file step file Frame & enclosure Link f3z file SLDASM file smg file step file Manual gimbal Link f3z file SLDASM file smg file step file Plant Imager Link f3z file SLDASM file smg file step file BOM & assembly instructions Link All bills of material are detailed here . Pan arm Link You can find the PDF with the BOM and assembly instructions here . Frame & enclosure Link You can find the PDF with the BOM and assembly instructions here . Manual gimbal Link You can find the PDF with the BOM and assembly instructions here . Plant Imager Link You can find the PDF with the BOM and assembly instructions here .","title":"Resources"},{"location":"plant_imager/build_v3/resources/#resources","text":"","title":"Resources"},{"location":"plant_imager/build_v3/resources/#file-formats","text":"a dfx file is a data file saved in a format developed by Autodesk and used for CAD (computer-aided design) vector image files, such as AutoCAD documents. DXF files are similar to .DWG files but are more compatible with other programs since they are ASCII (text) based fileinfo.com . an stl file is a 3D model saved in the stereolithography file format developed by 3D Systems. It contains plain text or binary data that describes a set of triangular facets, which comprise a model. STL files are quite common, and they can be opened in many CAD and 3D modeling programs fileinfo.com . a step file is a 3D model file formatted in STEP (Standard for the Exchange of Product Data), an ISO standard exchange format. It contains three-dimensional model data saved in a text format recognized by multiple computer-assisted design (CAD) programs wikipedia . an f3z file is a Zip-compressed archive used by Autodesk Fusion 360, a 3D CAD industrial and mechanical design tool fileinfo.com . an SLDASM file is a 3D assembly created by SolidWorks CAD software. SLDASM files are comprised of SolidWorks part (.SLDPRT) files, which users combine to form assemblies fileinfo.com . an smg file is a 3D CAD model saved in the SolidWorks Composer file format. It contains a 3D assembly, which may include one or more parts or sub-assemblies. SMG files also store document properties, which include information about the coordinate system and textures of the assembly. fileinfo.com .","title":"File formats"},{"location":"plant_imager/build_v3/resources/#cad-models","text":"","title":"CAD &amp; Models"},{"location":"plant_imager/build_v3/resources/#pan-arm","text":"f3z file SLDASM file smg file step file","title":"Pan arm"},{"location":"plant_imager/build_v3/resources/#frame-enclosure","text":"f3z file SLDASM file smg file step file","title":"Frame &amp; enclosure"},{"location":"plant_imager/build_v3/resources/#manual-gimbal","text":"f3z file SLDASM file smg file step file","title":"Manual gimbal"},{"location":"plant_imager/build_v3/resources/#plant-imager","text":"f3z file SLDASM file smg file step file","title":"Plant Imager"},{"location":"plant_imager/build_v3/resources/#bom-assembly-instructions","text":"All bills of material are detailed here .","title":"BOM &amp; assembly instructions"},{"location":"plant_imager/build_v3/resources/#pan-arm_1","text":"You can find the PDF with the BOM and assembly instructions here .","title":"Pan arm"},{"location":"plant_imager/build_v3/resources/#frame-enclosure_1","text":"You can find the PDF with the BOM and assembly instructions here .","title":"Frame &amp; enclosure"},{"location":"plant_imager/build_v3/resources/#manual-gimbal_1","text":"You can find the PDF with the BOM and assembly instructions here .","title":"Manual gimbal"},{"location":"plant_imager/build_v3/resources/#plant-imager_1","text":"You can find the PDF with the BOM and assembly instructions here .","title":"Plant Imager"},{"location":"plant_imager/developer/","text":"","title":"Home"},{"location":"plant_imager/developer/colmap_cli/","text":"COLMAP CLI Link Feature extraction Link colmap feature_extractor -h COLMAP 3.8 (Commit 31df46c on 2022-03-05 with CUDA) Options can either be specified via command-line or by defining them in a .ini project file passed to `--project_path`. -h [ --help ] --random_seed arg (=0) --log_to_stderr arg (=0) --log_level arg (=2) --project_path arg --database_path arg --image_path arg --camera_mode arg (=-1) --image_list_path arg --descriptor_normalization arg (=l1_root) {'l1_root', 'l2'} --ImageReader.mask_path arg --ImageReader.camera_model arg (=SIMPLE_RADIAL) --ImageReader.single_camera arg (=0) --ImageReader.single_camera_per_folder arg (=0) --ImageReader.single_camera_per_image arg (=0) --ImageReader.existing_camera_id arg (=-1) --ImageReader.camera_params arg --ImageReader.default_focal_length_factor arg (=1.2) --ImageReader.camera_mask_path arg --SiftExtraction.num_threads arg (=-1) --SiftExtraction.use_gpu arg (=1) --SiftExtraction.gpu_index arg (=-1) --SiftExtraction.max_image_size arg (=3200) --SiftExtraction.max_num_features arg (=8192) --SiftExtraction.first_octave arg (=-1) --SiftExtraction.num_octaves arg (=4) --SiftExtraction.octave_resolution arg (=3) --SiftExtraction.peak_threshold arg (=0.0066666666666666671) --SiftExtraction.edge_threshold arg (=10) --SiftExtraction.estimate_affine_shape arg (=0) --SiftExtraction.max_num_orientations arg (=2) --SiftExtraction.upright arg (=0) --SiftExtraction.domain_size_pooling arg (=0) --SiftExtraction.dsp_min_scale arg (=0.16666666666666666) --SiftExtraction.dsp_max_scale arg (=3) --SiftExtraction.dsp_num_scales arg (=10) Exhaustive matcher Link colmap exhaustive_matcher -h COLMAP 3.8 (Commit 31df46c on 2022-03-05 with CUDA) Options can either be specified via command-line or by defining them in a .ini project file passed to `--project_path`. -h [ --help ] --random_seed arg (=0) --log_to_stderr arg (=0) --log_level arg (=2) --project_path arg --database_path arg --SiftMatching.num_threads arg (=-1) --SiftMatching.use_gpu arg (=1) --SiftMatching.gpu_index arg (=-1) --SiftMatching.max_ratio arg (=0.80000000000000004) --SiftMatching.max_distance arg (=0.69999999999999996) --SiftMatching.cross_check arg (=1) --SiftMatching.max_error arg (=4) --SiftMatching.max_num_matches arg (=32768) --SiftMatching.confidence arg (=0.999) --SiftMatching.max_num_trials arg (=10000) --SiftMatching.min_inlier_ratio arg (=0.25) --SiftMatching.min_num_inliers arg (=15) --SiftMatching.multiple_models arg (=0) --SiftMatching.guided_matching arg (=0) --SiftMatching.planar_scene arg (=0) --ExhaustiveMatching.block_size arg (=50) Mapper Link colmap mapper -h COLMAP 3.8 (Commit 31df46c on 2022-03-05 with CUDA) Options can either be specified via command-line or by defining them in a .ini project file passed to `--project_path`. -h [ --help ] --random_seed arg (=0) --log_to_stderr arg (=0) --log_level arg (=2) --project_path arg --database_path arg --image_path arg --input_path arg --output_path arg --image_list_path arg --Mapper.min_num_matches arg (=15) --Mapper.ignore_watermarks arg (=0) --Mapper.multiple_models arg (=1) --Mapper.max_num_models arg (=50) --Mapper.max_model_overlap arg (=20) --Mapper.min_model_size arg (=10) --Mapper.init_image_id1 arg (=-1) --Mapper.init_image_id2 arg (=-1) --Mapper.init_num_trials arg (=200) --Mapper.extract_colors arg (=1) --Mapper.num_threads arg (=-1) --Mapper.min_focal_length_ratio arg (=0.10000000000000001) --Mapper.max_focal_length_ratio arg (=10) --Mapper.max_extra_param arg (=1) --Mapper.ba_refine_focal_length arg (=1) --Mapper.ba_refine_principal_point arg (=0) --Mapper.ba_refine_extra_params arg (=1) --Mapper.ba_min_num_residuals_for_multi_threading arg (=50000) --Mapper.ba_local_num_images arg (=6) --Mapper.ba_local_function_tolerance arg (=0) --Mapper.ba_local_max_num_iterations arg (=25) --Mapper.ba_global_use_pba arg (=0) --Mapper.ba_global_pba_gpu_index arg (=-1) --Mapper.ba_global_images_ratio arg (=1.1000000000000001) --Mapper.ba_global_points_ratio arg (=1.1000000000000001) --Mapper.ba_global_images_freq arg (=500) --Mapper.ba_global_points_freq arg (=250000) --Mapper.ba_global_function_tolerance arg (=0) --Mapper.ba_global_max_num_iterations arg (=50) --Mapper.ba_global_max_refinements arg (=5) --Mapper.ba_global_max_refinement_change arg (=0.00050000000000000001) --Mapper.ba_local_max_refinements arg (=2) --Mapper.ba_local_max_refinement_change arg (=0.001) --Mapper.snapshot_path arg --Mapper.snapshot_images_freq arg (=0) --Mapper.fix_existing_images arg (=0) --Mapper.init_min_num_inliers arg (=100) --Mapper.init_max_error arg (=4) --Mapper.init_max_forward_motion arg (=0.94999999999999996) --Mapper.init_min_tri_angle arg (=16) --Mapper.init_max_reg_trials arg (=2) --Mapper.abs_pose_max_error arg (=12) --Mapper.abs_pose_min_num_inliers arg (=30) --Mapper.abs_pose_min_inlier_ratio arg (=0.25) --Mapper.filter_max_reproj_error arg (=4) --Mapper.filter_min_tri_angle arg (=1.5) --Mapper.max_reg_trials arg (=3) --Mapper.local_ba_min_tri_angle arg (=6) --Mapper.tri_max_transitivity arg (=1) --Mapper.tri_create_max_angle_error arg (=2) --Mapper.tri_continue_max_angle_error arg (=2) --Mapper.tri_merge_max_reproj_error arg (=4) --Mapper.tri_complete_max_reproj_error arg (=4) --Mapper.tri_complete_max_transitivity arg (=5) --Mapper.tri_re_max_angle_error arg (=5) --Mapper.tri_re_min_ratio arg (=0.20000000000000001) --Mapper.tri_re_max_trials arg (=1) --Mapper.tri_min_angle arg (=1.5) --Mapper.tri_ignore_two_view_tracks arg (=1) Model alignment Link This step is used to \"geo-reference\" the reconstructed model. colmap model_aligner -h COLMAP 3.8 (Commit 31df46c on 2022-03-05 with CUDA) Options can either be specified via command-line or by defining them in a .ini project file passed to `--project_path`. -h [ --help ] --random_seed arg (=0) --log_to_stderr arg (=0) --log_level arg (=2) --project_path arg --input_path arg --output_path arg --database_path arg --ref_images_path arg --ref_is_gps arg (=1) --merge_image_and_ref_origins arg (=0) --transform_path arg --alignment_type arg (=custom) {plane, ecef, enu, enu-plane, enu-plane-unscaled, custom} --min_common_images arg (=3) --estimate_scale arg (=1) --robust_alignment arg (=1) --robust_alignment_max_error arg (=0)","title":"COLMAP"},{"location":"plant_imager/developer/colmap_cli/#colmap-cli","text":"","title":"COLMAP CLI"},{"location":"plant_imager/developer/colmap_cli/#feature-extraction","text":"colmap feature_extractor -h COLMAP 3.8 (Commit 31df46c on 2022-03-05 with CUDA) Options can either be specified via command-line or by defining them in a .ini project file passed to `--project_path`. -h [ --help ] --random_seed arg (=0) --log_to_stderr arg (=0) --log_level arg (=2) --project_path arg --database_path arg --image_path arg --camera_mode arg (=-1) --image_list_path arg --descriptor_normalization arg (=l1_root) {'l1_root', 'l2'} --ImageReader.mask_path arg --ImageReader.camera_model arg (=SIMPLE_RADIAL) --ImageReader.single_camera arg (=0) --ImageReader.single_camera_per_folder arg (=0) --ImageReader.single_camera_per_image arg (=0) --ImageReader.existing_camera_id arg (=-1) --ImageReader.camera_params arg --ImageReader.default_focal_length_factor arg (=1.2) --ImageReader.camera_mask_path arg --SiftExtraction.num_threads arg (=-1) --SiftExtraction.use_gpu arg (=1) --SiftExtraction.gpu_index arg (=-1) --SiftExtraction.max_image_size arg (=3200) --SiftExtraction.max_num_features arg (=8192) --SiftExtraction.first_octave arg (=-1) --SiftExtraction.num_octaves arg (=4) --SiftExtraction.octave_resolution arg (=3) --SiftExtraction.peak_threshold arg (=0.0066666666666666671) --SiftExtraction.edge_threshold arg (=10) --SiftExtraction.estimate_affine_shape arg (=0) --SiftExtraction.max_num_orientations arg (=2) --SiftExtraction.upright arg (=0) --SiftExtraction.domain_size_pooling arg (=0) --SiftExtraction.dsp_min_scale arg (=0.16666666666666666) --SiftExtraction.dsp_max_scale arg (=3) --SiftExtraction.dsp_num_scales arg (=10)","title":"Feature extraction"},{"location":"plant_imager/developer/colmap_cli/#exhaustive-matcher","text":"colmap exhaustive_matcher -h COLMAP 3.8 (Commit 31df46c on 2022-03-05 with CUDA) Options can either be specified via command-line or by defining them in a .ini project file passed to `--project_path`. -h [ --help ] --random_seed arg (=0) --log_to_stderr arg (=0) --log_level arg (=2) --project_path arg --database_path arg --SiftMatching.num_threads arg (=-1) --SiftMatching.use_gpu arg (=1) --SiftMatching.gpu_index arg (=-1) --SiftMatching.max_ratio arg (=0.80000000000000004) --SiftMatching.max_distance arg (=0.69999999999999996) --SiftMatching.cross_check arg (=1) --SiftMatching.max_error arg (=4) --SiftMatching.max_num_matches arg (=32768) --SiftMatching.confidence arg (=0.999) --SiftMatching.max_num_trials arg (=10000) --SiftMatching.min_inlier_ratio arg (=0.25) --SiftMatching.min_num_inliers arg (=15) --SiftMatching.multiple_models arg (=0) --SiftMatching.guided_matching arg (=0) --SiftMatching.planar_scene arg (=0) --ExhaustiveMatching.block_size arg (=50)","title":"Exhaustive matcher"},{"location":"plant_imager/developer/colmap_cli/#mapper","text":"colmap mapper -h COLMAP 3.8 (Commit 31df46c on 2022-03-05 with CUDA) Options can either be specified via command-line or by defining them in a .ini project file passed to `--project_path`. -h [ --help ] --random_seed arg (=0) --log_to_stderr arg (=0) --log_level arg (=2) --project_path arg --database_path arg --image_path arg --input_path arg --output_path arg --image_list_path arg --Mapper.min_num_matches arg (=15) --Mapper.ignore_watermarks arg (=0) --Mapper.multiple_models arg (=1) --Mapper.max_num_models arg (=50) --Mapper.max_model_overlap arg (=20) --Mapper.min_model_size arg (=10) --Mapper.init_image_id1 arg (=-1) --Mapper.init_image_id2 arg (=-1) --Mapper.init_num_trials arg (=200) --Mapper.extract_colors arg (=1) --Mapper.num_threads arg (=-1) --Mapper.min_focal_length_ratio arg (=0.10000000000000001) --Mapper.max_focal_length_ratio arg (=10) --Mapper.max_extra_param arg (=1) --Mapper.ba_refine_focal_length arg (=1) --Mapper.ba_refine_principal_point arg (=0) --Mapper.ba_refine_extra_params arg (=1) --Mapper.ba_min_num_residuals_for_multi_threading arg (=50000) --Mapper.ba_local_num_images arg (=6) --Mapper.ba_local_function_tolerance arg (=0) --Mapper.ba_local_max_num_iterations arg (=25) --Mapper.ba_global_use_pba arg (=0) --Mapper.ba_global_pba_gpu_index arg (=-1) --Mapper.ba_global_images_ratio arg (=1.1000000000000001) --Mapper.ba_global_points_ratio arg (=1.1000000000000001) --Mapper.ba_global_images_freq arg (=500) --Mapper.ba_global_points_freq arg (=250000) --Mapper.ba_global_function_tolerance arg (=0) --Mapper.ba_global_max_num_iterations arg (=50) --Mapper.ba_global_max_refinements arg (=5) --Mapper.ba_global_max_refinement_change arg (=0.00050000000000000001) --Mapper.ba_local_max_refinements arg (=2) --Mapper.ba_local_max_refinement_change arg (=0.001) --Mapper.snapshot_path arg --Mapper.snapshot_images_freq arg (=0) --Mapper.fix_existing_images arg (=0) --Mapper.init_min_num_inliers arg (=100) --Mapper.init_max_error arg (=4) --Mapper.init_max_forward_motion arg (=0.94999999999999996) --Mapper.init_min_tri_angle arg (=16) --Mapper.init_max_reg_trials arg (=2) --Mapper.abs_pose_max_error arg (=12) --Mapper.abs_pose_min_num_inliers arg (=30) --Mapper.abs_pose_min_inlier_ratio arg (=0.25) --Mapper.filter_max_reproj_error arg (=4) --Mapper.filter_min_tri_angle arg (=1.5) --Mapper.max_reg_trials arg (=3) --Mapper.local_ba_min_tri_angle arg (=6) --Mapper.tri_max_transitivity arg (=1) --Mapper.tri_create_max_angle_error arg (=2) --Mapper.tri_continue_max_angle_error arg (=2) --Mapper.tri_merge_max_reproj_error arg (=4) --Mapper.tri_complete_max_reproj_error arg (=4) --Mapper.tri_complete_max_transitivity arg (=5) --Mapper.tri_re_max_angle_error arg (=5) --Mapper.tri_re_min_ratio arg (=0.20000000000000001) --Mapper.tri_re_max_trials arg (=1) --Mapper.tri_min_angle arg (=1.5) --Mapper.tri_ignore_two_view_tracks arg (=1)","title":"Mapper"},{"location":"plant_imager/developer/colmap_cli/#model-alignment","text":"This step is used to \"geo-reference\" the reconstructed model. colmap model_aligner -h COLMAP 3.8 (Commit 31df46c on 2022-03-05 with CUDA) Options can either be specified via command-line or by defining them in a .ini project file passed to `--project_path`. -h [ --help ] --random_seed arg (=0) --log_to_stderr arg (=0) --log_level arg (=2) --project_path arg --input_path arg --output_path arg --database_path arg --ref_images_path arg --ref_is_gps arg (=1) --merge_image_and_ref_origins arg (=0) --transform_path arg --alignment_type arg (=custom) {plane, ecef, enu, enu-plane, enu-plane-unscaled, custom} --min_common_images arg (=3) --estimate_scale arg (=1) --robust_alignment arg (=1) --robust_alignment_max_error arg (=0)","title":"Model alignment"},{"location":"plant_imager/developer/conda/","text":"Conda Link Recipes to build conda packages can be found here . Follow these instructions to build conda packages. Warning Conda packages should be built from the base environment. conda activate base Requirements Link Install conda-build : Link Install conda-build , in the base environment, to be able to build conda package: conda install conda-build WARNING: For macOS, follow these instructions to install the required macOS 10.9 SDK . Optional - Install anaconda-client : Link To be able to upload your package on anaconda cloud you need to install anaconda-client : conda install anaconda-client Build conda packages: Link Build lettucethink : Link Using the given recipe, it is easy to build the lettucethink-python conda package: cd conda_recipes/ conda build lettucethink/ --user romi-eu Build plantdb : Link Using the given recipe, it is easy to build the plantdb conda package: cd conda_recipes/ conda build plantdb/ -c romi-eu -c open3d-admin --user romi-eu Build plant3dvision : Link Using the given recipe, it is easy to build the plant3dvision conda package: cd conda_recipes/ conda build plant3dvision/ -c romi-eu -c conda-forge -c open3d-admin --user romi-eu Build romi-plantviz : Link Using the given recipe, it is easy to build the romi-plantviz conda package: cd conda_recipes/ conda build romi-plantviz/ -c romi-eu -c conda-forge --user romi-eu Optional - Build dirsync package: Link To build dirsync you have to install hgsvn : sudo apt install hgsvn Using the given recipe, it is easy to build the dirsync conda package: cd conda_recipes conda build dirsync/recipe/ --user romi-eu Optional - Build opencv-python package: Link To build opencv-python you have to install qt4-qmake : sudo apt install qt4-qmake qt4-default Using the given recipe, it is easy to build the opencv-python conda package: cd conda_recipes conda build opencv-python/ -c conda-forge --user romi-eu Conda useful commands: Link Purge built packages: Link conda build purge Clean cache & unused packages: Link conda clean --all","title":"Conda"},{"location":"plant_imager/developer/conda/#conda","text":"Recipes to build conda packages can be found here . Follow these instructions to build conda packages. Warning Conda packages should be built from the base environment. conda activate base","title":"Conda"},{"location":"plant_imager/developer/conda/#requirements","text":"","title":"Requirements"},{"location":"plant_imager/developer/conda/#install-conda-build","text":"Install conda-build , in the base environment, to be able to build conda package: conda install conda-build WARNING: For macOS, follow these instructions to install the required macOS 10.9 SDK .","title":"Install conda-build:"},{"location":"plant_imager/developer/conda/#optional-install-anaconda-client","text":"To be able to upload your package on anaconda cloud you need to install anaconda-client : conda install anaconda-client","title":"Optional - Install anaconda-client:"},{"location":"plant_imager/developer/conda/#build-conda-packages","text":"","title":"Build conda packages:"},{"location":"plant_imager/developer/conda/#build-lettucethink","text":"Using the given recipe, it is easy to build the lettucethink-python conda package: cd conda_recipes/ conda build lettucethink/ --user romi-eu","title":"Build lettucethink:"},{"location":"plant_imager/developer/conda/#build-plantdb","text":"Using the given recipe, it is easy to build the plantdb conda package: cd conda_recipes/ conda build plantdb/ -c romi-eu -c open3d-admin --user romi-eu","title":"Build plantdb:"},{"location":"plant_imager/developer/conda/#build-plant3dvision","text":"Using the given recipe, it is easy to build the plant3dvision conda package: cd conda_recipes/ conda build plant3dvision/ -c romi-eu -c conda-forge -c open3d-admin --user romi-eu","title":"Build plant3dvision:"},{"location":"plant_imager/developer/conda/#build-romi-plantviz","text":"Using the given recipe, it is easy to build the romi-plantviz conda package: cd conda_recipes/ conda build romi-plantviz/ -c romi-eu -c conda-forge --user romi-eu","title":"Build romi-plantviz:"},{"location":"plant_imager/developer/conda/#optional-build-dirsync-package","text":"To build dirsync you have to install hgsvn : sudo apt install hgsvn Using the given recipe, it is easy to build the dirsync conda package: cd conda_recipes conda build dirsync/recipe/ --user romi-eu","title":"Optional - Build dirsync package:"},{"location":"plant_imager/developer/conda/#optional-build-opencv-python-package","text":"To build opencv-python you have to install qt4-qmake : sudo apt install qt4-qmake qt4-default Using the given recipe, it is easy to build the opencv-python conda package: cd conda_recipes conda build opencv-python/ -c conda-forge --user romi-eu","title":"Optional - Build opencv-python package:"},{"location":"plant_imager/developer/conda/#conda-useful-commands","text":"","title":"Conda useful commands:"},{"location":"plant_imager/developer/conda/#purge-built-packages","text":"conda build purge","title":"Purge built packages:"},{"location":"plant_imager/developer/conda/#clean-cache-unused-packages","text":"conda clean --all","title":"Clean cache &amp; unused packages:"},{"location":"plant_imager/developer/git_docReview/","text":"Contributing to docs Link From romi-robots-docs , we incorporate changes using a typical git workflow with commits and pull requests. The documentation is generated using MkDocs . Objectives Link At the end of this tutorial, you should be able to: create content for romi-robots-docs review modifications suggested for romi-robots-docs by you colleagues Prerequisite Link Install git Install Mkdocs 1. Install git Link Install git Example in Linux system with a Debian-based distribution (e.g. Ubuntu): git --version #verify that you have git installed: it should return the version (e.g. git version 2.25.1) #if git is not installed: $ sudo apt install git-all git clone romi-robots-docs git clone https://github.com/romi/romi-robots-docs.git cd romi-robots-docs #enter the cloned repository so that all git actions are available 2. Install mkdocs Link install command (e.g. on Linux, see the specific documentation for more detailed instructions ) sudo apt install mkdocs sudo pip3 install mkdocs-material #this extension of mkdocs is required Note: if the second installation fails, you may consider installing or updating pip3: sudo apt-get install python3-pip Then re-run gain the command: sudo pip3 install mkdocs-material Step-by-step tutorial Link if you only review an existing branch (without adding or modifying content), go directly to steps 8/9. 1. Create your local branch Link The default branch is master , that directly incorporate all changes. There is no dev branch. Never ever work on master : create a local branch to make changes, using the following commands: git checkout master # go to master git pull # update it with last changes git checkout -b my_branch # create local branch `my_branch` (it will derived from the last master) git push --set-upstream origin my_branch # attach branch `my_branch` to `origin/my_branch`. GitHub login/password will be asked for. 2. Visualize your changes locally on a web browser in an interactive manner Link mkdocs serve # reads the mkdocs.yml file to generate the web page. The terminal gives you information. The programs starts by building the documentation. As soon as you can read: [I 210323 08:58:22 server:335] Serving on http://127.0.0.1:8000 ..., you can connect your favorite web browser by copy-pasting the url or just (ctrl+click) on the address to open it in your default browser. The program mkdocs serve constantly watches for changes and refreshes the build as soon as they are detected, as indicated by the terminal: [I 210323 08:58:22 handlers:62] Start watching changes INFO - Start watching changes [I 210323 08:58:22 handlers:64] Start detecting changes INFO - Start detecting changes Since the refresh is very rapid upon changes, you can then see in live the effect of you modifications. In the terminal, possible issues are listed (INFO and WARNING),pointing to problems that should be fixed: In this case, pages should be added in the nav section of the mkdocs.yml file (see later point 4.). In the interactive browser, you cannot see and display pages that does not exist in the \"nav\" configuration. internal hyperlink issues (WARNING): WARNING - Documentation file 'xxx/xxxx/file1.md' contains a link to 'xxx/yyy/otherfile.md' which is not found in the documentation files. In this case, check and modify the hyperlink in file1.md to provide good redirection to otherfile.md 3. Adding images in the content Link store the image files in assets/. You can also directly provide html address for third party images if you are sure that the link will be stable over time. To have more flexibility and options for images layout, use the html command in your markdown file: <img src=\"/assets/images/my_image.png\" alt=\"name_displayed_if_error\" width=\"600\" style=\"display:block; margin-left: auto; margin-right: auto;\"> # here the style centers the picture 4. Modify the navigation in mkdocs.yml Link Open mkdocs.yml at the root of romi-robots-docs repo. Some changes must be reported in this file in the nav section: when you create a new page (a new file.md ) or a new directory, or modify the name of an existing file.md/directory. In the nav section, you can also enter the name given to pages in the menu. 5. Commit your changes Link This follow the classical git commit procedure: git status #list all files affected by changes git add/restore/rm <file> #do as many action as listed in red by the previous command ( git status ) #optional: verify that all changes are staged and ready for commit) git commit -m \"my awesome commit\" git push # push modification to `origin/my_branch` ( git log ) #optional: verify that your commit is recorded 6. Merge your working branch with current master: rebase may be needed Link Assuming that your working branch is called my_branch git checkout master #switch to master git pull #update in case changes were made in the meantime: now you have the latest master branch git checkout my_branch #switch again to the branch to merge git rebase master # rebase `master` branch onto `my_branch` if the last command indicates conflicts, it means that master and my_branch have diverged during your work. For each files listed by git: fix the conflicts by directly editing the file stage your changes with: git add file1 continue the rebase with: git rebase --continue Finally, once all conflicts have been resolved and changes staged, Push the rebasing to remote central repo: git push -f origin my_branch #-f (force) implies that login/password will be asked for. 7. Prepare a PR on GitHub webpage Link Go to the distant romi repository : https://github.com/romi/romi-robots-docs select you branch and prepare a PR: open a pull request (green button), enter a brief text to explain the modifications, assign reviewers (in the right column of the page), and press the green button 'create pull request'. 8. test a distant branch (e.g. for a pull request review from your colleagues) Link git checkout test_branch #switch to 'test_branch'. /!\\ do not switch to origin/test_branch since your working locally git fetch #Download objects and refs from the remote branch on /origin git pull #Incorporates changes from the remote repository into the current local `test_branch` mkdocs serve #serve the docs website on the local server You can view the display, test the links, etc... You can also create a new branch from it to modify it. 9. make your review on GitHub web interface Link Comment the pull requests (PR), file by file. Point to issues if any. 10. Use the project board (Kanban type) Link Go to: https://github.com/orgs/romi/projects/10 link your PR to existing issues move the corresponding note to the appropriate column (To do / In progress / Test / Done) TROUBLESHOOTING Link Warning messages Link They points to files reported in the mkdocs.yml but not existing in the current documentation. This is not an rugent issue sine the doculentation can be built despite these raised warnings. Consider modifying the indicated files. Fatal errors occuring after executing mkdocs serve Link module 'materialx.emoji' cannot find module 'materialx.emoji' (No module named 'materialx') solution : execute sudo pip3 install mkdocs-material 'gitsnippet' ERROR - Config value: 'plugins'. Error: The \"gitsnippet\" plugin is not installed solution : execute pip install mkdocs-gitsnippet-plugin 'decorator' ERROR - The 'decorator' distribution was not found and is required by the application solution : execute pip install decorator 'wcwidth' ERROR - The 'wcwidth' distribution was not found and is required by prompt-toolkit solution : execute pip install wcwidth","title":"Contribute to docs"},{"location":"plant_imager/developer/git_docReview/#contributing-to-docs","text":"From romi-robots-docs , we incorporate changes using a typical git workflow with commits and pull requests. The documentation is generated using MkDocs .","title":"Contributing to docs"},{"location":"plant_imager/developer/git_docReview/#objectives","text":"At the end of this tutorial, you should be able to: create content for romi-robots-docs review modifications suggested for romi-robots-docs by you colleagues","title":"Objectives"},{"location":"plant_imager/developer/git_docReview/#prerequisite","text":"Install git Install Mkdocs","title":"Prerequisite"},{"location":"plant_imager/developer/git_docReview/#1-install-git","text":"Install git Example in Linux system with a Debian-based distribution (e.g. Ubuntu): git --version #verify that you have git installed: it should return the version (e.g. git version 2.25.1) #if git is not installed: $ sudo apt install git-all git clone romi-robots-docs git clone https://github.com/romi/romi-robots-docs.git cd romi-robots-docs #enter the cloned repository so that all git actions are available","title":"1. Install git"},{"location":"plant_imager/developer/git_docReview/#2-install-mkdocs","text":"install command (e.g. on Linux, see the specific documentation for more detailed instructions ) sudo apt install mkdocs sudo pip3 install mkdocs-material #this extension of mkdocs is required Note: if the second installation fails, you may consider installing or updating pip3: sudo apt-get install python3-pip Then re-run gain the command: sudo pip3 install mkdocs-material","title":"2. Install mkdocs"},{"location":"plant_imager/developer/git_docReview/#step-by-step-tutorial","text":"if you only review an existing branch (without adding or modifying content), go directly to steps 8/9.","title":"Step-by-step tutorial"},{"location":"plant_imager/developer/git_docReview/#1-create-your-local-branch","text":"The default branch is master , that directly incorporate all changes. There is no dev branch. Never ever work on master : create a local branch to make changes, using the following commands: git checkout master # go to master git pull # update it with last changes git checkout -b my_branch # create local branch `my_branch` (it will derived from the last master) git push --set-upstream origin my_branch # attach branch `my_branch` to `origin/my_branch`. GitHub login/password will be asked for.","title":"1. Create your local branch"},{"location":"plant_imager/developer/git_docReview/#2-visualize-your-changes-locally-on-a-web-browser-in-an-interactive-manner","text":"mkdocs serve # reads the mkdocs.yml file to generate the web page. The terminal gives you information. The programs starts by building the documentation. As soon as you can read: [I 210323 08:58:22 server:335] Serving on http://127.0.0.1:8000 ..., you can connect your favorite web browser by copy-pasting the url or just (ctrl+click) on the address to open it in your default browser. The program mkdocs serve constantly watches for changes and refreshes the build as soon as they are detected, as indicated by the terminal: [I 210323 08:58:22 handlers:62] Start watching changes INFO - Start watching changes [I 210323 08:58:22 handlers:64] Start detecting changes INFO - Start detecting changes Since the refresh is very rapid upon changes, you can then see in live the effect of you modifications. In the terminal, possible issues are listed (INFO and WARNING),pointing to problems that should be fixed: In this case, pages should be added in the nav section of the mkdocs.yml file (see later point 4.). In the interactive browser, you cannot see and display pages that does not exist in the \"nav\" configuration. internal hyperlink issues (WARNING): WARNING - Documentation file 'xxx/xxxx/file1.md' contains a link to 'xxx/yyy/otherfile.md' which is not found in the documentation files. In this case, check and modify the hyperlink in file1.md to provide good redirection to otherfile.md","title":"2. Visualize your changes locally on a web browser in an interactive manner"},{"location":"plant_imager/developer/git_docReview/#3-adding-images-in-the-content","text":"store the image files in assets/. You can also directly provide html address for third party images if you are sure that the link will be stable over time. To have more flexibility and options for images layout, use the html command in your markdown file: <img src=\"/assets/images/my_image.png\" alt=\"name_displayed_if_error\" width=\"600\" style=\"display:block; margin-left: auto; margin-right: auto;\"> # here the style centers the picture","title":"3. Adding images in the content"},{"location":"plant_imager/developer/git_docReview/#4-modify-the-navigation-in-mkdocsyml","text":"Open mkdocs.yml at the root of romi-robots-docs repo. Some changes must be reported in this file in the nav section: when you create a new page (a new file.md ) or a new directory, or modify the name of an existing file.md/directory. In the nav section, you can also enter the name given to pages in the menu.","title":"4. Modify the navigation in mkdocs.yml"},{"location":"plant_imager/developer/git_docReview/#5-commit-your-changes","text":"This follow the classical git commit procedure: git status #list all files affected by changes git add/restore/rm <file> #do as many action as listed in red by the previous command ( git status ) #optional: verify that all changes are staged and ready for commit) git commit -m \"my awesome commit\" git push # push modification to `origin/my_branch` ( git log ) #optional: verify that your commit is recorded","title":"5. Commit your changes"},{"location":"plant_imager/developer/git_docReview/#6-merge-your-working-branch-with-current-master-rebase-may-be-needed","text":"Assuming that your working branch is called my_branch git checkout master #switch to master git pull #update in case changes were made in the meantime: now you have the latest master branch git checkout my_branch #switch again to the branch to merge git rebase master # rebase `master` branch onto `my_branch` if the last command indicates conflicts, it means that master and my_branch have diverged during your work. For each files listed by git: fix the conflicts by directly editing the file stage your changes with: git add file1 continue the rebase with: git rebase --continue Finally, once all conflicts have been resolved and changes staged, Push the rebasing to remote central repo: git push -f origin my_branch #-f (force) implies that login/password will be asked for.","title":"6. Merge your working branch with current master: rebase may be needed"},{"location":"plant_imager/developer/git_docReview/#7-prepare-a-pr-on-github-webpage","text":"Go to the distant romi repository : https://github.com/romi/romi-robots-docs select you branch and prepare a PR: open a pull request (green button), enter a brief text to explain the modifications, assign reviewers (in the right column of the page), and press the green button 'create pull request'.","title":"7. Prepare a PR on GitHub webpage"},{"location":"plant_imager/developer/git_docReview/#8-test-a-distant-branch-eg-for-a-pull-request-review-from-your-colleagues","text":"git checkout test_branch #switch to 'test_branch'. /!\\ do not switch to origin/test_branch since your working locally git fetch #Download objects and refs from the remote branch on /origin git pull #Incorporates changes from the remote repository into the current local `test_branch` mkdocs serve #serve the docs website on the local server You can view the display, test the links, etc... You can also create a new branch from it to modify it.","title":"8. test a distant branch (e.g. for a pull request review from your colleagues)"},{"location":"plant_imager/developer/git_docReview/#9-make-your-review-on-github-web-interface","text":"Comment the pull requests (PR), file by file. Point to issues if any.","title":"9. make your review on GitHub web interface"},{"location":"plant_imager/developer/git_docReview/#10-use-the-project-board-kanban-type","text":"Go to: https://github.com/orgs/romi/projects/10 link your PR to existing issues move the corresponding note to the appropriate column (To do / In progress / Test / Done)","title":"10. Use the project board (Kanban type)"},{"location":"plant_imager/developer/git_docReview/#troubleshooting","text":"","title":"TROUBLESHOOTING"},{"location":"plant_imager/developer/git_docReview/#warning-messages","text":"They points to files reported in the mkdocs.yml but not existing in the current documentation. This is not an rugent issue sine the doculentation can be built despite these raised warnings. Consider modifying the indicated files.","title":"Warning messages"},{"location":"plant_imager/developer/git_docReview/#fatal-errors-occuring-after-executing-mkdocs-serve","text":"module 'materialx.emoji' cannot find module 'materialx.emoji' (No module named 'materialx') solution : execute sudo pip3 install mkdocs-material 'gitsnippet' ERROR - Config value: 'plugins'. Error: The \"gitsnippet\" plugin is not installed solution : execute pip install mkdocs-gitsnippet-plugin 'decorator' ERROR - The 'decorator' distribution was not found and is required by the application solution : execute pip install decorator 'wcwidth' ERROR - The 'wcwidth' distribution was not found and is required by prompt-toolkit solution : execute pip install wcwidth","title":"Fatal errors occuring after executing mkdocs serve"},{"location":"plant_imager/developer/git_submodules_workflow/","text":"Git submodules in plant-3d-vision Link We make use of git submodules in the plant-3d-vision repository to tightly control the version of the other ROMI libraries used as dependencies. Hereafter we detail how to manage those submodules, especially how to update them. Getting started Link Clone the sources Link If you are joining the project start by cloning the sources: git clone https://github.com/romi/plant-3d-vision.git Initialize the submodules Link If you just cloned the repository or if the submodules folders ( romitask , romicgal , plantdb ...) are empty, you have to initialize the submodules in the plant-3d-vision folder with: cd plant-3d-vision git submodule init git submodule update You should now have submodules folders ( romitask , romicgal , plantdb ...) filled with the contents for the associated \"fixed commit\". To know the latest commit associated to a submodule, move to its folder and look-up the git log : cd plant-3d-vision/<submodule_root_dir> git log Tips Press key q to quit the log. Integrate the modifications of a submodule Link For the sake of clarity, let's assume you have worked on the dtw submodule, integrated your changes in the branch <my_branch> (usually main ) and you want to integrate these modifications to plant-3d-vision . On way to do it is this: cd plant-3d-vision/ git pull # It would be better to create an integration branch... cd dtw # could be another submodule than `dtw` git checkout <my_branch> # usually `main` or `dev`/`develop` git pull git log # check this is indeed the last commit that you want to integrate cd .. git status # should see `modifi\u00e9 : dtw (nouveaux commits)` git add dtw git status # should show added `dtw` in green git commit -m \"update dtw submodule\" git push Update the plant-3d-vision integration branch Link From the plant-3d-vision folder, checkout the dev branch (integration branch): git checkout dev git fetch git pull git submodule update git status Update the submodule branch Link To check the latest commit of a submodule do: git log To get the commit short hash: git rev-parse --short = 8 HEAD To update a submodule branch: cd <submodule_root_dir> git checkout <submodule_branch> # or commit short hash git status git log Commit the changes Link You may now commit the changes to plant-3d-vision : git commit -m \"Update romicgal submodule\"","title":"Git Submodules"},{"location":"plant_imager/developer/git_submodules_workflow/#git-submodules-in-plant-3d-vision","text":"We make use of git submodules in the plant-3d-vision repository to tightly control the version of the other ROMI libraries used as dependencies. Hereafter we detail how to manage those submodules, especially how to update them.","title":"Git submodules in plant-3d-vision"},{"location":"plant_imager/developer/git_submodules_workflow/#getting-started","text":"","title":"Getting started"},{"location":"plant_imager/developer/git_submodules_workflow/#clone-the-sources","text":"If you are joining the project start by cloning the sources: git clone https://github.com/romi/plant-3d-vision.git","title":"Clone the sources"},{"location":"plant_imager/developer/git_submodules_workflow/#initialize-the-submodules","text":"If you just cloned the repository or if the submodules folders ( romitask , romicgal , plantdb ...) are empty, you have to initialize the submodules in the plant-3d-vision folder with: cd plant-3d-vision git submodule init git submodule update You should now have submodules folders ( romitask , romicgal , plantdb ...) filled with the contents for the associated \"fixed commit\". To know the latest commit associated to a submodule, move to its folder and look-up the git log : cd plant-3d-vision/<submodule_root_dir> git log Tips Press key q to quit the log.","title":"Initialize the submodules"},{"location":"plant_imager/developer/git_submodules_workflow/#integrate-the-modifications-of-a-submodule","text":"For the sake of clarity, let's assume you have worked on the dtw submodule, integrated your changes in the branch <my_branch> (usually main ) and you want to integrate these modifications to plant-3d-vision . On way to do it is this: cd plant-3d-vision/ git pull # It would be better to create an integration branch... cd dtw # could be another submodule than `dtw` git checkout <my_branch> # usually `main` or `dev`/`develop` git pull git log # check this is indeed the last commit that you want to integrate cd .. git status # should see `modifi\u00e9 : dtw (nouveaux commits)` git add dtw git status # should show added `dtw` in green git commit -m \"update dtw submodule\" git push","title":"Integrate the modifications of a submodule"},{"location":"plant_imager/developer/git_submodules_workflow/#update-the-plant-3d-vision-integration-branch","text":"From the plant-3d-vision folder, checkout the dev branch (integration branch): git checkout dev git fetch git pull git submodule update git status","title":"Update the plant-3d-vision integration branch"},{"location":"plant_imager/developer/git_submodules_workflow/#update-the-submodule-branch","text":"To check the latest commit of a submodule do: git log To get the commit short hash: git rev-parse --short = 8 HEAD To update a submodule branch: cd <submodule_root_dir> git checkout <submodule_branch> # or commit short hash git status git log","title":"Update the submodule branch"},{"location":"plant_imager/developer/git_submodules_workflow/#commit-the-changes","text":"You may now commit the changes to plant-3d-vision : git commit -m \"Update romicgal submodule\"","title":"Commit the changes"},{"location":"plant_imager/developer/git_workflow/","text":"Git Workflow Link Many workflows are possible when using a version control system like git . To clarify how we use it in the ROMI project we hereafter details our choices and show ho to performs the most basic tasks to participate in the development of the ROMI libraries. Rules Link Here are some very important rules, be sure to understand them first! NEVER EVER work on master or dev , always on a branch! ALWAYS rebase your destination branch onto the one you want to merge before doing it! dev is the integration branch, master is the release branch Clone & configure the repository Link It all starts by cloning the repository you want to contribute to, e.g. plant3dvision : git clone https://github.com/romi/plant3dvision.git # clone the repository To use all possible git actions on this repository ('repo'), go the location of this local clone cd plant3dvision #the repo is cloned at the point where you executed the previous command (git clone). If you moved the clone repo, prefix with path like: cd path/to/yourcloned/plant3dvision Create development branch Link To contribute to development you have to create a branch on which you will work. Let's start by pulling the latest developments by updating our local dev branch git checkout dev # switch to your -local- `dev` branch git fetch # fetch changes from remote (`origin/dev`) git pull # pull changes (if any) from remote to local Then create your new branch <my_branch> and set tracking to remote central repo ( origin/<my_branch> ) git checkout -b <my_branch> # create local branch `<my_branch>` git push --set-upstream origin <my_branch> # attach local branch `<my_branch>` to remote `origin/<my_branch>`. Login/password will be asked for. Note Setting the branch tracking can be done later, even after committing changes to local repository! Work on your modifications Link We advise to use a proper IDE (like PyCharm or Atom) with an integrated or plugin based git tool for this part, as manually adding a lot of files can be time-consuming. Overall you will benefit from a nicer and faster integration of this particular step. Nevertheless, for the sake of clarity, hereafter we detail how to do that with the git command-line interface. Tracking new files Link If you create a new file, you will have to tell git to track its changes with: git add <my_new_file.py> Adding changes to local repository Link After editing your files ( e.g. <my_file1.py> <my_file2.py> ), tell git to validate the changes to these files by adding them to the list of tracked changes with: git add <my_file1.py> <my_file2.py> Then commit them to your local repository: git commit -m \"This is my awesome commit!\" Pushing changes to remote repository Link Once you are satisfied with the state of your work, you can push the locally committed changes to the remote central repository: git push # push modification to `origin/<my_branch>` Important Try to do this add/commit/push sequence as often as you can!! How do I not forget changes for committing ? Link Commits that affect only a limited number of files are preferable to track changes and history. However, some work require to modify several files. Especially when working with an IDE allowing easy exploring and modifications of all the repository content, you may forget some changes you did. To quickly identify all current files with uncommitted changes in your current branch, just simply check with git status on your local branch. The terminal lists in a red color all files requiring an action (git add, git restore, git remove): modified files, deleted files, newly added files. Note after acting on listed red files, typing git status should turn all previous files in green. Your branch is ready for committing. Then just proceed as above with git commit and git push . check your commit has been pushed Link After git push , you can get the list of pushed commits related to your current branch with git log (press q to exit the list in the terminal). Prepare your work for merging Link Once you are ready for creating a \"Pull Request\", let's update ( rebase in git) our branch with potential remote changes ( origin/dev ) since branching occurred. Important Start with step 1 & 2 and performs step 3 only if the branch where you are trying to integrate your work have diverged! Get the latest version of origin/dev : git checkout dev # checkout your local `dev` branch git fetch # fetch remote changes git pull # pull remote changes (if any) to local Rebase of origin/dev onto <my_branch> : git checkout <my_branch> # checkout the branch to rebase git rebase dev # rebase `dev` branch onto `<my_branch>` If dev has diverged during your work: if you have conflicts: fix them using an IDE say to git that conflicts are resolved ( e.g. for <my_file1.py> ): git add <my_file1.py> continue rebase until all changes are applied: git rebase --continue # to finish rebase push all changes (your rebased modifications) to the remote repository: git push -f origin <my_branch> Warning Using the -f option is necessary after a rebase as local and remote are now different (as show by a git status ). This will force push the changes done after rebasing. Then you can create your \"Pull Request\" from this latest commit using the GitHub interface. Don't forget to add reviewer. Now if you have a CI job checking the instability and performing tests, you may have to wait it all goes well before merging. Finalization: delete integrated branches. Link Check if you find all yours commits on origin/dev (either on GitHub interface of using git log in this branch), if yes: git branch --delete my_branch # delete local development branch git push origin :my_branch # delete development branch on origin Note git branch -a lists all the local branches first (with the current branch in green), followed by the remote-tracking branches in red. git branch only lists your local branches. Revert a commit Link You can revert the last commit with: git reset HEAD^ Update the project board (Kanban type) Link Go to: https://github.com/orgs/romi/projects choose the project board corresponding to your pull request (PR) link your PR to existing issues move the corresponding note to the appropriate column","title":"Git Workflow"},{"location":"plant_imager/developer/git_workflow/#git-workflow","text":"Many workflows are possible when using a version control system like git . To clarify how we use it in the ROMI project we hereafter details our choices and show ho to performs the most basic tasks to participate in the development of the ROMI libraries.","title":"Git Workflow"},{"location":"plant_imager/developer/git_workflow/#rules","text":"Here are some very important rules, be sure to understand them first! NEVER EVER work on master or dev , always on a branch! ALWAYS rebase your destination branch onto the one you want to merge before doing it! dev is the integration branch, master is the release branch","title":"Rules"},{"location":"plant_imager/developer/git_workflow/#clone-configure-the-repository","text":"It all starts by cloning the repository you want to contribute to, e.g. plant3dvision : git clone https://github.com/romi/plant3dvision.git # clone the repository To use all possible git actions on this repository ('repo'), go the location of this local clone cd plant3dvision #the repo is cloned at the point where you executed the previous command (git clone). If you moved the clone repo, prefix with path like: cd path/to/yourcloned/plant3dvision","title":"Clone &amp; configure the repository"},{"location":"plant_imager/developer/git_workflow/#create-development-branch","text":"To contribute to development you have to create a branch on which you will work. Let's start by pulling the latest developments by updating our local dev branch git checkout dev # switch to your -local- `dev` branch git fetch # fetch changes from remote (`origin/dev`) git pull # pull changes (if any) from remote to local Then create your new branch <my_branch> and set tracking to remote central repo ( origin/<my_branch> ) git checkout -b <my_branch> # create local branch `<my_branch>` git push --set-upstream origin <my_branch> # attach local branch `<my_branch>` to remote `origin/<my_branch>`. Login/password will be asked for. Note Setting the branch tracking can be done later, even after committing changes to local repository!","title":"Create development branch"},{"location":"plant_imager/developer/git_workflow/#work-on-your-modifications","text":"We advise to use a proper IDE (like PyCharm or Atom) with an integrated or plugin based git tool for this part, as manually adding a lot of files can be time-consuming. Overall you will benefit from a nicer and faster integration of this particular step. Nevertheless, for the sake of clarity, hereafter we detail how to do that with the git command-line interface.","title":"Work on your modifications"},{"location":"plant_imager/developer/git_workflow/#tracking-new-files","text":"If you create a new file, you will have to tell git to track its changes with: git add <my_new_file.py>","title":"Tracking new files"},{"location":"plant_imager/developer/git_workflow/#adding-changes-to-local-repository","text":"After editing your files ( e.g. <my_file1.py> <my_file2.py> ), tell git to validate the changes to these files by adding them to the list of tracked changes with: git add <my_file1.py> <my_file2.py> Then commit them to your local repository: git commit -m \"This is my awesome commit!\"","title":"Adding changes to local repository"},{"location":"plant_imager/developer/git_workflow/#pushing-changes-to-remote-repository","text":"Once you are satisfied with the state of your work, you can push the locally committed changes to the remote central repository: git push # push modification to `origin/<my_branch>` Important Try to do this add/commit/push sequence as often as you can!!","title":"Pushing changes to remote repository"},{"location":"plant_imager/developer/git_workflow/#how-do-i-not-forget-changes-for-committing","text":"Commits that affect only a limited number of files are preferable to track changes and history. However, some work require to modify several files. Especially when working with an IDE allowing easy exploring and modifications of all the repository content, you may forget some changes you did. To quickly identify all current files with uncommitted changes in your current branch, just simply check with git status on your local branch. The terminal lists in a red color all files requiring an action (git add, git restore, git remove): modified files, deleted files, newly added files. Note after acting on listed red files, typing git status should turn all previous files in green. Your branch is ready for committing. Then just proceed as above with git commit and git push .","title":"How do I not forget changes for committing ?"},{"location":"plant_imager/developer/git_workflow/#check-your-commit-has-been-pushed","text":"After git push , you can get the list of pushed commits related to your current branch with git log (press q to exit the list in the terminal).","title":"check your commit has been pushed"},{"location":"plant_imager/developer/git_workflow/#prepare-your-work-for-merging","text":"Once you are ready for creating a \"Pull Request\", let's update ( rebase in git) our branch with potential remote changes ( origin/dev ) since branching occurred. Important Start with step 1 & 2 and performs step 3 only if the branch where you are trying to integrate your work have diverged! Get the latest version of origin/dev : git checkout dev # checkout your local `dev` branch git fetch # fetch remote changes git pull # pull remote changes (if any) to local Rebase of origin/dev onto <my_branch> : git checkout <my_branch> # checkout the branch to rebase git rebase dev # rebase `dev` branch onto `<my_branch>` If dev has diverged during your work: if you have conflicts: fix them using an IDE say to git that conflicts are resolved ( e.g. for <my_file1.py> ): git add <my_file1.py> continue rebase until all changes are applied: git rebase --continue # to finish rebase push all changes (your rebased modifications) to the remote repository: git push -f origin <my_branch> Warning Using the -f option is necessary after a rebase as local and remote are now different (as show by a git status ). This will force push the changes done after rebasing. Then you can create your \"Pull Request\" from this latest commit using the GitHub interface. Don't forget to add reviewer. Now if you have a CI job checking the instability and performing tests, you may have to wait it all goes well before merging.","title":"Prepare your work for merging"},{"location":"plant_imager/developer/git_workflow/#finalization-delete-integrated-branches","text":"Check if you find all yours commits on origin/dev (either on GitHub interface of using git log in this branch), if yes: git branch --delete my_branch # delete local development branch git push origin :my_branch # delete development branch on origin Note git branch -a lists all the local branches first (with the current branch in green), followed by the remote-tracking branches in red. git branch only lists your local branches.","title":"Finalization: delete integrated branches."},{"location":"plant_imager/developer/git_workflow/#revert-a-commit","text":"You can revert the last commit with: git reset HEAD^","title":"Revert a commit"},{"location":"plant_imager/developer/git_workflow/#update-the-project-board-kanban-type","text":"Go to: https://github.com/orgs/romi/projects choose the project board corresponding to your pull request (PR) link your PR to existing issues move the corresponding note to the appropriate column","title":"Update the project board (Kanban type)"},{"location":"plant_imager/developer/pipeline_repeatability/","text":"Testing the reconstruction pipelines repeatability Link Objective Link The reconstruction pipeline aims to convert the series of RGB images output of the plant-imager to a reconstructed 3d object, here a plant, with the ultimate goal to obtain quantitative phenotype information. It is composed of a sequence of different tasks, each having a specific function. Some algorithms used during these tasks may be stochastic, hence their output might vary even tough we provide the same input. As it can impact the results of the analysis and is often not easily traceable, it is of interest to be able to quantify it. Two main things can be tested: * With a proper metric, estimate how much different the outputs of a task can be given the same input. * Evaluation of the repercussion in the phenotypic traits extraction of a pipeline including randomness. Prerequisite Link install romi plant-3d-vision (from source or using a docker image ) & read install procedure Create and activate isolated python environment (see the procedure here ) Step-by-step tutorial Link The robustness_evaluation script has been developed to quantify randomness in the pipeline and has 2 modes, it can either test the stochasticity of one task or of the full pipeline. Basically it compares outputs of a task given the same input (previous task output or acquisition output depending on the mode) on a fixed parameterizable number of replicates. robustness_evaluation -h usage: robustness_evaluation [-h] [-n REPLICATE_NUMBER] [-f] [-np] [-db TEST_DATABASE] [--models MODELS] scan {Clean,Colmap,Undistorted,Masks,Segmentation2D,Voxels,PointCloud,TriangleMesh,CurveSkeleton,TreeGraph,AnglesAndInternodes,Segmentation2d,SegmentedPointCloud,ClusteredMesh,OrganSegmentation} config_file ROMI reconstruction & analysis pipeline repeatability test procedure. Analyse the repeatability of a reconstruction & analysis pipeline by: 1. duplicating the scan in a temporary folder (and cleaning it if necessary) 2. running the pipeline up to the previous task of the task to test 3. copying this result to a new database and replicate the dataset 4. repeating the task to test for each replicate 5. comparing the results pair by pair. Comparison can be done at the scale of the files but also with metrics if a reference can be set. To create fully independent tests, we run the pipeline up to the task to test on each replicate. Note that in order to use the ML pipeline, you will first have to: 1. create an output directory 2. use the `--models` argument to copy the CNN trained models required to run the pipeline. positional arguments: scan scan to use for repeatability analysis {Clean,Colmap,Undistorted,Masks,Segmentation2D,Voxels,PointCloud,TriangleMesh,CurveSkeleton,TreeGraph,AnglesAndInternodes,Segmentation2d,SegmentedPointCloud,ClusteredMesh,OrganSegmentation} task to test, should be in: Clean, Colmap, Undistorted, Masks, Segmentation2D, Voxels, PointCloud, TriangleMesh, CurveSkeleton, TreeGraph, AnglesAndInternodes, Segmentation2d, SegmentedPointCloud, ClusteredMesh, OrganSegmentation config_file path to the TOML config file of the analysis pipeline optional arguments: -h, --help show this help message and exit -n REPLICATE_NUMBER, --replicate_number REPLICATE_NUMBER number of replicate to use for repeatability analysis -f, --full_pipe run the analysis pipeline on each replicate independently -np, --no_pipeline do not run the pipeline, only compare tasks outputs -db TEST_DATABASE, --test_database TEST_DATABASE test database location to use. Use at your own risks! --models MODELS models database location to use with ML pipeline. The metrics used are the same as the ones for an evaluation against ground truth 1. Test of a single task Link Example with the task TriangleMesh (whose goal is to compute a mesh from a point cloud): robustness_evaluation /path/db/scan TriangleMesh plant-3d-vision/config/pipeline.toml -n 10 Resulting: path/ \u251c\u2500\u2500 20210628123840_rep_test_TriangleMesh/ \u2502 \u251c\u2500\u2500 scan_0 \u2502 \u251c\u2500\u2500 scan_1 \u2502 \u251c\u2500\u2500 scan_2 \u2502 \u251c\u2500\u2500 scan_3 \u2502 \u251c\u2500\u2500 scan_4 \u2502 \u251c\u2500\u2500 scan_5 \u2502 \u251c\u2500\u2500 scan_6 \u2502 \u251c\u2500\u2500 scan_7 \u2502 \u251c\u2500\u2500 scan_8 \u2502 \u251c\u2500\u2500 scan_9 \u2502 \u251c\u2500\u2500 filebyfile_comparison.json \u2502 \u251c\u2500\u2500 romidb \u2502 \u251c\u2500\u2500 TriangleMesh_comparison.json \u2514\u2500\u2500 db/scan The scan datasets are identical up to PointCloud then the TriangleMesh task is run separately on each one. Results with the appropriate metric are in the TriangleMesh_comparison.json file. 2. Independent tests Link If the goal is to see what are the impacts of randomness through the pipeline in the output of the task TriangleMesh , perform an independent test thanks to the -f parameter: robustness_evaluation /path/db/scan TriangleMesh plant-3d-vision/config/pipeline.toml -n 10 -f With a similar tree result: path/ \u251c\u2500\u2500 20210628123840_rep_test_TriangleMesh/ \u2502 \u251c\u2500\u2500 scan_0 \u2502 \u251c\u2500\u2500 scan_1 \u2502 \u251c\u2500\u2500 scan_2 \u2502 \u251c\u2500\u2500 scan_3 \u2502 \u251c\u2500\u2500 scan_4 \u2502 \u251c\u2500\u2500 scan_5 \u2502 \u251c\u2500\u2500 scan_6 \u2502 \u251c\u2500\u2500 scan_7 \u2502 \u251c\u2500\u2500 scan_8 \u2502 \u251c\u2500\u2500 scan_9 \u2502 \u251c\u2500\u2500 filebyfile_comparison.json \u2502 \u251c\u2500\u2500 romidb \u2502 \u251c\u2500\u2500 TriangleMesh_comparison.json \u2514\u2500\u2500 db/scan Note To run tests on an existing database the -db parameter is configurable but be careful of what is to be tested","title":"Pipelines repeatability"},{"location":"plant_imager/developer/pipeline_repeatability/#testing-the-reconstruction-pipelines-repeatability","text":"","title":"Testing the reconstruction pipelines repeatability"},{"location":"plant_imager/developer/pipeline_repeatability/#objective","text":"The reconstruction pipeline aims to convert the series of RGB images output of the plant-imager to a reconstructed 3d object, here a plant, with the ultimate goal to obtain quantitative phenotype information. It is composed of a sequence of different tasks, each having a specific function. Some algorithms used during these tasks may be stochastic, hence their output might vary even tough we provide the same input. As it can impact the results of the analysis and is often not easily traceable, it is of interest to be able to quantify it. Two main things can be tested: * With a proper metric, estimate how much different the outputs of a task can be given the same input. * Evaluation of the repercussion in the phenotypic traits extraction of a pipeline including randomness.","title":"Objective"},{"location":"plant_imager/developer/pipeline_repeatability/#prerequisite","text":"install romi plant-3d-vision (from source or using a docker image ) & read install procedure Create and activate isolated python environment (see the procedure here )","title":"Prerequisite"},{"location":"plant_imager/developer/pipeline_repeatability/#step-by-step-tutorial","text":"The robustness_evaluation script has been developed to quantify randomness in the pipeline and has 2 modes, it can either test the stochasticity of one task or of the full pipeline. Basically it compares outputs of a task given the same input (previous task output or acquisition output depending on the mode) on a fixed parameterizable number of replicates. robustness_evaluation -h usage: robustness_evaluation [-h] [-n REPLICATE_NUMBER] [-f] [-np] [-db TEST_DATABASE] [--models MODELS] scan {Clean,Colmap,Undistorted,Masks,Segmentation2D,Voxels,PointCloud,TriangleMesh,CurveSkeleton,TreeGraph,AnglesAndInternodes,Segmentation2d,SegmentedPointCloud,ClusteredMesh,OrganSegmentation} config_file ROMI reconstruction & analysis pipeline repeatability test procedure. Analyse the repeatability of a reconstruction & analysis pipeline by: 1. duplicating the scan in a temporary folder (and cleaning it if necessary) 2. running the pipeline up to the previous task of the task to test 3. copying this result to a new database and replicate the dataset 4. repeating the task to test for each replicate 5. comparing the results pair by pair. Comparison can be done at the scale of the files but also with metrics if a reference can be set. To create fully independent tests, we run the pipeline up to the task to test on each replicate. Note that in order to use the ML pipeline, you will first have to: 1. create an output directory 2. use the `--models` argument to copy the CNN trained models required to run the pipeline. positional arguments: scan scan to use for repeatability analysis {Clean,Colmap,Undistorted,Masks,Segmentation2D,Voxels,PointCloud,TriangleMesh,CurveSkeleton,TreeGraph,AnglesAndInternodes,Segmentation2d,SegmentedPointCloud,ClusteredMesh,OrganSegmentation} task to test, should be in: Clean, Colmap, Undistorted, Masks, Segmentation2D, Voxels, PointCloud, TriangleMesh, CurveSkeleton, TreeGraph, AnglesAndInternodes, Segmentation2d, SegmentedPointCloud, ClusteredMesh, OrganSegmentation config_file path to the TOML config file of the analysis pipeline optional arguments: -h, --help show this help message and exit -n REPLICATE_NUMBER, --replicate_number REPLICATE_NUMBER number of replicate to use for repeatability analysis -f, --full_pipe run the analysis pipeline on each replicate independently -np, --no_pipeline do not run the pipeline, only compare tasks outputs -db TEST_DATABASE, --test_database TEST_DATABASE test database location to use. Use at your own risks! --models MODELS models database location to use with ML pipeline. The metrics used are the same as the ones for an evaluation against ground truth","title":"Step-by-step tutorial"},{"location":"plant_imager/developer/pipeline_repeatability/#1-test-of-a-single-task","text":"Example with the task TriangleMesh (whose goal is to compute a mesh from a point cloud): robustness_evaluation /path/db/scan TriangleMesh plant-3d-vision/config/pipeline.toml -n 10 Resulting: path/ \u251c\u2500\u2500 20210628123840_rep_test_TriangleMesh/ \u2502 \u251c\u2500\u2500 scan_0 \u2502 \u251c\u2500\u2500 scan_1 \u2502 \u251c\u2500\u2500 scan_2 \u2502 \u251c\u2500\u2500 scan_3 \u2502 \u251c\u2500\u2500 scan_4 \u2502 \u251c\u2500\u2500 scan_5 \u2502 \u251c\u2500\u2500 scan_6 \u2502 \u251c\u2500\u2500 scan_7 \u2502 \u251c\u2500\u2500 scan_8 \u2502 \u251c\u2500\u2500 scan_9 \u2502 \u251c\u2500\u2500 filebyfile_comparison.json \u2502 \u251c\u2500\u2500 romidb \u2502 \u251c\u2500\u2500 TriangleMesh_comparison.json \u2514\u2500\u2500 db/scan The scan datasets are identical up to PointCloud then the TriangleMesh task is run separately on each one. Results with the appropriate metric are in the TriangleMesh_comparison.json file.","title":"1. Test of a single task"},{"location":"plant_imager/developer/pipeline_repeatability/#2-independent-tests","text":"If the goal is to see what are the impacts of randomness through the pipeline in the output of the task TriangleMesh , perform an independent test thanks to the -f parameter: robustness_evaluation /path/db/scan TriangleMesh plant-3d-vision/config/pipeline.toml -n 10 -f With a similar tree result: path/ \u251c\u2500\u2500 20210628123840_rep_test_TriangleMesh/ \u2502 \u251c\u2500\u2500 scan_0 \u2502 \u251c\u2500\u2500 scan_1 \u2502 \u251c\u2500\u2500 scan_2 \u2502 \u251c\u2500\u2500 scan_3 \u2502 \u251c\u2500\u2500 scan_4 \u2502 \u251c\u2500\u2500 scan_5 \u2502 \u251c\u2500\u2500 scan_6 \u2502 \u251c\u2500\u2500 scan_7 \u2502 \u251c\u2500\u2500 scan_8 \u2502 \u251c\u2500\u2500 scan_9 \u2502 \u251c\u2500\u2500 filebyfile_comparison.json \u2502 \u251c\u2500\u2500 romidb \u2502 \u251c\u2500\u2500 TriangleMesh_comparison.json \u2514\u2500\u2500 db/scan Note To run tests on an existing database the -db parameter is configurable but be careful of what is to be tested","title":"2. Independent tests"},{"location":"plant_imager/developer/tutorial_template/","text":"How to make a good romi tutorial ? Link Format : starts with \"How\", is a question, should be focused to a user-oriented question content : try to build a streamline procedure. If the procedure is too complicated (like different case scenarios, if you need to explain 'if you want to ... and/or if you want to...' ), consider splitting your tutorial in as many tutorials as needed to obtain a streamline procedure. Here a suggested sections for our romi tutorials: Objectives Link At the end of this tutorial, you should be able to: write a good tutorial review others' tutorials Prerequisite Link Installations link(s) to other tutorials Short theoretical primer Link (if needed) Step-by-step tutorial Link 1. This is step 1 Link For non-computer experts, please remember to provide detailed information each time a command line is needed: explain if a new terminal should be open or not ; precise from where the command(s) should be executed ; indicate whether a particular isolated environment should be activated (e.g. conda environment) ; give precise and detailed command lines needed (make sure that all commands are given) cd my repo #precise from where the command should be executed command 1 #code comments are welcome ! command 2 # this is for.... Provide ideas of the expected results to check that the commands were successfully run. examples: a new folder/object is created the terminal says \"useful information\" 2. This is step 2 Link","title":"Make a tutorial"},{"location":"plant_imager/developer/tutorial_template/#how-to-make-a-good-romi-tutorial","text":"Format : starts with \"How\", is a question, should be focused to a user-oriented question content : try to build a streamline procedure. If the procedure is too complicated (like different case scenarios, if you need to explain 'if you want to ... and/or if you want to...' ), consider splitting your tutorial in as many tutorials as needed to obtain a streamline procedure. Here a suggested sections for our romi tutorials:","title":"How to make a good romi tutorial ?"},{"location":"plant_imager/developer/tutorial_template/#objectives","text":"At the end of this tutorial, you should be able to: write a good tutorial review others' tutorials","title":"Objectives"},{"location":"plant_imager/developer/tutorial_template/#prerequisite","text":"Installations link(s) to other tutorials","title":"Prerequisite"},{"location":"plant_imager/developer/tutorial_template/#short-theoretical-primer","text":"(if needed)","title":"Short theoretical primer"},{"location":"plant_imager/developer/tutorial_template/#step-by-step-tutorial","text":"","title":"Step-by-step tutorial"},{"location":"plant_imager/developer/tutorial_template/#1-this-is-step-1","text":"For non-computer experts, please remember to provide detailed information each time a command line is needed: explain if a new terminal should be open or not ; precise from where the command(s) should be executed ; indicate whether a particular isolated environment should be activated (e.g. conda environment) ; give precise and detailed command lines needed (make sure that all commands are given) cd my repo #precise from where the command should be executed command 1 #code comments are welcome ! command 2 # this is for.... Provide ideas of the expected results to check that the commands were successfully run. examples: a new folder/object is created the terminal says \"useful information\"","title":"1. This is step 1"},{"location":"plant_imager/developer/tutorial_template/#2-this-is-step-2","text":"","title":"2. This is step 2"},{"location":"plant_imager/developer/user_case_scenarios/","text":"User case scenarios for the plant scanner Link (or the biologists wish list!) Last edited on 7 Nov 2018. We here describe the scenario followed by a biologist experimenter to acquire and reconstruct the plant architecture using the phenotyping station or 3D scanner. Minimal scenario: command line interface (CLI) Link We here define a minimal scenario using simple CLI to develop and test the workflow. the experimenter prepares the plant if needed; the plant is placed (upright) at the centre of the phenotyping station; the experimenter checks the JSON file defining biological metadata the experimenter starts a \"circular scan\" using a CLI: images are acquired and saved locally (computer controlling the 3D scanner); they are later organised using the Database API; the reconstruction is automatically started after the previous step; Final scenario: graphical user interface (GUI) Link This is the final scenario we want to set up. the experimenter prepares the plant if needed; the plant is placed (upright) at the centre of the phenotyping station; the experimenter login to the GUI, if not done already; under the \"metadata\" tab, the experimenter defines the biological metadata related to its plan and should be able to check the used hardware metadata : he or she defines the biological metadata using predefined fields and values (should be possible to add more); he or she validates by clicking a \"save\" button; under the \"acquisition\" tab, the experimenter defines the acquisition method and settings (the type of scan, number of images, ...) and initiate the acquisition: he or she defines the acquisition settings using predefined fields and values (should be possible to add more); he or she starts this step by clicking an \"acquire\" button; under the \"reconstruction\" tab, the experimenter can access a list of datasets (he owns or accessible to him) and initiate reconstruction(s): he or she can select one dataset (or more?), he or she selects a 3D reconstruction method, he or she defines its settings, he or she starts this step by clicking a \"reconstruct\" button; under the \"quantification\" tab, the experimenter can access a list of 3D structures (he owns or accessible to him) and initiate quantification(s): he or she can select one 3D structure (or more?), he or she selects a quantification method, he or she defines its settings, he or she starts this step by clicking a \"quantify\" button; We would probably need an \"OMERO\" tab to: select/change the \"group\" to which the dataset should be sent to; change the URL and port (change the used OMERO database); change the user logged to the OMERO database;","title":"User case scenarios"},{"location":"plant_imager/developer/user_case_scenarios/#user-case-scenarios-for-the-plant-scanner","text":"(or the biologists wish list!) Last edited on 7 Nov 2018. We here describe the scenario followed by a biologist experimenter to acquire and reconstruct the plant architecture using the phenotyping station or 3D scanner.","title":"User case scenarios for the plant scanner"},{"location":"plant_imager/developer/user_case_scenarios/#minimal-scenario-command-line-interface-cli","text":"We here define a minimal scenario using simple CLI to develop and test the workflow. the experimenter prepares the plant if needed; the plant is placed (upright) at the centre of the phenotyping station; the experimenter checks the JSON file defining biological metadata the experimenter starts a \"circular scan\" using a CLI: images are acquired and saved locally (computer controlling the 3D scanner); they are later organised using the Database API; the reconstruction is automatically started after the previous step;","title":"Minimal scenario: command line interface (CLI)"},{"location":"plant_imager/developer/user_case_scenarios/#final-scenario-graphical-user-interface-gui","text":"This is the final scenario we want to set up. the experimenter prepares the plant if needed; the plant is placed (upright) at the centre of the phenotyping station; the experimenter login to the GUI, if not done already; under the \"metadata\" tab, the experimenter defines the biological metadata related to its plan and should be able to check the used hardware metadata : he or she defines the biological metadata using predefined fields and values (should be possible to add more); he or she validates by clicking a \"save\" button; under the \"acquisition\" tab, the experimenter defines the acquisition method and settings (the type of scan, number of images, ...) and initiate the acquisition: he or she defines the acquisition settings using predefined fields and values (should be possible to add more); he or she starts this step by clicking an \"acquire\" button; under the \"reconstruction\" tab, the experimenter can access a list of datasets (he owns or accessible to him) and initiate reconstruction(s): he or she can select one dataset (or more?), he or she selects a 3D reconstruction method, he or she defines its settings, he or she starts this step by clicking a \"reconstruct\" button; under the \"quantification\" tab, the experimenter can access a list of 3D structures (he owns or accessible to him) and initiate quantification(s): he or she can select one 3D structure (or more?), he or she selects a quantification method, he or she defines its settings, he or she starts this step by clicking a \"quantify\" button; We would probably need an \"OMERO\" tab to: select/change the \"group\" to which the dataset should be sent to; change the URL and port (change the used OMERO database); change the user logged to the OMERO database;","title":"Final scenario: graphical user interface (GUI)"},{"location":"plant_imager/docker/","text":"Docker containers for ROMI Link The official dockerhub repository for the ROMI project is roboticsmicrofarms . List of docker containers Link We hereafter list the docker containers, their availability and provides link to their location & usage instructions: plantdb is available here and explanations there plantimager is not available yet and explanations there plant-3d-vision is available there plant-3d-explorer is available here and explanations there Use cases with docker-compose Link In this section we reference the \"real-life\" use cases of our software. Use the plant 3d explorer on a local database directory Link The easiest way to use the plant-3d-explorer on a local database directory without installing the ROMI libraries (and their dependencies) is to use the pre-built docker image and add a docker-compose YAML recipe. See here for more details. Getting started with docker Link In order to be able to use the ROMI docker images you have to install docker-ce and nvidia-docker2 . Installing docker Link To install docker-ce , please refer to the official documentation . Install nvidia-docker Link To install nvidia-docker2 , please refer to the official documentation . DockerHub Link ROMI repository Link The Docker hub repository for the ROMI project is here: https://hub.docker.com/orgs/roboticsmicrofarms . Colmap Link Docker images for the Colmap open source project: https://hub.docker.com/r/colmap/colmap nvidia/cuda with Colmap - (compatible with Driver Version: 418.67 CUDA Version: 10.1) https://hub.docker.com/r/geki/colmap","title":"Home"},{"location":"plant_imager/docker/#docker-containers-for-romi","text":"The official dockerhub repository for the ROMI project is roboticsmicrofarms .","title":"Docker containers for ROMI"},{"location":"plant_imager/docker/#list-of-docker-containers","text":"We hereafter list the docker containers, their availability and provides link to their location & usage instructions: plantdb is available here and explanations there plantimager is not available yet and explanations there plant-3d-vision is available there plant-3d-explorer is available here and explanations there","title":"List of docker containers"},{"location":"plant_imager/docker/#use-cases-with-docker-compose","text":"In this section we reference the \"real-life\" use cases of our software.","title":"Use cases with docker-compose"},{"location":"plant_imager/docker/#use-the-plant-3d-explorer-on-a-local-database-directory","text":"The easiest way to use the plant-3d-explorer on a local database directory without installing the ROMI libraries (and their dependencies) is to use the pre-built docker image and add a docker-compose YAML recipe. See here for more details.","title":"Use the plant 3d explorer on a local database directory"},{"location":"plant_imager/docker/#getting-started-with-docker","text":"In order to be able to use the ROMI docker images you have to install docker-ce and nvidia-docker2 .","title":"Getting started with docker"},{"location":"plant_imager/docker/#installing-docker","text":"To install docker-ce , please refer to the official documentation .","title":"Installing docker"},{"location":"plant_imager/docker/#install-nvidia-docker","text":"To install nvidia-docker2 , please refer to the official documentation .","title":"Install nvidia-docker"},{"location":"plant_imager/docker/#dockerhub","text":"","title":"DockerHub"},{"location":"plant_imager/docker/#romi-repository","text":"The Docker hub repository for the ROMI project is here: https://hub.docker.com/orgs/roboticsmicrofarms .","title":"ROMI repository"},{"location":"plant_imager/docker/#colmap","text":"Docker images for the Colmap open source project: https://hub.docker.com/r/colmap/colmap nvidia/cuda with Colmap - (compatible with Driver Version: 418.67 CUDA Version: 10.1) https://hub.docker.com/r/geki/colmap","title":"Colmap"},{"location":"plant_imager/docker/colmap/","text":"Colmap docker image Link As we want the possibility to choose the Python version we use, for example 3.8, and the provided docker image are based on Ubuntu 18.04 that ship Python 3.6 & 3.7, we need to create our own Dockerfile . This has been done in docker/colmap/Dockerfile with: Python 3.8 Colmap 3.7 Build the docker image Link To build the docker image, use the Dockerfile in docker/colmap/ : image_name = \"roboticsmicrofarms/colmap\" image_version = \"3.7\" docker build -t = \" $image_name : $image_version \" docker/colmap/. docker tag \" $image_name : $image_version \" \" $image_name :latest\" Test the container Link Let's test the image we just created! Start a container Link You can start by creating a running container with: docker run -it --gpus all \\ -v /tmp/integration_tests/2019-02-01_10-56-33/:/tmp/ \\ roboticsmicrofarms/colmap:3.7 Try to call the colmap executable to get the version number with: colmap -v As we also installed Python, try to call it after activating the venv with: . /venv/bin/activate python -V Get a test dataset Link To further test the built image, let's try to use colmap on a typical set of data. If you do not have your own dataset, we provide a test dataset that you can download (to the temporary folder) as follows: cd /tmp wget https://db.romi-project.eu/models/test_db.tar.gz tar -xf test_db.tar.gz Extract images poses Link We use the \"poses\" (camera locations) provided in the images' metadata (JSON file associated to each images) by the plant-imager to create a poses.txt file containing each image coordinates: import os import json posefile = open ( f \"/tmp/poses.txt\" , mode = 'w' ) # - Try to get the pose from each file metadata: for i , file in enumerate ( sorted ( os . listdir ( \"/tmp/metadata/images\" ))): with open ( f \"/tmp/metadata/images/ { file } \" , mode = 'r' ) as f : jdict = json . load ( f ) # print(jdict) try : p = jdict [ 'approximate_pose' ] except KeyError : p = jdict [ 'pose' ] # backward compatibility, should work for provided test dataset s = ' %s %d %d %d \\n ' % ( file . split ( '.' )[ 0 ] + \".jpg\" , p [ 0 ], p [ 1 ], p [ 2 ]) posefile . write ( s ) posefile . close () Test colmap tools Link You can test that the colmap tools are working properly by calling them as follows: DATASET_PATH = /tmp colmap feature_extractor \\ --database_path $DATASET_PATH /database.db \\ --image_path $DATASET_PATH /images colmap exhaustive_matcher \\ --database_path $DATASET_PATH /database.db mkdir $DATASET_PATH /sparse colmap mapper \\ --database_path $DATASET_PATH /database.db \\ --image_path $DATASET_PATH /images \\ --output_path $DATASET_PATH /sparse colmap model_aligner \\ --ref_images_path $DATASET_PATH /poses.txt \\ --input_path $DATASET_PATH /sparse/0 \\ --output_path $DATASET_PATH /sparse/0 \\ --ref_is_gps 0 \\ --robust_alignment_max_error 10 colmap model_converter \\ --input_path $DATASET_PATH /sparse/0 \\ --output_path $DATASET_PATH /sparse/0/sparse.ply \\ --output_type PLY mkdir $DATASET_PATH /dense colmap image_undistorter \\ --image_path $DATASET_PATH /images \\ --input_path $DATASET_PATH /sparse/0 \\ --output_path $DATASET_PATH /dense \\ --output_type COLMAP \\ --max_image_size 2000 colmap patch_match_stereo \\ --workspace_path $DATASET_PATH /dense \\ --workspace_format COLMAP \\ --PatchMatchStereo.geom_consistency true colmap stereo_fusion \\ --workspace_path $DATASET_PATH /dense \\ --workspace_format COLMAP \\ --input_type geometric \\ --output_path $DATASET_PATH /dense/fused.ply Test geometric pipeline Link If you have plant-3d-vision installed on your machine, you can further test the colmap image with the reconstruction pipelines using our test scripts and datasets. Test it on real data Link export COLMAP_EXE = \"roboticsmicrofarms/colmap\" ./tests/check_geom_pipe.sh --tmp Test it on virtual data Link export COLMAP_EXE = \"roboticsmicrofarms/colmap\" ./tests/check_geom_pipe.sh --tmp --virtual Test it on your (real) data Link export COLMAP_EXE = \"roboticsmicrofarms/colmap\" ./tests/check_geom_pipe.sh --tmp --database /path/to/my/dataset Upload the built image Link Once you have checked the obtained image leads to functional containers, you can upload the image to docker hub! Start by login in to docker hub with: docker login Then you can upload with: docker push \" $image_name \"","title":"Colmap"},{"location":"plant_imager/docker/colmap/#colmap-docker-image","text":"As we want the possibility to choose the Python version we use, for example 3.8, and the provided docker image are based on Ubuntu 18.04 that ship Python 3.6 & 3.7, we need to create our own Dockerfile . This has been done in docker/colmap/Dockerfile with: Python 3.8 Colmap 3.7","title":"Colmap docker image"},{"location":"plant_imager/docker/colmap/#build-the-docker-image","text":"To build the docker image, use the Dockerfile in docker/colmap/ : image_name = \"roboticsmicrofarms/colmap\" image_version = \"3.7\" docker build -t = \" $image_name : $image_version \" docker/colmap/. docker tag \" $image_name : $image_version \" \" $image_name :latest\"","title":"Build the docker image"},{"location":"plant_imager/docker/colmap/#test-the-container","text":"Let's test the image we just created!","title":"Test the container"},{"location":"plant_imager/docker/colmap/#start-a-container","text":"You can start by creating a running container with: docker run -it --gpus all \\ -v /tmp/integration_tests/2019-02-01_10-56-33/:/tmp/ \\ roboticsmicrofarms/colmap:3.7 Try to call the colmap executable to get the version number with: colmap -v As we also installed Python, try to call it after activating the venv with: . /venv/bin/activate python -V","title":"Start a container"},{"location":"plant_imager/docker/colmap/#get-a-test-dataset","text":"To further test the built image, let's try to use colmap on a typical set of data. If you do not have your own dataset, we provide a test dataset that you can download (to the temporary folder) as follows: cd /tmp wget https://db.romi-project.eu/models/test_db.tar.gz tar -xf test_db.tar.gz","title":"Get a test dataset"},{"location":"plant_imager/docker/colmap/#extract-images-poses","text":"We use the \"poses\" (camera locations) provided in the images' metadata (JSON file associated to each images) by the plant-imager to create a poses.txt file containing each image coordinates: import os import json posefile = open ( f \"/tmp/poses.txt\" , mode = 'w' ) # - Try to get the pose from each file metadata: for i , file in enumerate ( sorted ( os . listdir ( \"/tmp/metadata/images\" ))): with open ( f \"/tmp/metadata/images/ { file } \" , mode = 'r' ) as f : jdict = json . load ( f ) # print(jdict) try : p = jdict [ 'approximate_pose' ] except KeyError : p = jdict [ 'pose' ] # backward compatibility, should work for provided test dataset s = ' %s %d %d %d \\n ' % ( file . split ( '.' )[ 0 ] + \".jpg\" , p [ 0 ], p [ 1 ], p [ 2 ]) posefile . write ( s ) posefile . close ()","title":"Extract images poses"},{"location":"plant_imager/docker/colmap/#test-colmap-tools","text":"You can test that the colmap tools are working properly by calling them as follows: DATASET_PATH = /tmp colmap feature_extractor \\ --database_path $DATASET_PATH /database.db \\ --image_path $DATASET_PATH /images colmap exhaustive_matcher \\ --database_path $DATASET_PATH /database.db mkdir $DATASET_PATH /sparse colmap mapper \\ --database_path $DATASET_PATH /database.db \\ --image_path $DATASET_PATH /images \\ --output_path $DATASET_PATH /sparse colmap model_aligner \\ --ref_images_path $DATASET_PATH /poses.txt \\ --input_path $DATASET_PATH /sparse/0 \\ --output_path $DATASET_PATH /sparse/0 \\ --ref_is_gps 0 \\ --robust_alignment_max_error 10 colmap model_converter \\ --input_path $DATASET_PATH /sparse/0 \\ --output_path $DATASET_PATH /sparse/0/sparse.ply \\ --output_type PLY mkdir $DATASET_PATH /dense colmap image_undistorter \\ --image_path $DATASET_PATH /images \\ --input_path $DATASET_PATH /sparse/0 \\ --output_path $DATASET_PATH /dense \\ --output_type COLMAP \\ --max_image_size 2000 colmap patch_match_stereo \\ --workspace_path $DATASET_PATH /dense \\ --workspace_format COLMAP \\ --PatchMatchStereo.geom_consistency true colmap stereo_fusion \\ --workspace_path $DATASET_PATH /dense \\ --workspace_format COLMAP \\ --input_type geometric \\ --output_path $DATASET_PATH /dense/fused.ply","title":"Test colmap tools"},{"location":"plant_imager/docker/colmap/#test-geometric-pipeline","text":"If you have plant-3d-vision installed on your machine, you can further test the colmap image with the reconstruction pipelines using our test scripts and datasets.","title":"Test geometric pipeline"},{"location":"plant_imager/docker/colmap/#test-it-on-real-data","text":"export COLMAP_EXE = \"roboticsmicrofarms/colmap\" ./tests/check_geom_pipe.sh --tmp","title":"Test it on real data"},{"location":"plant_imager/docker/colmap/#test-it-on-virtual-data","text":"export COLMAP_EXE = \"roboticsmicrofarms/colmap\" ./tests/check_geom_pipe.sh --tmp --virtual","title":"Test it on virtual data"},{"location":"plant_imager/docker/colmap/#test-it-on-your-real-data","text":"export COLMAP_EXE = \"roboticsmicrofarms/colmap\" ./tests/check_geom_pipe.sh --tmp --database /path/to/my/dataset","title":"Test it on your (real) data"},{"location":"plant_imager/docker/colmap/#upload-the-built-image","text":"Once you have checked the obtained image leads to functional containers, you can upload the image to docker hub! Start by login in to docker hub with: docker login Then you can upload with: docker push \" $image_name \"","title":"Upload the built image"},{"location":"plant_imager/docker/docker_compose/","text":"Docker compose Link In the following sections we will propose several use cases combining docker images thanks to docker-compose . Note You need docker-compose installed, see here . Database & plant 3D explorer Link To use your own local database, we provide a docker compose recipe that: start a database container using roboticsmicrofarms/plantdb start a plant-3d-explorer container using roboticsmicrofarms/plant-3d-explorer Use pre-built docker image Link You can use the pre-built images plantdb & plantviewer , accessible from the ROMI dockerhub, to easily test & use the plant 3d explorer with your own database 1 . The docker-compose.yml look like this: version : '3' services : db : image : \"roboticsmicrofarms/plantdb\" volumes : - ${ROMI_DB}:/myapp/db expose : - \"5000\" healthcheck : test : \"exit 0\" viewer : image : \"roboticsmicrofarms/plant-3d-explorer\" depends_on : - db environment : REACT_APP_API_URL : http://172.21.0.2:5000 ports : - \"3000:3000\" From the root directory of plant-3d-explorer containing the docker-compose.yml in a terminal: export ROMI_DB = <path/to/db> docker-compose up -d Important Do not forget to set the path to the database. Warning If you have other containers running it might not work since it assumes the plantdb container will have the 172.21.0.2 IP address! To stop the containers: docker-compose stop Note To use local builds, change the image YAML parameter to match your images names & tag. Force local builds Link To force builds at compose startup, for development or debugging purposes, use the build YAML parameter instead of image 2 . It is possible to keep the image YAML parameter to tag the built images 3 . The docker-compose.yml should look like this: version : '3' services : db : build : ../plantdb/. image : db:debug volumes : - ${ROMI_DB}:/myapp/db expose : - \"5000\" healthcheck : test : \"exit 0\" viewer : build : ../developer image : viewer:debug depends_on : - db environment : REACT_APP_API_URL : http://172.21.0.2:5000 ports : - \"3000:3000\" Warning This assumes that you have to plantdb repository cloned next to the one of plant-3d-explorer . https://docs.docker.com/compose/compose-file/#image \u21a9 https://docs.docker.com/compose/gettingstarted/ \u21a9 https://docs.docker.com/compose/compose-file/#build \u21a9","title":"Docker compose"},{"location":"plant_imager/docker/docker_compose/#docker-compose","text":"In the following sections we will propose several use cases combining docker images thanks to docker-compose . Note You need docker-compose installed, see here .","title":"Docker compose"},{"location":"plant_imager/docker/docker_compose/#database-plant-3d-explorer","text":"To use your own local database, we provide a docker compose recipe that: start a database container using roboticsmicrofarms/plantdb start a plant-3d-explorer container using roboticsmicrofarms/plant-3d-explorer","title":"Database &amp; plant 3D explorer"},{"location":"plant_imager/docker/docker_compose/#use-pre-built-docker-image","text":"You can use the pre-built images plantdb & plantviewer , accessible from the ROMI dockerhub, to easily test & use the plant 3d explorer with your own database 1 . The docker-compose.yml look like this: version : '3' services : db : image : \"roboticsmicrofarms/plantdb\" volumes : - ${ROMI_DB}:/myapp/db expose : - \"5000\" healthcheck : test : \"exit 0\" viewer : image : \"roboticsmicrofarms/plant-3d-explorer\" depends_on : - db environment : REACT_APP_API_URL : http://172.21.0.2:5000 ports : - \"3000:3000\" From the root directory of plant-3d-explorer containing the docker-compose.yml in a terminal: export ROMI_DB = <path/to/db> docker-compose up -d Important Do not forget to set the path to the database. Warning If you have other containers running it might not work since it assumes the plantdb container will have the 172.21.0.2 IP address! To stop the containers: docker-compose stop Note To use local builds, change the image YAML parameter to match your images names & tag.","title":"Use pre-built docker image"},{"location":"plant_imager/docker/docker_compose/#force-local-builds","text":"To force builds at compose startup, for development or debugging purposes, use the build YAML parameter instead of image 2 . It is possible to keep the image YAML parameter to tag the built images 3 . The docker-compose.yml should look like this: version : '3' services : db : build : ../plantdb/. image : db:debug volumes : - ${ROMI_DB}:/myapp/db expose : - \"5000\" healthcheck : test : \"exit 0\" viewer : build : ../developer image : viewer:debug depends_on : - db environment : REACT_APP_API_URL : http://172.21.0.2:5000 ports : - \"3000:3000\" Warning This assumes that you have to plantdb repository cloned next to the one of plant-3d-explorer . https://docs.docker.com/compose/compose-file/#image \u21a9 https://docs.docker.com/compose/gettingstarted/ \u21a9 https://docs.docker.com/compose/compose-file/#build \u21a9","title":"Force local builds"},{"location":"plant_imager/docker/plant-3d-vision_docker/","text":"Docker container for ROMI plantinterpreter Link Important An existing local database directory is required , it will be mounted at container startup. To see how to create a local database directory, look here . Use pre-built docker image Link Assuming you have a valid ROMI database directory under /data/ROMI/DB , you can easily download and start the pre-built roboticsmicrofarms/plant-3d-vision docker image with: export ROMI_DB = /data/ROMI/DB docker run --runtime = nvidia --gpus all \\ --env PYOPENCL_CTX = '0' \\ -v $ROMI_DB :/myapp/db \\ -it roboticsmicrofarms/plant-3d-vision:latest This should start the latest pre-built roboticsmicrofarms/plant-3d-vision docker image in interactive mode. The database location inside the docker container is /myapp/db . Note -v $ROMI_DB:/myapp/db performs a bind mount to enable access to the local database by the docker image. See the official documentation . Build docker image Link We provide a convenience bash script to ease the build of roboticsmicrofarms/plant-3d-vision docker image. You can choose to use this script OR to \"manually\" call the docker build command. Provided convenience build.sh script Link To build the image with the provided build script, from the root directory: ./docker/build.sh You can also pass some options, use ./docker/build.sh -h to get more details about usage, options and default values. Tips To be sure to always pull the latest parent image, you may add the --pull option! Manually call the docker build command Link To build the image, from the root directory: export VTAG = \"latest\" docker build -t roboticsmicrofarms/plant-3d-vision: $VTAG . Publish docker image Link Push it on docker hub: docker push roboticsmicrofarms/plantdb: $VTAG This requires a valid account & token on dockerhub! Usage Link Requirements Link To run it, you need to have a valid local ROMI database, look here for instructions and here for an example database. Starting the plant-3d-vision docker image Link Provided run.sh script Link To start the container, in interactive mode, with the provided run script in plant3dvision/docker , use: ./run.sh You can also pass some options, use ./run.sh -h to get more details about usage and options, notably to mount a local romi database. NVIDIA GPU test Link To make sure the started container will be able to access the host GPU, use: ./run.sh --gpu_test Pipelines tests Link To performs test reconstructions, you have several possibilities: test the geometric pipeline: ./run.sh --geom_pipeline_test test the machine learning pipeline: ./run.sh --ml_pipeline_test test both pipelines: ./run.sh --pipeline_test Note This use test data & test models (for ML) provided with plant3dvision in plant3dvision/tests/testdata . Manually Link Assuming you have a valid ROMI database directory under /data/ROMI/DB , you can start the roboticsmicrofarms/plant-3d-vision docker image with: export ROMI_DB = /data/ROMI/DB docker run --runtime = nvidia --gpus all \\ --env PYOPENCL_CTX = '0' \\ -v $ROMI_DB :/myapp/db \\ -it roboticsmicrofarms/plant-3d-vision: $VTAG bash This should start the built roboticsmicrofarms/plant-3d-vision docker image in interactive mode. The database location inside the docker container is ~/db . Note that: you are using the docker image roboticsmicrofarms/plant-3d-vision:$VTAG you mount the host directory $ROMI_DB \"inside\" the running container in the ~/db directory you activate all GPUs within the container with --gpus all declaring the environment variable PYOPENCL_CTX='0' select the first CUDA GPU capable -it & bash returns an interactive bash shell You may want to name the running container (with --name <my_name> ) if you \"demonize\" it (with -d ). Executing a ROMI task Link Once you are inside the running docker container, you may call the ROMI tasks. romi_run_task AnglesAndInternodes db/<my_scan_000>/ --config plant3dvision/configs/original_pipe_0.toml Note You may have to source the .profile file before calling romi_run_task . You can give a command to execute at container start-up using the -c, --cmd option. For example: export ROMI_DB = /data/ROMI/DB ./run.sh -p $ROMI_DB -u scanner -c \"source .profile && romi_run_task AnglesAndInternodes db/<my_scan_000>/ --config plant3dvision/configs/original_pipe_0.toml\" Important source .profile is important to add .local/bin/ to the $PATH environment variable. If you don't do this, you might not be able to access the romi_run_task binary from bash in the docker container.","title":"Plant Interpreter"},{"location":"plant_imager/docker/plant-3d-vision_docker/#docker-container-for-romi-plantinterpreter","text":"Important An existing local database directory is required , it will be mounted at container startup. To see how to create a local database directory, look here .","title":"Docker container for ROMI plantinterpreter"},{"location":"plant_imager/docker/plant-3d-vision_docker/#use-pre-built-docker-image","text":"Assuming you have a valid ROMI database directory under /data/ROMI/DB , you can easily download and start the pre-built roboticsmicrofarms/plant-3d-vision docker image with: export ROMI_DB = /data/ROMI/DB docker run --runtime = nvidia --gpus all \\ --env PYOPENCL_CTX = '0' \\ -v $ROMI_DB :/myapp/db \\ -it roboticsmicrofarms/plant-3d-vision:latest This should start the latest pre-built roboticsmicrofarms/plant-3d-vision docker image in interactive mode. The database location inside the docker container is /myapp/db . Note -v $ROMI_DB:/myapp/db performs a bind mount to enable access to the local database by the docker image. See the official documentation .","title":"Use pre-built docker image"},{"location":"plant_imager/docker/plant-3d-vision_docker/#build-docker-image","text":"We provide a convenience bash script to ease the build of roboticsmicrofarms/plant-3d-vision docker image. You can choose to use this script OR to \"manually\" call the docker build command.","title":"Build docker image"},{"location":"plant_imager/docker/plant-3d-vision_docker/#provided-convenience-buildsh-script","text":"To build the image with the provided build script, from the root directory: ./docker/build.sh You can also pass some options, use ./docker/build.sh -h to get more details about usage, options and default values. Tips To be sure to always pull the latest parent image, you may add the --pull option!","title":"Provided convenience build.sh script"},{"location":"plant_imager/docker/plant-3d-vision_docker/#manually-call-the-docker-build-command","text":"To build the image, from the root directory: export VTAG = \"latest\" docker build -t roboticsmicrofarms/plant-3d-vision: $VTAG .","title":"Manually call the docker build command"},{"location":"plant_imager/docker/plant-3d-vision_docker/#publish-docker-image","text":"Push it on docker hub: docker push roboticsmicrofarms/plantdb: $VTAG This requires a valid account & token on dockerhub!","title":"Publish docker image"},{"location":"plant_imager/docker/plant-3d-vision_docker/#usage","text":"","title":"Usage"},{"location":"plant_imager/docker/plant-3d-vision_docker/#requirements","text":"To run it, you need to have a valid local ROMI database, look here for instructions and here for an example database.","title":"Requirements"},{"location":"plant_imager/docker/plant-3d-vision_docker/#starting-the-plant-3d-vision-docker-image","text":"","title":"Starting the plant-3d-vision docker image"},{"location":"plant_imager/docker/plant-3d-vision_docker/#provided-runsh-script","text":"To start the container, in interactive mode, with the provided run script in plant3dvision/docker , use: ./run.sh You can also pass some options, use ./run.sh -h to get more details about usage and options, notably to mount a local romi database.","title":"Provided run.sh script"},{"location":"plant_imager/docker/plant-3d-vision_docker/#nvidia-gpu-test","text":"To make sure the started container will be able to access the host GPU, use: ./run.sh --gpu_test","title":"NVIDIA GPU test"},{"location":"plant_imager/docker/plant-3d-vision_docker/#pipelines-tests","text":"To performs test reconstructions, you have several possibilities: test the geometric pipeline: ./run.sh --geom_pipeline_test test the machine learning pipeline: ./run.sh --ml_pipeline_test test both pipelines: ./run.sh --pipeline_test Note This use test data & test models (for ML) provided with plant3dvision in plant3dvision/tests/testdata .","title":"Pipelines tests"},{"location":"plant_imager/docker/plant-3d-vision_docker/#manually","text":"Assuming you have a valid ROMI database directory under /data/ROMI/DB , you can start the roboticsmicrofarms/plant-3d-vision docker image with: export ROMI_DB = /data/ROMI/DB docker run --runtime = nvidia --gpus all \\ --env PYOPENCL_CTX = '0' \\ -v $ROMI_DB :/myapp/db \\ -it roboticsmicrofarms/plant-3d-vision: $VTAG bash This should start the built roboticsmicrofarms/plant-3d-vision docker image in interactive mode. The database location inside the docker container is ~/db . Note that: you are using the docker image roboticsmicrofarms/plant-3d-vision:$VTAG you mount the host directory $ROMI_DB \"inside\" the running container in the ~/db directory you activate all GPUs within the container with --gpus all declaring the environment variable PYOPENCL_CTX='0' select the first CUDA GPU capable -it & bash returns an interactive bash shell You may want to name the running container (with --name <my_name> ) if you \"demonize\" it (with -d ).","title":"Manually"},{"location":"plant_imager/docker/plant-3d-vision_docker/#executing-a-romi-task","text":"Once you are inside the running docker container, you may call the ROMI tasks. romi_run_task AnglesAndInternodes db/<my_scan_000>/ --config plant3dvision/configs/original_pipe_0.toml Note You may have to source the .profile file before calling romi_run_task . You can give a command to execute at container start-up using the -c, --cmd option. For example: export ROMI_DB = /data/ROMI/DB ./run.sh -p $ROMI_DB -u scanner -c \"source .profile && romi_run_task AnglesAndInternodes db/<my_scan_000>/ --config plant3dvision/configs/original_pipe_0.toml\" Important source .profile is important to add .local/bin/ to the $PATH environment variable. If you don't do this, you might not be able to access the romi_run_task binary from bash in the docker container.","title":"Executing a ROMI task"},{"location":"plant_imager/docker/plant3dexplorer_docker/","text":"Docker container for ROMI plant 3d explorer Link The plant visualizer is a webapp that dialog with the database to display images & some quantitative traits. It is based on Ubuntu 18.04. Note that we tag the different versions, the default is to use the latest, but you can also specify a specific version by changing the value of the environment variable $VTAG , e.g. export VTAG=\"2.1\" . Look here for a list of available tags: https://hub.docker.com/repository/docker/roboticsmicrofarms/plantviewer Requirements Link The docker image does not contain any plant scans and does not come with a working ROMI local database. To quickly create an example DB you can use: wget https://db.romi-project.eu/models/test_db.tar.gz tar -xf test_db.tar.gz This will create a integration_tests folder with a ready to use test database. To create a local ROMI database: python package install, look here . plantdb docker image, look here . docker-compose YAML recipe (start both a plantdb & a plant-3d-explorer docker image connected to the db), look here . Use pre-built docker image Link You can easily download and start the pre-built plant-3d-explorer docker image with: docker run -p 3000 :3000 roboticsmicrofarms/plant-3d-explorer: $VTAG By default, the docker image will create a container pointing toward the official ROMI database https://db.romi-project.eu . To change that, e.g. to a local running database at '0.0.0.0', do 1 : docker run --env REACT_APP_API_URL = '0.0.0.0' -p 3000 :3000 roboticsmicrofarms/plant-3d-explorer: $VTAG Build docker image Link To build the image, from the plant-3d-explorer root directory, run: export VTAG = \"latest\" docker build -t roboticsmicrofarms/plant-3d-explorer: $VTAG . To start the container using the built image: docker run -p 3000 :3000 roboticsmicrofarms/plant-3d-explorer: $VTAG Once it's up and running, you should be able to access the viewer using a browser here: http://localhost:3000/ Note If you omit the -p 3000:3000 you can still access the interface using the docker ip, something like http://172.17.0.2:3000/ Important Use chrome as firefox has some issues with the used JavaScript libraries! Publish docker image Link To push it on the roboticsmicrofarms docker hub: docker push roboticsmicrofarms/plant-3d-explorer: $VTAG This requires a valid account and token on dockerhub! https://docs.docker.com/engine/reference/commandline/run/#set-environment-variables--e---env---env-file \u21a9","title":"plant-3d-explorer"},{"location":"plant_imager/docker/plant3dexplorer_docker/#docker-container-for-romi-plant-3d-explorer","text":"The plant visualizer is a webapp that dialog with the database to display images & some quantitative traits. It is based on Ubuntu 18.04. Note that we tag the different versions, the default is to use the latest, but you can also specify a specific version by changing the value of the environment variable $VTAG , e.g. export VTAG=\"2.1\" . Look here for a list of available tags: https://hub.docker.com/repository/docker/roboticsmicrofarms/plantviewer","title":"Docker container for ROMI plant 3d explorer"},{"location":"plant_imager/docker/plant3dexplorer_docker/#requirements","text":"The docker image does not contain any plant scans and does not come with a working ROMI local database. To quickly create an example DB you can use: wget https://db.romi-project.eu/models/test_db.tar.gz tar -xf test_db.tar.gz This will create a integration_tests folder with a ready to use test database. To create a local ROMI database: python package install, look here . plantdb docker image, look here . docker-compose YAML recipe (start both a plantdb & a plant-3d-explorer docker image connected to the db), look here .","title":"Requirements"},{"location":"plant_imager/docker/plant3dexplorer_docker/#use-pre-built-docker-image","text":"You can easily download and start the pre-built plant-3d-explorer docker image with: docker run -p 3000 :3000 roboticsmicrofarms/plant-3d-explorer: $VTAG By default, the docker image will create a container pointing toward the official ROMI database https://db.romi-project.eu . To change that, e.g. to a local running database at '0.0.0.0', do 1 : docker run --env REACT_APP_API_URL = '0.0.0.0' -p 3000 :3000 roboticsmicrofarms/plant-3d-explorer: $VTAG","title":"Use pre-built docker image"},{"location":"plant_imager/docker/plant3dexplorer_docker/#build-docker-image","text":"To build the image, from the plant-3d-explorer root directory, run: export VTAG = \"latest\" docker build -t roboticsmicrofarms/plant-3d-explorer: $VTAG . To start the container using the built image: docker run -p 3000 :3000 roboticsmicrofarms/plant-3d-explorer: $VTAG Once it's up and running, you should be able to access the viewer using a browser here: http://localhost:3000/ Note If you omit the -p 3000:3000 you can still access the interface using the docker ip, something like http://172.17.0.2:3000/ Important Use chrome as firefox has some issues with the used JavaScript libraries!","title":"Build docker image"},{"location":"plant_imager/docker/plant3dexplorer_docker/#publish-docker-image","text":"To push it on the roboticsmicrofarms docker hub: docker push roboticsmicrofarms/plant-3d-explorer: $VTAG This requires a valid account and token on dockerhub! https://docs.docker.com/engine/reference/commandline/run/#set-environment-variables--e---env---env-file \u21a9","title":"Publish docker image"},{"location":"plant_imager/docker/plantdb_docker/","text":"Docker container for ROMI database Link Important An existing local database directory is required , it will be mounted at container startup. To see how to create a local database directory, look here . Use pre-built docker image Link Assuming you have a valid ROMI database directory under /data/ROMI/DB , you can easily download and start the pre-built plantdb docker image with: export ROMI_DB = /data/ROMI/DB docker run -it -p 5000 :5000 \\ -v $ROMI_DB :/myapp/db \\ roboticsmicrofarms/plantdb:latest You should be able to access it here: http://localhost:5000/ Note -v $ROMI_DB:/myapp/db performs a bind mount to enable access to the local database by the docker image. See the official documentation . Build docker image Link We provide a convenience bash script to ease the build of plantdb docker image. You can choose to use this script OR to \"manually\" call the docker build command. Provided convenience build.sh script Link To build the image with the provided build script, from the plantdb/docker directory: ./build.sh You can also pass some options, use ./build.sh -h to get more details about usage, options and default values. Manually call the docker build command Link To build the image, from the plantdb root directory: export VTAG = \"latest\" docker build -t roboticsmicrofarms/plantdb: $VTAG . You can use the following optional arguments: --build-arg USER_NAME=<user> : change the default user in container; --build-arg PLANTDB_BRANCH=<git_branch> : change the cloned git branch from plantdb . Publish docker image Link Push it on docker hub: docker push roboticsmicrofarms/plantdb: $VTAG This requires a valid account & token on dockerhub! Usage Link Requirements Link To run it, you need to have a valid local ROMI database, look here for instructions and here for an example database. Starting the plantdb docker image Link Provided run.sh script Link To start the container with the provided run script in plantdb/docker , use: ./run.sh You can also pass some options, use ./run.sh -h to get more details about usage and options. Manually Link Assuming you extracted it in your home folder ( /home/$USER/integration_tests ), you can start the plantdb docker image with: docker run -it -p 5000 :5000 -v /home/ $USER /integration_tests:/myapp/db plantdb: $VTAG In both cases, you should see something like: n scans = 2 * Serving Flask app \"romi_scanner_rest_api\" (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit) Tip -v /home/$USER/integration_tests:/myapp/db performs a bind mount to enable access to the local database by the docker image. See the official Docker documentation . Accessing the REST API Link Once it's up, you should be able to access the REST API here: http://localhost:5000/ To access the REST API, open your favorite browser and use URLs to access: the list of all scans: http://0.0.0.0:5000/scans the '2018-12-17_17-05-35' dataset: http://0.0.0.0:5000/scans/2018-12-17_17-05-35 You should see JSON formatted text.","title":"Database"},{"location":"plant_imager/docker/plantdb_docker/#docker-container-for-romi-database","text":"Important An existing local database directory is required , it will be mounted at container startup. To see how to create a local database directory, look here .","title":"Docker container for ROMI database"},{"location":"plant_imager/docker/plantdb_docker/#use-pre-built-docker-image","text":"Assuming you have a valid ROMI database directory under /data/ROMI/DB , you can easily download and start the pre-built plantdb docker image with: export ROMI_DB = /data/ROMI/DB docker run -it -p 5000 :5000 \\ -v $ROMI_DB :/myapp/db \\ roboticsmicrofarms/plantdb:latest You should be able to access it here: http://localhost:5000/ Note -v $ROMI_DB:/myapp/db performs a bind mount to enable access to the local database by the docker image. See the official documentation .","title":"Use pre-built docker image"},{"location":"plant_imager/docker/plantdb_docker/#build-docker-image","text":"We provide a convenience bash script to ease the build of plantdb docker image. You can choose to use this script OR to \"manually\" call the docker build command.","title":"Build docker image"},{"location":"plant_imager/docker/plantdb_docker/#provided-convenience-buildsh-script","text":"To build the image with the provided build script, from the plantdb/docker directory: ./build.sh You can also pass some options, use ./build.sh -h to get more details about usage, options and default values.","title":"Provided convenience build.sh script"},{"location":"plant_imager/docker/plantdb_docker/#manually-call-the-docker-build-command","text":"To build the image, from the plantdb root directory: export VTAG = \"latest\" docker build -t roboticsmicrofarms/plantdb: $VTAG . You can use the following optional arguments: --build-arg USER_NAME=<user> : change the default user in container; --build-arg PLANTDB_BRANCH=<git_branch> : change the cloned git branch from plantdb .","title":"Manually call the docker build command"},{"location":"plant_imager/docker/plantdb_docker/#publish-docker-image","text":"Push it on docker hub: docker push roboticsmicrofarms/plantdb: $VTAG This requires a valid account & token on dockerhub!","title":"Publish docker image"},{"location":"plant_imager/docker/plantdb_docker/#usage","text":"","title":"Usage"},{"location":"plant_imager/docker/plantdb_docker/#requirements","text":"To run it, you need to have a valid local ROMI database, look here for instructions and here for an example database.","title":"Requirements"},{"location":"plant_imager/docker/plantdb_docker/#starting-the-plantdb-docker-image","text":"","title":"Starting the plantdb docker image"},{"location":"plant_imager/docker/plantdb_docker/#provided-runsh-script","text":"To start the container with the provided run script in plantdb/docker , use: ./run.sh You can also pass some options, use ./run.sh -h to get more details about usage and options.","title":"Provided run.sh script"},{"location":"plant_imager/docker/plantdb_docker/#manually","text":"Assuming you extracted it in your home folder ( /home/$USER/integration_tests ), you can start the plantdb docker image with: docker run -it -p 5000 :5000 -v /home/ $USER /integration_tests:/myapp/db plantdb: $VTAG In both cases, you should see something like: n scans = 2 * Serving Flask app \"romi_scanner_rest_api\" (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit) Tip -v /home/$USER/integration_tests:/myapp/db performs a bind mount to enable access to the local database by the docker image. See the official Docker documentation .","title":"Manually"},{"location":"plant_imager/docker/plantdb_docker/#accessing-the-rest-api","text":"Once it's up, you should be able to access the REST API here: http://localhost:5000/ To access the REST API, open your favorite browser and use URLs to access: the list of all scans: http://0.0.0.0:5000/scans the '2018-12-17_17-05-35' dataset: http://0.0.0.0:5000/scans/2018-12-17_17-05-35 You should see JSON formatted text.","title":"Accessing the REST API"},{"location":"plant_imager/docker/plantimager_docker/","text":"Docker container for ROMI plant-imager Link","title":"Plant Imager"},{"location":"plant_imager/docker/plantimager_docker/#docker-container-for-romi-plant-imager","text":"","title":"Docker container for ROMI plant-imager"},{"location":"plant_imager/docker/virtualplantimager_docker/","text":"Docker container for ROMI virtual plant imager Link Objective Link The following sections aim to show you how to build the docker image and run the corresponding container of the Virtual Plant Imager Prerequisites Link In addition to having docker installed in your system, you must also install the nvidia gpu drivers, nvidia-docker (v2.0) and nvidia-container-toolkit. This docker image has been tested successfully on: docker --version=19.03.6 | nvidia driver version=450.102.04 | CUDA version=11.0 Building the Docker image Link In this repository, you will find a script build.sh in the docker directory. git clone https://github.com/romi/plant-imager.git cd plant-imager/ cd docker/ ./build.sh This will create by default a docker image plantimager:latest . Inside the docker image, a user is created and named as the one currently used by your system. If you want more build options (specific branches, tags...etc), type ./build.sh --help . Running the docker container Link In the docker directory, you will find also a script named run.sh . To show more options, type ./run.sh --help Pre-requisites Link For clarity let us defines some variables here: ROMI_DB : the ROMI database root directory (should contain a plantdb file); ROMI_CFG : the directory containing the ROMI configurations (TOML files); To defines these variable, in a terminal: export ROMI_DB = /data/ROMI/DB export ROMI_CFG = /data/ROMI/configs Get an example archive with arabidopsis model Link Download & extract the example archive at the root directory of the romi database: wget --progress = bar -P $ROMI_DB https://media.romi-project.eu/data/vscan_data.tar.xz tar -C $ROMI_DB / -xvJf $ROMI_DB /vscan_data.tar.xz TOML config Link Use the following configuration, replacing <my_vscan> with the name of the virtual scan dataset to create, e.g. vscan_007 . [ObjFileset] scan_id = \"<my_vscan>\" [HdriFileset] scan_id = \"vscan_data\" [LpyFileset] scan_id = \"vscan_data\" [PaletteFileset] scan_id = \"vscan_data\" [ScanPath] class_name = \"Circle\" [ScanPath.kwargs] center_x = -2 center_y = 3 z = 34.17519302880196 tilt = 8 radius = 30 n_points = 72 [VirtualScan] obj_fileset = \"ObjFileset\" use_palette = true use_hdri = true load_scene = false scene_file_id = \"pot\" render_ground_truth = true [VirtualScan.scanner] width = 896 height = 896 focal = 24 flash = true add_leaf_displacement = true [Voxels] type = \"averaging\" voxel_size = 0.05 Virtual scan of a model plant Link Start the docker container Link Use the roboticsmicrofarms/plantimager docker image: export ROMI_DB = /data/ROMI/DB export ROMI_CFG = /data/ROMI/configs docker run --runtime = nvidia --gpus all \\ -v $ROMI_DB :/myapp/db \\ -v $ROMI_CFG :/myapp/configs \\ -it roboticsmicrofarms/plantimager:latest bash Initialize a scan dataset Link Use the romi_import_folder tool to import the required data into a new scan dataset, e.g. vscan_007 : romi_import_folder ~/db/vscan_data/data/ ~/db/vscan_007/ --metadata ~/db/vscan_data/files.json Start a VirtualScan romi task Link cd plantimager/bin romi_run_task VirtualScan ~/db/vscan_007 --config ~/plantimager/config/vscan_obj.toml","title":"Virtual Plant Imager"},{"location":"plant_imager/docker/virtualplantimager_docker/#docker-container-for-romi-virtual-plant-imager","text":"","title":"Docker container for ROMI virtual plant imager"},{"location":"plant_imager/docker/virtualplantimager_docker/#objective","text":"The following sections aim to show you how to build the docker image and run the corresponding container of the Virtual Plant Imager","title":"Objective"},{"location":"plant_imager/docker/virtualplantimager_docker/#prerequisites","text":"In addition to having docker installed in your system, you must also install the nvidia gpu drivers, nvidia-docker (v2.0) and nvidia-container-toolkit. This docker image has been tested successfully on: docker --version=19.03.6 | nvidia driver version=450.102.04 | CUDA version=11.0","title":"Prerequisites"},{"location":"plant_imager/docker/virtualplantimager_docker/#building-the-docker-image","text":"In this repository, you will find a script build.sh in the docker directory. git clone https://github.com/romi/plant-imager.git cd plant-imager/ cd docker/ ./build.sh This will create by default a docker image plantimager:latest . Inside the docker image, a user is created and named as the one currently used by your system. If you want more build options (specific branches, tags...etc), type ./build.sh --help .","title":"Building the Docker image"},{"location":"plant_imager/docker/virtualplantimager_docker/#running-the-docker-container","text":"In the docker directory, you will find also a script named run.sh . To show more options, type ./run.sh --help","title":"Running the docker container"},{"location":"plant_imager/docker/virtualplantimager_docker/#pre-requisites","text":"For clarity let us defines some variables here: ROMI_DB : the ROMI database root directory (should contain a plantdb file); ROMI_CFG : the directory containing the ROMI configurations (TOML files); To defines these variable, in a terminal: export ROMI_DB = /data/ROMI/DB export ROMI_CFG = /data/ROMI/configs","title":"Pre-requisites"},{"location":"plant_imager/docker/virtualplantimager_docker/#get-an-example-archive-with-arabidopsis-model","text":"Download & extract the example archive at the root directory of the romi database: wget --progress = bar -P $ROMI_DB https://media.romi-project.eu/data/vscan_data.tar.xz tar -C $ROMI_DB / -xvJf $ROMI_DB /vscan_data.tar.xz","title":"Get an example archive with arabidopsis model"},{"location":"plant_imager/docker/virtualplantimager_docker/#toml-config","text":"Use the following configuration, replacing <my_vscan> with the name of the virtual scan dataset to create, e.g. vscan_007 . [ObjFileset] scan_id = \"<my_vscan>\" [HdriFileset] scan_id = \"vscan_data\" [LpyFileset] scan_id = \"vscan_data\" [PaletteFileset] scan_id = \"vscan_data\" [ScanPath] class_name = \"Circle\" [ScanPath.kwargs] center_x = -2 center_y = 3 z = 34.17519302880196 tilt = 8 radius = 30 n_points = 72 [VirtualScan] obj_fileset = \"ObjFileset\" use_palette = true use_hdri = true load_scene = false scene_file_id = \"pot\" render_ground_truth = true [VirtualScan.scanner] width = 896 height = 896 focal = 24 flash = true add_leaf_displacement = true [Voxels] type = \"averaging\" voxel_size = 0.05","title":"TOML config"},{"location":"plant_imager/docker/virtualplantimager_docker/#virtual-scan-of-a-model-plant","text":"","title":"Virtual scan of a model plant"},{"location":"plant_imager/docker/virtualplantimager_docker/#start-the-docker-container","text":"Use the roboticsmicrofarms/plantimager docker image: export ROMI_DB = /data/ROMI/DB export ROMI_CFG = /data/ROMI/configs docker run --runtime = nvidia --gpus all \\ -v $ROMI_DB :/myapp/db \\ -v $ROMI_CFG :/myapp/configs \\ -it roboticsmicrofarms/plantimager:latest bash","title":"Start the docker container"},{"location":"plant_imager/docker/virtualplantimager_docker/#initialize-a-scan-dataset","text":"Use the romi_import_folder tool to import the required data into a new scan dataset, e.g. vscan_007 : romi_import_folder ~/db/vscan_data/data/ ~/db/vscan_007/ --metadata ~/db/vscan_data/files.json","title":"Initialize a scan dataset"},{"location":"plant_imager/docker/virtualplantimager_docker/#start-a-virtualscan-romi-task","text":"cd plantimager/bin romi_run_task VirtualScan ~/db/vscan_007 --config ~/plantimager/config/vscan_obj.toml","title":"Start a VirtualScan romi task"},{"location":"plant_imager/explanations/general_design/","text":"Overview of the modules interactions Link The following figure shows a use case of the ROMI modules, and the way they interact, to design an efficient plant phenotyping platform used in research. PlantDB Link Should be totally independent of the rest since it could be uses in other parts of the ROMI project (Rover, Cable bot, ...) through the abstract class DB or even the local database class FSDB . Plant Imager Link It requires a physical connection to the hardware ( pyserial ) to control. It also needs an active ROMI database to export acquired datasets (plant images). Virtual Plant Imager Link It requires a connection to an active ROMI database to export generated datasets (virtual plant images). In case of machine learning methods, a database would also provide training datasets. Plant 3D Vision Link It requires connection to an active ROMI database to import datasets to process and export the results. Two plant reconstruction approaches are available in the SmartInterpreter: Geometry based, try to infer the plant's geometry using structure from motion algorithms and space carving to first reconstruct a point cloud. Machine learning based, try to infer the plant's geometry using semantic (organ) segmentation of pictures and space carving to first reconstruct a labelled point cloud. Then meshing and skeletonization finally enables to extract the plant's phyllotaxis. Plant 3D Explorer Link It requires a database with datasets to browse and represent. Research oriented user story Link The user put his/her plant inside the scanner and run acquisitions , which returns a set of images per plant. These images are uploaded to a central database . The user defines a pipeline to reconstruct and quantify plants architecture by choosing among a set of predefined methods and algorithms. These instructions may be run by a distant server. Finally, the user can access the acquisitions, reconstructions & quantitative data by connecting to a visualization server using his/her computer","title":"General design"},{"location":"plant_imager/explanations/general_design/#overview-of-the-modules-interactions","text":"The following figure shows a use case of the ROMI modules, and the way they interact, to design an efficient plant phenotyping platform used in research.","title":"Overview of the modules interactions"},{"location":"plant_imager/explanations/general_design/#plantdb","text":"Should be totally independent of the rest since it could be uses in other parts of the ROMI project (Rover, Cable bot, ...) through the abstract class DB or even the local database class FSDB .","title":"PlantDB"},{"location":"plant_imager/explanations/general_design/#plant-imager","text":"It requires a physical connection to the hardware ( pyserial ) to control. It also needs an active ROMI database to export acquired datasets (plant images).","title":"Plant Imager"},{"location":"plant_imager/explanations/general_design/#virtual-plant-imager","text":"It requires a connection to an active ROMI database to export generated datasets (virtual plant images). In case of machine learning methods, a database would also provide training datasets.","title":"Virtual Plant Imager"},{"location":"plant_imager/explanations/general_design/#plant-3d-vision","text":"It requires connection to an active ROMI database to import datasets to process and export the results. Two plant reconstruction approaches are available in the SmartInterpreter: Geometry based, try to infer the plant's geometry using structure from motion algorithms and space carving to first reconstruct a point cloud. Machine learning based, try to infer the plant's geometry using semantic (organ) segmentation of pictures and space carving to first reconstruct a labelled point cloud. Then meshing and skeletonization finally enables to extract the plant's phyllotaxis.","title":"Plant 3D Vision"},{"location":"plant_imager/explanations/general_design/#plant-3d-explorer","text":"It requires a database with datasets to browse and represent.","title":"Plant 3D Explorer"},{"location":"plant_imager/explanations/general_design/#research-oriented-user-story","text":"The user put his/her plant inside the scanner and run acquisitions , which returns a set of images per plant. These images are uploaded to a central database . The user defines a pipeline to reconstruct and quantify plants architecture by choosing among a set of predefined methods and algorithms. These instructions may be run by a distant server. Finally, the user can access the acquisitions, reconstructions & quantitative data by connecting to a visualization server using his/her computer","title":"Research oriented user story"},{"location":"plant_imager/explanations/masks/","text":"Masks Link Here we show the effects of the type of filter to use in the Mask task. Method linear Link The image have been generated with the following python code: import numpy as np import matplotlib.pyplot as plt from imageio import imread from skimage.exposure import rescale_intensity from plant3dvision import test_db_path from plant3dvision.proc2d import linear , dilation path = test_db_path () im = imread ( path . joinpath ( 'real_plant/images/00000_rgb.jpg' )) im = np . asarray ( im , dtype = float ) # transform the uint8 RGB image into a float RGB numpy array im = rescale_intensity ( im , out_range = ( 0. , 1. )) # rescale to [0., 1.] threshold = 0.4 filter = linear ( im , [ 0. , 1. , 0. ]) # apply `linear` filter mask = filter > threshold # convert to binary mask using threshold mask = dilation ( mask , 2 ) # apply a dilation to binary mask # Plot the original image, filtered image and binary mask fig , ax = plt . subplots ( 1 , 3 ) ax [ 0 ] . imshow ( im ) ax [ 0 ] . set_title ( \"Original image\" ) ax [ 1 ] . imshow ( filter ) ax [ 1 ] . set_title ( \"Filtered image\" ) ax [ 2 ] . imshow ( mask , cmap = 'gray' ) ax [ 2 ] . set_title ( \"Mask image\" ) Important Pay attention to the values used for threshold and dilation . Method excess_green Link The image have been generated with the following python code: import numpy as np import matplotlib.pyplot as plt from imageio import imread from skimage.exposure import rescale_intensity from plant3dvision import test_db_path from plant3dvision.proc2d import excess_green , dilation path = test_db_path () im = imread ( path . joinpath ( 'real_plant/images/00000_rgb.jpg' )) im = np . asarray ( im , dtype = float ) # transform the uint8 RGB image into a float RGB numpy array im = rescale_intensity ( im , out_range = ( 0. , 1. )) # rescale to [0., 1.] threshold = 0.4 filter = excess_green ( im ) # apply `excess_green` filter mask = filter > threshold # convert to binary mask using threshold mask = dilation ( mask , 2 ) # apply a dilation to binary mask # Plot the original image, filtered image and binary mask fig , ax = plt . subplots ( 1 , 3 ) ax [ 0 ] . imshow ( im ) ax [ 0 ] . set_title ( \"Original image\" ) ax [ 1 ] . imshow ( filter ) ax [ 1 ] . set_title ( \"Filtered image\" ) ax [ 2 ] . imshow ( mask , cmap = 'gray' ) ax [ 2 ] . set_title ( \"Mask image\" ) Important Pay attention to the values used for threshold and dilation .","title":"Mask"},{"location":"plant_imager/explanations/masks/#masks","text":"Here we show the effects of the type of filter to use in the Mask task.","title":"Masks"},{"location":"plant_imager/explanations/masks/#method-linear","text":"The image have been generated with the following python code: import numpy as np import matplotlib.pyplot as plt from imageio import imread from skimage.exposure import rescale_intensity from plant3dvision import test_db_path from plant3dvision.proc2d import linear , dilation path = test_db_path () im = imread ( path . joinpath ( 'real_plant/images/00000_rgb.jpg' )) im = np . asarray ( im , dtype = float ) # transform the uint8 RGB image into a float RGB numpy array im = rescale_intensity ( im , out_range = ( 0. , 1. )) # rescale to [0., 1.] threshold = 0.4 filter = linear ( im , [ 0. , 1. , 0. ]) # apply `linear` filter mask = filter > threshold # convert to binary mask using threshold mask = dilation ( mask , 2 ) # apply a dilation to binary mask # Plot the original image, filtered image and binary mask fig , ax = plt . subplots ( 1 , 3 ) ax [ 0 ] . imshow ( im ) ax [ 0 ] . set_title ( \"Original image\" ) ax [ 1 ] . imshow ( filter ) ax [ 1 ] . set_title ( \"Filtered image\" ) ax [ 2 ] . imshow ( mask , cmap = 'gray' ) ax [ 2 ] . set_title ( \"Mask image\" ) Important Pay attention to the values used for threshold and dilation .","title":"Method linear"},{"location":"plant_imager/explanations/masks/#method-excess_green","text":"The image have been generated with the following python code: import numpy as np import matplotlib.pyplot as plt from imageio import imread from skimage.exposure import rescale_intensity from plant3dvision import test_db_path from plant3dvision.proc2d import excess_green , dilation path = test_db_path () im = imread ( path . joinpath ( 'real_plant/images/00000_rgb.jpg' )) im = np . asarray ( im , dtype = float ) # transform the uint8 RGB image into a float RGB numpy array im = rescale_intensity ( im , out_range = ( 0. , 1. )) # rescale to [0., 1.] threshold = 0.4 filter = excess_green ( im ) # apply `excess_green` filter mask = filter > threshold # convert to binary mask using threshold mask = dilation ( mask , 2 ) # apply a dilation to binary mask # Plot the original image, filtered image and binary mask fig , ax = plt . subplots ( 1 , 3 ) ax [ 0 ] . imshow ( im ) ax [ 0 ] . set_title ( \"Original image\" ) ax [ 1 ] . imshow ( filter ) ax [ 1 ] . set_title ( \"Filtered image\" ) ax [ 2 ] . imshow ( mask , cmap = 'gray' ) ax [ 2 ] . set_title ( \"Mask image\" ) Important Pay attention to the values used for threshold and dilation .","title":"Method excess_green"},{"location":"plant_imager/explanations/segmentation/","text":"Segmentation of images Link The segmentation of an image consists in assigning a label to each of its pixels. For the 3d reconstruction of a plant, we need at least the segmentation of the images into 2 classes: plant and background . For a reconstruction with semantic labeling of the point cloud, we will need a semantic segmentation of the images giving one label for each organ type (e.g. { leaf , stem , pedicel , flower , fruit }). The figure below shows the binary and multi-class segmentations for a virtual plant. Example image of virtual arabidopsis (left) with binary (middle) and multi-class segmentation (right). Binary segmentation Link The binary segmentation of an image into plant and background is performed with the following command: romi_run_task Masks scan_id --config myconfig.toml with upstream task being ImagesFilesetExists when processing the raw RGB images or Undistorded when processing images corrected using the intrinsic parameters of the camera. The task takes this set of images as an input and produce one binary mask for each image. There are 2 methods available to compute indices for binary segmentation: Excess Green Index and Linear SVM. For each method, we provide an example configuration file in the Index computation section. Index computation Link Linear support vector machine (SVM) Link A linear combination of R, G and B is used to compute the index for pixel (i,j) (i,j) : S_{ij}=w_0 R_{ij} + w_1 G_{ij} +w_2 B_{ij} S_{ij}=w_0 R_{ij} + w_1 G_{ij} +w_2 B_{ij} where w w is the parameters vector specified in the configuration file. A simple vector, like w=(0,1,0) w=(0,1,0) may be used for example. Alternatively, you can train an SVM to learn those weights, and the threshold to be provided in the configuration file. For this, we consider you have a sample image and a ground truth binary mask. A ground truth may be produced using a manual annotation tool like LabelMe . Using for example a list of N randomly selected pixels as X_{train} X_{train} (array of size [N,3]) and their corresponding labels as Y_{train} Y_{train} (array of size N), a linear SVM is trained using from sklearn import svm X_train , Y_train = ... clf = svm . SVC ( kernel = 'linear' ) clf . fit ( X_train , y_train ) the weights can then be retrieved as clf.coef_ and the threshold as -clf.intercept_ Configuration file Link [Masks] upstream_task = \"ImagesFilesetExists\" # other option \"Undistorted\" type = \"linear\" parameters = \"[0,1,0]\" threshold = 0.5 Excess green Link This segmentation method is assuming the plant is green and the background is not. It has no parameter but it may be less robust than the linear SVM. We compute the normalized RGB values x \\in {r,g,b} x \\in {r,g,b} for each pixel (i,j) (i,j) : x_{ij} = \\frac{X_{ij}}{R_{ij}+G_{ij}+B_{ij}} x_{ij} = \\frac{X_{ij}}{R_{ij}+G_{ij}+B_{ij}} where X \\in {R, G, B} X \\in {R, G, B} is the red, green or blue image Then, the green excess index is computed as: \\operatorname{ExG}=2g-r-b \\operatorname{ExG}=2g-r-b Configuration file Link [Masks] upstream_task = \"ImagesFilesetExists\" # other option \"Undistorted\" type = \"excess_green\" threshold = 0.2 Multi-class segmentation Link The Segmentation2D task performs the semantic segmentation of images using a deep neural network (DNN). The command to run this task is: romi_run_task Segmentation2D scan_id my_config.toml This will produce a series of binary masks, one for each class on which the network was trained. Generic encoder/decoder architecture for semantic segmentation (U-net). The architecture of the network is inspired from the U-net 1 , with a ResNet encoder 2 . It consists in encoding and decoding pathways with skip connections between the 2. Along the encoding pathways, there is a sequence of convolutions and the image signal is upsampled along the decoding pathway. The network is trained for segmenting images of a size (S_x,S_y) (S_x,S_y) which is not necessarily the image size of the acquired images. Those parameters Sx and Sy should be provided in the configuration file. The images will be cropped to (S_x,S_y) (S_x,S_y) before being fed to the DNN and it is then resized to the original size as an output of the task. Configuration File Link [Segmentation2D] model_id = \"Resnetdataset_gl_png_896_896_epoch50\" # no default value Sx = 896 Sy = 896 threshold = 0.01 DNN model Link The neural architecture weights are obtained through training on an annotated dataset (see How to train a DNN for semantic segmentation). Those weights should be stored in the database (at <database>/models/models ) and the name of the weights file should be provided as the model_id parameter in the configuration. You can use our model trained on virtual arabidopsis here Binarization Link A binary mask m m is produced from the index or from the output of the DNN, I , by applying a threshold \\theta \\theta on I for each pixel (i,j) (i,j) : \\begin{equation} m_{ij} = \\begin{cases} 255 & \\text{if $I_{ij}>\\theta$}\\\\ 0 & \\text{otherwise} \\end{cases} \\end{equation} \\begin{equation} m_{ij} = \\begin{cases} 255 & \\text{if $I_{ij}>\\theta$}\\\\ 0 & \\text{otherwise} \\end{cases} \\end{equation} This threshold may be chosen empirically, or it may be learnt from annotated data (see linear SVM section). Dilation Link If the integer dilation parameter is non-zero a morphological dilation is applied to the image using the function binary_dilation from the skimage.morphology module. The dilation parameter sets the number of times binary_dilation is iteratively applied. For a faithful reconstruction this parameter should be set to 0 0 but in practice you may want to have a coarser point cloud. This is true when your segmentation is not perfect, dilation will fill the holes or when the reconstructed mesh is broken because the point-cloud is too thin. Working with data from the virtual scanner Link When working with data generated with the virtual scanner, the images folder contains multiple channels corresponding to the various class for which images were generated ( stem , flower , fruit , leaf , pedicel ). You have to select the rgb channel using the query parameter. Configuration File Link [Masks] type = \"excess_green\" threshold = 0.2 query = \"{\\\"channel\\\":\\\"rgb\\\"}\" Ronneberger, O., Fischer, P., & Brox, T. (2015, October). U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention (pp. 234-241). Springer, Cham. \u21a9 Zhang, Z., Liu, Q., & Wang, Y. (2018). Road extraction by deep residual u-net. IEEE Geoscience and Remote Sensing Letters, 15(5), 749-753. \u21a9","title":"Segmentation of images"},{"location":"plant_imager/explanations/segmentation/#segmentation-of-images","text":"The segmentation of an image consists in assigning a label to each of its pixels. For the 3d reconstruction of a plant, we need at least the segmentation of the images into 2 classes: plant and background . For a reconstruction with semantic labeling of the point cloud, we will need a semantic segmentation of the images giving one label for each organ type (e.g. { leaf , stem , pedicel , flower , fruit }). The figure below shows the binary and multi-class segmentations for a virtual plant. Example image of virtual arabidopsis (left) with binary (middle) and multi-class segmentation (right).","title":"Segmentation of images"},{"location":"plant_imager/explanations/segmentation/#binary-segmentation","text":"The binary segmentation of an image into plant and background is performed with the following command: romi_run_task Masks scan_id --config myconfig.toml with upstream task being ImagesFilesetExists when processing the raw RGB images or Undistorded when processing images corrected using the intrinsic parameters of the camera. The task takes this set of images as an input and produce one binary mask for each image. There are 2 methods available to compute indices for binary segmentation: Excess Green Index and Linear SVM. For each method, we provide an example configuration file in the Index computation section.","title":"Binary segmentation"},{"location":"plant_imager/explanations/segmentation/#index-computation","text":"","title":"Index computation"},{"location":"plant_imager/explanations/segmentation/#linear-support-vector-machine-svm","text":"A linear combination of R, G and B is used to compute the index for pixel (i,j) (i,j) : S_{ij}=w_0 R_{ij} + w_1 G_{ij} +w_2 B_{ij} S_{ij}=w_0 R_{ij} + w_1 G_{ij} +w_2 B_{ij} where w w is the parameters vector specified in the configuration file. A simple vector, like w=(0,1,0) w=(0,1,0) may be used for example. Alternatively, you can train an SVM to learn those weights, and the threshold to be provided in the configuration file. For this, we consider you have a sample image and a ground truth binary mask. A ground truth may be produced using a manual annotation tool like LabelMe . Using for example a list of N randomly selected pixels as X_{train} X_{train} (array of size [N,3]) and their corresponding labels as Y_{train} Y_{train} (array of size N), a linear SVM is trained using from sklearn import svm X_train , Y_train = ... clf = svm . SVC ( kernel = 'linear' ) clf . fit ( X_train , y_train ) the weights can then be retrieved as clf.coef_ and the threshold as -clf.intercept_","title":"Linear support vector machine (SVM)"},{"location":"plant_imager/explanations/segmentation/#configuration-file","text":"[Masks] upstream_task = \"ImagesFilesetExists\" # other option \"Undistorted\" type = \"linear\" parameters = \"[0,1,0]\" threshold = 0.5","title":"Configuration file"},{"location":"plant_imager/explanations/segmentation/#excess-green","text":"This segmentation method is assuming the plant is green and the background is not. It has no parameter but it may be less robust than the linear SVM. We compute the normalized RGB values x \\in {r,g,b} x \\in {r,g,b} for each pixel (i,j) (i,j) : x_{ij} = \\frac{X_{ij}}{R_{ij}+G_{ij}+B_{ij}} x_{ij} = \\frac{X_{ij}}{R_{ij}+G_{ij}+B_{ij}} where X \\in {R, G, B} X \\in {R, G, B} is the red, green or blue image Then, the green excess index is computed as: \\operatorname{ExG}=2g-r-b \\operatorname{ExG}=2g-r-b","title":"Excess green"},{"location":"plant_imager/explanations/segmentation/#configuration-file_1","text":"[Masks] upstream_task = \"ImagesFilesetExists\" # other option \"Undistorted\" type = \"excess_green\" threshold = 0.2","title":"Configuration file"},{"location":"plant_imager/explanations/segmentation/#multi-class-segmentation","text":"The Segmentation2D task performs the semantic segmentation of images using a deep neural network (DNN). The command to run this task is: romi_run_task Segmentation2D scan_id my_config.toml This will produce a series of binary masks, one for each class on which the network was trained. Generic encoder/decoder architecture for semantic segmentation (U-net). The architecture of the network is inspired from the U-net 1 , with a ResNet encoder 2 . It consists in encoding and decoding pathways with skip connections between the 2. Along the encoding pathways, there is a sequence of convolutions and the image signal is upsampled along the decoding pathway. The network is trained for segmenting images of a size (S_x,S_y) (S_x,S_y) which is not necessarily the image size of the acquired images. Those parameters Sx and Sy should be provided in the configuration file. The images will be cropped to (S_x,S_y) (S_x,S_y) before being fed to the DNN and it is then resized to the original size as an output of the task.","title":"Multi-class segmentation"},{"location":"plant_imager/explanations/segmentation/#configuration-file_2","text":"[Segmentation2D] model_id = \"Resnetdataset_gl_png_896_896_epoch50\" # no default value Sx = 896 Sy = 896 threshold = 0.01","title":"Configuration File"},{"location":"plant_imager/explanations/segmentation/#dnn-model","text":"The neural architecture weights are obtained through training on an annotated dataset (see How to train a DNN for semantic segmentation). Those weights should be stored in the database (at <database>/models/models ) and the name of the weights file should be provided as the model_id parameter in the configuration. You can use our model trained on virtual arabidopsis here","title":"DNN model"},{"location":"plant_imager/explanations/segmentation/#binarization","text":"A binary mask m m is produced from the index or from the output of the DNN, I , by applying a threshold \\theta \\theta on I for each pixel (i,j) (i,j) : \\begin{equation} m_{ij} = \\begin{cases} 255 & \\text{if $I_{ij}>\\theta$}\\\\ 0 & \\text{otherwise} \\end{cases} \\end{equation} \\begin{equation} m_{ij} = \\begin{cases} 255 & \\text{if $I_{ij}>\\theta$}\\\\ 0 & \\text{otherwise} \\end{cases} \\end{equation} This threshold may be chosen empirically, or it may be learnt from annotated data (see linear SVM section).","title":"Binarization"},{"location":"plant_imager/explanations/segmentation/#dilation","text":"If the integer dilation parameter is non-zero a morphological dilation is applied to the image using the function binary_dilation from the skimage.morphology module. The dilation parameter sets the number of times binary_dilation is iteratively applied. For a faithful reconstruction this parameter should be set to 0 0 but in practice you may want to have a coarser point cloud. This is true when your segmentation is not perfect, dilation will fill the holes or when the reconstructed mesh is broken because the point-cloud is too thin.","title":"Dilation"},{"location":"plant_imager/explanations/segmentation/#working-with-data-from-the-virtual-scanner","text":"When working with data generated with the virtual scanner, the images folder contains multiple channels corresponding to the various class for which images were generated ( stem , flower , fruit , leaf , pedicel ). You have to select the rgb channel using the query parameter.","title":"Working with data from the virtual scanner"},{"location":"plant_imager/explanations/segmentation/#configuration-file_3","text":"[Masks] type = \"excess_green\" threshold = 0.2 query = \"{\\\"channel\\\":\\\"rgb\\\"}\" Ronneberger, O., Fischer, P., & Brox, T. (2015, October). U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention (pp. 234-241). Springer, Cham. \u21a9 Zhang, Z., Liu, Q., & Wang, Y. (2018). Road extraction by deep residual u-net. IEEE Geoscience and Remote Sensing Letters, 15(5), 749-753. \u21a9","title":"Configuration File"},{"location":"plant_imager/how_to/","text":"","title":"Home"},{"location":"plant_imager/how_to/create_new_evaluation_task/","text":"How-to create a new ROMI evaluation task Link In order to evaluate the reconstruction and quantification tasks accuracy , we offer the possibility to also create evaluation tasks . The idea is to use a digital twin to generates the expected outcome of a task and use it as ground truth to challenge the reconstruction task. To do so, you will have to create two tasks: ground truth task : it should generate the expected outcome of the evaluated task from the digital twin; evaluation task : it will compare the output of the evaluated task against the ground truth. For example, the Voxels task has a VoxelGroundTruth task and a VoxelEvaluation task. Ground truth task Link Ground truth tasks should be defined in plant-3d-vision/plant3dvision/tasks/ground_truth.py . It should inherit from RomiTask and define a run method exporting the ground truth later use as reference in the evaluation task. Warning Do not forget to reference the task in romitask/romitask/modules.py . Evaluation task Link Evaluation tasks should be defined in plant-3d-vision/plant3dvision/tasks/evaluation.py . The evaluation task that you will write should inherit from EvaluationTask that defines: the requires method to use an upstream_task and ground_truth ; the output method to create the corresponding evaluation dataset the evaluate method that you should override; the run method that call evaluate and save the results as a JSON file. Warning Do not forget to reference the task in romitask/romitask/modules.py .","title":"Create a new evaluation task"},{"location":"plant_imager/how_to/create_new_evaluation_task/#how-to-create-a-new-romi-evaluation-task","text":"In order to evaluate the reconstruction and quantification tasks accuracy , we offer the possibility to also create evaluation tasks . The idea is to use a digital twin to generates the expected outcome of a task and use it as ground truth to challenge the reconstruction task. To do so, you will have to create two tasks: ground truth task : it should generate the expected outcome of the evaluated task from the digital twin; evaluation task : it will compare the output of the evaluated task against the ground truth. For example, the Voxels task has a VoxelGroundTruth task and a VoxelEvaluation task.","title":"How-to create a new ROMI evaluation task"},{"location":"plant_imager/how_to/create_new_evaluation_task/#ground-truth-task","text":"Ground truth tasks should be defined in plant-3d-vision/plant3dvision/tasks/ground_truth.py . It should inherit from RomiTask and define a run method exporting the ground truth later use as reference in the evaluation task. Warning Do not forget to reference the task in romitask/romitask/modules.py .","title":"Ground truth task"},{"location":"plant_imager/how_to/create_new_evaluation_task/#evaluation-task","text":"Evaluation tasks should be defined in plant-3d-vision/plant3dvision/tasks/evaluation.py . The evaluation task that you will write should inherit from EvaluationTask that defines: the requires method to use an upstream_task and ground_truth ; the output method to create the corresponding evaluation dataset the evaluate method that you should override; the run method that call evaluate and save the results as a JSON file. Warning Do not forget to reference the task in romitask/romitask/modules.py .","title":"Evaluation task"},{"location":"plant_imager/how_to/create_new_task/","text":"How-to create a new ROMI task Link We hereafter details how you can make your own algorithms available to the ROMI reconstruction and analysis pipeline by creating a task and registering it as an available module. For the sake of clarity and to illustrate how-to create a ROMI task from scratch, in this guide we will assume you want to add something quite different from what is already there. Important ROMI task usually have a semantic meaning and for example, the task AnglesAndInternodes may take several types of object in input (mesh, point-cloud & skeletons) but always output the JSON file with the obtained measures. So, to decide if you have to create a new task or add your algorithm to an existing task, following this rule should help: at a given step of the pipeline, if the output change, this is a NEW task! Add your algorithm to plant3dvision Link You first have to add a file (or append to an existing one), e.g. named algo.py , under the plant-3d-vision/plant3dvision directory. Let's assume the previously added file has a main function called my_algo like this: def my_algo ( data , * params , ** kwargs ): # Do something to data with given parameters to return transformed data `out_data` return out_data , error It has: data input(s) ( e.g. images, point clouds, meshes, ...) that will often be the output of a previous task in the pipeline parameter(s) , specific to the algorithm you want to add output(s) , the transformed dataset that will often be the input of a following task in the pipeline Create a ROMI task Link Dependency to luigi Link We use luigi to manage the pipeline execution and handle requirements & tasks dependencies. To create a task you will thus have to create a new Python class MyTask inheriting from the RomiTask class and creates a few methods and at least a run method used by luigi . Dependency to plantdb Link To manage the files, inputs and outputs, we use the plantdb package implementing a local file system database written in pure python. It provides classes and methods that simplifies and normalize the creation and use of the tasks outputs and inputs. New RomiTask template Link You will create a new python file my_task.py in the tasks submodule: plant-3d-vision/plant3dvision/tasks/my_task.py #!/usr/bin/env python # -*- coding: utf-8 -*- \"\"\" Briefly describe your module here. \"\"\" import luigi from plantdb import RomiTask from plantdb import io from plant3dvision.log import logger # Use this as logging method from plant3dvision.tasks.proc3d import SegmentedPointCloud # Now import your main method: from plant3dvision.algo import my_algo def MyTask ( RomiTask ): \"\"\"My algorithm is the best! Attributes ---------- upstream_task : luigi.TaskParameter Upstream task that will provides the data to your algorithm, here `SegmentedPointCloud`. param1 : luigi.FloatParameter An example float parameter parsed from the TOML config file. Set to `2.0` by default. param2 : luigi.IntParameter An example float parameter parsed from the TOML config file. Set to `5` by default. log : luigi.BoolParameter An example boolean parameter. \"\"\" # No need to write an `__init__` section, declare your class attributes as task parameters: upstream_task = luigi . TaskParameter ( default = SegmentedPointCloud ) param1 = luigi . FloatParameter ( default = 2.0 ) param2 = luigi . IntParameter ( default = 5 ) log = luigi . BoolParameter ( default = False ) def requires ( self ): \"\"\"Used by luigi to check you task dependencies.\"\"\" # By default a RomiTask requires a luigi.TaskParameter called `upstream_task`. # So no need to declare this method if you don't requires more than one upstream task! # Else you can override with something like (should be of type `luigi.TaskParameter`!): #return [self.upstream_task1(), self.upstream_task1()] pass def run ( self ): \"\"\"Called by luigi, it will run your algorithm. Usually consist of 3 steps: 1. Get the input(s) data from the previous task, eg. images or point clouds 2. Run you algorithm on input data 3. Save the result(s) of your method, eg. as a JSON file Notes ----- The parameters for your algorithms have been declared at class instantiation! \"\"\" # -1- Get the input(s) data from the previous task # To access the single file output of the upstream task use: uptask_input_file = self . input_file () # Read it with the proper reader, here a point-cloud reader (SegmentedPointCloud): in_data = io . read_point_cloud ( uptask_input_file ) # -2- Run you algorithm on input data out_data , error = my_algo ( in_data ) # Use example for boolean parameter & logger with 'info' level if self . log : logger . info ( \"My task ran perfectly!\" ) # -3- Write a single output (eg. a JSON file)... # Create the output `File` object task_output_file = self . output_file () # Write a JSON file with your method results io . write_json ( task_output_file , out_data ) # Add metadata to your file, eg. some error measure you don't want to include in the main output file: task_output_file . set_metadata ( \"my_error\" , error ) The corresponding TOML configuration file ( my_pipeline.toml ) controlling your task behaviour would look like this: [MyTask] upstream_task = 'SegmentedPointCloud' param1 = 6.0 param2 = 3 log = true Note You may need to add methods to read and write data, this should be done in the plantdb library using the plantdb/plantdb/io.py file! Multiple I/O for a task Link Your method (or the upstream task) may produce a set of object you want to save as separates files. In such case, use Filset objects. For example to output multiple JSON files: list_of_jsonifyable = [ ... ] task_output_fs = self . output () . get () for i , json_data in enumerate ( list_of_jsonifyable ): f = task_output_fs . create_file ( f \"my_json_ { i } \" ) # no extension! io . write_json ( f , json_data ) # Add some metadata to this `File` object f . set_metadata ( \"foo\" , f \"bar { i } \" ) Test your task Link You should now be able to test your newly created task MyTask with romi_run_task : romi_run_task MyTask /path/to/dataset --config /path/to/my_pipeline.toml --module plant3dvision.tasks.my_algo Using the --module option you can test your task without registering it. Register your task Link Once you are satisfied, you can add it to romitask/modules.py by referring to the task class name & its python module location: MODULES = { # ... \"MyTask\" : \"plant3dvision.tasks.my_algo\" , # ... } Use your newly created task Link Finally, you should now be able to use your newly created task MyTask with romi_run_task : romi_run_task MyTask /path/to/dataset --config /path/to/my_pipeline.toml Warning Use of absolute path is highly recommended as you may experience some difficulties from luigi otherwise!","title":"Create a new task"},{"location":"plant_imager/how_to/create_new_task/#how-to-create-a-new-romi-task","text":"We hereafter details how you can make your own algorithms available to the ROMI reconstruction and analysis pipeline by creating a task and registering it as an available module. For the sake of clarity and to illustrate how-to create a ROMI task from scratch, in this guide we will assume you want to add something quite different from what is already there. Important ROMI task usually have a semantic meaning and for example, the task AnglesAndInternodes may take several types of object in input (mesh, point-cloud & skeletons) but always output the JSON file with the obtained measures. So, to decide if you have to create a new task or add your algorithm to an existing task, following this rule should help: at a given step of the pipeline, if the output change, this is a NEW task!","title":"How-to create a new ROMI task"},{"location":"plant_imager/how_to/create_new_task/#add-your-algorithm-to-plant3dvision","text":"You first have to add a file (or append to an existing one), e.g. named algo.py , under the plant-3d-vision/plant3dvision directory. Let's assume the previously added file has a main function called my_algo like this: def my_algo ( data , * params , ** kwargs ): # Do something to data with given parameters to return transformed data `out_data` return out_data , error It has: data input(s) ( e.g. images, point clouds, meshes, ...) that will often be the output of a previous task in the pipeline parameter(s) , specific to the algorithm you want to add output(s) , the transformed dataset that will often be the input of a following task in the pipeline","title":"Add your algorithm to plant3dvision"},{"location":"plant_imager/how_to/create_new_task/#create-a-romi-task","text":"","title":"Create a ROMI task"},{"location":"plant_imager/how_to/create_new_task/#dependency-to-luigi","text":"We use luigi to manage the pipeline execution and handle requirements & tasks dependencies. To create a task you will thus have to create a new Python class MyTask inheriting from the RomiTask class and creates a few methods and at least a run method used by luigi .","title":"Dependency to luigi"},{"location":"plant_imager/how_to/create_new_task/#dependency-to-plantdb","text":"To manage the files, inputs and outputs, we use the plantdb package implementing a local file system database written in pure python. It provides classes and methods that simplifies and normalize the creation and use of the tasks outputs and inputs.","title":"Dependency to plantdb"},{"location":"plant_imager/how_to/create_new_task/#new-romitask-template","text":"You will create a new python file my_task.py in the tasks submodule: plant-3d-vision/plant3dvision/tasks/my_task.py #!/usr/bin/env python # -*- coding: utf-8 -*- \"\"\" Briefly describe your module here. \"\"\" import luigi from plantdb import RomiTask from plantdb import io from plant3dvision.log import logger # Use this as logging method from plant3dvision.tasks.proc3d import SegmentedPointCloud # Now import your main method: from plant3dvision.algo import my_algo def MyTask ( RomiTask ): \"\"\"My algorithm is the best! Attributes ---------- upstream_task : luigi.TaskParameter Upstream task that will provides the data to your algorithm, here `SegmentedPointCloud`. param1 : luigi.FloatParameter An example float parameter parsed from the TOML config file. Set to `2.0` by default. param2 : luigi.IntParameter An example float parameter parsed from the TOML config file. Set to `5` by default. log : luigi.BoolParameter An example boolean parameter. \"\"\" # No need to write an `__init__` section, declare your class attributes as task parameters: upstream_task = luigi . TaskParameter ( default = SegmentedPointCloud ) param1 = luigi . FloatParameter ( default = 2.0 ) param2 = luigi . IntParameter ( default = 5 ) log = luigi . BoolParameter ( default = False ) def requires ( self ): \"\"\"Used by luigi to check you task dependencies.\"\"\" # By default a RomiTask requires a luigi.TaskParameter called `upstream_task`. # So no need to declare this method if you don't requires more than one upstream task! # Else you can override with something like (should be of type `luigi.TaskParameter`!): #return [self.upstream_task1(), self.upstream_task1()] pass def run ( self ): \"\"\"Called by luigi, it will run your algorithm. Usually consist of 3 steps: 1. Get the input(s) data from the previous task, eg. images or point clouds 2. Run you algorithm on input data 3. Save the result(s) of your method, eg. as a JSON file Notes ----- The parameters for your algorithms have been declared at class instantiation! \"\"\" # -1- Get the input(s) data from the previous task # To access the single file output of the upstream task use: uptask_input_file = self . input_file () # Read it with the proper reader, here a point-cloud reader (SegmentedPointCloud): in_data = io . read_point_cloud ( uptask_input_file ) # -2- Run you algorithm on input data out_data , error = my_algo ( in_data ) # Use example for boolean parameter & logger with 'info' level if self . log : logger . info ( \"My task ran perfectly!\" ) # -3- Write a single output (eg. a JSON file)... # Create the output `File` object task_output_file = self . output_file () # Write a JSON file with your method results io . write_json ( task_output_file , out_data ) # Add metadata to your file, eg. some error measure you don't want to include in the main output file: task_output_file . set_metadata ( \"my_error\" , error ) The corresponding TOML configuration file ( my_pipeline.toml ) controlling your task behaviour would look like this: [MyTask] upstream_task = 'SegmentedPointCloud' param1 = 6.0 param2 = 3 log = true Note You may need to add methods to read and write data, this should be done in the plantdb library using the plantdb/plantdb/io.py file!","title":"New RomiTask template"},{"location":"plant_imager/how_to/create_new_task/#multiple-io-for-a-task","text":"Your method (or the upstream task) may produce a set of object you want to save as separates files. In such case, use Filset objects. For example to output multiple JSON files: list_of_jsonifyable = [ ... ] task_output_fs = self . output () . get () for i , json_data in enumerate ( list_of_jsonifyable ): f = task_output_fs . create_file ( f \"my_json_ { i } \" ) # no extension! io . write_json ( f , json_data ) # Add some metadata to this `File` object f . set_metadata ( \"foo\" , f \"bar { i } \" )","title":"Multiple I/O for a task"},{"location":"plant_imager/how_to/create_new_task/#test-your-task","text":"You should now be able to test your newly created task MyTask with romi_run_task : romi_run_task MyTask /path/to/dataset --config /path/to/my_pipeline.toml --module plant3dvision.tasks.my_algo Using the --module option you can test your task without registering it.","title":"Test your task"},{"location":"plant_imager/how_to/create_new_task/#register-your-task","text":"Once you are satisfied, you can add it to romitask/modules.py by referring to the task class name & its python module location: MODULES = { # ... \"MyTask\" : \"plant3dvision.tasks.my_algo\" , # ... }","title":"Register your task"},{"location":"plant_imager/how_to/create_new_task/#use-your-newly-created-task","text":"Finally, you should now be able to use your newly created task MyTask with romi_run_task : romi_run_task MyTask /path/to/dataset --config /path/to/my_pipeline.toml Warning Use of absolute path is highly recommended as you may experience some difficulties from luigi otherwise!","title":"Use your newly created task"},{"location":"plant_imager/how_to/import_files/","text":"How-to import files in ROMI database Link Importing external images as a dataset Link In order to be able to use external images, i.e. images that were not acquired with the software & hardware developed by ROMI, we provide tools to import them as a scan dataset in the ROMI database. One example could be a set of pictures (of a plant) acquired with your phone that you would like to reconstruct and maybe analyse with our software. To do so, you may use the romi_import_folder or romi_import_file executables from plantdb . For example, you have a set of 10 RGB pictures named img_00*.jpg in a folder my_plant/ that you would like to import as outdoor_plant_1 in a romi database located under /data/romi/db . First you have to move the pictures to an \u00ecmages sub-directory & create a metadata.json describing the object under study: cd my_plant mkdir images mv *.jpg images/. touch metadata.json An example of a metadata.json : { \"object\" : { \"age\" : \"N/A\" , \"culture\" : \"N/A\" , \"environment\" : \"outdoor\" , \"experiment_id\" : \"romi demo outdoor plant\" , \"object\" : \"plant\" , \"plant_id\" : \"Chirsuta_1\" , \"sample\" : \"whole plant\" , \"species\" : \"Cardamine hirsuta\" , \"stock\" : \"WT\" , \"treatment\" : \"none\" } } To summarize you now should have the following folder structure: my_plant/ \u251c\u2500\u2500 images/ \u2502 \u251c\u2500\u2500 img_001.jpg \u2502 \u251c\u2500\u2500 [...] \u2502 \u2514\u2500\u2500 img_010.jpg \u2514\u2500\u2500 metadata.json Then you can perform the 'import to the database' operation with romi_import_folder : romi_import_folder my_plant/\u00ecmages/ /data/romi/db/outdoor_plant_1 --metadata my_plant/metadata.json That's it! Your manual acquisition is ready to be used by the romi_run_task tool for reconstruction.","title":"Import external resources"},{"location":"plant_imager/how_to/import_files/#how-to-import-files-in-romi-database","text":"","title":"How-to import files in ROMI database"},{"location":"plant_imager/how_to/import_files/#importing-external-images-as-a-dataset","text":"In order to be able to use external images, i.e. images that were not acquired with the software & hardware developed by ROMI, we provide tools to import them as a scan dataset in the ROMI database. One example could be a set of pictures (of a plant) acquired with your phone that you would like to reconstruct and maybe analyse with our software. To do so, you may use the romi_import_folder or romi_import_file executables from plantdb . For example, you have a set of 10 RGB pictures named img_00*.jpg in a folder my_plant/ that you would like to import as outdoor_plant_1 in a romi database located under /data/romi/db . First you have to move the pictures to an \u00ecmages sub-directory & create a metadata.json describing the object under study: cd my_plant mkdir images mv *.jpg images/. touch metadata.json An example of a metadata.json : { \"object\" : { \"age\" : \"N/A\" , \"culture\" : \"N/A\" , \"environment\" : \"outdoor\" , \"experiment_id\" : \"romi demo outdoor plant\" , \"object\" : \"plant\" , \"plant_id\" : \"Chirsuta_1\" , \"sample\" : \"whole plant\" , \"species\" : \"Cardamine hirsuta\" , \"stock\" : \"WT\" , \"treatment\" : \"none\" } } To summarize you now should have the following folder structure: my_plant/ \u251c\u2500\u2500 images/ \u2502 \u251c\u2500\u2500 img_001.jpg \u2502 \u251c\u2500\u2500 [...] \u2502 \u2514\u2500\u2500 img_010.jpg \u2514\u2500\u2500 metadata.json Then you can perform the 'import to the database' operation with romi_import_folder : romi_import_folder my_plant/\u00ecmages/ /data/romi/db/outdoor_plant_1 --metadata my_plant/metadata.json That's it! Your manual acquisition is ready to be used by the romi_run_task tool for reconstruction.","title":"Importing external images as a dataset"},{"location":"plant_imager/install/","text":"Installing the ROMI software Link Use cases Link In the following subsections we will detail how to install ROMI software for a few usage cases: Create a database server here . Plant scans acquisition using the ROMI plant scanner to a database here . Plant reconstruction pipelines from existing plant scans in a database here . Virtual plant creation (3D modelling of plant architecture with LPY), virtual scan (mimic plant scanner with blender) & reconstruction (same as 2.) here . Create a web server hosting the plant 3d explorer GUI here . Note You can find docker images for use cases #1, #3 & #5 in the dockerhub repository of the ROMI project here . General requirements Link Cloning sources Link To clone the git repository, you will need: git ca-certificates Start with these system dependencies: sudo apt-get install git ca-certificates Downloading from URLs Link Sometimes the documentation will provide commands with wget to download archives or other types of files, here is the command line to install it if you do not have it: sudo apt install wget Creating isolated Python environments Link Important We recommend using conda to create isolated environments as some packages, like openalea.lpy , are available as conda packages but not from pip and can be tricky to install from sources! Follow this link to learn how to install miniconda3 & create isolated Python environments with conda . If you have no idea why you should use isolated Python environments, here is a quote from the official Python documentation: Quote venv (for Python 3) and virtualenv (for Python 2) allow you to manage separate package installations for different projects. They essentially allow you to create a \"virtual\" isolated Python installation and install packages into that virtual installation. When you switch projects, you can simply create a new virtual environment and not have to worry about breaking the packages installed in the other environments. It is always recommended using a virtual environment while developing Python applications. List of sources Link For the ROMI projects, several libraries have been developed in various languages and made available on GitHub. Here is a list of the important repositories for the plant scanner project: plantdb : the database module is accessible here ; plantimager : the scanner interface, and the virtual scanner is accessible here ; plant3dvision : the computer vision algorithms to reconstruct the plants is accessible here ; romiseg : the ML-based plant segmentation models is accessible here ; plant-3d-explorer : the Node JS web viewer for plant scan, reconstruction and quantification is accessible here Additionally, we also have: romicgal : some CGAL bindings used for skeletonization & meshing is accessible here bldc_featherwing : the controller for BLDC motor on a feather wing is accessible here","title":"Home"},{"location":"plant_imager/install/#installing-the-romi-software","text":"","title":"Installing the ROMI software"},{"location":"plant_imager/install/#use-cases","text":"In the following subsections we will detail how to install ROMI software for a few usage cases: Create a database server here . Plant scans acquisition using the ROMI plant scanner to a database here . Plant reconstruction pipelines from existing plant scans in a database here . Virtual plant creation (3D modelling of plant architecture with LPY), virtual scan (mimic plant scanner with blender) & reconstruction (same as 2.) here . Create a web server hosting the plant 3d explorer GUI here . Note You can find docker images for use cases #1, #3 & #5 in the dockerhub repository of the ROMI project here .","title":"Use cases"},{"location":"plant_imager/install/#general-requirements","text":"","title":"General requirements"},{"location":"plant_imager/install/#cloning-sources","text":"To clone the git repository, you will need: git ca-certificates Start with these system dependencies: sudo apt-get install git ca-certificates","title":"Cloning sources"},{"location":"plant_imager/install/#downloading-from-urls","text":"Sometimes the documentation will provide commands with wget to download archives or other types of files, here is the command line to install it if you do not have it: sudo apt install wget","title":"Downloading from URLs"},{"location":"plant_imager/install/#creating-isolated-python-environments","text":"Important We recommend using conda to create isolated environments as some packages, like openalea.lpy , are available as conda packages but not from pip and can be tricky to install from sources! Follow this link to learn how to install miniconda3 & create isolated Python environments with conda . If you have no idea why you should use isolated Python environments, here is a quote from the official Python documentation: Quote venv (for Python 3) and virtualenv (for Python 2) allow you to manage separate package installations for different projects. They essentially allow you to create a \"virtual\" isolated Python installation and install packages into that virtual installation. When you switch projects, you can simply create a new virtual environment and not have to worry about breaking the packages installed in the other environments. It is always recommended using a virtual environment while developing Python applications.","title":"Creating isolated Python environments"},{"location":"plant_imager/install/#list-of-sources","text":"For the ROMI projects, several libraries have been developed in various languages and made available on GitHub. Here is a list of the important repositories for the plant scanner project: plantdb : the database module is accessible here ; plantimager : the scanner interface, and the virtual scanner is accessible here ; plant3dvision : the computer vision algorithms to reconstruct the plants is accessible here ; romiseg : the ML-based plant segmentation models is accessible here ; plant-3d-explorer : the Node JS web viewer for plant scan, reconstruction and quantification is accessible here Additionally, we also have: romicgal : some CGAL bindings used for skeletonization & meshing is accessible here bldc_featherwing : the controller for BLDC motor on a feather wing is accessible here","title":"List of sources"},{"location":"plant_imager/install/create_env/","text":"Creating isolated Python environments Link You can use venv or conda to create isolated Python environments. Warning Some ROMI libraries have dependencies relying on specific Python versions. Make sure that the isolated environment you create match these requirements! Isolated environments with venv Link Requirements Link Python 3 & pip are required. On Debian-like OS, use the following command to install them: sudo apt-get install python3 python3-pip For more details & explanations, follow this official guide to learn how to install packages using pip and virtual environments. Environment creation Link To create a new environment, named plant_imager , use python3 and the venv module: python3 -m venv plant_imager Note This will create a plant_imager folder in the current working directory and place the \"environment files\" there! We thus advise to gather all your environment in a common folder like ~/envs . To activate it: source plant_imager/bin/activate Usage Link Now you can easily install Python packages, for example NumPy , as follow: pip3 install numpy Note Use deactivate or kill terminal to leave it! You can now use this environment to install the ROMI software & dependencies. Isolated environments with miniconda Link Requirements Link In this case you do not need Python to be installed on your system, all you need it to install miniconda3 . You can download the latest miniconda3 version with: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh On Debian-like OS, use the following command to install it: bash Miniconda3-latest-Linux-x86_64.sh For more details & explanations, follow this official guide to learn how to install miniconda . Environment creation Link To create a new conda environment, named plant_imager with Python 3.7: conda create --name plant_imager python == 3 .7 To activate it: conda activate plant_imager Usage Link Now you can now easily install Python packages, for example NumPy , as follow: conda install numpy Note Use conda deactivate or kill terminal to leave it! You can now use this environment to install the ROMI software & dependencies.","title":"Create python environments"},{"location":"plant_imager/install/create_env/#creating-isolated-python-environments","text":"You can use venv or conda to create isolated Python environments. Warning Some ROMI libraries have dependencies relying on specific Python versions. Make sure that the isolated environment you create match these requirements!","title":"Creating isolated Python environments"},{"location":"plant_imager/install/create_env/#isolated-environments-with-venv","text":"","title":"Isolated environments with venv"},{"location":"plant_imager/install/create_env/#requirements","text":"Python 3 & pip are required. On Debian-like OS, use the following command to install them: sudo apt-get install python3 python3-pip For more details & explanations, follow this official guide to learn how to install packages using pip and virtual environments.","title":"Requirements"},{"location":"plant_imager/install/create_env/#environment-creation","text":"To create a new environment, named plant_imager , use python3 and the venv module: python3 -m venv plant_imager Note This will create a plant_imager folder in the current working directory and place the \"environment files\" there! We thus advise to gather all your environment in a common folder like ~/envs . To activate it: source plant_imager/bin/activate","title":"Environment creation"},{"location":"plant_imager/install/create_env/#usage","text":"Now you can easily install Python packages, for example NumPy , as follow: pip3 install numpy Note Use deactivate or kill terminal to leave it! You can now use this environment to install the ROMI software & dependencies.","title":"Usage"},{"location":"plant_imager/install/create_env/#isolated-environments-with-miniconda","text":"","title":"Isolated environments with miniconda"},{"location":"plant_imager/install/create_env/#requirements_1","text":"In this case you do not need Python to be installed on your system, all you need it to install miniconda3 . You can download the latest miniconda3 version with: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh On Debian-like OS, use the following command to install it: bash Miniconda3-latest-Linux-x86_64.sh For more details & explanations, follow this official guide to learn how to install miniconda .","title":"Requirements"},{"location":"plant_imager/install/create_env/#environment-creation_1","text":"To create a new conda environment, named plant_imager with Python 3.7: conda create --name plant_imager python == 3 .7 To activate it: conda activate plant_imager","title":"Environment creation"},{"location":"plant_imager/install/create_env/#usage_1","text":"Now you can now easily install Python packages, for example NumPy , as follow: conda install numpy Note Use conda deactivate or kill terminal to leave it! You can now use this environment to install the ROMI software & dependencies.","title":"Usage"},{"location":"plant_imager/install/plant3dexplorer_setup/","text":"Install the ROMI Plant 3d explorer Link To follow this guide you should have a conda environment, see here . For the sake of clarity it will be called Plant 3d explorer . Note If you do not want the hassle of having to create environment & install python libraries, there is a pre-built docker image, with usage instructions here . Pre-requisite Link The Plant 3d explorer relies on: node npm Install node and npm , on ubuntu: sudo apt install npm The packaged version ot npm is probably out of date (require npm>=5 ), to update it: npm install npm@latest -g Install ROMI packages & their dependencies: Link Activate your plant_imager environment! Clone the visualizer git repository : git clone https://github.com/romi/plant-3d-explorer.git cd plant-3d-explorer Install node packages and build the pages: npm install Use the Plant 3d explorer Link With the official ROMI database Link You can use the ROMI database to test the installation of the Plant 3d explorer : export REACT_APP_API_URL = 'https://db.romi-project.eu' npm start With a running local database Link If you have followed the installation instructions of the ROMI database ( here ), you can use it with the Plant 3d explorer : export REACT_APP_API_URL = '0.0.0.0' npm start Tip To permanently set this URL as the location of the DB, add it to your ~/.bashrc file. echo 'export REACT_APP_API_URL=0.0.0.0' >> ~/.bashrc","title":"plant-3d-explorer software setup"},{"location":"plant_imager/install/plant3dexplorer_setup/#install-the-romi-plant-3d-explorer","text":"To follow this guide you should have a conda environment, see here . For the sake of clarity it will be called Plant 3d explorer . Note If you do not want the hassle of having to create environment & install python libraries, there is a pre-built docker image, with usage instructions here .","title":"Install the ROMI Plant 3d explorer"},{"location":"plant_imager/install/plant3dexplorer_setup/#pre-requisite","text":"The Plant 3d explorer relies on: node npm Install node and npm , on ubuntu: sudo apt install npm The packaged version ot npm is probably out of date (require npm>=5 ), to update it: npm install npm@latest -g","title":"Pre-requisite"},{"location":"plant_imager/install/plant3dexplorer_setup/#install-romi-packages-their-dependencies","text":"Activate your plant_imager environment! Clone the visualizer git repository : git clone https://github.com/romi/plant-3d-explorer.git cd plant-3d-explorer Install node packages and build the pages: npm install","title":"Install ROMI packages &amp; their dependencies:"},{"location":"plant_imager/install/plant3dexplorer_setup/#use-the-plant-3d-explorer","text":"","title":"Use the Plant 3d explorer"},{"location":"plant_imager/install/plant3dexplorer_setup/#with-the-official-romi-database","text":"You can use the ROMI database to test the installation of the Plant 3d explorer : export REACT_APP_API_URL = 'https://db.romi-project.eu' npm start","title":"With the official ROMI database"},{"location":"plant_imager/install/plant3dexplorer_setup/#with-a-running-local-database","text":"If you have followed the installation instructions of the ROMI database ( here ), you can use it with the Plant 3d explorer : export REACT_APP_API_URL = '0.0.0.0' npm start Tip To permanently set this URL as the location of the DB, add it to your ~/.bashrc file. echo 'export REACT_APP_API_URL=0.0.0.0' >> ~/.bashrc","title":"With a running local database"},{"location":"plant_imager/install/plant_imager_setup/","text":"Install ROMI software for the plant imager Link To follows this guide you should have a conda or a Python venv , see here . For the sake of clarity the environment will be called plant_imager . Install ROMI packages with pip : Link Activate your plant_imager environment! conda activate plant_imager Note Since this is still under development, the packages are installed in \"editable mode\" with the -e option. Install plant-imager sources: Link To pilot the hardware you have to install the plant-imager package: python3 -m pip install -e git+https://github.com/romi/plant-imager.git@dev Install plant-3d-vision sources: Link To start \"acquisition jobs\", you have to install the plant-3d-vision package: python3 -m pip install -e git+https://github.com/romi/plant3dvision.git@dev Install plantdb sources: Link Since we will need an active database to export the acquisitions, you have to install the plantdb package: python3 -m pip install -e git+https://github.com/romi/plantdb.git@dev Example database Link To quickly create an example DB you can use: wget https://db.romi-project.eu/models/test_db.tar.gz tar -xf test_db.tar.gz This will create a integration_tests folder with a ready to use test database. You should now be ready to perform \"plant acquisitions\" following the dedicated user guide. Troubleshooting Link Serial access denied Link Look here if you can not communicate with the scanner using usb.","title":"Plant imager software setup"},{"location":"plant_imager/install/plant_imager_setup/#install-romi-software-for-the-plant-imager","text":"To follows this guide you should have a conda or a Python venv , see here . For the sake of clarity the environment will be called plant_imager .","title":"Install ROMI software for the plant imager"},{"location":"plant_imager/install/plant_imager_setup/#install-romi-packages-with-pip","text":"Activate your plant_imager environment! conda activate plant_imager Note Since this is still under development, the packages are installed in \"editable mode\" with the -e option.","title":"Install ROMI packages with pip:"},{"location":"plant_imager/install/plant_imager_setup/#install-plant-imager-sources","text":"To pilot the hardware you have to install the plant-imager package: python3 -m pip install -e git+https://github.com/romi/plant-imager.git@dev","title":"Install plant-imager sources:"},{"location":"plant_imager/install/plant_imager_setup/#install-plant-3d-vision-sources","text":"To start \"acquisition jobs\", you have to install the plant-3d-vision package: python3 -m pip install -e git+https://github.com/romi/plant3dvision.git@dev","title":"Install plant-3d-vision sources:"},{"location":"plant_imager/install/plant_imager_setup/#install-plantdb-sources","text":"Since we will need an active database to export the acquisitions, you have to install the plantdb package: python3 -m pip install -e git+https://github.com/romi/plantdb.git@dev","title":"Install plantdb sources:"},{"location":"plant_imager/install/plant_imager_setup/#example-database","text":"To quickly create an example DB you can use: wget https://db.romi-project.eu/models/test_db.tar.gz tar -xf test_db.tar.gz This will create a integration_tests folder with a ready to use test database. You should now be ready to perform \"plant acquisitions\" following the dedicated user guide.","title":"Example database"},{"location":"plant_imager/install/plant_imager_setup/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"plant_imager/install/plant_imager_setup/#serial-access-denied","text":"Look here if you can not communicate with the scanner using usb.","title":"Serial access denied"},{"location":"plant_imager/install/plant_reconstruction_setup/","text":"Install ROMI software for plants reconstruction Link To follows this guide you should have an existing conda or a Python venv , see here . We prefer to use conda and will use examples command lines with it. Simply replace the conda activate plant_imager commands with source plant_imager/bin/activate . For the sake of clarity the environment will be called plant_imager . Requirements: Link Colmap Link To save you the hassle of installing Colmap & its dependencies, we wrote a mechanism allowing you to run the Colmap command straight into a docker container where it is already done! Note Choose between option A (recommended) OR B! A - Use of docker image Link You can use a pre-built docker image with Colmap & its dependencies installed (named geki/colmap ). This requires to install the docker-engine. To do so, follows the official instructions here: https://docs.docker.com/get-docker/ It uses the Python docker SDK docker available on PyPi , to learn more read the official documentation. If you upgrade from an older install, you may have to install the Python docker SDK: conda activate plant_imager python -m pip install docker Important Make sure you can access the docker engine as a non-root user! On Linux: 1. Create the docker group : `$ sudo groupadd docker ` 2. Add your user to the docker group : `$ sudo usermod - aG docker $ USER ` 3. Log out and log back in so that your group membership is re - evaluated . Official instructions [ here ] ( https : // docs . docker . com / engine / install / linux - postinstall / ) B - System install Link If you are a warrior or a computer expert, you can follow the procedure from the official documentation here . Make sure to use version 3.6. Note If you are using a conda environment, you can install ceres-solver dependency for Colmap from the conda-forge channel: conda activate plant_imager conda install ceres-solver -c conda-forge Important By default we use the docker mechanism, to enable the system install you need to export the environment variable COLMAP_EXE='colmap' . Install ROMI packages with pip : Link Activate your plant_imager environment! Note Since this is still under development, the packages are installed in \"editable mode\" with the -e option. Important You need an active ROMI database to import the images fileset to reconstruct. If it's not done yet, follow the installation instructions of a ROMI database ( here ). Install plant3dvision sources: Link To start \"reconstruction jobs\", you have to install the plant3dvision package. Here we use the submodules but if you wish to edit other packages than plant3dvision , e.g. plantdb , install them from source! conda activate plant_imager git clone --branch dev https://github.com/romi/plant3dvision.git cd plant3dvision git submodule init git submodule update python3 -m pip install -e ./plantdb/ --no-cache-dir python3 -m pip install -e ./romitask/ --no-cache-dir python3 -m pip install -e ./romiseg/ --no-cache-dir python3 -m pip install -e ./romicgal/ --no-cache-dir python3 -m pip install -e ./dtw/ --no-cache-dir python3 -m pip install -r requirements.txt --no-cache-dir python3 -m pip install -e . --no-cache-dir You should now be ready to performs \"plant reconstructions\" following the dedicated user guide. Install romicgal sources Link We use some algorithms from CGAL and propose a minimal python wrapper called romicgal . To install it: conda activate plant_imager python3 -m pip install -e git+https://github.com/romi/romicgal.git@master Note This takes some time since it has to download dependencies ( CGAL-0.5 & boost-1.72.0 ) and compile them. Install romiseg sources Link To install the additional Machine Learning based segmentation module: conda activate plant_imager python3 -m pip install -e git+https://github.com/romi/romiseg@dev Warning If not using CUDA 10.*, you have to install the matching pytorch distribution. For example, for CUDA 9.2, use: conda activate plant_imager pip install torch == 1 .4.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html To upgrade to Cuda 11.1, type conda install pytorch == 1 .8.0 torchvision == 0 .9.0 torchaudio == 0 .8.0 cudatoolkit = 11 .1 -c pytorch -c conda-forge See the PyTorch webpage for compatible Torch and Cuda combinations.","title":"Plant reconstruction software setup"},{"location":"plant_imager/install/plant_reconstruction_setup/#install-romi-software-for-plants-reconstruction","text":"To follows this guide you should have an existing conda or a Python venv , see here . We prefer to use conda and will use examples command lines with it. Simply replace the conda activate plant_imager commands with source plant_imager/bin/activate . For the sake of clarity the environment will be called plant_imager .","title":"Install ROMI software for plants reconstruction"},{"location":"plant_imager/install/plant_reconstruction_setup/#requirements","text":"","title":"Requirements:"},{"location":"plant_imager/install/plant_reconstruction_setup/#colmap","text":"To save you the hassle of installing Colmap & its dependencies, we wrote a mechanism allowing you to run the Colmap command straight into a docker container where it is already done! Note Choose between option A (recommended) OR B!","title":"Colmap"},{"location":"plant_imager/install/plant_reconstruction_setup/#a-use-of-docker-image","text":"You can use a pre-built docker image with Colmap & its dependencies installed (named geki/colmap ). This requires to install the docker-engine. To do so, follows the official instructions here: https://docs.docker.com/get-docker/ It uses the Python docker SDK docker available on PyPi , to learn more read the official documentation. If you upgrade from an older install, you may have to install the Python docker SDK: conda activate plant_imager python -m pip install docker Important Make sure you can access the docker engine as a non-root user! On Linux: 1. Create the docker group : `$ sudo groupadd docker ` 2. Add your user to the docker group : `$ sudo usermod - aG docker $ USER ` 3. Log out and log back in so that your group membership is re - evaluated . Official instructions [ here ] ( https : // docs . docker . com / engine / install / linux - postinstall / )","title":"A - Use of docker image"},{"location":"plant_imager/install/plant_reconstruction_setup/#b-system-install","text":"If you are a warrior or a computer expert, you can follow the procedure from the official documentation here . Make sure to use version 3.6. Note If you are using a conda environment, you can install ceres-solver dependency for Colmap from the conda-forge channel: conda activate plant_imager conda install ceres-solver -c conda-forge Important By default we use the docker mechanism, to enable the system install you need to export the environment variable COLMAP_EXE='colmap' .","title":"B - System install"},{"location":"plant_imager/install/plant_reconstruction_setup/#install-romi-packages-with-pip","text":"Activate your plant_imager environment! Note Since this is still under development, the packages are installed in \"editable mode\" with the -e option. Important You need an active ROMI database to import the images fileset to reconstruct. If it's not done yet, follow the installation instructions of a ROMI database ( here ).","title":"Install ROMI packages with pip:"},{"location":"plant_imager/install/plant_reconstruction_setup/#install-plant3dvision-sources","text":"To start \"reconstruction jobs\", you have to install the plant3dvision package. Here we use the submodules but if you wish to edit other packages than plant3dvision , e.g. plantdb , install them from source! conda activate plant_imager git clone --branch dev https://github.com/romi/plant3dvision.git cd plant3dvision git submodule init git submodule update python3 -m pip install -e ./plantdb/ --no-cache-dir python3 -m pip install -e ./romitask/ --no-cache-dir python3 -m pip install -e ./romiseg/ --no-cache-dir python3 -m pip install -e ./romicgal/ --no-cache-dir python3 -m pip install -e ./dtw/ --no-cache-dir python3 -m pip install -r requirements.txt --no-cache-dir python3 -m pip install -e . --no-cache-dir You should now be ready to performs \"plant reconstructions\" following the dedicated user guide.","title":"Install plant3dvision sources:"},{"location":"plant_imager/install/plant_reconstruction_setup/#install-romicgal-sources","text":"We use some algorithms from CGAL and propose a minimal python wrapper called romicgal . To install it: conda activate plant_imager python3 -m pip install -e git+https://github.com/romi/romicgal.git@master Note This takes some time since it has to download dependencies ( CGAL-0.5 & boost-1.72.0 ) and compile them.","title":"Install romicgal sources"},{"location":"plant_imager/install/plant_reconstruction_setup/#install-romiseg-sources","text":"To install the additional Machine Learning based segmentation module: conda activate plant_imager python3 -m pip install -e git+https://github.com/romi/romiseg@dev Warning If not using CUDA 10.*, you have to install the matching pytorch distribution. For example, for CUDA 9.2, use: conda activate plant_imager pip install torch == 1 .4.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html To upgrade to Cuda 11.1, type conda install pytorch == 1 .8.0 torchvision == 0 .9.0 torchaudio == 0 .8.0 cudatoolkit = 11 .1 -c pytorch -c conda-forge See the PyTorch webpage for compatible Torch and Cuda combinations.","title":"Install romiseg sources"},{"location":"plant_imager/install/plantdb_setup/","text":"Create a ROMI database to host, receive & serve plant scans Link To follow this guide you should have a conda environment, see here . For the sake of clarity it will be called plantscans_db . Note Since this is still under development, the packages are installed in \"editable mode\" with the -e option. Note If you do not want the hassle of having to create environment & install python libraries, there is a pre-built docker image, with usage instructions here . Install plantdb sources Link Activate your plantscans_db environment! conda activate plantscans_db To create an active ROMI database, you have to install the plantdb package: git clone https://github.com/romi/plantdb.git && \\ cd plantdb && \\ git checkout dev && \\ python3.7 -m pip install setuptools setuptools-scm && \\ python3.7 -m pip install luigi pillow && \\ python3.7 -m pip install flask flask-restful flask-cors && \\ python3.7 -m pip install . Initialize a ROMI database Link The FSDB class from the plantdb module is used to manage a local file system for data storage. A database is any folder which contains a file named plantdb . To create an empty database, just create a new folder, and an empty file named plantdb in it. For example: mkdir /data/romi_db touch /data/romi_db/plantdb Then define its location in an environment variable DB_LOCATION : export DB_LOCATION = '/data/ROMI/DB' Note To permanently set this directory as the location of the DB, add it to your ~/.bashrc file. echo 'export DB_LOCATION=/data/ROMI/DB' >> ~/.bashrc Serve the REST API Link Then you can start the REST API with romi_scanner_rest_api : romi_scanner_rest_api You should see something like: n scans = 2 * Serving Flask app \"romi_scanner_rest_api\" (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit) To access the REST API, open your favorite browser and use URLs to access: the list of all scans: http://0.0.0.0:5000/scans the '2018-12-17_17-05-35' dataset: http://0.0.0.0:5000/scans/2018-12-17_17-05-35 You should see JSON formatted text. Troubleshooting : When starting the REST API with romi_scanner_rest_api , if you get an error message about this executable not being found, it may be missing from the $PATH environment variable. Add it with: export PATH = $PATH : \"/home/ $USER /.local/bin\" Note To permanently set this in your bash terminal, add it to your ~/.bashrc file. echo 'export PATH=$PATH:/home/$USER/.local/bin' >> ~/.bashrc","title":"Database setup"},{"location":"plant_imager/install/plantdb_setup/#create-a-romi-database-to-host-receive-serve-plant-scans","text":"To follow this guide you should have a conda environment, see here . For the sake of clarity it will be called plantscans_db . Note Since this is still under development, the packages are installed in \"editable mode\" with the -e option. Note If you do not want the hassle of having to create environment & install python libraries, there is a pre-built docker image, with usage instructions here .","title":"Create a ROMI database to host, receive &amp; serve plant scans"},{"location":"plant_imager/install/plantdb_setup/#install-plantdb-sources","text":"Activate your plantscans_db environment! conda activate plantscans_db To create an active ROMI database, you have to install the plantdb package: git clone https://github.com/romi/plantdb.git && \\ cd plantdb && \\ git checkout dev && \\ python3.7 -m pip install setuptools setuptools-scm && \\ python3.7 -m pip install luigi pillow && \\ python3.7 -m pip install flask flask-restful flask-cors && \\ python3.7 -m pip install .","title":"Install plantdb sources"},{"location":"plant_imager/install/plantdb_setup/#initialize-a-romi-database","text":"The FSDB class from the plantdb module is used to manage a local file system for data storage. A database is any folder which contains a file named plantdb . To create an empty database, just create a new folder, and an empty file named plantdb in it. For example: mkdir /data/romi_db touch /data/romi_db/plantdb Then define its location in an environment variable DB_LOCATION : export DB_LOCATION = '/data/ROMI/DB' Note To permanently set this directory as the location of the DB, add it to your ~/.bashrc file. echo 'export DB_LOCATION=/data/ROMI/DB' >> ~/.bashrc","title":"Initialize a ROMI database"},{"location":"plant_imager/install/plantdb_setup/#serve-the-rest-api","text":"Then you can start the REST API with romi_scanner_rest_api : romi_scanner_rest_api You should see something like: n scans = 2 * Serving Flask app \"romi_scanner_rest_api\" (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit) To access the REST API, open your favorite browser and use URLs to access: the list of all scans: http://0.0.0.0:5000/scans the '2018-12-17_17-05-35' dataset: http://0.0.0.0:5000/scans/2018-12-17_17-05-35 You should see JSON formatted text. Troubleshooting : When starting the REST API with romi_scanner_rest_api , if you get an error message about this executable not being found, it may be missing from the $PATH environment variable. Add it with: export PATH = $PATH : \"/home/ $USER /.local/bin\" Note To permanently set this in your bash terminal, add it to your ~/.bashrc file. echo 'export PATH=$PATH:/home/$USER/.local/bin' >> ~/.bashrc","title":"Serve the REST API"},{"location":"plant_imager/install/virtual_plant_setup/","text":"Install ROMI software for virtual plants acquisition & reconstruction Link To follows this guide you should have a conda or a Python venv , see here . For the sake of clarity the environment will be called virtual_plants . Notice for using the virtual scanner Link If you want to use the virtual scanner, the modified python version bundled with blender and the environment python version have to match. To obtain the python version bundled with your distribution of blender, type: blender -b --python-expr \"import sys; print(sys.version)\" It will output something like: Blender 2.82 (sub 7) (hash 5b416ffb848e built 2020-02-14 16:19:45) ALSA lib pcm_dmix.c:1089:(snd_pcm_dmix_open) unable to open slave 3.8.1 (default, Jan 22 2020, 06:38:00) [GCC 9.2.0] Blender quit In this case, this means Blender bundle Python 3.8, and you should too. In the following, we will assume that you are using conda environments. If not, adapt with corresponding virtualenv commands. Install ROMI packages with pip : Link Activate your virtual_plants environment! Note Since this is still under development, the packages are installed in \"editable mode\" with the -e option. Install openalea.lpy Link If you're using python>=3.7 and conda , just install lpy from conda: conda install -c conda-forge -c fredboudon openalea.lpy Install romicgal sources Link To pilot the hardware you have to install the plantimager package: python3 -m pip install -e git+https://github.com/romi/romicgal.git@dev Note This takes some time since it has to download dependencies ( CGAL-0.5 & boost-1.72.0 ) and compile them. Install plant3dvision sources Link To start \"acquisition jobs\", you have to install the plant3dvision package: python3 -m pip install -e git+https://github.com/romi/plant3dvision.git@dev Install plantdb sources Link Since we will need an active database to export the acquisitions, you have to install the plantdb package: python3 -m pip install -e git+https://github.com/romi/plantdb.git@dev Install romiseg sources Link To install the additional segmentation module: python3 -m pip install git+https://github.com/romi/romiseg@dev Warning If not using CUDA 10.*, you have to install the matching pytorch distribution. For example, for CUDA 9.2, use: pip install torch == 1 .4.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html Example database Link To quickly create an example DB you can use: wget https://db.romi-project.eu/models/test_db.tar.gz tar -xf test_db.tar.gz This will create a integration_tests folder with a ready to use test database. You should now be ready to perform tasks on virtual plants such as \"creation\", \"acquisition\" & \"reconstruction\" following the dedicated user guide.","title":"Virtual plants scanner software setup"},{"location":"plant_imager/install/virtual_plant_setup/#install-romi-software-for-virtual-plants-acquisition-reconstruction","text":"To follows this guide you should have a conda or a Python venv , see here . For the sake of clarity the environment will be called virtual_plants .","title":"Install ROMI software for virtual plants acquisition &amp; reconstruction"},{"location":"plant_imager/install/virtual_plant_setup/#notice-for-using-the-virtual-scanner","text":"If you want to use the virtual scanner, the modified python version bundled with blender and the environment python version have to match. To obtain the python version bundled with your distribution of blender, type: blender -b --python-expr \"import sys; print(sys.version)\" It will output something like: Blender 2.82 (sub 7) (hash 5b416ffb848e built 2020-02-14 16:19:45) ALSA lib pcm_dmix.c:1089:(snd_pcm_dmix_open) unable to open slave 3.8.1 (default, Jan 22 2020, 06:38:00) [GCC 9.2.0] Blender quit In this case, this means Blender bundle Python 3.8, and you should too. In the following, we will assume that you are using conda environments. If not, adapt with corresponding virtualenv commands.","title":"Notice for using the virtual scanner"},{"location":"plant_imager/install/virtual_plant_setup/#install-romi-packages-with-pip","text":"Activate your virtual_plants environment! Note Since this is still under development, the packages are installed in \"editable mode\" with the -e option.","title":"Install ROMI packages with pip:"},{"location":"plant_imager/install/virtual_plant_setup/#install-openalealpy","text":"If you're using python>=3.7 and conda , just install lpy from conda: conda install -c conda-forge -c fredboudon openalea.lpy","title":"Install openalea.lpy"},{"location":"plant_imager/install/virtual_plant_setup/#install-romicgal-sources","text":"To pilot the hardware you have to install the plantimager package: python3 -m pip install -e git+https://github.com/romi/romicgal.git@dev Note This takes some time since it has to download dependencies ( CGAL-0.5 & boost-1.72.0 ) and compile them.","title":"Install romicgal sources"},{"location":"plant_imager/install/virtual_plant_setup/#install-plant3dvision-sources","text":"To start \"acquisition jobs\", you have to install the plant3dvision package: python3 -m pip install -e git+https://github.com/romi/plant3dvision.git@dev","title":"Install plant3dvision sources"},{"location":"plant_imager/install/virtual_plant_setup/#install-plantdb-sources","text":"Since we will need an active database to export the acquisitions, you have to install the plantdb package: python3 -m pip install -e git+https://github.com/romi/plantdb.git@dev","title":"Install plantdb sources"},{"location":"plant_imager/install/virtual_plant_setup/#install-romiseg-sources","text":"To install the additional segmentation module: python3 -m pip install git+https://github.com/romi/romiseg@dev Warning If not using CUDA 10.*, you have to install the matching pytorch distribution. For example, for CUDA 9.2, use: pip install torch == 1 .4.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html","title":"Install romiseg sources"},{"location":"plant_imager/install/virtual_plant_setup/#example-database","text":"To quickly create an example DB you can use: wget https://db.romi-project.eu/models/test_db.tar.gz tar -xf test_db.tar.gz This will create a integration_tests folder with a ready to use test database. You should now be ready to perform tasks on virtual plants such as \"creation\", \"acquisition\" & \"reconstruction\" following the dedicated user guide.","title":"Example database"},{"location":"plant_imager/metadata/","text":"FAIR data for the ROMI plant scanner project Link FAIR data Link We aim at using FAIR data principles in the ROMI plant scanner project. Quoting the GoFAIR website: Quote In 2016, the \" FAIR Guiding Principles for scientific data management and stewardship \" were published in Scientific Data. The authors intended to provide guidelines to improve the Findability , Accessibility , Interoperability , and Reuse of digital assets. The principles emphasise machine-actionability (i.e., the capacity of computational systems to find, access, interoperate, and reuse data with none or minimal human intervention) because humans increasingly rely on computational support to deal with data as a result of the increase in volume, complexity, and creation speed of data. In our context, a biological dataset is a set of RAW images (eg: RGB images), used to reconstruct the plant 3D structure, associated with a set of metadata of different nature: biological, hardware & software. ROMI plant scanner metadata Link Metadata are stored hierarchically. We currently use the JSON format. There are many JSON files containing metadata in the metadata directory attached to a dataset. General metadata Link The first you should consider is metadata/metadata.json . Its top-level entries are: \"scanner\", the hardware metadata (see here ) \"object\", the biological metadata (see here ) \"path\", the parameter values used for the task Scan (see here ) \"computed\", the parameter values used for the task Colmap (see here ) \"measures\", the parameter values used for the task AnglesAndInternodes (see here ) Todo Remove top-level entries \"path\", \"computed\" & \"measures\", they look like duplicates from their respective task metadata. Scanning operation Link Found under the path top level section, it contains: the trajectory of the camera under path Example: \"path\" : { \"args\" : { \"filetype\" : \"jpg\" , \"num_points\" : 72 , \"radius\" : 350 , \"tilt\" : 0.45 , \"xc\" : 400 , \"yc\" : 400 , \"z\" : 0 }, \"id\" : \"circular_72\" , \"type\" : \"circular\" } Todo Potential duplication from the Scan task metadata! Colmap reconstruction Link Found under the computed top level section, it contains the camera_model used by Colmap. Example: \"computed\" : { \"camera_model\" : { \"height\" : 1080 , \"id\" : 1 , \"model\" : \"OPENCV\" , \"params\" : [ 1102.2709767952233 , 1102.2709767952233 , 808.0 , 540.0 , -0.015118876273724434 , -0.015118876273724434 , 0.0 , 0.0 ], \"width\" : 1616 } } Todo Potential duplication from the Colmap task metadata! Measures of angles and internodes Link Found under the measures top level section: Measured angles are under angles Measured internodes are under internodes Example: \"measures\" : { \"angles\" : [ 2.6179938779914944 , 1.3089969389957472 , ... 2.0943951023931953 ], \"internodes\" : [ 41 , 32 , ... 1 ] } Todo Potential duplication from the AnglesAndInternodes task metadata! Tasks metadata Link Then there are the JSON files attached to each task, e.g. Colmap_True____feature_extrac_3bbfcb1413.json , they contain the parameter used to run this task. Their content and meaning is explained in the task metadata section. Images metadata Link The image JSON file metadata/images.json contains ??? and is produced by ???. Example: { \"task_params\" : { \"fileset_id\" : \"images\" , \"output_file_id\" : \"out\" , \"scan_id\" : \"\" } } Found under the metadata/images directory, these JSON files contains ??? and is produced by ???. Note Sub-section camera_model seems redundant with same section in reconstruction . Example: { \"calibrated_pose\" : [ 49.04537654162613 , 401.1470121046677 , -0.10613970524327433 ], \"colmap_camera\" : { \"camera_model\" : { \"height\" : 1080 , \"id\" : 1 , \"model\" : \"OPENCV\" , \"params\" : [ 1106.9593323985682 , 1106.9593323985682 , 808.0 , 540.0 , -0.012379986602455324 , -0.012379986602455324 , 0.0 , 0.0 ], \"width\" : 1616 }, \"rotmat\" : [ [ -0.07758281248083276 , 0.9961595266033345 , 0.040584538496628464 ], [ -0.4230067224604736 , -0.069751540308706 , 0.9034378979087665 ], [ 0.9027991027691664 , 0.05292372040950383 , 0.42679369706827314 ] ], \"tvec\" : [ -397.2357284138349 , 49.50722946972662 , -66.64522999892229 ] }, \"pose\" : [ 50.0 , 400.0 , 0 , 0.0 , 0.45 ] } Visualization metadata Link The visualization JSON file metadata/Visualization.json contains ??? and is produced by ???. Important Run the Visualization task to complete this section! Example: { \"files\" : { \"angles\" : \"AnglesAndInternodes\" , \"images\" : [ \"image_rgb-000\" , \"image_rgb-001\" , \"...\" \"image_rgb-071\" ], \"mesh\" : \"TriangleMesh\" , \"point_cloud\" : \"PointCloud\" , \"poses\" : \"images\" , \"skeleton\" : \"CurveSkeleton\" , \"thumbnails\" : [ \"thumbnail_rgb-000\" , \"thumbnail_rgb-001\" , \"...\" \"thumbnail_rgb-071\" ], \"zip\" : \"scan\" } } Found under the metadata/Visualization/ directory, there are two category of JSON files: image_*.json contains ??? and is produced by ???. thumbnail_*.json contains ??? and is produced by ???. Example for image_*.json : { \"image_id\" : \"rgb-000\" } Example for thumbnail_*.json : { \"image_id\" : \"rgb-000\" } Other metadata Link Todo Explain what are & who produce: - the image JSON file metadata/images.json - the image JSON files found under metadata/images/*.json - the visualization JSON file metadata/Visualization.json - the visualization JSON files found under metadata/Visualization/*.json","title":"Home"},{"location":"plant_imager/metadata/#fair-data-for-the-romi-plant-scanner-project","text":"","title":"FAIR data for the ROMI plant scanner project"},{"location":"plant_imager/metadata/#fair-data","text":"We aim at using FAIR data principles in the ROMI plant scanner project. Quoting the GoFAIR website: Quote In 2016, the \" FAIR Guiding Principles for scientific data management and stewardship \" were published in Scientific Data. The authors intended to provide guidelines to improve the Findability , Accessibility , Interoperability , and Reuse of digital assets. The principles emphasise machine-actionability (i.e., the capacity of computational systems to find, access, interoperate, and reuse data with none or minimal human intervention) because humans increasingly rely on computational support to deal with data as a result of the increase in volume, complexity, and creation speed of data. In our context, a biological dataset is a set of RAW images (eg: RGB images), used to reconstruct the plant 3D structure, associated with a set of metadata of different nature: biological, hardware & software.","title":"FAIR data"},{"location":"plant_imager/metadata/#romi-plant-scanner-metadata","text":"Metadata are stored hierarchically. We currently use the JSON format. There are many JSON files containing metadata in the metadata directory attached to a dataset.","title":"ROMI plant scanner metadata"},{"location":"plant_imager/metadata/#general-metadata","text":"The first you should consider is metadata/metadata.json . Its top-level entries are: \"scanner\", the hardware metadata (see here ) \"object\", the biological metadata (see here ) \"path\", the parameter values used for the task Scan (see here ) \"computed\", the parameter values used for the task Colmap (see here ) \"measures\", the parameter values used for the task AnglesAndInternodes (see here ) Todo Remove top-level entries \"path\", \"computed\" & \"measures\", they look like duplicates from their respective task metadata.","title":"General metadata"},{"location":"plant_imager/metadata/#scanning-operation","text":"Found under the path top level section, it contains: the trajectory of the camera under path Example: \"path\" : { \"args\" : { \"filetype\" : \"jpg\" , \"num_points\" : 72 , \"radius\" : 350 , \"tilt\" : 0.45 , \"xc\" : 400 , \"yc\" : 400 , \"z\" : 0 }, \"id\" : \"circular_72\" , \"type\" : \"circular\" } Todo Potential duplication from the Scan task metadata!","title":"Scanning operation"},{"location":"plant_imager/metadata/#colmap-reconstruction","text":"Found under the computed top level section, it contains the camera_model used by Colmap. Example: \"computed\" : { \"camera_model\" : { \"height\" : 1080 , \"id\" : 1 , \"model\" : \"OPENCV\" , \"params\" : [ 1102.2709767952233 , 1102.2709767952233 , 808.0 , 540.0 , -0.015118876273724434 , -0.015118876273724434 , 0.0 , 0.0 ], \"width\" : 1616 } } Todo Potential duplication from the Colmap task metadata!","title":"Colmap reconstruction"},{"location":"plant_imager/metadata/#measures-of-angles-and-internodes","text":"Found under the measures top level section: Measured angles are under angles Measured internodes are under internodes Example: \"measures\" : { \"angles\" : [ 2.6179938779914944 , 1.3089969389957472 , ... 2.0943951023931953 ], \"internodes\" : [ 41 , 32 , ... 1 ] } Todo Potential duplication from the AnglesAndInternodes task metadata!","title":"Measures of angles and internodes"},{"location":"plant_imager/metadata/#tasks-metadata","text":"Then there are the JSON files attached to each task, e.g. Colmap_True____feature_extrac_3bbfcb1413.json , they contain the parameter used to run this task. Their content and meaning is explained in the task metadata section.","title":"Tasks metadata"},{"location":"plant_imager/metadata/#images-metadata","text":"The image JSON file metadata/images.json contains ??? and is produced by ???. Example: { \"task_params\" : { \"fileset_id\" : \"images\" , \"output_file_id\" : \"out\" , \"scan_id\" : \"\" } } Found under the metadata/images directory, these JSON files contains ??? and is produced by ???. Note Sub-section camera_model seems redundant with same section in reconstruction . Example: { \"calibrated_pose\" : [ 49.04537654162613 , 401.1470121046677 , -0.10613970524327433 ], \"colmap_camera\" : { \"camera_model\" : { \"height\" : 1080 , \"id\" : 1 , \"model\" : \"OPENCV\" , \"params\" : [ 1106.9593323985682 , 1106.9593323985682 , 808.0 , 540.0 , -0.012379986602455324 , -0.012379986602455324 , 0.0 , 0.0 ], \"width\" : 1616 }, \"rotmat\" : [ [ -0.07758281248083276 , 0.9961595266033345 , 0.040584538496628464 ], [ -0.4230067224604736 , -0.069751540308706 , 0.9034378979087665 ], [ 0.9027991027691664 , 0.05292372040950383 , 0.42679369706827314 ] ], \"tvec\" : [ -397.2357284138349 , 49.50722946972662 , -66.64522999892229 ] }, \"pose\" : [ 50.0 , 400.0 , 0 , 0.0 , 0.45 ] }","title":"Images metadata"},{"location":"plant_imager/metadata/#visualization-metadata","text":"The visualization JSON file metadata/Visualization.json contains ??? and is produced by ???. Important Run the Visualization task to complete this section! Example: { \"files\" : { \"angles\" : \"AnglesAndInternodes\" , \"images\" : [ \"image_rgb-000\" , \"image_rgb-001\" , \"...\" \"image_rgb-071\" ], \"mesh\" : \"TriangleMesh\" , \"point_cloud\" : \"PointCloud\" , \"poses\" : \"images\" , \"skeleton\" : \"CurveSkeleton\" , \"thumbnails\" : [ \"thumbnail_rgb-000\" , \"thumbnail_rgb-001\" , \"...\" \"thumbnail_rgb-071\" ], \"zip\" : \"scan\" } } Found under the metadata/Visualization/ directory, there are two category of JSON files: image_*.json contains ??? and is produced by ???. thumbnail_*.json contains ??? and is produced by ???. Example for image_*.json : { \"image_id\" : \"rgb-000\" } Example for thumbnail_*.json : { \"image_id\" : \"rgb-000\" }","title":"Visualization metadata"},{"location":"plant_imager/metadata/#other-metadata","text":"Todo Explain what are & who produce: - the image JSON file metadata/images.json - the image JSON files found under metadata/images/*.json - the visualization JSON file metadata/Visualization.json - the visualization JSON files found under metadata/Visualization/*.json","title":"Other metadata"},{"location":"plant_imager/metadata/biological_metadata/","text":"Biological Metadata Link Biological metadata are informative of the biological object and its growth conditions. Definitions Link Here is a list of biological metadata and their definition: species : the species of the biological object analysed, eg : \"Arabidopsis thaliana\"; seed stock : an identifier of the seed stock used, eg : \"Col-0\", \"186.AV.L1\", ...; plant id : an identifier for the plant, eg : \"GT1\"; growth environment : , eg : \"Lyon - Indoor\"; growth conditions : growth condition used, eg : \"LD\", \"SD\", \"LD+SD\"; treatment : specific treatment applied, if any, eg : \"Auxin 1mM\"; DAG : Days After Germination or age of the plant in days, eg : 40; sample : part of the plant used, if any, eg : \"main stem\"; experiment id : an identifier for the experiment, eg : \"dry plant\"; dataset id : the Omero dataset identifier for the biological dataset, eg : 12; Configuration Link Todo How is it defined in a TOML configuration file ? Database location Link Located in metadata/metadata.json and found under the object top level section, it contains biologically relevant information such as the studied species, its age and growth conditions. This information is not restricted in its format but should contain a minimal set of entries. Todo Defines the minimal set of entries! Use the MIAPPE standard? JSON example Link Example of a metadata/metadata.json file for biological metadata: \"object\" : { \"age\" : \"62d\" , \"culture\" : \"LD\" , \"environment\" : \"Lyon indoor\" , \"experiment_id\" : \"living plant\" , \"object\" : \"plant\" , \"plant_id\" : \"Col0_26_10_2018_B\" , \"sample\" : \"main stem\" , \"species\" : \"Arabidopsis thaliana\" , \"stock\" : \"186AV.L1\" , \"treatment\" : \"none\" }","title":"Biological metadata"},{"location":"plant_imager/metadata/biological_metadata/#biological-metadata","text":"Biological metadata are informative of the biological object and its growth conditions.","title":"Biological Metadata"},{"location":"plant_imager/metadata/biological_metadata/#definitions","text":"Here is a list of biological metadata and their definition: species : the species of the biological object analysed, eg : \"Arabidopsis thaliana\"; seed stock : an identifier of the seed stock used, eg : \"Col-0\", \"186.AV.L1\", ...; plant id : an identifier for the plant, eg : \"GT1\"; growth environment : , eg : \"Lyon - Indoor\"; growth conditions : growth condition used, eg : \"LD\", \"SD\", \"LD+SD\"; treatment : specific treatment applied, if any, eg : \"Auxin 1mM\"; DAG : Days After Germination or age of the plant in days, eg : 40; sample : part of the plant used, if any, eg : \"main stem\"; experiment id : an identifier for the experiment, eg : \"dry plant\"; dataset id : the Omero dataset identifier for the biological dataset, eg : 12;","title":"Definitions"},{"location":"plant_imager/metadata/biological_metadata/#configuration","text":"Todo How is it defined in a TOML configuration file ?","title":"Configuration"},{"location":"plant_imager/metadata/biological_metadata/#database-location","text":"Located in metadata/metadata.json and found under the object top level section, it contains biologically relevant information such as the studied species, its age and growth conditions. This information is not restricted in its format but should contain a minimal set of entries. Todo Defines the minimal set of entries! Use the MIAPPE standard?","title":"Database location"},{"location":"plant_imager/metadata/biological_metadata/#json-example","text":"Example of a metadata/metadata.json file for biological metadata: \"object\" : { \"age\" : \"62d\" , \"culture\" : \"LD\" , \"environment\" : \"Lyon indoor\" , \"experiment_id\" : \"living plant\" , \"object\" : \"plant\" , \"plant_id\" : \"Col0_26_10_2018_B\" , \"sample\" : \"main stem\" , \"species\" : \"Arabidopsis thaliana\" , \"stock\" : \"186AV.L1\" , \"treatment\" : \"none\" }","title":"JSON example"},{"location":"plant_imager/metadata/hardware_metadata/","text":"Hardware metadata & scan settings Link Hardware metadata are informative of the hardware setup like the used camera, its firmwares or the workspace size. Definitions Link Here is a list of hardware metadata and their definition: frame : scanner frame type and version, eg : \"30profile v1\"; X_motor : type of motor used for the X axis, eg : \"X-Carve NEMA23\"; Y_motor : type of motor used for the Y axis, eg : \"X-Carve NEMA23\"; Z_motor : type of motor used for the Z axis, eg : \"X-Carve NEMA23\"; pan_motor : , type of motor used for the camera pan axis, eg : \"Dynamixel\"; tilt_motor : , type of motor used for the camera tilt axis, eg : \"gimbal\"; sensor : type of sensor used during acquisition, eg : \"Sony alpha\"; scan_os : , eg : \"\"; The metadata dictionary made of frame , X_motor , Y_motor , Z_motor , pan_motor & tilt_motor define the hardware_id used in the README.md . The sensor metadata could be more detailed, for example as a dictionary or a reference to a sensor_id database? Configuration Link Todo How is it defined in a TOML configuration file ? Database location Link Located in metadata/metadata.json and found under the scanner top level section, it contains information about the hardware and software used for the scan: the used camera with camera_args , camera_firmware , camera_hardware & camera_lens the model and version of the scanning station with id list of hardware and software components and their versions with cnc_args , cnc_firmware , cnc_hardware , frame , gimbal_args , gimbal_firmware , gimbal_hardware the used workspace with workspace JSON example Link Example of a metadata/metadata.json file for hardware metadata: \"scanner\" : { \"camera_args\" : { \"api_url\" : \"http://192.168.122.1:8080\" }, \"camera_firmware\" : \"sony_wifi\" , \"camera_hardware\" : \"Sony Alpha 5100\" , \"camera_lens\" : \"16-35 stock\" , \"cnc_args\" : { \"homing\" : true , \"port\" : \"/dev/ttyUSB1\" }, \"cnc_firmware\" : \"grbl-v1.1\" , \"cnc_hardware\" : \"xcarve-v2\" , \"frame\" : \"alu 40mm\" , \"gimbal_args\" : { \"baud_rate\" : 57600 , \"dev\" : \"/dev/ttyUSB0\" , \"tilt0\" : 3072 }, \"gimbal_firmware\" : \"dynamixel-usb2dynamixel\" , \"gimbal_hardware\" : \"dynamixel\" , \"id\" : \"lyon_1\" , \"workspace\" : { \"x\" : [ 200 , 600 ], \"y\" : [ 200 , 600 ], \"z\" : [ -180 , 260 ] } } Todo Gather all camera parameters under a camera section? Gather all cnc parameters under a cnc section? Gather all gimbal parameters under a gimbal section?","title":"Hardware metadata"},{"location":"plant_imager/metadata/hardware_metadata/#hardware-metadata-scan-settings","text":"Hardware metadata are informative of the hardware setup like the used camera, its firmwares or the workspace size.","title":"Hardware metadata &amp; scan settings"},{"location":"plant_imager/metadata/hardware_metadata/#definitions","text":"Here is a list of hardware metadata and their definition: frame : scanner frame type and version, eg : \"30profile v1\"; X_motor : type of motor used for the X axis, eg : \"X-Carve NEMA23\"; Y_motor : type of motor used for the Y axis, eg : \"X-Carve NEMA23\"; Z_motor : type of motor used for the Z axis, eg : \"X-Carve NEMA23\"; pan_motor : , type of motor used for the camera pan axis, eg : \"Dynamixel\"; tilt_motor : , type of motor used for the camera tilt axis, eg : \"gimbal\"; sensor : type of sensor used during acquisition, eg : \"Sony alpha\"; scan_os : , eg : \"\"; The metadata dictionary made of frame , X_motor , Y_motor , Z_motor , pan_motor & tilt_motor define the hardware_id used in the README.md . The sensor metadata could be more detailed, for example as a dictionary or a reference to a sensor_id database?","title":"Definitions"},{"location":"plant_imager/metadata/hardware_metadata/#configuration","text":"Todo How is it defined in a TOML configuration file ?","title":"Configuration"},{"location":"plant_imager/metadata/hardware_metadata/#database-location","text":"Located in metadata/metadata.json and found under the scanner top level section, it contains information about the hardware and software used for the scan: the used camera with camera_args , camera_firmware , camera_hardware & camera_lens the model and version of the scanning station with id list of hardware and software components and their versions with cnc_args , cnc_firmware , cnc_hardware , frame , gimbal_args , gimbal_firmware , gimbal_hardware the used workspace with workspace","title":"Database location"},{"location":"plant_imager/metadata/hardware_metadata/#json-example","text":"Example of a metadata/metadata.json file for hardware metadata: \"scanner\" : { \"camera_args\" : { \"api_url\" : \"http://192.168.122.1:8080\" }, \"camera_firmware\" : \"sony_wifi\" , \"camera_hardware\" : \"Sony Alpha 5100\" , \"camera_lens\" : \"16-35 stock\" , \"cnc_args\" : { \"homing\" : true , \"port\" : \"/dev/ttyUSB1\" }, \"cnc_firmware\" : \"grbl-v1.1\" , \"cnc_hardware\" : \"xcarve-v2\" , \"frame\" : \"alu 40mm\" , \"gimbal_args\" : { \"baud_rate\" : 57600 , \"dev\" : \"/dev/ttyUSB0\" , \"tilt0\" : 3072 }, \"gimbal_firmware\" : \"dynamixel-usb2dynamixel\" , \"gimbal_hardware\" : \"dynamixel\" , \"id\" : \"lyon_1\" , \"workspace\" : { \"x\" : [ 200 , 600 ], \"y\" : [ 200 , 600 ], \"z\" : [ -180 , 260 ] } } Todo Gather all camera parameters under a camera section? Gather all cnc parameters under a cnc section? Gather all gimbal parameters under a gimbal section?","title":"JSON example"},{"location":"plant_imager/metadata/software_metadata/","text":"Software metadata: versioning and history Link They aim at keeping track of the algorithm versions, using their git tag, during any ROMI task. They enable to trace back results and compare them.","title":"Software metadata"},{"location":"plant_imager/metadata/software_metadata/#software-metadata-versioning-and-history","text":"They aim at keeping track of the algorithm versions, using their git tag, during any ROMI task. They enable to trace back results and compare them.","title":"Software metadata: versioning and history"},{"location":"plant_imager/metadata/tasks_metadata/","text":"Tasks metadata: parametrization Link Aim Link They aim at keeping track of the parameters used by algorithms during any ROMI task. They enable to trace back results and compare them. Database location Link Located under the metadata directory of the plant scan dataset, these JSON files contain the parameters used to run the task and are produced by each task. Examples of task metadata JSON file names: AnglesAndInternodes_1_0_2_0_0_1_9e87e344e6.json Colmap_True____feature_extrac_3bbfcb1413.json Masks_True_5_out_9adb9db801.json TreeGraph_out__CurveSkeleton_5dca9a2821.json Undistorted_out_____fb3e3fa0ff.json Voxels_False___background___False_0ac9c133f7.json Visualization.json The tasks ids are a concatenation of the task name, the first values of the first 3 parameters sorted by parameter name and a md5hash of the name/parameters as a cananocalised json (from luigi documentation of task_id_str) AnglesAndInternodes task Link Configuration Link To configure this task, we use the [AnglesAndInternodes] section in the TOML configuration file. For example: [AnglesAndInternodes] upstream_task = \"TreeGraph\" characteristic_length = 1.0 stem_axis_inverted = false Database location Link Found under metadata/AnglesAndInternodes_*.json . JSON example Link Example of metadata/AnglesAndInternodes_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"characteristic_length\" : 1.0 , \"min_elongation_ratio\" : 2.0 , \"min_fruit_size\" : 0.1 , \"number_nn\" : 50 , \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"stem_axis\" : 2 , \"stem_axis_inverted\" : \"False\" , \"upstream_task\" : \"ClusteredMesh\" } } ClusteredMesh task Link Configuration Link To configure this task, we use the [Colmap] section in the TOML configuration file. For example: [ClusteredMesh] upstream_task = \"SegmentedPointCloud\" min_vol = 1.0 min_length = 10.0 Database location Link Found under metadata/ClusteredMesh_*.json . JSON example Link Example of metadata/ClusteredMesh_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"min_length\" : 10.0 , \"min_vol\" : 1.0 , \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"upstream_task\" : \"SegmentedPointCloud\" } } Colmap task Link Configuration Link To configure this task, we use the [Colmap] section in the TOML configuration file. For example: [Colmap] matcher = \"exhaustive\" compute_dense = false [Colmap.cli_args.feature_extractor] \"--ImageReader.single_camera\" = \"1\" \"--SiftExtraction.use_gpu\" = \"1\" [Colmap.cli_args.exhaustive_matcher] \"--SiftMatching.use_gpu\" = \"1\" [Colmap.cli_args.model_aligner] \"--robust_alignment_max_error\" = \"10\" [ Colmap . bounding_box ] # default to None x = [ 150 , 650 ] y = [ 150 , 650 ] z = [ -90 , 300 ] Database location Link Found under metadata/Colmap_*.json . JSON example Link Example of metadata/Colmap_*.json corresponding to the example TOML configuration file: { \"bounding_box\" : { \"x\" : [ 282.3466418101626 , 590.7798175997629 ], \"y\" : [ 386.77943280470265 , 589.1307313142569 ], \"z\" : [ 22.820168903922998 , 259.7594718279537 ] }, \"task_params\" : { \"align_pcd\" : \"True\" , \"calibration_scan_id\" : \"\" , \"cli_args\" : { \"exhaustive_matcher\" : { \"--SiftMatching.use_gpu\" : \"1\" }, \"feature_extractor\" : { \"--ImageReader.single_camera\" : \"1\" , \"--SiftExtraction.use_gpu\" : \"1\" }, \"model_aligner\" : { \"--robust_alignment_max_error\" : \"10\" } }, \"compute_dense\" : \"False\" , \"matcher\" : \"exhaustive\" , \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"upstream_task\" : \"ImagesFilesetExists\" } } CurveSkeleton task Link Configuration Link To configure this task, we use the [CurveSkeleton] section in the TOML configuration file. For example: [CurveSkeleton] upstream_task = \"TriangleMesh\" Database location Link Found under metadata/CurveSkeleton_*.json . JSON example Link Example of metadata/CurveSkeleton_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"upstream_task\" : \"TriangleMesh\" } } Masks task Link Configuration Link To configure this task, we use the [Masks] section in the TOML configuration file. Default parametrization based on linear masking. For example: [Masks] upstream_task = \"Undistorted\" type = \"linear\" parameters = \"[0,1,0]\" dilation = 5 binarize = true threshold = 0.3 Database location Link Found under metadata/Masks_*.json . JSON example Link Example of metadata/Masks_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"binarize\" : \"True\" , \"dilation\" : 5 , \"output_file_id\" : \"out\" , \"parameters\" : [ 0 , 1 , 0 ], \"query\" : {}, \"scan_id\" : \"\" , \"threshold\" : 0.3 , \"type\" : \"linear\" , \"upstream_task\" : \"ImagesFilesetExists\" } } PointCloud task Link Configuration Link To configure this task, we use the [PointCloud] section in the TOML configuration file. For example: [PointCloud] upstream_task = \"Voxels\" level_set_value = 1.0 log = false background_prior = -200 min_contrast = 10.0 min_score = 0.2 Database location Link Found under metadata/PointCloud_*.json . JSON example Link Example of metadata/PointCloud_*.json : { \"task_params\" : { \"background_prior\" : 0.5 , \"level_set_value\" : 1 , \"log\" : \"False\" , \"min_contrast\" : 10.0 , \"min_score\" : 0.2 , \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"upstream_task\" : \"Voxels\" } } Segmentation2D task Link Configuration Link To configure this task, we use the [Segmentation2D] section in the TOML configuration file. For example: [Segmentation2D] upstream_task = \"Undistorted\" model_fileset = \"ModelFileset\" query = \"{\\\"channel\\\":\\\"rgb\\\"}\" model_id = \"Resnetdataset_gl_png_896_896_epoch50\" resize = true binarize = true dilation = 1 Sx = 896 Sy = 896 epochs = 1 batch = 1 learning_rate = 0.0001 threshold = 0.0035 [ModelFileset] scan_id = \"models\" Database location Link Found under metadata/Segmentation2D_*.json . JSON example Link Example of metadata/Segmentation2D_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"Sx\" : 896 , \"Sy\" : 896 , \"binarize\" : \"True\" , \"dilation\" : 1 , \"inverted_labels\" : [ \"background\" ], \"labels\" : [], \"model_fileset\" : \"ModelFileset\" , \"model_id\" : \"Resnetdataset_gl_png_896_896_epoch50\" , \"output_file_id\" : \"out\" , \"query\" : { \"channel\" : \"rgb\" }, \"resize\" : \"True\" , \"scan_id\" : \"\" , \"threshold\" : 0.0035 , \"upstream_task\" : \"Undistorted\" } } SegmentedPointCloud task Link Configuration Link To configure this task, we use the [SegmentedPointCloud] section in the TOML configuration file. For example: [SegmentedPointCloud] upstream_segmentation = \"Segmentation2D\" upstream_task = \"PointCloud\" use_colmap_poses = true Database location Link Found under metadata/SegmentedPointCloud_*.json . JSON example Link Example of metadata/SegmentedPointCloud_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"upstream_segmentation\" : \"Segmentation2D\" , \"upstream_task\" : \"PointCloud\" , \"use_colmap_poses\" : \"True\" } } TreeGraph task Link Configuration Link To configure this task, we use the [TreeGraph] section in the TOML configuration file. For example: [TreeGraph] upstream_task = \"CurveSkeleton\" z_axis = 2 stem_axis_inverted = false Database location Link Found under metadata/TreeGraph_*.json . JSON example Link Example of metadata/TreeGraph_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"upstream_task\" : \"CurveSkeleton\" , \"z_axis\" : 2 , \"z_orientation\" : 1 } } TriangleMesh task Link Configuration Link To configure this task, we use the [TriangleMesh] section in the TOML configuration file. For example: [TriangleMesh] upstream_task = \"PointCloud\" Database location Link Found under metadata/TriangleMesh_*.json . JSON example Link Example of metadata/TriangleMesh_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"upstream_task\" : \"PointCloud\" } } Undistorted task Link Configuration Link To configure this task, we use the [Undistorted] section in the TOML configuration file. For example: [Undistorted] upstream_task = \"ImagesFilesetExists\" Database location Link Found under metadata/Undistorted_*.json . JSON example Link Example of metadata/Undistorted_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"output_file_id\" : \"out\" , \"query\" : {}, \"scan_id\" : \"\" , \"upstream_task\" : \"ImagesFilesetExists\" } } Visualization task Link Configuration Link To configure this task, we use the [Visualization] section in the TOML configuration file. For example: [Visualization] upstream_point_cloud = \"PointCloud\" upstream_mesh = \"TriangleMesh\" upstream_colmap = \"Colmap\" upstream_angles = \"AnglesAndInternodes\" upstream_skeleton = \"CurveSkeleton\" upstream_images = \"ImagesFilesetExists\" max_image_size = 1500 max_point_cloud_size = 10000 thumbnail_size = 150 Database location Link Found under metadata/Visualization.json . JSON example Link Example of metadata/Visualization.json corresponding to the example TOML configuration file: { \"files\" : { \"angles\" : \"AnglesAndInternodes\" , \"camera\" : \"cameras\" , \"images\" : [ \"image_00000_rgb\" , \"image_00001_rgb\" , \"image_00002_rgb\" , \"image_00003_rgb\" , \"image_00004_rgb\" , \"image_00005_rgb\" , \"image_00006_rgb\" , \"image_00007_rgb\" , \"image_00008_rgb\" , \"image_00009_rgb\" , \"image_00010_rgb\" , \"image_00011_rgb\" , \"image_00012_rgb\" , \"image_00013_rgb\" , \"image_00014_rgb\" , \"image_00015_rgb\" , \"image_00016_rgb\" , \"image_00017_rgb\" , \"image_00018_rgb\" , \"image_00019_rgb\" , \"image_00020_rgb\" , \"image_00021_rgb\" , \"image_00022_rgb\" , \"image_00023_rgb\" , \"image_00024_rgb\" , \"image_00025_rgb\" , \"image_00026_rgb\" , \"image_00027_rgb\" , \"image_00028_rgb\" , \"image_00029_rgb\" , \"image_00030_rgb\" , \"image_00031_rgb\" , \"image_00032_rgb\" , \"image_00033_rgb\" , \"image_00034_rgb\" , \"image_00035_rgb\" , \"image_00036_rgb\" , \"image_00037_rgb\" , \"image_00038_rgb\" , \"image_00039_rgb\" , \"image_00040_rgb\" , \"image_00041_rgb\" , \"image_00042_rgb\" , \"image_00043_rgb\" , \"image_00044_rgb\" , \"image_00045_rgb\" , \"image_00046_rgb\" , \"image_00047_rgb\" , \"image_00048_rgb\" , \"image_00049_rgb\" , \"image_00050_rgb\" , \"image_00051_rgb\" , \"image_00052_rgb\" , \"image_00053_rgb\" , \"image_00054_rgb\" , \"image_00055_rgb\" , \"image_00056_rgb\" , \"image_00057_rgb\" , \"image_00058_rgb\" , \"image_00059_rgb\" ], \"measures\" : \"measures\" , \"mesh\" : \"TriangleMesh\" , \"point_cloud\" : \"PointCloud\" , \"poses\" : \"images\" , \"skeleton\" : \"CurveSkeleton\" , \"thumbnails\" : [ \"thumbnail_00000_rgb\" , \"thumbnail_00001_rgb\" , \"thumbnail_00002_rgb\" , \"thumbnail_00003_rgb\" , \"thumbnail_00004_rgb\" , \"thumbnail_00005_rgb\" , \"thumbnail_00006_rgb\" , \"thumbnail_00007_rgb\" , \"thumbnail_00008_rgb\" , \"thumbnail_00009_rgb\" , \"thumbnail_00010_rgb\" , \"thumbnail_00011_rgb\" , \"thumbnail_00012_rgb\" , \"thumbnail_00013_rgb\" , \"thumbnail_00014_rgb\" , \"thumbnail_00015_rgb\" , \"thumbnail_00016_rgb\" , \"thumbnail_00017_rgb\" , \"thumbnail_00018_rgb\" , \"thumbnail_00019_rgb\" , \"thumbnail_00020_rgb\" , \"thumbnail_00021_rgb\" , \"thumbnail_00022_rgb\" , \"thumbnail_00023_rgb\" , \"thumbnail_00024_rgb\" , \"thumbnail_00025_rgb\" , \"thumbnail_00026_rgb\" , \"thumbnail_00027_rgb\" , \"thumbnail_00028_rgb\" , \"thumbnail_00029_rgb\" , \"thumbnail_00030_rgb\" , \"thumbnail_00031_rgb\" , \"thumbnail_00032_rgb\" , \"thumbnail_00033_rgb\" , \"thumbnail_00034_rgb\" , \"thumbnail_00035_rgb\" , \"thumbnail_00036_rgb\" , \"thumbnail_00037_rgb\" , \"thumbnail_00038_rgb\" , \"thumbnail_00039_rgb\" , \"thumbnail_00040_rgb\" , \"thumbnail_00041_rgb\" , \"thumbnail_00042_rgb\" , \"thumbnail_00043_rgb\" , \"thumbnail_00044_rgb\" , \"thumbnail_00045_rgb\" , \"thumbnail_00046_rgb\" , \"thumbnail_00047_rgb\" , \"thumbnail_00048_rgb\" , \"thumbnail_00049_rgb\" , \"thumbnail_00050_rgb\" , \"thumbnail_00051_rgb\" , \"thumbnail_00052_rgb\" , \"thumbnail_00053_rgb\" , \"thumbnail_00054_rgb\" , \"thumbnail_00055_rgb\" , \"thumbnail_00056_rgb\" , \"thumbnail_00057_rgb\" , \"thumbnail_00058_rgb\" , \"thumbnail_00059_rgb\" ], \"zip\" : \"scan\" }, \"task_params\" : { \"max_image_size\" : 1500 , \"max_point_cloud_size\" : 10000 , \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"thumbnail_size\" : 150 , \"upstream_angles\" : \"AnglesAndInternodes\" , \"upstream_colmap\" : \"Colmap\" , \"upstream_images\" : \"Undistorted\" , \"upstream_mesh\" : \"TriangleMesh\" , \"upstream_point_cloud\" : \"PointCloud\" , \"upstream_skeleton\" : \"CurveSkeleton\" } } Voxels task Link Configuration Link To configure this task, we use the [Voxels] section in the TOML configuration file. For example: [Voxels] upstream_mask = \"Masks\" upstream_colmap = \"Colmap\" use_colmap_poses = true voxel_size = 1.0 type = \"carving\" log = false invert = false labels = \"[]\" Database location Link Found under metadata/Voxels_*.json . JSON example Link Example of metadata/Voxels_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"invert\" : \"False\" , \"labels\" : [ \"background\" ], \"log\" : \"False\" , \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"type\" : \"averaging\" , \"upstream_colmap\" : \"Colmap\" , \"upstream_mask\" : \"Segmentation2D\" , \"use_colmap_poses\" : \"True\" , \"voxel_size\" : 0.01 } } Scan task Link Todo For some reason the parameters are defined in the metadata/metadata.json file. Definition can be found here .","title":"Tasks metadata"},{"location":"plant_imager/metadata/tasks_metadata/#tasks-metadata-parametrization","text":"","title":"Tasks metadata: parametrization"},{"location":"plant_imager/metadata/tasks_metadata/#aim","text":"They aim at keeping track of the parameters used by algorithms during any ROMI task. They enable to trace back results and compare them.","title":"Aim"},{"location":"plant_imager/metadata/tasks_metadata/#database-location","text":"Located under the metadata directory of the plant scan dataset, these JSON files contain the parameters used to run the task and are produced by each task. Examples of task metadata JSON file names: AnglesAndInternodes_1_0_2_0_0_1_9e87e344e6.json Colmap_True____feature_extrac_3bbfcb1413.json Masks_True_5_out_9adb9db801.json TreeGraph_out__CurveSkeleton_5dca9a2821.json Undistorted_out_____fb3e3fa0ff.json Voxels_False___background___False_0ac9c133f7.json Visualization.json The tasks ids are a concatenation of the task name, the first values of the first 3 parameters sorted by parameter name and a md5hash of the name/parameters as a cananocalised json (from luigi documentation of task_id_str)","title":"Database location"},{"location":"plant_imager/metadata/tasks_metadata/#anglesandinternodes-task","text":"","title":"AnglesAndInternodes task"},{"location":"plant_imager/metadata/tasks_metadata/#configuration","text":"To configure this task, we use the [AnglesAndInternodes] section in the TOML configuration file. For example: [AnglesAndInternodes] upstream_task = \"TreeGraph\" characteristic_length = 1.0 stem_axis_inverted = false","title":"Configuration"},{"location":"plant_imager/metadata/tasks_metadata/#database-location_1","text":"Found under metadata/AnglesAndInternodes_*.json .","title":"Database location"},{"location":"plant_imager/metadata/tasks_metadata/#json-example","text":"Example of metadata/AnglesAndInternodes_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"characteristic_length\" : 1.0 , \"min_elongation_ratio\" : 2.0 , \"min_fruit_size\" : 0.1 , \"number_nn\" : 50 , \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"stem_axis\" : 2 , \"stem_axis_inverted\" : \"False\" , \"upstream_task\" : \"ClusteredMesh\" } }","title":"JSON example"},{"location":"plant_imager/metadata/tasks_metadata/#clusteredmesh-task","text":"","title":"ClusteredMesh task"},{"location":"plant_imager/metadata/tasks_metadata/#configuration_1","text":"To configure this task, we use the [Colmap] section in the TOML configuration file. For example: [ClusteredMesh] upstream_task = \"SegmentedPointCloud\" min_vol = 1.0 min_length = 10.0","title":"Configuration"},{"location":"plant_imager/metadata/tasks_metadata/#database-location_2","text":"Found under metadata/ClusteredMesh_*.json .","title":"Database location"},{"location":"plant_imager/metadata/tasks_metadata/#json-example_1","text":"Example of metadata/ClusteredMesh_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"min_length\" : 10.0 , \"min_vol\" : 1.0 , \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"upstream_task\" : \"SegmentedPointCloud\" } }","title":"JSON example"},{"location":"plant_imager/metadata/tasks_metadata/#colmap-task","text":"","title":"Colmap task"},{"location":"plant_imager/metadata/tasks_metadata/#configuration_2","text":"To configure this task, we use the [Colmap] section in the TOML configuration file. For example: [Colmap] matcher = \"exhaustive\" compute_dense = false [Colmap.cli_args.feature_extractor] \"--ImageReader.single_camera\" = \"1\" \"--SiftExtraction.use_gpu\" = \"1\" [Colmap.cli_args.exhaustive_matcher] \"--SiftMatching.use_gpu\" = \"1\" [Colmap.cli_args.model_aligner] \"--robust_alignment_max_error\" = \"10\" [ Colmap . bounding_box ] # default to None x = [ 150 , 650 ] y = [ 150 , 650 ] z = [ -90 , 300 ]","title":"Configuration"},{"location":"plant_imager/metadata/tasks_metadata/#database-location_3","text":"Found under metadata/Colmap_*.json .","title":"Database location"},{"location":"plant_imager/metadata/tasks_metadata/#json-example_2","text":"Example of metadata/Colmap_*.json corresponding to the example TOML configuration file: { \"bounding_box\" : { \"x\" : [ 282.3466418101626 , 590.7798175997629 ], \"y\" : [ 386.77943280470265 , 589.1307313142569 ], \"z\" : [ 22.820168903922998 , 259.7594718279537 ] }, \"task_params\" : { \"align_pcd\" : \"True\" , \"calibration_scan_id\" : \"\" , \"cli_args\" : { \"exhaustive_matcher\" : { \"--SiftMatching.use_gpu\" : \"1\" }, \"feature_extractor\" : { \"--ImageReader.single_camera\" : \"1\" , \"--SiftExtraction.use_gpu\" : \"1\" }, \"model_aligner\" : { \"--robust_alignment_max_error\" : \"10\" } }, \"compute_dense\" : \"False\" , \"matcher\" : \"exhaustive\" , \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"upstream_task\" : \"ImagesFilesetExists\" } }","title":"JSON example"},{"location":"plant_imager/metadata/tasks_metadata/#curveskeleton-task","text":"","title":"CurveSkeleton task"},{"location":"plant_imager/metadata/tasks_metadata/#configuration_3","text":"To configure this task, we use the [CurveSkeleton] section in the TOML configuration file. For example: [CurveSkeleton] upstream_task = \"TriangleMesh\"","title":"Configuration"},{"location":"plant_imager/metadata/tasks_metadata/#database-location_4","text":"Found under metadata/CurveSkeleton_*.json .","title":"Database location"},{"location":"plant_imager/metadata/tasks_metadata/#json-example_3","text":"Example of metadata/CurveSkeleton_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"upstream_task\" : \"TriangleMesh\" } }","title":"JSON example"},{"location":"plant_imager/metadata/tasks_metadata/#masks-task","text":"","title":"Masks task"},{"location":"plant_imager/metadata/tasks_metadata/#configuration_4","text":"To configure this task, we use the [Masks] section in the TOML configuration file. Default parametrization based on linear masking. For example: [Masks] upstream_task = \"Undistorted\" type = \"linear\" parameters = \"[0,1,0]\" dilation = 5 binarize = true threshold = 0.3","title":"Configuration"},{"location":"plant_imager/metadata/tasks_metadata/#database-location_5","text":"Found under metadata/Masks_*.json .","title":"Database location"},{"location":"plant_imager/metadata/tasks_metadata/#json-example_4","text":"Example of metadata/Masks_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"binarize\" : \"True\" , \"dilation\" : 5 , \"output_file_id\" : \"out\" , \"parameters\" : [ 0 , 1 , 0 ], \"query\" : {}, \"scan_id\" : \"\" , \"threshold\" : 0.3 , \"type\" : \"linear\" , \"upstream_task\" : \"ImagesFilesetExists\" } }","title":"JSON example"},{"location":"plant_imager/metadata/tasks_metadata/#pointcloud-task","text":"","title":"PointCloud task"},{"location":"plant_imager/metadata/tasks_metadata/#configuration_5","text":"To configure this task, we use the [PointCloud] section in the TOML configuration file. For example: [PointCloud] upstream_task = \"Voxels\" level_set_value = 1.0 log = false background_prior = -200 min_contrast = 10.0 min_score = 0.2","title":"Configuration"},{"location":"plant_imager/metadata/tasks_metadata/#database-location_6","text":"Found under metadata/PointCloud_*.json .","title":"Database location"},{"location":"plant_imager/metadata/tasks_metadata/#json-example_5","text":"Example of metadata/PointCloud_*.json : { \"task_params\" : { \"background_prior\" : 0.5 , \"level_set_value\" : 1 , \"log\" : \"False\" , \"min_contrast\" : 10.0 , \"min_score\" : 0.2 , \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"upstream_task\" : \"Voxels\" } }","title":"JSON example"},{"location":"plant_imager/metadata/tasks_metadata/#segmentation2d-task","text":"","title":"Segmentation2D task"},{"location":"plant_imager/metadata/tasks_metadata/#configuration_6","text":"To configure this task, we use the [Segmentation2D] section in the TOML configuration file. For example: [Segmentation2D] upstream_task = \"Undistorted\" model_fileset = \"ModelFileset\" query = \"{\\\"channel\\\":\\\"rgb\\\"}\" model_id = \"Resnetdataset_gl_png_896_896_epoch50\" resize = true binarize = true dilation = 1 Sx = 896 Sy = 896 epochs = 1 batch = 1 learning_rate = 0.0001 threshold = 0.0035 [ModelFileset] scan_id = \"models\"","title":"Configuration"},{"location":"plant_imager/metadata/tasks_metadata/#database-location_7","text":"Found under metadata/Segmentation2D_*.json .","title":"Database location"},{"location":"plant_imager/metadata/tasks_metadata/#json-example_6","text":"Example of metadata/Segmentation2D_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"Sx\" : 896 , \"Sy\" : 896 , \"binarize\" : \"True\" , \"dilation\" : 1 , \"inverted_labels\" : [ \"background\" ], \"labels\" : [], \"model_fileset\" : \"ModelFileset\" , \"model_id\" : \"Resnetdataset_gl_png_896_896_epoch50\" , \"output_file_id\" : \"out\" , \"query\" : { \"channel\" : \"rgb\" }, \"resize\" : \"True\" , \"scan_id\" : \"\" , \"threshold\" : 0.0035 , \"upstream_task\" : \"Undistorted\" } }","title":"JSON example"},{"location":"plant_imager/metadata/tasks_metadata/#segmentedpointcloud-task","text":"","title":"SegmentedPointCloud task"},{"location":"plant_imager/metadata/tasks_metadata/#configuration_7","text":"To configure this task, we use the [SegmentedPointCloud] section in the TOML configuration file. For example: [SegmentedPointCloud] upstream_segmentation = \"Segmentation2D\" upstream_task = \"PointCloud\" use_colmap_poses = true","title":"Configuration"},{"location":"plant_imager/metadata/tasks_metadata/#database-location_8","text":"Found under metadata/SegmentedPointCloud_*.json .","title":"Database location"},{"location":"plant_imager/metadata/tasks_metadata/#json-example_7","text":"Example of metadata/SegmentedPointCloud_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"upstream_segmentation\" : \"Segmentation2D\" , \"upstream_task\" : \"PointCloud\" , \"use_colmap_poses\" : \"True\" } }","title":"JSON example"},{"location":"plant_imager/metadata/tasks_metadata/#treegraph-task","text":"","title":"TreeGraph task"},{"location":"plant_imager/metadata/tasks_metadata/#configuration_8","text":"To configure this task, we use the [TreeGraph] section in the TOML configuration file. For example: [TreeGraph] upstream_task = \"CurveSkeleton\" z_axis = 2 stem_axis_inverted = false","title":"Configuration"},{"location":"plant_imager/metadata/tasks_metadata/#database-location_9","text":"Found under metadata/TreeGraph_*.json .","title":"Database location"},{"location":"plant_imager/metadata/tasks_metadata/#json-example_8","text":"Example of metadata/TreeGraph_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"upstream_task\" : \"CurveSkeleton\" , \"z_axis\" : 2 , \"z_orientation\" : 1 } }","title":"JSON example"},{"location":"plant_imager/metadata/tasks_metadata/#trianglemesh-task","text":"","title":"TriangleMesh task"},{"location":"plant_imager/metadata/tasks_metadata/#configuration_9","text":"To configure this task, we use the [TriangleMesh] section in the TOML configuration file. For example: [TriangleMesh] upstream_task = \"PointCloud\"","title":"Configuration"},{"location":"plant_imager/metadata/tasks_metadata/#database-location_10","text":"Found under metadata/TriangleMesh_*.json .","title":"Database location"},{"location":"plant_imager/metadata/tasks_metadata/#json-example_9","text":"Example of metadata/TriangleMesh_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"upstream_task\" : \"PointCloud\" } }","title":"JSON example"},{"location":"plant_imager/metadata/tasks_metadata/#undistorted-task","text":"","title":"Undistorted task"},{"location":"plant_imager/metadata/tasks_metadata/#configuration_10","text":"To configure this task, we use the [Undistorted] section in the TOML configuration file. For example: [Undistorted] upstream_task = \"ImagesFilesetExists\"","title":"Configuration"},{"location":"plant_imager/metadata/tasks_metadata/#database-location_11","text":"Found under metadata/Undistorted_*.json .","title":"Database location"},{"location":"plant_imager/metadata/tasks_metadata/#json-example_10","text":"Example of metadata/Undistorted_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"output_file_id\" : \"out\" , \"query\" : {}, \"scan_id\" : \"\" , \"upstream_task\" : \"ImagesFilesetExists\" } }","title":"JSON example"},{"location":"plant_imager/metadata/tasks_metadata/#visualization-task","text":"","title":"Visualization task"},{"location":"plant_imager/metadata/tasks_metadata/#configuration_11","text":"To configure this task, we use the [Visualization] section in the TOML configuration file. For example: [Visualization] upstream_point_cloud = \"PointCloud\" upstream_mesh = \"TriangleMesh\" upstream_colmap = \"Colmap\" upstream_angles = \"AnglesAndInternodes\" upstream_skeleton = \"CurveSkeleton\" upstream_images = \"ImagesFilesetExists\" max_image_size = 1500 max_point_cloud_size = 10000 thumbnail_size = 150","title":"Configuration"},{"location":"plant_imager/metadata/tasks_metadata/#database-location_12","text":"Found under metadata/Visualization.json .","title":"Database location"},{"location":"plant_imager/metadata/tasks_metadata/#json-example_11","text":"Example of metadata/Visualization.json corresponding to the example TOML configuration file: { \"files\" : { \"angles\" : \"AnglesAndInternodes\" , \"camera\" : \"cameras\" , \"images\" : [ \"image_00000_rgb\" , \"image_00001_rgb\" , \"image_00002_rgb\" , \"image_00003_rgb\" , \"image_00004_rgb\" , \"image_00005_rgb\" , \"image_00006_rgb\" , \"image_00007_rgb\" , \"image_00008_rgb\" , \"image_00009_rgb\" , \"image_00010_rgb\" , \"image_00011_rgb\" , \"image_00012_rgb\" , \"image_00013_rgb\" , \"image_00014_rgb\" , \"image_00015_rgb\" , \"image_00016_rgb\" , \"image_00017_rgb\" , \"image_00018_rgb\" , \"image_00019_rgb\" , \"image_00020_rgb\" , \"image_00021_rgb\" , \"image_00022_rgb\" , \"image_00023_rgb\" , \"image_00024_rgb\" , \"image_00025_rgb\" , \"image_00026_rgb\" , \"image_00027_rgb\" , \"image_00028_rgb\" , \"image_00029_rgb\" , \"image_00030_rgb\" , \"image_00031_rgb\" , \"image_00032_rgb\" , \"image_00033_rgb\" , \"image_00034_rgb\" , \"image_00035_rgb\" , \"image_00036_rgb\" , \"image_00037_rgb\" , \"image_00038_rgb\" , \"image_00039_rgb\" , \"image_00040_rgb\" , \"image_00041_rgb\" , \"image_00042_rgb\" , \"image_00043_rgb\" , \"image_00044_rgb\" , \"image_00045_rgb\" , \"image_00046_rgb\" , \"image_00047_rgb\" , \"image_00048_rgb\" , \"image_00049_rgb\" , \"image_00050_rgb\" , \"image_00051_rgb\" , \"image_00052_rgb\" , \"image_00053_rgb\" , \"image_00054_rgb\" , \"image_00055_rgb\" , \"image_00056_rgb\" , \"image_00057_rgb\" , \"image_00058_rgb\" , \"image_00059_rgb\" ], \"measures\" : \"measures\" , \"mesh\" : \"TriangleMesh\" , \"point_cloud\" : \"PointCloud\" , \"poses\" : \"images\" , \"skeleton\" : \"CurveSkeleton\" , \"thumbnails\" : [ \"thumbnail_00000_rgb\" , \"thumbnail_00001_rgb\" , \"thumbnail_00002_rgb\" , \"thumbnail_00003_rgb\" , \"thumbnail_00004_rgb\" , \"thumbnail_00005_rgb\" , \"thumbnail_00006_rgb\" , \"thumbnail_00007_rgb\" , \"thumbnail_00008_rgb\" , \"thumbnail_00009_rgb\" , \"thumbnail_00010_rgb\" , \"thumbnail_00011_rgb\" , \"thumbnail_00012_rgb\" , \"thumbnail_00013_rgb\" , \"thumbnail_00014_rgb\" , \"thumbnail_00015_rgb\" , \"thumbnail_00016_rgb\" , \"thumbnail_00017_rgb\" , \"thumbnail_00018_rgb\" , \"thumbnail_00019_rgb\" , \"thumbnail_00020_rgb\" , \"thumbnail_00021_rgb\" , \"thumbnail_00022_rgb\" , \"thumbnail_00023_rgb\" , \"thumbnail_00024_rgb\" , \"thumbnail_00025_rgb\" , \"thumbnail_00026_rgb\" , \"thumbnail_00027_rgb\" , \"thumbnail_00028_rgb\" , \"thumbnail_00029_rgb\" , \"thumbnail_00030_rgb\" , \"thumbnail_00031_rgb\" , \"thumbnail_00032_rgb\" , \"thumbnail_00033_rgb\" , \"thumbnail_00034_rgb\" , \"thumbnail_00035_rgb\" , \"thumbnail_00036_rgb\" , \"thumbnail_00037_rgb\" , \"thumbnail_00038_rgb\" , \"thumbnail_00039_rgb\" , \"thumbnail_00040_rgb\" , \"thumbnail_00041_rgb\" , \"thumbnail_00042_rgb\" , \"thumbnail_00043_rgb\" , \"thumbnail_00044_rgb\" , \"thumbnail_00045_rgb\" , \"thumbnail_00046_rgb\" , \"thumbnail_00047_rgb\" , \"thumbnail_00048_rgb\" , \"thumbnail_00049_rgb\" , \"thumbnail_00050_rgb\" , \"thumbnail_00051_rgb\" , \"thumbnail_00052_rgb\" , \"thumbnail_00053_rgb\" , \"thumbnail_00054_rgb\" , \"thumbnail_00055_rgb\" , \"thumbnail_00056_rgb\" , \"thumbnail_00057_rgb\" , \"thumbnail_00058_rgb\" , \"thumbnail_00059_rgb\" ], \"zip\" : \"scan\" }, \"task_params\" : { \"max_image_size\" : 1500 , \"max_point_cloud_size\" : 10000 , \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"thumbnail_size\" : 150 , \"upstream_angles\" : \"AnglesAndInternodes\" , \"upstream_colmap\" : \"Colmap\" , \"upstream_images\" : \"Undistorted\" , \"upstream_mesh\" : \"TriangleMesh\" , \"upstream_point_cloud\" : \"PointCloud\" , \"upstream_skeleton\" : \"CurveSkeleton\" } }","title":"JSON example"},{"location":"plant_imager/metadata/tasks_metadata/#voxels-task","text":"","title":"Voxels task"},{"location":"plant_imager/metadata/tasks_metadata/#configuration_12","text":"To configure this task, we use the [Voxels] section in the TOML configuration file. For example: [Voxels] upstream_mask = \"Masks\" upstream_colmap = \"Colmap\" use_colmap_poses = true voxel_size = 1.0 type = \"carving\" log = false invert = false labels = \"[]\"","title":"Configuration"},{"location":"plant_imager/metadata/tasks_metadata/#database-location_13","text":"Found under metadata/Voxels_*.json .","title":"Database location"},{"location":"plant_imager/metadata/tasks_metadata/#json-example_12","text":"Example of metadata/Voxels_*.json corresponding to the example TOML configuration file: { \"task_params\" : { \"invert\" : \"False\" , \"labels\" : [ \"background\" ], \"log\" : \"False\" , \"output_file_id\" : \"out\" , \"scan_id\" : \"\" , \"type\" : \"averaging\" , \"upstream_colmap\" : \"Colmap\" , \"upstream_mask\" : \"Segmentation2D\" , \"use_colmap_poses\" : \"True\" , \"voxel_size\" : 0.01 } }","title":"JSON example"},{"location":"plant_imager/metadata/tasks_metadata/#scan-task","text":"Todo For some reason the parameters are defined in the metadata/metadata.json file. Definition can be found here .","title":"Scan task"},{"location":"plant_imager/specifications/","text":"","title":"Home"},{"location":"plant_imager/specifications/configuration_files/","text":"Tasks configuration Link Introduction Link Each task has some default configuration but to create a working pipeline, you need to set some parameters in a TOML configuration file. There are two types of parameters: those associated with luigi: upstream tasks, requirements & outputs those tuning the used algorithms: variables & parameters For examples look at the configurations examples provided in plant3dvision/configs . I/O tasks Link These tasks refer to the inputs and outputs of each algorithmic tasks. They are often related to task defined in the plantdb library. FilesetExists Link Check that the given Fileset id exists (in dataset). No luigi parameters (no upstream task). List of task variables: fileset_id : the id ( str ) to check for existence. ImagesFilesetExists(FilesetExists) Link Check that the image file set exists. No luigi parameters (no upstream task). List of task variables: fileset_id : the id ( str ) to check for existence 'images' by default. ModelFileset(FilesetExists) Link Check that the trained ML models file set exists. No luigi parameters (no upstream task). List of task variables: fileset_id : the id ( str ) to check for existence 'models' by default. Example: [ModelFileset] scan_id = \"models\" Defines the location of the trained ML models in a dataset named 'models' . Database tasks Link Clean Link Defined in plantdb.task , it is used to clean a scan dataset from the \"processing folders\". No luigi parameters (no upstream task). List of task variables: no_confirm : boolean indicating whether a confirmation is required to clean the dataset, False by default, i.e. confirmation required. Example: [Clean] no_confirm = true Use this to clean without confirmation. Algorithmic tasks Link Colmap task Link Defined in plant3dvision.task.colmap , it is used to match scan images and estimate camera poses. It can also be used to compute a sparse and/or dense point cloud. List of luigi task parameters: upstream_task : task upstream of the Colmap task, default is ImagesFilesetExists List of task variables: matcher : images matching method, can be either \"exhaustive\" (default) or \"sequential\"; compute_dense : boolean indicating whether to run the dense Colmap to obtain a dense point cloud, False by default; cli_args : dictionary of parameters for Colmap command line prompts; align_pcd : boolean indicating whether to align point cloud on calibrated or metadata poses, True by default; calibration_scan_id : ID of the calibration scan, requires ???. Example: [Colmap] matcher = \"exhaustive\" compute_dense = false calibration_scan_id = \"calib_scan_shortpath\" [Colmap.cli_args.feature_extractor] \"--ImageReader.single_camera\" = \"1\" \"--SiftExtraction.use_gpu\" = \"1\" [Colmap.cli_args.exhaustive_matcher] \"--SiftMatching.use_gpu\" = \"1\" [Colmap.cli_args.model_aligner] \"--robust_alignment_max_error\" = \"10\" Todo Add a compute_sparse ? Undistorted task Link Defined in plant3dvision.task.proc2d , it is used to create undistorted images from pre-computed intrinsic camera parameters. Masks task Link Defined in plant3dvision.task.proc2d , it is used to create binary mask of the plant location within each image. List of luigi task parameters: upstream_task : task upstream of the Masks task, default is Undistorted but can be None . List of task variables: type : method to use to compute masks, choices are: 'linear' , 'excess_green' , 'vesselness' , 'invert' ; parameters : list of scalar parameters, depends on type hence no default values ; dilation : integer defining the dilation factor to apply when using a binary mask, no default values ; binarize : boolean indicating whether to binarize the mask, default is True ; threshold : float used as threshold for binarization step, default is 0.0 ; Voxels task Link Defined in plant3dvision.task.cl , it is used to reconstruct the 3D structure of the plant from binary or segmented masks. List of luigi task parameters: upstream_task : task upstream of the Colmap task, default is ImagesFilesetExists List of task variables: type : ; Example: [Voxels] upstream_mask = \"Segmentation2D\" labels = \"[\\\"background\\\"]\" voxel_size = 0.3 type = \"averaging\" invert = false use_colmap_poses = true log = false Note Choose 'Segmentation2D' as ` for ML pipeline or 'Masks'` for geometrical pipeline. PointCloud task Link Example: [PointCloud] level_set_value = 1 background_prior = 0.5 log = false SegmentedPointCloud task Link Example: [SegmentedPointCloud] upstream_segmentation = \"Segmentation2D\" upstream_task = \"PointCloud\" use_colmap_poses = true Segmentation2D task Link Example: [Segmentation2D] upstream_task = \"Undistorted\" query = \"{\\\"channel\\\":\\\"rgb\\\"}\" model_id = \"Resnetdataset_gl_png_896_896_epoch50\" resize = true binarize = true dilation = 1 Sx = 896 Sy = 896 epochs = 1 batch = 1 learning_rate = 0.0001 threshold = 0.0035 Visualization task Link Example: [Visualization] max_image_size = 1500 max_point_cloud_size = 10000 thumbnail_size = 150 pcd_source = \"vox2pcd\" mesh_source = \"delaunay\" ClusteredMesh task Link Example: [ClusteredMesh] upstream_task = \"SegmentedPointCloud\" AnglesAndInternodes task Link Example: [AnglesAndInternodes] upstream_task = \"ClusteredMesh\" Evaluation tasks Link VoxelGroundTruth Link Example: PointCloudGroundTruth Link Example: [PointCloudGroundTruth] pcd_size = 10000 ClusteredMeshGroundTruth Link Example: PointCloudEvaluation task Link Example: [PointCloudEvaluation] max_distance = 0.2 PointCloudSegmentationEvaluation task Link Segmentation2DEvaluation task Link","title":"Configuration files"},{"location":"plant_imager/specifications/configuration_files/#tasks-configuration","text":"","title":"Tasks configuration"},{"location":"plant_imager/specifications/configuration_files/#introduction","text":"Each task has some default configuration but to create a working pipeline, you need to set some parameters in a TOML configuration file. There are two types of parameters: those associated with luigi: upstream tasks, requirements & outputs those tuning the used algorithms: variables & parameters For examples look at the configurations examples provided in plant3dvision/configs .","title":"Introduction"},{"location":"plant_imager/specifications/configuration_files/#io-tasks","text":"These tasks refer to the inputs and outputs of each algorithmic tasks. They are often related to task defined in the plantdb library.","title":"I/O tasks"},{"location":"plant_imager/specifications/configuration_files/#filesetexists","text":"Check that the given Fileset id exists (in dataset). No luigi parameters (no upstream task). List of task variables: fileset_id : the id ( str ) to check for existence.","title":"FilesetExists"},{"location":"plant_imager/specifications/configuration_files/#imagesfilesetexistsfilesetexists","text":"Check that the image file set exists. No luigi parameters (no upstream task). List of task variables: fileset_id : the id ( str ) to check for existence 'images' by default.","title":"ImagesFilesetExists(FilesetExists)"},{"location":"plant_imager/specifications/configuration_files/#modelfilesetfilesetexists","text":"Check that the trained ML models file set exists. No luigi parameters (no upstream task). List of task variables: fileset_id : the id ( str ) to check for existence 'models' by default. Example: [ModelFileset] scan_id = \"models\" Defines the location of the trained ML models in a dataset named 'models' .","title":"ModelFileset(FilesetExists)"},{"location":"plant_imager/specifications/configuration_files/#database-tasks","text":"","title":"Database tasks"},{"location":"plant_imager/specifications/configuration_files/#clean","text":"Defined in plantdb.task , it is used to clean a scan dataset from the \"processing folders\". No luigi parameters (no upstream task). List of task variables: no_confirm : boolean indicating whether a confirmation is required to clean the dataset, False by default, i.e. confirmation required. Example: [Clean] no_confirm = true Use this to clean without confirmation.","title":"Clean"},{"location":"plant_imager/specifications/configuration_files/#algorithmic-tasks","text":"","title":"Algorithmic tasks"},{"location":"plant_imager/specifications/configuration_files/#colmap-task","text":"Defined in plant3dvision.task.colmap , it is used to match scan images and estimate camera poses. It can also be used to compute a sparse and/or dense point cloud. List of luigi task parameters: upstream_task : task upstream of the Colmap task, default is ImagesFilesetExists List of task variables: matcher : images matching method, can be either \"exhaustive\" (default) or \"sequential\"; compute_dense : boolean indicating whether to run the dense Colmap to obtain a dense point cloud, False by default; cli_args : dictionary of parameters for Colmap command line prompts; align_pcd : boolean indicating whether to align point cloud on calibrated or metadata poses, True by default; calibration_scan_id : ID of the calibration scan, requires ???. Example: [Colmap] matcher = \"exhaustive\" compute_dense = false calibration_scan_id = \"calib_scan_shortpath\" [Colmap.cli_args.feature_extractor] \"--ImageReader.single_camera\" = \"1\" \"--SiftExtraction.use_gpu\" = \"1\" [Colmap.cli_args.exhaustive_matcher] \"--SiftMatching.use_gpu\" = \"1\" [Colmap.cli_args.model_aligner] \"--robust_alignment_max_error\" = \"10\" Todo Add a compute_sparse ?","title":"Colmap task"},{"location":"plant_imager/specifications/configuration_files/#undistorted-task","text":"Defined in plant3dvision.task.proc2d , it is used to create undistorted images from pre-computed intrinsic camera parameters.","title":"Undistorted task"},{"location":"plant_imager/specifications/configuration_files/#masks-task","text":"Defined in plant3dvision.task.proc2d , it is used to create binary mask of the plant location within each image. List of luigi task parameters: upstream_task : task upstream of the Masks task, default is Undistorted but can be None . List of task variables: type : method to use to compute masks, choices are: 'linear' , 'excess_green' , 'vesselness' , 'invert' ; parameters : list of scalar parameters, depends on type hence no default values ; dilation : integer defining the dilation factor to apply when using a binary mask, no default values ; binarize : boolean indicating whether to binarize the mask, default is True ; threshold : float used as threshold for binarization step, default is 0.0 ;","title":"Masks task"},{"location":"plant_imager/specifications/configuration_files/#voxels-task","text":"Defined in plant3dvision.task.cl , it is used to reconstruct the 3D structure of the plant from binary or segmented masks. List of luigi task parameters: upstream_task : task upstream of the Colmap task, default is ImagesFilesetExists List of task variables: type : ; Example: [Voxels] upstream_mask = \"Segmentation2D\" labels = \"[\\\"background\\\"]\" voxel_size = 0.3 type = \"averaging\" invert = false use_colmap_poses = true log = false Note Choose 'Segmentation2D' as ` for ML pipeline or 'Masks'` for geometrical pipeline.","title":"Voxels task"},{"location":"plant_imager/specifications/configuration_files/#pointcloud-task","text":"Example: [PointCloud] level_set_value = 1 background_prior = 0.5 log = false","title":"PointCloud task"},{"location":"plant_imager/specifications/configuration_files/#segmentedpointcloud-task","text":"Example: [SegmentedPointCloud] upstream_segmentation = \"Segmentation2D\" upstream_task = \"PointCloud\" use_colmap_poses = true","title":"SegmentedPointCloud task"},{"location":"plant_imager/specifications/configuration_files/#segmentation2d-task","text":"Example: [Segmentation2D] upstream_task = \"Undistorted\" query = \"{\\\"channel\\\":\\\"rgb\\\"}\" model_id = \"Resnetdataset_gl_png_896_896_epoch50\" resize = true binarize = true dilation = 1 Sx = 896 Sy = 896 epochs = 1 batch = 1 learning_rate = 0.0001 threshold = 0.0035","title":"Segmentation2D task"},{"location":"plant_imager/specifications/configuration_files/#visualization-task","text":"Example: [Visualization] max_image_size = 1500 max_point_cloud_size = 10000 thumbnail_size = 150 pcd_source = \"vox2pcd\" mesh_source = \"delaunay\"","title":"Visualization task"},{"location":"plant_imager/specifications/configuration_files/#clusteredmesh-task","text":"Example: [ClusteredMesh] upstream_task = \"SegmentedPointCloud\"","title":"ClusteredMesh task"},{"location":"plant_imager/specifications/configuration_files/#anglesandinternodes-task","text":"Example: [AnglesAndInternodes] upstream_task = \"ClusteredMesh\"","title":"AnglesAndInternodes task"},{"location":"plant_imager/specifications/configuration_files/#evaluation-tasks","text":"","title":"Evaluation tasks"},{"location":"plant_imager/specifications/configuration_files/#voxelgroundtruth","text":"Example:","title":"VoxelGroundTruth"},{"location":"plant_imager/specifications/configuration_files/#pointcloudgroundtruth","text":"Example: [PointCloudGroundTruth] pcd_size = 10000","title":"PointCloudGroundTruth"},{"location":"plant_imager/specifications/configuration_files/#clusteredmeshgroundtruth","text":"Example:","title":"ClusteredMeshGroundTruth"},{"location":"plant_imager/specifications/configuration_files/#pointcloudevaluation-task","text":"Example: [PointCloudEvaluation] max_distance = 0.2","title":"PointCloudEvaluation task"},{"location":"plant_imager/specifications/configuration_files/#pointcloudsegmentationevaluation-task","text":"","title":"PointCloudSegmentationEvaluation task"},{"location":"plant_imager/specifications/configuration_files/#segmentation2devaluation-task","text":"","title":"Segmentation2DEvaluation task"},{"location":"plant_imager/specifications/data/","text":"This page describes how to use the romi package plantdb accessible here . A shared example datasets is accessible here . Getting started Link Installation Link Warning If you intend to contribute to the development of plantdb or want to be able to edit the code and test your changes, you should choose editable mode . Non-editable mode Link Install from GitHub using pip : pip install git+ssh://git@github.com/romi/plantdb.git#dev Note This uses ssh and thus requires to be registered as part of the project and to deploy ssh keys. Editable mode Link Clone from GitHub and install using pip : git clone https://github.com/romi/plantdb.git cd plantdb pip install -e . Minimal working example Link Let's assume you have a list of images of a given object and that you want to add them to a ROMI database as a \"scan\". 1 - Initialize database Link First create the directory for the database and add the romidb marker to it: from os.path import join from tempfile import mkdtemp mydb = mkdtemp ( prefix = 'romidb_' ) open ( join ( mydb , 'romidb' ), 'w' ) . close () Now you can initialize a ROMI FSDB database object: from plantdb.fsdb import FSDB db = FSDB ( mydb ) db . connect () # Locks the database and allows access 2 - Create a new dataset Link To create a new dataset, here named myscan_001 , do: scan = db . create_scan ( \"myscan_001\" ) To add scan metadata ( eg. camera settings, biological metadata, hardware metadata...), do: scan . set_metadata ({ \"scanner\" : { \"harware\" : 'test' }}) This will results in several changes in the local database: Add a myscan_001 sub-directory in the database root directory; Add a metadata sub-directory in myscan_001 and a metadata.json gathering the given scan metadata . 3 - Add images as new fileset Link OPTIONAL - create a list of RGB images If you do not have a scan datasets available, either download a shared datasets here or generate a list of images as follows: import numpy as np # Generate random noise images n_images = 99 imgs = [] for i in range ( n_images ): img = 256 * np . random . rand ( 256 , 256 , 3 ) img = np . array ( img , dtype = np . uint8 ) imgs . append ( img ) Create a new fileset : fileset = scan . create_fileset ( \"images\" ) Add the images to the fileset: Load the file list (or skip if you generated random images): from os import listdir imgs = listdir ( \"</path/to/my/scan/images>\" ) Then loop the images list and add them to the fileset , optionally attach some metadata to each image: from plantdb import io for i , img in enumerate ( imgs ): file = fileset . create_file ( \" %i \" % i ) io . write_image ( file , img ) file . set_metadata ( \"key\" , \" %i \" % i ) This will results in several changes in the local database: Reference the image by its file name by adding an entry in files.json ; Write a scan_img_1.jpeg image in the images sub-directory of the scan \"myscan\" . Add an images sub-directory in the metadata sub-directory, and JSON files with the image id as name to store the image metadata . 4 - Access image files in a fileset Link To access the image files in a fileset (in a datasets, itself in an existing and accessible database), proceed as follows: from plantdb.fsdb import FSDB db = FSDB ( mydb ) db . connect () # Locks the database and allows access scan = db . get_scan ( \"myscan\" ) fileset = scan . get_fileset ( \"images\" ) for f in fileset . get_files (): im = io . read_image ( f ) # reads image data print ( f . get_metadata ( \"key\" )) # i db . disconnect () Examples Link from plantdb.fsdb import FSDB from plantdb import io import numpy as np # Generate random noise images n_images = 100 imgs = [] for i in range ( n_images ): img = 256 * np . random . rand ( 256 , 256 , 3 ) img = np . array ( img , dtype = np . uint8 ) imgs . append ( img ) from os import listdir from os.path import join from tempfile import mkdtemp # Create a temporary DB directory: mydb = mkdtemp ( prefix = 'romidb_' ) # Create the `romidb` file in previously created temporary DB directory: open ( join ( mydb , 'romidb' ), 'w' ) . close () listdir ( mydb ) # Connect to the DB: db = FSDB ( mydb ) db . connect () # Locks the database and allows access # Add a scan datasets to the DB: scan = db . create_scan ( \"myscan_001\" ) listdir ( mydb ) # Add metadata to a scan datasets: scan . set_metadata ({ \"scanner\" : { \"hardware\" : 'test' }}) listdir ( join ( mydb , \"myscan_001\" )) listdir ( join ( mydb , \"myscan_001\" , \"metadata\" )) fileset = scan . create_fileset ( \"images\" ) listdir ( join ( mydb , \"myscan_001\" )) for i , img in enumerate ( imgs ): file = fileset . create_file ( \" %i \" % i ) io . write_image ( file , img ) file . set_metadata ( \"key\" , \" %i \" % i ) # Add some metadata # read files in the fileset: scan = db . get_scan ( \"myscan\" ) fileset = scan . get_fileset ( \"images\" ) for f in fileset . get_files (): im = io . read_image ( f ) # reads image data print ( f . get_metadata ( \"key\" )) # i db . disconnect () Database structure Link Overview Link Hereafter we give an overview of the database structure using the ROMI database terminology: plantdb_root/ \u251c\u2500\u2500 dataset_001/ \u2502 \u251c\u2500\u2500 fileset_A/ \u2502 \u2502 \u251c\u2500\u2500 file_A_001.ext \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2514\u2500\u2500 file_A_009.ext \u2502 \u251c\u2500\u2500 fileset_B/ \u2502 \u2502 \u251c\u2500\u2500 file_B_001.ext \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2514\u2500\u2500 file_B_009.ext \u2502 \u251c\u2500\u2500 metadata \u2502 \u2502 \u251c\u2500\u2500 fileset_A.json \u2502 \u2502 \u251c\u2500\u2500 fileset_B.json \u2502 \u2502 \u2514\u2500\u2500 metadata.json \u2502 \u2514\u2500\u2500 files.json \u251c\u2500\u2500 dataset_002/ \u2502 \u2514\u2500\u2500 [...] \u251c\u2500\u2500 [...] \u251c\u2500\u2500 (lock) \u2514\u2500\u2500 romidb Database root directory Link A root database directory is defined, eg. mydb/ . Inside this directory we need to define (add) the romidb marker, so it may be used by FSDB class. We may also find the lock file used to limit the access to the database to only one user. Note that the database initialization part is manual. To create them, in a terminal: mkdir mydb touch mydb/romidb We just created the following tree structure: mydb/ \u2514\u2500\u2500 romidb Once you have created this root directory and the romidb marker file, you can initialize a ROMI FSDB database object in Python: from plantdb.fsdb import FSDB db = FSDB ( \"mydb\" ) db . connect () The method FSDB.connect() locks the database with a lock file at root directory and allows access. To disconnect and free the database do: db . disconnect () If for some reason the Python terminal unexpectedly terminate without a call to the disconnect method, you may have to remove the lock file manually. Check that no one else is using the database! Within this root database directory you will find other directories corresponding to datasets . Datasets directories Link At the next level, we find the datasets directory(s), eg. named myscan_001 . Their names must be uniques, and you create them as follows: scan = db . create_scan ( \"myscan_001\" ) If you add scan metadata ( eg. camera settings, biological metadata, hardware metadata...) with scan.set_metadata() , you get another directory metadata with a metadata.json file. scan . set_metadata ({ \"scanner\" : { \"hardware\" : 'test' }}) We now have the following tree structure: mydb/ \u251c\u2500\u2500 myscan_001/ \u2502 \u2514\u2500\u2500 metadata/ \u2502 \u2514\u2500\u2500 metadata.json \u2514\u2500\u2500 romidb And the file metadata.json should look like this: { \"scanner\" : { \"hardware\" : \"test\" } } Images directories Link Inside myscan_001/ , we find the datasets or fileset in plantdb terminology. In the case of the \"plant scanner\", this is a list of RGB image files acquired by a camera moving around the plant. To store the datasets, we thus name the created fileset \"images\": fileset = scan . create_fileset ( \"images\" ) This creates an images directory and a files.json at the dataset root directory. We now have the following tree structure: mydb/ \u251c\u2500\u2500 myscan_001/ \u2502 \u251c\u2500\u2500 images/ \u2502 \u251c\u2500\u2500 metadata/ \u2502 \u2502 \u2514\u2500\u2500 metadata.json \u2502 \u2514\u2500\u2500 files.json \u2514\u2500\u2500 romidb The JSON should look like this: { \"filesets\" : [ { \"files\" : [], \"id\" : \"images\" } ] } We then create random RGB images to add to the dataset: import numpy as np # Generate random noise images n_images = 100 imgs = [] for i in range ( n_images ): img = 256 * np . random . rand ( 256 , 256 , 3 ) img = np . array ( img , dtype = np . uint8 ) imgs . append ( img ) And we add them with their metadata to the database: for i , img in enumerate ( imgs ): fname = f \"img_ { str ( i ) . zfill ( 2 ) } .png\" file = fileset . create_file ( fname ) io . write_image ( file , img ) file . set_metadata ( \"key\" , fname ) file . set_metadata ( \"id\" , i ) Inside this images/ directory will reside the images added to the database. At the same time you added images with REF_TO_TUTO , you created an entry in a JSON file referencing the files. If you added metadata along with the files ( eg. camera poses, jpeg metadata...) it should be referenced in metadata/images/ eg. metadata/images/<scan_img_01>.json . mydb/ \u251c\u2500\u2500 myscan_001/ \u2502 \u251c\u2500\u2500 files.json \u2502 \u251c\u2500\u2500 images/ \u2502 \u2502 \u251c\u2500\u2500 scan_img_01.jpg \u2502 \u2502 \u251c\u2500\u2500 scan_img_02.jpg \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2514\u2500\u2500 scan_img_99.jpg \u2502 \u251c\u2500\u2500 metadata/ \u2502 \u2502 \u251c\u2500\u2500 images \u2502 \u2502 \u2502 \u251c\u2500\u2500 scan_img_01.json \u2502 \u2502 \u2502 \u251c\u2500\u2500 scan_img_02.json \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2502 \u2514\u2500\u2500 scan_img_99.json \u2502 \u2502 \u2514\u2500\u2500 metadata.json \u2514\u2500\u2500 romidb Example Link mydb/ \u251c\u2500\u2500 myscan_001/ \u2502 \u251c\u2500\u2500 AnglesAndInternodes_1_0_2_0_0_1_dd8d67653a \u2502 \u2502 \u2514\u2500\u2500 AnglesAndInternodes.json \u2502 \u251c\u2500\u2500 Colmap_True____feature_extrac_3bbfcb1413 \u2502 \u2502 \u251c\u2500\u2500 cameras.json \u2502 \u2502 \u251c\u2500\u2500 images.json \u2502 \u2502 \u251c\u2500\u2500 points3d.json \u2502 \u2502 \u2514\u2500\u2500 sparse.ply \u2502 \u251c\u2500\u2500 CurveSkeleton_out__TriangleMesh_6a92751c20 \u2502 \u2502 \u2514\u2500\u2500 CurveSkeleton.json \u2502 \u251c\u2500\u2500 files.json \u2502 \u251c\u2500\u2500 images \u2502 \u2502 \u251c\u2500\u2500 pict20190201_110110_0.jpg \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2514\u2500\u2500 pict20190201_111209_0.jpg \u2502 \u251c\u2500\u2500 Masks_True_5_out_9adb9db801 \u2502 \u2502 \u251c\u2500\u2500 pict20190201_110110_0.jpg \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2514\u2500\u2500 pict20190201_111209_0.jpg \u2502 \u251c\u2500\u2500 measures.csv \u2502 \u251c\u2500\u2500 metadata \u2502 \u2502 \u251c\u2500\u2500 AnglesAndInternodes_1_0_2_0_0_1_dd8d67653a.json \u2502 \u2502 \u251c\u2500\u2500 Colmap_True____feature_extrac_3bbfcb1413.json \u2502 \u2502 \u251c\u2500\u2500 CurveSkeleton_out__TriangleMesh_6a92751c20.json \u2502 \u2502 \u251c\u2500\u2500 images \u2502 \u2502 \u2502 \u251c\u2500\u2500 pict20190201_110110_0.json \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2502 \u2514\u2500\u2500 pict20190201_111209_0.json \u2502 \u2502 \u251c\u2500\u2500 images.json \u2502 \u2502 \u251c\u2500\u2500 Masks_True_5_out_9adb9db801 \u2502 \u2502 \u2502 \u251c\u2500\u2500 pict20190201_110110_0.json \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2502 \u2514\u2500\u2500 pict20190201_111209_0.json \u2502 \u2502 \u251c\u2500\u2500 Masks_True_5_out_e90d1804eb.json \u2502 \u2502 \u251c\u2500\u2500 metadata.json \u2502 \u2502 \u251c\u2500\u2500 PointCloud_1_0_1_0_False_9ab5a15d9b \u2502 \u2502 \u2502 \u2514\u2500\u2500 PointCloud.json \u2502 \u2502 \u251c\u2500\u2500 PointCloud_1_0_1_0_False_9ab5a15d9b.json \u2502 \u2502 \u251c\u2500\u2500 PointCloud__200_0_1_0_False_4ce2e46446.json \u2502 \u2502 \u251c\u2500\u2500 TreeGraph_out__CurveSkeleton_5dca9a2821.json \u2502 \u2502 \u251c\u2500\u2500 TriangleMesh_out__PointCloud_80dc94ac81.json \u2502 \u2502 \u251c\u2500\u2500 Undistorted_out_____fb3e3fa0ff \u2502 \u2502 \u2502 \u251c\u2500\u2500 pict20190201_110110_0.json \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2502 \u2514\u2500\u2500 pict20190201_111209_0.json \u2502 \u2502 \u251c\u2500\u2500 Undistorted_out_____fb3e3fa0ff.json \u2502 \u2502 \u251c\u2500\u2500 Voxels_False____False_567dc7f48b \u2502 \u2502 \u2502 \u2514\u2500\u2500 Voxels.json \u2502 \u2502 \u251c\u2500\u2500 Voxels_False____False_567dc7f48b.json \u2502 \u2502 \u251c\u2500\u2500 Voxels_False____True_af037e876e.json \u2502 \u2502 \u2514\u2500\u2500 Voxels_False____True_cd9a5ff06b.json \u2502 \u251c\u2500\u2500 pipeline.toml \u2502 \u251c\u2500\u2500 PointCloud_1_0_1_0_False_9ab5a15d9b \u2502 \u2502 \u2514\u2500\u2500 PointCloud.ply \u2502 \u251c\u2500\u2500 TreeGraph_out__CurveSkeleton_5dca9a2821 \u2502 \u2502 \u2514\u2500\u2500 TreeGraph.p \u2502 \u251c\u2500\u2500 TriangleMesh_out__PointCloud_80dc94ac81 \u2502 \u2502 \u2514\u2500\u2500 TriangleMesh.ply \u2502 \u2514\u2500\u2500 Voxels_False____False_567dc7f48b \u2502 \u2514\u2500\u2500 Voxels.npz \u251c\u2500\u2500 colmap_log.txt \u251c\u2500\u2500 lock \u2514\u2500\u2500 romidb","title":"Data"},{"location":"plant_imager/specifications/data/#getting-started","text":"","title":"Getting started"},{"location":"plant_imager/specifications/data/#installation","text":"Warning If you intend to contribute to the development of plantdb or want to be able to edit the code and test your changes, you should choose editable mode .","title":"Installation"},{"location":"plant_imager/specifications/data/#non-editable-mode","text":"Install from GitHub using pip : pip install git+ssh://git@github.com/romi/plantdb.git#dev Note This uses ssh and thus requires to be registered as part of the project and to deploy ssh keys.","title":"Non-editable mode"},{"location":"plant_imager/specifications/data/#editable-mode","text":"Clone from GitHub and install using pip : git clone https://github.com/romi/plantdb.git cd plantdb pip install -e .","title":"Editable mode"},{"location":"plant_imager/specifications/data/#minimal-working-example","text":"Let's assume you have a list of images of a given object and that you want to add them to a ROMI database as a \"scan\".","title":"Minimal working example"},{"location":"plant_imager/specifications/data/#1-initialize-database","text":"First create the directory for the database and add the romidb marker to it: from os.path import join from tempfile import mkdtemp mydb = mkdtemp ( prefix = 'romidb_' ) open ( join ( mydb , 'romidb' ), 'w' ) . close () Now you can initialize a ROMI FSDB database object: from plantdb.fsdb import FSDB db = FSDB ( mydb ) db . connect () # Locks the database and allows access","title":"1 - Initialize database"},{"location":"plant_imager/specifications/data/#2-create-a-new-dataset","text":"To create a new dataset, here named myscan_001 , do: scan = db . create_scan ( \"myscan_001\" ) To add scan metadata ( eg. camera settings, biological metadata, hardware metadata...), do: scan . set_metadata ({ \"scanner\" : { \"harware\" : 'test' }}) This will results in several changes in the local database: Add a myscan_001 sub-directory in the database root directory; Add a metadata sub-directory in myscan_001 and a metadata.json gathering the given scan metadata .","title":"2 - Create a new dataset"},{"location":"plant_imager/specifications/data/#3-add-images-as-new-fileset","text":"OPTIONAL - create a list of RGB images If you do not have a scan datasets available, either download a shared datasets here or generate a list of images as follows: import numpy as np # Generate random noise images n_images = 99 imgs = [] for i in range ( n_images ): img = 256 * np . random . rand ( 256 , 256 , 3 ) img = np . array ( img , dtype = np . uint8 ) imgs . append ( img ) Create a new fileset : fileset = scan . create_fileset ( \"images\" ) Add the images to the fileset: Load the file list (or skip if you generated random images): from os import listdir imgs = listdir ( \"</path/to/my/scan/images>\" ) Then loop the images list and add them to the fileset , optionally attach some metadata to each image: from plantdb import io for i , img in enumerate ( imgs ): file = fileset . create_file ( \" %i \" % i ) io . write_image ( file , img ) file . set_metadata ( \"key\" , \" %i \" % i ) This will results in several changes in the local database: Reference the image by its file name by adding an entry in files.json ; Write a scan_img_1.jpeg image in the images sub-directory of the scan \"myscan\" . Add an images sub-directory in the metadata sub-directory, and JSON files with the image id as name to store the image metadata .","title":"3 - Add images as new fileset"},{"location":"plant_imager/specifications/data/#4-access-image-files-in-a-fileset","text":"To access the image files in a fileset (in a datasets, itself in an existing and accessible database), proceed as follows: from plantdb.fsdb import FSDB db = FSDB ( mydb ) db . connect () # Locks the database and allows access scan = db . get_scan ( \"myscan\" ) fileset = scan . get_fileset ( \"images\" ) for f in fileset . get_files (): im = io . read_image ( f ) # reads image data print ( f . get_metadata ( \"key\" )) # i db . disconnect ()","title":"4 - Access image files in a fileset"},{"location":"plant_imager/specifications/data/#examples","text":"from plantdb.fsdb import FSDB from plantdb import io import numpy as np # Generate random noise images n_images = 100 imgs = [] for i in range ( n_images ): img = 256 * np . random . rand ( 256 , 256 , 3 ) img = np . array ( img , dtype = np . uint8 ) imgs . append ( img ) from os import listdir from os.path import join from tempfile import mkdtemp # Create a temporary DB directory: mydb = mkdtemp ( prefix = 'romidb_' ) # Create the `romidb` file in previously created temporary DB directory: open ( join ( mydb , 'romidb' ), 'w' ) . close () listdir ( mydb ) # Connect to the DB: db = FSDB ( mydb ) db . connect () # Locks the database and allows access # Add a scan datasets to the DB: scan = db . create_scan ( \"myscan_001\" ) listdir ( mydb ) # Add metadata to a scan datasets: scan . set_metadata ({ \"scanner\" : { \"hardware\" : 'test' }}) listdir ( join ( mydb , \"myscan_001\" )) listdir ( join ( mydb , \"myscan_001\" , \"metadata\" )) fileset = scan . create_fileset ( \"images\" ) listdir ( join ( mydb , \"myscan_001\" )) for i , img in enumerate ( imgs ): file = fileset . create_file ( \" %i \" % i ) io . write_image ( file , img ) file . set_metadata ( \"key\" , \" %i \" % i ) # Add some metadata # read files in the fileset: scan = db . get_scan ( \"myscan\" ) fileset = scan . get_fileset ( \"images\" ) for f in fileset . get_files (): im = io . read_image ( f ) # reads image data print ( f . get_metadata ( \"key\" )) # i db . disconnect ()","title":"Examples"},{"location":"plant_imager/specifications/data/#database-structure","text":"","title":"Database structure"},{"location":"plant_imager/specifications/data/#overview","text":"Hereafter we give an overview of the database structure using the ROMI database terminology: plantdb_root/ \u251c\u2500\u2500 dataset_001/ \u2502 \u251c\u2500\u2500 fileset_A/ \u2502 \u2502 \u251c\u2500\u2500 file_A_001.ext \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2514\u2500\u2500 file_A_009.ext \u2502 \u251c\u2500\u2500 fileset_B/ \u2502 \u2502 \u251c\u2500\u2500 file_B_001.ext \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2514\u2500\u2500 file_B_009.ext \u2502 \u251c\u2500\u2500 metadata \u2502 \u2502 \u251c\u2500\u2500 fileset_A.json \u2502 \u2502 \u251c\u2500\u2500 fileset_B.json \u2502 \u2502 \u2514\u2500\u2500 metadata.json \u2502 \u2514\u2500\u2500 files.json \u251c\u2500\u2500 dataset_002/ \u2502 \u2514\u2500\u2500 [...] \u251c\u2500\u2500 [...] \u251c\u2500\u2500 (lock) \u2514\u2500\u2500 romidb","title":"Overview"},{"location":"plant_imager/specifications/data/#database-root-directory","text":"A root database directory is defined, eg. mydb/ . Inside this directory we need to define (add) the romidb marker, so it may be used by FSDB class. We may also find the lock file used to limit the access to the database to only one user. Note that the database initialization part is manual. To create them, in a terminal: mkdir mydb touch mydb/romidb We just created the following tree structure: mydb/ \u2514\u2500\u2500 romidb Once you have created this root directory and the romidb marker file, you can initialize a ROMI FSDB database object in Python: from plantdb.fsdb import FSDB db = FSDB ( \"mydb\" ) db . connect () The method FSDB.connect() locks the database with a lock file at root directory and allows access. To disconnect and free the database do: db . disconnect () If for some reason the Python terminal unexpectedly terminate without a call to the disconnect method, you may have to remove the lock file manually. Check that no one else is using the database! Within this root database directory you will find other directories corresponding to datasets .","title":"Database root directory"},{"location":"plant_imager/specifications/data/#datasets-directories","text":"At the next level, we find the datasets directory(s), eg. named myscan_001 . Their names must be uniques, and you create them as follows: scan = db . create_scan ( \"myscan_001\" ) If you add scan metadata ( eg. camera settings, biological metadata, hardware metadata...) with scan.set_metadata() , you get another directory metadata with a metadata.json file. scan . set_metadata ({ \"scanner\" : { \"hardware\" : 'test' }}) We now have the following tree structure: mydb/ \u251c\u2500\u2500 myscan_001/ \u2502 \u2514\u2500\u2500 metadata/ \u2502 \u2514\u2500\u2500 metadata.json \u2514\u2500\u2500 romidb And the file metadata.json should look like this: { \"scanner\" : { \"hardware\" : \"test\" } }","title":"Datasets directories"},{"location":"plant_imager/specifications/data/#images-directories","text":"Inside myscan_001/ , we find the datasets or fileset in plantdb terminology. In the case of the \"plant scanner\", this is a list of RGB image files acquired by a camera moving around the plant. To store the datasets, we thus name the created fileset \"images\": fileset = scan . create_fileset ( \"images\" ) This creates an images directory and a files.json at the dataset root directory. We now have the following tree structure: mydb/ \u251c\u2500\u2500 myscan_001/ \u2502 \u251c\u2500\u2500 images/ \u2502 \u251c\u2500\u2500 metadata/ \u2502 \u2502 \u2514\u2500\u2500 metadata.json \u2502 \u2514\u2500\u2500 files.json \u2514\u2500\u2500 romidb The JSON should look like this: { \"filesets\" : [ { \"files\" : [], \"id\" : \"images\" } ] } We then create random RGB images to add to the dataset: import numpy as np # Generate random noise images n_images = 100 imgs = [] for i in range ( n_images ): img = 256 * np . random . rand ( 256 , 256 , 3 ) img = np . array ( img , dtype = np . uint8 ) imgs . append ( img ) And we add them with their metadata to the database: for i , img in enumerate ( imgs ): fname = f \"img_ { str ( i ) . zfill ( 2 ) } .png\" file = fileset . create_file ( fname ) io . write_image ( file , img ) file . set_metadata ( \"key\" , fname ) file . set_metadata ( \"id\" , i ) Inside this images/ directory will reside the images added to the database. At the same time you added images with REF_TO_TUTO , you created an entry in a JSON file referencing the files. If you added metadata along with the files ( eg. camera poses, jpeg metadata...) it should be referenced in metadata/images/ eg. metadata/images/<scan_img_01>.json . mydb/ \u251c\u2500\u2500 myscan_001/ \u2502 \u251c\u2500\u2500 files.json \u2502 \u251c\u2500\u2500 images/ \u2502 \u2502 \u251c\u2500\u2500 scan_img_01.jpg \u2502 \u2502 \u251c\u2500\u2500 scan_img_02.jpg \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2514\u2500\u2500 scan_img_99.jpg \u2502 \u251c\u2500\u2500 metadata/ \u2502 \u2502 \u251c\u2500\u2500 images \u2502 \u2502 \u2502 \u251c\u2500\u2500 scan_img_01.json \u2502 \u2502 \u2502 \u251c\u2500\u2500 scan_img_02.json \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2502 \u2514\u2500\u2500 scan_img_99.json \u2502 \u2502 \u2514\u2500\u2500 metadata.json \u2514\u2500\u2500 romidb","title":"Images directories"},{"location":"plant_imager/specifications/data/#example","text":"mydb/ \u251c\u2500\u2500 myscan_001/ \u2502 \u251c\u2500\u2500 AnglesAndInternodes_1_0_2_0_0_1_dd8d67653a \u2502 \u2502 \u2514\u2500\u2500 AnglesAndInternodes.json \u2502 \u251c\u2500\u2500 Colmap_True____feature_extrac_3bbfcb1413 \u2502 \u2502 \u251c\u2500\u2500 cameras.json \u2502 \u2502 \u251c\u2500\u2500 images.json \u2502 \u2502 \u251c\u2500\u2500 points3d.json \u2502 \u2502 \u2514\u2500\u2500 sparse.ply \u2502 \u251c\u2500\u2500 CurveSkeleton_out__TriangleMesh_6a92751c20 \u2502 \u2502 \u2514\u2500\u2500 CurveSkeleton.json \u2502 \u251c\u2500\u2500 files.json \u2502 \u251c\u2500\u2500 images \u2502 \u2502 \u251c\u2500\u2500 pict20190201_110110_0.jpg \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2514\u2500\u2500 pict20190201_111209_0.jpg \u2502 \u251c\u2500\u2500 Masks_True_5_out_9adb9db801 \u2502 \u2502 \u251c\u2500\u2500 pict20190201_110110_0.jpg \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2514\u2500\u2500 pict20190201_111209_0.jpg \u2502 \u251c\u2500\u2500 measures.csv \u2502 \u251c\u2500\u2500 metadata \u2502 \u2502 \u251c\u2500\u2500 AnglesAndInternodes_1_0_2_0_0_1_dd8d67653a.json \u2502 \u2502 \u251c\u2500\u2500 Colmap_True____feature_extrac_3bbfcb1413.json \u2502 \u2502 \u251c\u2500\u2500 CurveSkeleton_out__TriangleMesh_6a92751c20.json \u2502 \u2502 \u251c\u2500\u2500 images \u2502 \u2502 \u2502 \u251c\u2500\u2500 pict20190201_110110_0.json \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2502 \u2514\u2500\u2500 pict20190201_111209_0.json \u2502 \u2502 \u251c\u2500\u2500 images.json \u2502 \u2502 \u251c\u2500\u2500 Masks_True_5_out_9adb9db801 \u2502 \u2502 \u2502 \u251c\u2500\u2500 pict20190201_110110_0.json \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2502 \u2514\u2500\u2500 pict20190201_111209_0.json \u2502 \u2502 \u251c\u2500\u2500 Masks_True_5_out_e90d1804eb.json \u2502 \u2502 \u251c\u2500\u2500 metadata.json \u2502 \u2502 \u251c\u2500\u2500 PointCloud_1_0_1_0_False_9ab5a15d9b \u2502 \u2502 \u2502 \u2514\u2500\u2500 PointCloud.json \u2502 \u2502 \u251c\u2500\u2500 PointCloud_1_0_1_0_False_9ab5a15d9b.json \u2502 \u2502 \u251c\u2500\u2500 PointCloud__200_0_1_0_False_4ce2e46446.json \u2502 \u2502 \u251c\u2500\u2500 TreeGraph_out__CurveSkeleton_5dca9a2821.json \u2502 \u2502 \u251c\u2500\u2500 TriangleMesh_out__PointCloud_80dc94ac81.json \u2502 \u2502 \u251c\u2500\u2500 Undistorted_out_____fb3e3fa0ff \u2502 \u2502 \u2502 \u251c\u2500\u2500 pict20190201_110110_0.json \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2502 \u2514\u2500\u2500 pict20190201_111209_0.json \u2502 \u2502 \u251c\u2500\u2500 Undistorted_out_____fb3e3fa0ff.json \u2502 \u2502 \u251c\u2500\u2500 Voxels_False____False_567dc7f48b \u2502 \u2502 \u2502 \u2514\u2500\u2500 Voxels.json \u2502 \u2502 \u251c\u2500\u2500 Voxels_False____False_567dc7f48b.json \u2502 \u2502 \u251c\u2500\u2500 Voxels_False____True_af037e876e.json \u2502 \u2502 \u2514\u2500\u2500 Voxels_False____True_cd9a5ff06b.json \u2502 \u251c\u2500\u2500 pipeline.toml \u2502 \u251c\u2500\u2500 PointCloud_1_0_1_0_False_9ab5a15d9b \u2502 \u2502 \u2514\u2500\u2500 PointCloud.ply \u2502 \u251c\u2500\u2500 TreeGraph_out__CurveSkeleton_5dca9a2821 \u2502 \u2502 \u2514\u2500\u2500 TreeGraph.p \u2502 \u251c\u2500\u2500 TriangleMesh_out__PointCloud_80dc94ac81 \u2502 \u2502 \u2514\u2500\u2500 TriangleMesh.ply \u2502 \u2514\u2500\u2500 Voxels_False____False_567dc7f48b \u2502 \u2514\u2500\u2500 Voxels.npz \u251c\u2500\u2500 colmap_log.txt \u251c\u2500\u2500 lock \u2514\u2500\u2500 romidb","title":"Example"},{"location":"plant_imager/specifications/hardware/","text":"Hardware setup and instructions Link Network overview Link The general network design of the ROMI Plant Imager is as the following: The raspberry pi controls the movements of the camera thanks to the CNC (for the x,y,z coordinates) and the Gimbal (pan and tilt). Both of them are connected to the pi by USB cables. For the camera, several configurations exists. It is possible to retrieve the photos either by Wi-Fi (might lead to a lower resolution) or directly via a micro USB. Hardware configuration files Link To gather configuration information on the hardware during an acquisition with the plant imager we use toml files. For example, saving the following lines in a config.toml : [ScanPath] class_name = \"Circle\" # Circle, Line, Cylinder [ScanPath.kwargs] center_x = 375 center_y = 375 z = 80 tilt = 0 radius = 300 n_points = 60 [Scan.scanner.cnc] module = \"plantimager.grbl\" [Scan.scanner.cnc.kwargs] homing = true port = \"/dev/ttyACM0\" [Scan.scanner.gimbal] module = \"plantimager.blgimbal\" [Scan.scanner.gimbal.kwargs] port = \"/dev/ttyACM1\" has_tilt = false zero_pan = 0 invert_rotation = true [Scan.scanner.camera] module = \"plantimager.sony\" [Scan.scanner.camera.kwargs] device_ip = \"192.168.122.1\" api_port = \"10000\" postview = true use_flashair = false rotation = 270 [Scan.metadata.object] species = \"chenopodium album\" seed_stock = \"Col-0\" plant_id = \"3dt_chenoA\" growth_environment = \"Lyon-indoor\" growth_conditions = \"SD+LD\" treatment = \"None\" DAG = 40 sample = \"main_stem\" experiment_id = \"3dt_2021-01\" dataset_id = \"3dt\" [Scan.metadata.hardware] frame = \"30profile v1\" X_motor = \"X-Carve NEMA23\" Y_motor = \"X-Carve NEMA23\" Z_motor = \"X-Carve NEMA23\" pan_motor = \"iPower Motor GM4108H-120T Brushless Gimbal Motor\" tilt_motor = \"None\" sensor = \"RX0\" [Scan.metadata.workspace] x = [ 200 , 600 , ] y = [ 200 , 600 , ] z = [ -100 , 300 , ] Some arguments in this example have default values and for others (commented \"mandatory\" in the following description) it has to be specified in the configuration file. Here, a more detailed explanation with a full default parameters list: The acquisition path: [ ScanPath ] # mandatory class_name = \"Circle\" class_name defines the type of path the robotic arm will follow. In this case it will be a circle, the other possibilities are commented next to the variable in the example above. [ ScanPath . kwargs ] # mandatory center_x = 375 center_y = 375 z = 80 tilt = 0 radius = 300 n_points = 60 The kwargs related to the path are in this section. The arm will perform a circle of 300 around the point (375, 375) with a fixed z (80) and a tilt of 0\u00b0. The angle between each pose will be 5\u00b0 because the n_points is 60 on a 360\u00b0 circle. The center_x , center_y , z and radius parameters are expressed in mm and are related to the axis of the CNC. To have an idea of valid values for those it's possible to get the limits of the CNC axis with steps described in the cnc calibration description. Needed parameters for connection between hardware components (CNC, Gimbal and camera) and software: [ Scan . scanner . cnc ] # mandatory module = \"plantimager.grbl\" Here for example for the CNC you will have to inform about the python module required to connect to the hardware. It will depend on the type of the device. [Scan.scanner.cnc.kwargs] homing = true port = \"/dev/ttyUSB0\" baud_rate = 115200 x_lims = None y_lims = None z_lims = None safe_start = True invert_x = true invert_y = true invert_z = true The arguments all have default values here, but you might need to change the port (check with dmesg -w ). [ Scan . scanner . gimbal ] # mandatory module = \"plantimager.blgimbal\" [Scan.scanner.gimbal.kwargs] port = \"/dev/ttyUSB0\" has_tilt = True steps_per_turn = 360 zero_pan = 0 zero_tilt = 0 invert_rotation = False Similarly, for the Gimbal, again with default arguments that could be changed depending on your setup [ Scan . scanner . camera ] # mandatory module = \"plantimager.sony\" # or plantimager.gp2 [Scan.scanner.camera.kwargs] device_ip = \"192.168.122.1\" # mandatory api_port = \"10000\" # mandatory timeout : time_s = 10 postview = false use_adb = false use_flashair = false flashair_host = None camera_params = None rotation = 0 Finally, the camera (in this case the SONY RX0 communicating via Wi-Fi) with more specific arguments that will depend on the type of sensor used. A more precise documentation on several cameras and their associated parameters can be found here Object metadata: In principle, you can put any information that appear important as part of an experiment but to have a guideline of relevant parameters in the context of phenotyping you might want to check the biological metadata documentation Hardware metadata: Again here, some guidelines for this section can be found in the hardware metadata description. [ Scan . metadata . workspace ] # mandatory x = [ 200 , 600 , ] y = [ 200 , 600 , ] z = [ -100 , 300 , ] Concerning the workspace, it is not properly required for the scan to perform but if a reconstruction is to be made it will be needed. As for the path, appropriate coordinates can be collected from information contained in the cnc calibration . To load the config file in python: >> > import toml >> > conf = toml . load ( open ( 'config.toml' )) >> > print ( conf ) { 'Scan' : { 'scanner' : { 'camera_firmware' : 'sony_wifi' , 'cnc_firmware' : 'grbl-v1.1' , 'gimbal_firmware' : 'blgimbal' }}} >> > print ( conf [ \"Scan\" ][ \"scanner\" ][ \"camera_firmware\" ]) \"sony_wifi\" PiZero camera rovercam Link WORK IN PROGRESS!!!!! Configuring the access point host software (hostapd) Link Source: Raspberry Foundation website . 1. General setup Link Switch over to systemd-networkd : # remove classic networking sudo apt --autoremove purge ifupdown dhcpcd5 isc-dhcp-client isc-dhcp-common rm -r /etc/network /etc/dhcp # enable systemd-networkd systemctl enable systemd-networkd.service # setup systemd-resolved systemctl enable systemd-resolved.service apt --autoremove purge avahi-daemon apt install libnss-resolve ln -sf /run/systemd/resolve/stub-resolv.conf /etc/resolv.conf 2. Configure wpa_supplicant as access point Link To configure wpa_supplicant as access point create this file with your settings for country= , ssid= , psk= and maybe frequency= . You can just copy and paste this in one block to your command line beginning with cat and including both EOF (delimiter EOF will not get part of the file): cat > /etc/wpa_supplicant/wpa_supplicant-wlan0.conf <<EOF country=DE ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 network={ ssid=\"RPiNet\" mode=2 frequency=2437 #key_mgmt=NONE # uncomment this for an open hotspot # delete next 3 lines if key_mgmt=NONE key_mgmt=WPA-PSK proto=RSN WPA psk=\"password\" } EOF chmod 600 /etc/wpa_supplicant/wpa_supplicant-wlan0.conf systemctl disable wpa_supplicant.service systemctl enable wpa_supplicant@wlan0.service Setting up a stand alone access point Link Example for this setup: wifi mobile-phone <~.~.~.~.~> ( wlan0 ) RPi ( eth0 ) \\ / ( dhcp ) 192 .168.4.1 Do \"General setup\" then create the following file to configure wlan0 . We only have the access point. There is no ethernet device configured. cat > /etc/systemd/network/08-wlan0.network <<EOF [Match] Name=wlan0 [Network] Address=192.168.4.1/24 MulticastDNS=yes DHCPServer=yes EOF If you want this then reboot. That's it. Otherwise, go on, no need to reboot this time. Troubleshooting Link Serial access denied Link Look here if you can not communicate with the scanner using usb.","title":"Hardware"},{"location":"plant_imager/specifications/hardware/#hardware-setup-and-instructions","text":"","title":"Hardware setup and instructions"},{"location":"plant_imager/specifications/hardware/#network-overview","text":"The general network design of the ROMI Plant Imager is as the following: The raspberry pi controls the movements of the camera thanks to the CNC (for the x,y,z coordinates) and the Gimbal (pan and tilt). Both of them are connected to the pi by USB cables. For the camera, several configurations exists. It is possible to retrieve the photos either by Wi-Fi (might lead to a lower resolution) or directly via a micro USB.","title":"Network overview"},{"location":"plant_imager/specifications/hardware/#hardware-configuration-files","text":"To gather configuration information on the hardware during an acquisition with the plant imager we use toml files. For example, saving the following lines in a config.toml : [ScanPath] class_name = \"Circle\" # Circle, Line, Cylinder [ScanPath.kwargs] center_x = 375 center_y = 375 z = 80 tilt = 0 radius = 300 n_points = 60 [Scan.scanner.cnc] module = \"plantimager.grbl\" [Scan.scanner.cnc.kwargs] homing = true port = \"/dev/ttyACM0\" [Scan.scanner.gimbal] module = \"plantimager.blgimbal\" [Scan.scanner.gimbal.kwargs] port = \"/dev/ttyACM1\" has_tilt = false zero_pan = 0 invert_rotation = true [Scan.scanner.camera] module = \"plantimager.sony\" [Scan.scanner.camera.kwargs] device_ip = \"192.168.122.1\" api_port = \"10000\" postview = true use_flashair = false rotation = 270 [Scan.metadata.object] species = \"chenopodium album\" seed_stock = \"Col-0\" plant_id = \"3dt_chenoA\" growth_environment = \"Lyon-indoor\" growth_conditions = \"SD+LD\" treatment = \"None\" DAG = 40 sample = \"main_stem\" experiment_id = \"3dt_2021-01\" dataset_id = \"3dt\" [Scan.metadata.hardware] frame = \"30profile v1\" X_motor = \"X-Carve NEMA23\" Y_motor = \"X-Carve NEMA23\" Z_motor = \"X-Carve NEMA23\" pan_motor = \"iPower Motor GM4108H-120T Brushless Gimbal Motor\" tilt_motor = \"None\" sensor = \"RX0\" [Scan.metadata.workspace] x = [ 200 , 600 , ] y = [ 200 , 600 , ] z = [ -100 , 300 , ] Some arguments in this example have default values and for others (commented \"mandatory\" in the following description) it has to be specified in the configuration file. Here, a more detailed explanation with a full default parameters list: The acquisition path: [ ScanPath ] # mandatory class_name = \"Circle\" class_name defines the type of path the robotic arm will follow. In this case it will be a circle, the other possibilities are commented next to the variable in the example above. [ ScanPath . kwargs ] # mandatory center_x = 375 center_y = 375 z = 80 tilt = 0 radius = 300 n_points = 60 The kwargs related to the path are in this section. The arm will perform a circle of 300 around the point (375, 375) with a fixed z (80) and a tilt of 0\u00b0. The angle between each pose will be 5\u00b0 because the n_points is 60 on a 360\u00b0 circle. The center_x , center_y , z and radius parameters are expressed in mm and are related to the axis of the CNC. To have an idea of valid values for those it's possible to get the limits of the CNC axis with steps described in the cnc calibration description. Needed parameters for connection between hardware components (CNC, Gimbal and camera) and software: [ Scan . scanner . cnc ] # mandatory module = \"plantimager.grbl\" Here for example for the CNC you will have to inform about the python module required to connect to the hardware. It will depend on the type of the device. [Scan.scanner.cnc.kwargs] homing = true port = \"/dev/ttyUSB0\" baud_rate = 115200 x_lims = None y_lims = None z_lims = None safe_start = True invert_x = true invert_y = true invert_z = true The arguments all have default values here, but you might need to change the port (check with dmesg -w ). [ Scan . scanner . gimbal ] # mandatory module = \"plantimager.blgimbal\" [Scan.scanner.gimbal.kwargs] port = \"/dev/ttyUSB0\" has_tilt = True steps_per_turn = 360 zero_pan = 0 zero_tilt = 0 invert_rotation = False Similarly, for the Gimbal, again with default arguments that could be changed depending on your setup [ Scan . scanner . camera ] # mandatory module = \"plantimager.sony\" # or plantimager.gp2 [Scan.scanner.camera.kwargs] device_ip = \"192.168.122.1\" # mandatory api_port = \"10000\" # mandatory timeout : time_s = 10 postview = false use_adb = false use_flashair = false flashair_host = None camera_params = None rotation = 0 Finally, the camera (in this case the SONY RX0 communicating via Wi-Fi) with more specific arguments that will depend on the type of sensor used. A more precise documentation on several cameras and their associated parameters can be found here Object metadata: In principle, you can put any information that appear important as part of an experiment but to have a guideline of relevant parameters in the context of phenotyping you might want to check the biological metadata documentation Hardware metadata: Again here, some guidelines for this section can be found in the hardware metadata description. [ Scan . metadata . workspace ] # mandatory x = [ 200 , 600 , ] y = [ 200 , 600 , ] z = [ -100 , 300 , ] Concerning the workspace, it is not properly required for the scan to perform but if a reconstruction is to be made it will be needed. As for the path, appropriate coordinates can be collected from information contained in the cnc calibration . To load the config file in python: >> > import toml >> > conf = toml . load ( open ( 'config.toml' )) >> > print ( conf ) { 'Scan' : { 'scanner' : { 'camera_firmware' : 'sony_wifi' , 'cnc_firmware' : 'grbl-v1.1' , 'gimbal_firmware' : 'blgimbal' }}} >> > print ( conf [ \"Scan\" ][ \"scanner\" ][ \"camera_firmware\" ]) \"sony_wifi\"","title":"Hardware configuration files"},{"location":"plant_imager/specifications/hardware/#pizero-camera-rovercam","text":"WORK IN PROGRESS!!!!!","title":"PiZero camera rovercam"},{"location":"plant_imager/specifications/hardware/#configuring-the-access-point-host-software-hostapd","text":"Source: Raspberry Foundation website .","title":"Configuring the access point host software (hostapd)"},{"location":"plant_imager/specifications/hardware/#1-general-setup","text":"Switch over to systemd-networkd : # remove classic networking sudo apt --autoremove purge ifupdown dhcpcd5 isc-dhcp-client isc-dhcp-common rm -r /etc/network /etc/dhcp # enable systemd-networkd systemctl enable systemd-networkd.service # setup systemd-resolved systemctl enable systemd-resolved.service apt --autoremove purge avahi-daemon apt install libnss-resolve ln -sf /run/systemd/resolve/stub-resolv.conf /etc/resolv.conf","title":"1. General setup"},{"location":"plant_imager/specifications/hardware/#2-configure-wpa_supplicant-as-access-point","text":"To configure wpa_supplicant as access point create this file with your settings for country= , ssid= , psk= and maybe frequency= . You can just copy and paste this in one block to your command line beginning with cat and including both EOF (delimiter EOF will not get part of the file): cat > /etc/wpa_supplicant/wpa_supplicant-wlan0.conf <<EOF country=DE ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 network={ ssid=\"RPiNet\" mode=2 frequency=2437 #key_mgmt=NONE # uncomment this for an open hotspot # delete next 3 lines if key_mgmt=NONE key_mgmt=WPA-PSK proto=RSN WPA psk=\"password\" } EOF chmod 600 /etc/wpa_supplicant/wpa_supplicant-wlan0.conf systemctl disable wpa_supplicant.service systemctl enable wpa_supplicant@wlan0.service","title":"2. Configure wpa_supplicant as access point"},{"location":"plant_imager/specifications/hardware/#setting-up-a-stand-alone-access-point","text":"Example for this setup: wifi mobile-phone <~.~.~.~.~> ( wlan0 ) RPi ( eth0 ) \\ / ( dhcp ) 192 .168.4.1 Do \"General setup\" then create the following file to configure wlan0 . We only have the access point. There is no ethernet device configured. cat > /etc/systemd/network/08-wlan0.network <<EOF [Match] Name=wlan0 [Network] Address=192.168.4.1/24 MulticastDNS=yes DHCPServer=yes EOF If you want this then reboot. That's it. Otherwise, go on, no need to reboot this time.","title":"Setting up a stand alone access point"},{"location":"plant_imager/specifications/hardware/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"plant_imager/specifications/hardware/#serial-access-denied","text":"Look here if you can not communicate with the scanner using usb.","title":"Serial access denied"},{"location":"plant_imager/specifications/luigi/","text":"Luigi in the ROMI Plant Scanner project Link Hereafter we explain how we used the luigi Python package in the ROMI Plant Scanner project. If you are not familiar with luigi , let's just say that it is useful to create long-running batch processes where you want to chain many tasks with dependencies and requirements. In the context of the ROMI Plant Scanner project, we faced the challenge of creating complex pipelines, especially for the 3D reconstruction & analysis of a plant structure after its acquisition in the form of a series of RGB images. Several complex and fairly distinct algorithm are required to achieve our goals, and we thus decided to break down this chain of tasks to achieve greater modularity and robustness. Using luigi led us to abstract several concepts like Task , Target & Parameter : a task is limited to a single algorithmic operation with input(s), output(s) & parameter(s); a target is a (set of) file(s) that can be the input required by a task or (one of) its output(s); a parameter is a value controlling the algorithm; Parameters & configuration Link As we run luigi using the command-line tool, and the constructed workflow can be made of many tasks each with several parameters, we use TOML configuration files to define them. In addition to the TOML configuration file, the romi_run_task script requires the definition of two values: the name of the ROMI task to run; the name of the Scan dataset on which to run the ROMI task; As we will see later, each \"computational task\" has an upstream task. Using these tasks dependencies luigi will create the required workflow. Hence you do not have to know or defines the required workflow to run a task, just call it and luigi will do the rest for you! Target subclass Link In order to check for a task requirement(s) or handle its output(s) luigi implement the Target class. As we use our own Python database implementation FSDB from plantdb , and it implements the concept of a set of files as a Fileset class, we subclassed the luigi.Target class as TargetFileset . It is thus used to get/create files from the FSDB database by our luigi.Task subclasses. For example, the raw RGB images set obtained after a Scan task is the 'images' Fileset . Task subclasses Link This is where the computation is done with the run() method and targets are controlled with the requires() & output() methods. Note ROMI tasks do not use a Scan dataset identifier as it is assumed that they only work on one Scan at a time.","title":"Luigi"},{"location":"plant_imager/specifications/luigi/#luigi-in-the-romi-plant-scanner-project","text":"Hereafter we explain how we used the luigi Python package in the ROMI Plant Scanner project. If you are not familiar with luigi , let's just say that it is useful to create long-running batch processes where you want to chain many tasks with dependencies and requirements. In the context of the ROMI Plant Scanner project, we faced the challenge of creating complex pipelines, especially for the 3D reconstruction & analysis of a plant structure after its acquisition in the form of a series of RGB images. Several complex and fairly distinct algorithm are required to achieve our goals, and we thus decided to break down this chain of tasks to achieve greater modularity and robustness. Using luigi led us to abstract several concepts like Task , Target & Parameter : a task is limited to a single algorithmic operation with input(s), output(s) & parameter(s); a target is a (set of) file(s) that can be the input required by a task or (one of) its output(s); a parameter is a value controlling the algorithm;","title":"Luigi in the ROMI Plant Scanner project"},{"location":"plant_imager/specifications/luigi/#parameters-configuration","text":"As we run luigi using the command-line tool, and the constructed workflow can be made of many tasks each with several parameters, we use TOML configuration files to define them. In addition to the TOML configuration file, the romi_run_task script requires the definition of two values: the name of the ROMI task to run; the name of the Scan dataset on which to run the ROMI task; As we will see later, each \"computational task\" has an upstream task. Using these tasks dependencies luigi will create the required workflow. Hence you do not have to know or defines the required workflow to run a task, just call it and luigi will do the rest for you!","title":"Parameters &amp; configuration"},{"location":"plant_imager/specifications/luigi/#target-subclass","text":"In order to check for a task requirement(s) or handle its output(s) luigi implement the Target class. As we use our own Python database implementation FSDB from plantdb , and it implements the concept of a set of files as a Fileset class, we subclassed the luigi.Target class as TargetFileset . It is thus used to get/create files from the FSDB database by our luigi.Task subclasses. For example, the raw RGB images set obtained after a Scan task is the 'images' Fileset .","title":"Target subclass"},{"location":"plant_imager/specifications/luigi/#task-subclasses","text":"This is where the computation is done with the run() method and targets are controlled with the requires() & output() methods. Note ROMI tasks do not use a Scan dataset identifier as it is assumed that they only work on one Scan at a time.","title":"Task subclasses"},{"location":"plant_imager/specifications/pipelines/","text":"Warning This is a work in progress... the original author has no idea what he is doing! Legend Link Let's start with a description of the used symbols: Note shaped boxes are RomiConfig , they are TOML files that contains parameters for each task. Round shaped boxes are RomiTasks with their name on the first level, then the module names ( --module option in romi_run_task ) and a quick description of the tasks at hand. Folder shaped boxes are RomiTarget , they indicate files input/output and the file extension is given between parenthesis. Acquisitions Link Acquisition of real plant datasets Link Acquisition of virtual plant datasets Link Plant Reconstruction from RGB images Link 3D Plant Phenotyping Link Geometric approach Link Machine Learning approach Link Evaluation Link Mask task evaluation Link Voxel task evaluation Link PointCloud task evaluation Link","title":"Pipelines"},{"location":"plant_imager/specifications/pipelines/#legend","text":"Let's start with a description of the used symbols: Note shaped boxes are RomiConfig , they are TOML files that contains parameters for each task. Round shaped boxes are RomiTasks with their name on the first level, then the module names ( --module option in romi_run_task ) and a quick description of the tasks at hand. Folder shaped boxes are RomiTarget , they indicate files input/output and the file extension is given between parenthesis.","title":"Legend"},{"location":"plant_imager/specifications/pipelines/#acquisitions","text":"","title":"Acquisitions"},{"location":"plant_imager/specifications/pipelines/#acquisition-of-real-plant-datasets","text":"","title":"Acquisition of real plant datasets"},{"location":"plant_imager/specifications/pipelines/#acquisition-of-virtual-plant-datasets","text":"","title":"Acquisition of virtual plant datasets"},{"location":"plant_imager/specifications/pipelines/#plant-reconstruction-from-rgb-images","text":"","title":"Plant Reconstruction from RGB images"},{"location":"plant_imager/specifications/pipelines/#3d-plant-phenotyping","text":"","title":"3D Plant Phenotyping"},{"location":"plant_imager/specifications/pipelines/#geometric-approach","text":"","title":"Geometric approach"},{"location":"plant_imager/specifications/pipelines/#machine-learning-approach","text":"","title":"Machine Learning approach"},{"location":"plant_imager/specifications/pipelines/#evaluation","text":"","title":"Evaluation"},{"location":"plant_imager/specifications/pipelines/#mask-task-evaluation","text":"","title":"Mask task evaluation"},{"location":"plant_imager/specifications/pipelines/#voxel-task-evaluation","text":"","title":"Voxel task evaluation"},{"location":"plant_imager/specifications/pipelines/#pointcloud-task-evaluation","text":"","title":"PointCloud task evaluation"},{"location":"plant_imager/specifications/virtual-plant-imager/","text":"Instructions and main specifications for the virtual plant imager Link Warning clearly separate Lpy and the VPI General description Link The Virtual Plant Imager functions as a digital twin of the real romi robot Plant imager : it takes rgb images of one to several virtual 3D plant models and generate all data and metadata necessary to proceed downstream with a training of a neural network or with an analysis pipeline of the plant-3d-vision tool suite. As input , it takes a 3D model ( .obj ) file ; As output , it provides one to several datasets in a romi database format , ready for downstream use (training or analysis). db/ \u251c\u2500\u2500 virtual_imageset/ \u2502 \u251c\u2500\u2500 images/ \u2502 \u251c\u2500\u2500 metadata/ \u2502 \u2502 \u2514\u2500\u2500 images/ \u2502 \u2502 \u2514\u2500\u2500 images.json \u2502 \u2514\u2500\u2500 files.json \u2502 \u2514\u2500\u2500 scan.json \u2514\u2500\u2500 romidb The Virtual Plant Imager relies on Blender v2.81a to generate at set of (2D) RGB images from a plant 3d model. An HTTP server acts as an interface to drive Blender generation scripts. Note The Virtual Plant Imager is closely integrated with the plant generator Lpy . For information related to the generation of virtual 3D model of plants, you will be redirected to other LPy documentation. Input data (for romi_run_task VirtualScan) Link As for all ROMI tools, the Virtual Plant Imager requires a proper database to store, access and generate new data. Let's call virtual_db this database. In particular, it contains data for the virtual plant generation and/or imaging grouped in a so-called vscan_data folder: Legend : (*) the name is fixed and cannot be changed (!) the folder/file must exist (no tag means that the folder is not required for the program to run) virtual_db \u2502 romidb (!*) # a (empty) marker file for recognition by the plantdb module \u2514\u2500\u2500\u2500vscan_data (!*) \u2502 \u2514\u2500\u2500\u2500hdri (*) \u2502 \u2502 hdri_file1.hdr \u2502 \u2502 hdri_file2.hdr \u2502 \u2502 etc... \u2502 \u2514\u2500\u2500\u2500lpy (*) \u2502 \u2502 my_plant_species_model.lpy \u2502 \u2514\u2500\u2500\u2500obj (*) \u2502 \u2502 VirtualPlant.obj \u2502 \u2502 VirtualPlant_mtl \u2502 \u2514\u2500\u2500\u2500metadata (!*) \u2502 \u2502 hdri.json \u2502 \u2502 lpy.json \u2502 \u2502 obj.json \u2502 \u2502 palette.json \u2502 \u2502 scenes.json \u2502 \u2514\u2500\u2500\u2500palette (*) \u2502 \u2502 my_plant_species_model.png \u2502 \u2514\u2500\u2500\u2500scenes (*) \u2502 files.json Following sections detail the content of each files and subfolders. hdri Link If no hdri files are provided, a uniform black background will be applied. Even if they are provided, hdri backgrounds can be deactivated in the .toml file (related romi_run task: VirtualScan ) and the default black background will be applied. New background HDRI files can be downloaded from hdri haven and store in the hdri folder. Limited resolution is enough for downstream applications (we recommend 2K resolution, 6.2 MB total download). ! Note if there are several hdri files -> which one is used ? lpy Link .lpy files contain a parametric model of a plant that Lpy will use to generate a 3D model as an .obj file. With the above data_example, we provide the file arabidopsis_notex.lpy , a Lpy model for the laboratory model plant Arabidopsis thaliana (copyright C. Godin, Inria - RDP Mosaic). You can replace this file with any other Lpy model file. The correct name of the file must then be specified in the configuration .toml file (see below) [VirtualPlant] lpy_file_id = \"arabidopsis_notex\" #base name of the .lpy to be used by Lpy If you do not want to use Lpy to generate your virtual plant, you can also directly import your custom 3D plant (generated elsewhere), which must be at the .obj format. In this case, lpy subfolder is dispensable, store data in the obj subfolder (see next) obj Link Alternatively to Lpy-generated, custom 3D plant model can be provided. Data must consist in a obj and a mtl files. In the .obj , each semantic type of label desired to segment the plant must correspond to a distinct mesh. Each of these meshes must have a single material whose name is the name of the label. (note QUESTION: and then, what are the groundtruth ? how are they provided ?) metadata Link lpy.json { \"task_params\" : { \"output_file_id\" : \"out\" , \"scan_id\" : \"vscan_data\" } } other files (hdri/palette/scenes.json): { \"task_parameters\" : {}, \"task_params\" : { \"output_file_id\" : \"out\" , \"scan_id\" : \"vscan_data\" } } palette Link a .png file containing textures that will LPy will apply on the virtual plant. scenes Link files.json Link fileset descriptor with \"id\" definitions. { \"filesets\" : [ { \"files\" : [ { \"file\" : \"felsenlabyrinth_2k.hdr\" , \"id\" : \"felsenlabyrinth_2k\" }, { \"file\" : \"forest_slope_2k.hdr\" , \"id\" : \"forest_slope_2k\" } ], \"id\" : \"hdri\" }, { \"files\" : [ { \"file\" : \"arabidopsis_notex.lpy\" , \"id\" : \"arabidopsis_notex\" } ], \"id\" : \"lpy\" }, { \"etc...\" } ] } Running the virtual plant imager Link Basic command lines Link Corresponding task with romi_run_task : VirtualScan romi_run_task VirtualScan \\ --config vpi_single_dataset.toml \\ path/to/db/generated_dataset the content of the configuration file vpi_single_dataset.toml will be detailed un the next section ### configuration file for the Task Virtual Scan see also the page on specifications>tasks>VPI (does not exist yet, to be created)","title":"Virtual Plant Imager"},{"location":"plant_imager/specifications/virtual-plant-imager/#instructions-and-main-specifications-for-the-virtual-plant-imager","text":"Warning clearly separate Lpy and the VPI","title":"Instructions and main specifications for the virtual plant imager"},{"location":"plant_imager/specifications/virtual-plant-imager/#general-description","text":"The Virtual Plant Imager functions as a digital twin of the real romi robot Plant imager : it takes rgb images of one to several virtual 3D plant models and generate all data and metadata necessary to proceed downstream with a training of a neural network or with an analysis pipeline of the plant-3d-vision tool suite. As input , it takes a 3D model ( .obj ) file ; As output , it provides one to several datasets in a romi database format , ready for downstream use (training or analysis). db/ \u251c\u2500\u2500 virtual_imageset/ \u2502 \u251c\u2500\u2500 images/ \u2502 \u251c\u2500\u2500 metadata/ \u2502 \u2502 \u2514\u2500\u2500 images/ \u2502 \u2502 \u2514\u2500\u2500 images.json \u2502 \u2514\u2500\u2500 files.json \u2502 \u2514\u2500\u2500 scan.json \u2514\u2500\u2500 romidb The Virtual Plant Imager relies on Blender v2.81a to generate at set of (2D) RGB images from a plant 3d model. An HTTP server acts as an interface to drive Blender generation scripts. Note The Virtual Plant Imager is closely integrated with the plant generator Lpy . For information related to the generation of virtual 3D model of plants, you will be redirected to other LPy documentation.","title":"General description"},{"location":"plant_imager/specifications/virtual-plant-imager/#input-data-for-romi_run_task-virtualscan","text":"As for all ROMI tools, the Virtual Plant Imager requires a proper database to store, access and generate new data. Let's call virtual_db this database. In particular, it contains data for the virtual plant generation and/or imaging grouped in a so-called vscan_data folder: Legend : (*) the name is fixed and cannot be changed (!) the folder/file must exist (no tag means that the folder is not required for the program to run) virtual_db \u2502 romidb (!*) # a (empty) marker file for recognition by the plantdb module \u2514\u2500\u2500\u2500vscan_data (!*) \u2502 \u2514\u2500\u2500\u2500hdri (*) \u2502 \u2502 hdri_file1.hdr \u2502 \u2502 hdri_file2.hdr \u2502 \u2502 etc... \u2502 \u2514\u2500\u2500\u2500lpy (*) \u2502 \u2502 my_plant_species_model.lpy \u2502 \u2514\u2500\u2500\u2500obj (*) \u2502 \u2502 VirtualPlant.obj \u2502 \u2502 VirtualPlant_mtl \u2502 \u2514\u2500\u2500\u2500metadata (!*) \u2502 \u2502 hdri.json \u2502 \u2502 lpy.json \u2502 \u2502 obj.json \u2502 \u2502 palette.json \u2502 \u2502 scenes.json \u2502 \u2514\u2500\u2500\u2500palette (*) \u2502 \u2502 my_plant_species_model.png \u2502 \u2514\u2500\u2500\u2500scenes (*) \u2502 files.json Following sections detail the content of each files and subfolders.","title":"Input data (for romi_run_task VirtualScan)"},{"location":"plant_imager/specifications/virtual-plant-imager/#hdri","text":"If no hdri files are provided, a uniform black background will be applied. Even if they are provided, hdri backgrounds can be deactivated in the .toml file (related romi_run task: VirtualScan ) and the default black background will be applied. New background HDRI files can be downloaded from hdri haven and store in the hdri folder. Limited resolution is enough for downstream applications (we recommend 2K resolution, 6.2 MB total download). ! Note if there are several hdri files -> which one is used ?","title":"hdri"},{"location":"plant_imager/specifications/virtual-plant-imager/#lpy","text":".lpy files contain a parametric model of a plant that Lpy will use to generate a 3D model as an .obj file. With the above data_example, we provide the file arabidopsis_notex.lpy , a Lpy model for the laboratory model plant Arabidopsis thaliana (copyright C. Godin, Inria - RDP Mosaic). You can replace this file with any other Lpy model file. The correct name of the file must then be specified in the configuration .toml file (see below) [VirtualPlant] lpy_file_id = \"arabidopsis_notex\" #base name of the .lpy to be used by Lpy If you do not want to use Lpy to generate your virtual plant, you can also directly import your custom 3D plant (generated elsewhere), which must be at the .obj format. In this case, lpy subfolder is dispensable, store data in the obj subfolder (see next)","title":"lpy"},{"location":"plant_imager/specifications/virtual-plant-imager/#obj","text":"Alternatively to Lpy-generated, custom 3D plant model can be provided. Data must consist in a obj and a mtl files. In the .obj , each semantic type of label desired to segment the plant must correspond to a distinct mesh. Each of these meshes must have a single material whose name is the name of the label. (note QUESTION: and then, what are the groundtruth ? how are they provided ?)","title":"obj"},{"location":"plant_imager/specifications/virtual-plant-imager/#metadata","text":"lpy.json { \"task_params\" : { \"output_file_id\" : \"out\" , \"scan_id\" : \"vscan_data\" } } other files (hdri/palette/scenes.json): { \"task_parameters\" : {}, \"task_params\" : { \"output_file_id\" : \"out\" , \"scan_id\" : \"vscan_data\" } }","title":"metadata"},{"location":"plant_imager/specifications/virtual-plant-imager/#palette","text":"a .png file containing textures that will LPy will apply on the virtual plant.","title":"palette"},{"location":"plant_imager/specifications/virtual-plant-imager/#scenes","text":"","title":"scenes"},{"location":"plant_imager/specifications/virtual-plant-imager/#filesjson","text":"fileset descriptor with \"id\" definitions. { \"filesets\" : [ { \"files\" : [ { \"file\" : \"felsenlabyrinth_2k.hdr\" , \"id\" : \"felsenlabyrinth_2k\" }, { \"file\" : \"forest_slope_2k.hdr\" , \"id\" : \"forest_slope_2k\" } ], \"id\" : \"hdri\" }, { \"files\" : [ { \"file\" : \"arabidopsis_notex.lpy\" , \"id\" : \"arabidopsis_notex\" } ], \"id\" : \"lpy\" }, { \"etc...\" } ] }","title":"files.json"},{"location":"plant_imager/specifications/virtual-plant-imager/#running-the-virtual-plant-imager","text":"","title":"Running the virtual plant imager"},{"location":"plant_imager/specifications/virtual-plant-imager/#basic-command-lines","text":"Corresponding task with romi_run_task : VirtualScan romi_run_task VirtualScan \\ --config vpi_single_dataset.toml \\ path/to/db/generated_dataset the content of the configuration file vpi_single_dataset.toml will be detailed un the next section ### configuration file for the Task Virtual Scan see also the page on specifications>tasks>VPI (does not exist yet, to be created)","title":"Basic command lines"},{"location":"plant_imager/specifications/tasks/","text":"Task definitions Link As we have seen previously, we are using the luigi paradigm and have defined a series of tasks to create flexible and modular pipelines. This section is a more general overview, For more details see the reference documentation ( TODO )! Base task class Link RomiTask is the base abstract class for the ROMI Plant Scanner project and subclass luigi.Task . It implements the following methods: requires() , by default an upstream task is required; output() , by default the output of a task is a Fileset with the task's name as an identifier; input_file() , helper method to get the output of the upstream task; output_file() , helper method to create the output of the current task;","title":"Home"},{"location":"plant_imager/specifications/tasks/#task-definitions","text":"As we have seen previously, we are using the luigi paradigm and have defined a series of tasks to create flexible and modular pipelines. This section is a more general overview, For more details see the reference documentation ( TODO )!","title":"Task definitions"},{"location":"plant_imager/specifications/tasks/#base-task-class","text":"RomiTask is the base abstract class for the ROMI Plant Scanner project and subclass luigi.Task . It implements the following methods: requires() , by default an upstream task is required; output() , by default the output of a task is a Fileset with the task's name as an identifier; input_file() , helper method to get the output of the upstream task; output_file() , helper method to create the output of the current task;","title":"Base task class"},{"location":"plant_imager/specifications/tasks/acquisition_tasks/","text":"Acquisition-related tasks Link Scan task Link This task class is used to acquire a set of RGB images from a real plant to study using the Plant imager hardware. It produces a Fileset named 'images' (designated as raw scan) that may go with a metadata dictionary if provided as parameter. VirtualScan task Link This task class is used to acquire a set of RGB images from a mesh computer model (OBJ file) representing a plant using blender. It produces a Fileset named 'images' that may be accompanied by a metadata dictionary if provided as parameter. VirtualPlant task Link This task class is used to generate a mesh computer model (OBJ file) representing a plant using a programmable plant model generator called LPY. It produces ??? ( TODO ) CalibrationScan task Link This task class is used to ??? ( TODO )","title":"Acquisition tasks"},{"location":"plant_imager/specifications/tasks/acquisition_tasks/#acquisition-related-tasks","text":"","title":"Acquisition-related tasks"},{"location":"plant_imager/specifications/tasks/acquisition_tasks/#scan-task","text":"This task class is used to acquire a set of RGB images from a real plant to study using the Plant imager hardware. It produces a Fileset named 'images' (designated as raw scan) that may go with a metadata dictionary if provided as parameter.","title":"Scan task"},{"location":"plant_imager/specifications/tasks/acquisition_tasks/#virtualscan-task","text":"This task class is used to acquire a set of RGB images from a mesh computer model (OBJ file) representing a plant using blender. It produces a Fileset named 'images' that may be accompanied by a metadata dictionary if provided as parameter.","title":"VirtualScan task"},{"location":"plant_imager/specifications/tasks/acquisition_tasks/#virtualplant-task","text":"This task class is used to generate a mesh computer model (OBJ file) representing a plant using a programmable plant model generator called LPY. It produces ??? ( TODO )","title":"VirtualPlant task"},{"location":"plant_imager/specifications/tasks/acquisition_tasks/#calibrationscan-task","text":"This task class is used to ??? ( TODO )","title":"CalibrationScan task"},{"location":"plant_imager/specifications/tasks/evaluation_tasks/","text":"Evaluation-related tasks Link These tasks require the definition of a ground truth . EvaluationTask Link This is the base (abstract) evaluation task. PointCloudSegmentationEvaluation Link PointCloudEvaluation Link Segmentation2DEvaluation Link VoxelsEvaluation Link CylinderRadiusEvaluation Link Thanks to ground truth derived from virtual plants data, most of the reconstruction tasks can be properly evaluated. However, the acquisition made by the plant imager can not be assessed with virtual ground truth and needs some from real world with an adapted comparison method. The 2 main outputs of the plant imager are a set of images and the corresponding positions from the robotic arm. Those data are used later in the reconstruction pipeline, therefore their precision can be evaluated by the precision of the reconstructed object. The evaluation of this acquisition component will be described in this section. Objective Link The goal here is to be able to evaluate the precision of an acquisition by comparing the measurement of a specific trait in an actual object with the same feature in its reconstruction. One basic shape that can often be encountered in plants is the cylinder (stem are often cylindrical, and sometimes so are fruits). It has been therefore chosen as model from which to extract main characteristics (as the height or the radius). In this evaluation we focused on the radius estimation. The several steps are: Make an acquisition of a cylindrical object Generate a point cloud from the collected images Estimate the radius of the reconstructed object and compare it with the \"ground truth\" radius Prerequisite Link install romi plant-imager (from source or using a docker image ) & read install procedure install romi plant-3d-vision (from source or using a docker image ) & read install procedure Create and activate isolated python environment (see the procedure here ) Step-by-step tutorial Link 1. Image a cylindrical object Link In order to have a precise measurement of the object radius, it can be a good idea to use manufactured items with a calibrated size. In our case we chose to evaluate our plant imager with a can. Note Be careful to be able to detect your object from the background in the future reconstruction process. Different segmentation methods are described here and as we use the \"Binary segmentation\" algorithm the can has been painted in a very tasteful green. The procedure to take images is described in the plant imager tutorial but here are the basic steps: DB creation mkdir path/to/db touch path/to/db/romidb Run an acquisition with the Scan task, and a hardware.toml configuration file in the newly created DB romi_run_task Scan /path/to/db/imageset_id/ --config plant-imager/config/hardware.toml The imageset_id fileset is now filled : db/ \u251c\u2500\u2500 imageset_id/ \u2502 \u251c\u2500\u2500 files.json \u2502 \u251c\u2500\u2500 images/ \u2502 \u251c\u2500\u2500 metadata/ \u2502 \u2502 \u2514\u2500\u2500 images/ \u2502 \u2502 \u2514\u2500\u2500 images.json \u2502 \u2514\u2500\u2500 scan.toml \u2514\u2500\u2500 romidb As for the AnglesAndInternodes measurement, in order for the CylinderRadiusEvaluation to retrieve the ground truth radius value, it must be added manually in a imageset_id/measures.json file: { \"radius\" : 7.95 } 2. Point cloud reconstruction Link The next step is to compute a point cloud from the acquired data. As before, the full explanation of the operations concerning the reconstruction pipeline can be found in this tutorial but mainly are: romi_run_task PointCloud /path/to/db/imageset_id/ --config plant-3d-vision/config/pipeline.toml Resulting an equivalent of this tree structure (depending on the used configuration file): db/ \u251c\u2500\u2500 imageset_id/ \u2502 \u251c\u2500\u2500 Colmap__/ \u2502 \u251c\u2500\u2500 files.json \u2502 \u251c\u2500\u2500 images/ \u2502 \u251c\u2500\u2500 Masks__/ \u2502 \u251c\u2500\u2500 measures.json \u2502 \u251c\u2500\u2500 metadata/ \u2502 \u2502 \u2514\u2500\u2500 images/ \u2502 \u2502 \u2514\u2500\u2500 images.json \u2502 \u251c\u2500\u2500 pipeline.toml \u2502 \u251c\u2500\u2500 PointCloud__/ \u2502 \u251c\u2500\u2500 scan.toml \u2502 \u251c\u2500\u2500 Undistorted__/ \u2502 \u2514\u2500\u2500 Voxels__/ \u2514\u2500\u2500 romidb Note Be sure to obtain a proper reconstructed cylinder (as maybe a section of the object) by checking if you are satisfied with the PointCloud_created_fileset/PointCloud.ply point cloud in you favorite 3d software. If not, you can try to modify parameters in the pipeline.toml configuration file as: - the bounding box in the Colmap section - the threshold parameter (or equivalent) of the Masks task - the voxel_size linked to the Voxels task 3. Cylinder Radius Evaluation Link It is now possible to extract the radius from the point cloud using the CylinderRadiusEvaluation . Parameters in the pipeline.toml associated to the task must be defined accordingly: [CylinderRadiusEvaluation] upstream_task = \"PointCloud\" Note By default the upstream_task parameter of CylinderRadiusEvaluation is CylinderRadiusGroundTruth With the following command line: romi_run_task CylinderRadiusEvaluation /path/to/db/imageset_id/ --config plant-3d-vision/config/pipeline.toml --module plant3dvision.tasks.evaluation The CylinderRadiusEvaluation.json can be found in the CylinderRadiusEvaluation__ fileset with the output results: { \"calculated_radius\" : 8.265318865620745 , \"gt_radius\" : 7.95 , \"err (%)\" : 3.97 } Evaluate the cylinder radius estimation task Link To assess the accuracy of the cylinder radius estimate from a point-cloud, we developed a CylinderRadiusGroundTruth task. It produces a ground truth point-cloud with a cylinder shape. By default, random dimensions (height and radius) are assumed. To generate the cylinder point-cloud, assuming you did set a ROMI_DB environment variable to your local database: 0. Set a ROMI_DB environment variable pointing to your local database: export ROMI_DB = $USER \"/romi_db/\" 1. Create a scan folder, e.g. virtual_cylinder : mkdir $ROMI_DB /virtual_cylinder 2. Then, it can be run the same way as any other task: romi_run_task CylinderRadiusGroundTruth $ROMI_DB /virtual_cylinder/ --module plant3dvision.tasks.evaluation You may want to specify a radius and/or height in the toml config: [CylinderRadiusGroundTruth] radius = 66.6 height = 13.0 To evaluate of the CylinderRadiusEvaluation task thanks to the virtual cylinder generated by CylinderRadiusGroundTruth : 0. Set a ROMI_DB environment variable pointing to your local database: export ROMI_DB = $USER \"/romi_db/\" 1. Then, run the CylinderRadiusEvaluation task with: romi_run_task CylinderRadiusEvaluation $ROMI_DB /virtual_cylinder/ --config plant-3d-vision/config/virtual_cylinder.toml --module plant3dvision.tasks.evaluation","title":"Evaluation tasks"},{"location":"plant_imager/specifications/tasks/evaluation_tasks/#evaluation-related-tasks","text":"These tasks require the definition of a ground truth .","title":"Evaluation-related tasks"},{"location":"plant_imager/specifications/tasks/evaluation_tasks/#evaluationtask","text":"This is the base (abstract) evaluation task.","title":"EvaluationTask"},{"location":"plant_imager/specifications/tasks/evaluation_tasks/#pointcloudsegmentationevaluation","text":"","title":"PointCloudSegmentationEvaluation"},{"location":"plant_imager/specifications/tasks/evaluation_tasks/#pointcloudevaluation","text":"","title":"PointCloudEvaluation"},{"location":"plant_imager/specifications/tasks/evaluation_tasks/#segmentation2devaluation","text":"","title":"Segmentation2DEvaluation"},{"location":"plant_imager/specifications/tasks/evaluation_tasks/#voxelsevaluation","text":"","title":"VoxelsEvaluation"},{"location":"plant_imager/specifications/tasks/evaluation_tasks/#cylinderradiusevaluation","text":"Thanks to ground truth derived from virtual plants data, most of the reconstruction tasks can be properly evaluated. However, the acquisition made by the plant imager can not be assessed with virtual ground truth and needs some from real world with an adapted comparison method. The 2 main outputs of the plant imager are a set of images and the corresponding positions from the robotic arm. Those data are used later in the reconstruction pipeline, therefore their precision can be evaluated by the precision of the reconstructed object. The evaluation of this acquisition component will be described in this section.","title":"CylinderRadiusEvaluation"},{"location":"plant_imager/specifications/tasks/evaluation_tasks/#objective","text":"The goal here is to be able to evaluate the precision of an acquisition by comparing the measurement of a specific trait in an actual object with the same feature in its reconstruction. One basic shape that can often be encountered in plants is the cylinder (stem are often cylindrical, and sometimes so are fruits). It has been therefore chosen as model from which to extract main characteristics (as the height or the radius). In this evaluation we focused on the radius estimation. The several steps are: Make an acquisition of a cylindrical object Generate a point cloud from the collected images Estimate the radius of the reconstructed object and compare it with the \"ground truth\" radius","title":"Objective"},{"location":"plant_imager/specifications/tasks/evaluation_tasks/#prerequisite","text":"install romi plant-imager (from source or using a docker image ) & read install procedure install romi plant-3d-vision (from source or using a docker image ) & read install procedure Create and activate isolated python environment (see the procedure here )","title":"Prerequisite"},{"location":"plant_imager/specifications/tasks/evaluation_tasks/#step-by-step-tutorial","text":"","title":"Step-by-step tutorial"},{"location":"plant_imager/specifications/tasks/evaluation_tasks/#1-image-a-cylindrical-object","text":"In order to have a precise measurement of the object radius, it can be a good idea to use manufactured items with a calibrated size. In our case we chose to evaluate our plant imager with a can. Note Be careful to be able to detect your object from the background in the future reconstruction process. Different segmentation methods are described here and as we use the \"Binary segmentation\" algorithm the can has been painted in a very tasteful green. The procedure to take images is described in the plant imager tutorial but here are the basic steps: DB creation mkdir path/to/db touch path/to/db/romidb Run an acquisition with the Scan task, and a hardware.toml configuration file in the newly created DB romi_run_task Scan /path/to/db/imageset_id/ --config plant-imager/config/hardware.toml The imageset_id fileset is now filled : db/ \u251c\u2500\u2500 imageset_id/ \u2502 \u251c\u2500\u2500 files.json \u2502 \u251c\u2500\u2500 images/ \u2502 \u251c\u2500\u2500 metadata/ \u2502 \u2502 \u2514\u2500\u2500 images/ \u2502 \u2502 \u2514\u2500\u2500 images.json \u2502 \u2514\u2500\u2500 scan.toml \u2514\u2500\u2500 romidb As for the AnglesAndInternodes measurement, in order for the CylinderRadiusEvaluation to retrieve the ground truth radius value, it must be added manually in a imageset_id/measures.json file: { \"radius\" : 7.95 }","title":"1. Image a cylindrical object"},{"location":"plant_imager/specifications/tasks/evaluation_tasks/#2-point-cloud-reconstruction","text":"The next step is to compute a point cloud from the acquired data. As before, the full explanation of the operations concerning the reconstruction pipeline can be found in this tutorial but mainly are: romi_run_task PointCloud /path/to/db/imageset_id/ --config plant-3d-vision/config/pipeline.toml Resulting an equivalent of this tree structure (depending on the used configuration file): db/ \u251c\u2500\u2500 imageset_id/ \u2502 \u251c\u2500\u2500 Colmap__/ \u2502 \u251c\u2500\u2500 files.json \u2502 \u251c\u2500\u2500 images/ \u2502 \u251c\u2500\u2500 Masks__/ \u2502 \u251c\u2500\u2500 measures.json \u2502 \u251c\u2500\u2500 metadata/ \u2502 \u2502 \u2514\u2500\u2500 images/ \u2502 \u2502 \u2514\u2500\u2500 images.json \u2502 \u251c\u2500\u2500 pipeline.toml \u2502 \u251c\u2500\u2500 PointCloud__/ \u2502 \u251c\u2500\u2500 scan.toml \u2502 \u251c\u2500\u2500 Undistorted__/ \u2502 \u2514\u2500\u2500 Voxels__/ \u2514\u2500\u2500 romidb Note Be sure to obtain a proper reconstructed cylinder (as maybe a section of the object) by checking if you are satisfied with the PointCloud_created_fileset/PointCloud.ply point cloud in you favorite 3d software. If not, you can try to modify parameters in the pipeline.toml configuration file as: - the bounding box in the Colmap section - the threshold parameter (or equivalent) of the Masks task - the voxel_size linked to the Voxels task","title":"2. Point cloud reconstruction"},{"location":"plant_imager/specifications/tasks/evaluation_tasks/#3-cylinder-radius-evaluation","text":"It is now possible to extract the radius from the point cloud using the CylinderRadiusEvaluation . Parameters in the pipeline.toml associated to the task must be defined accordingly: [CylinderRadiusEvaluation] upstream_task = \"PointCloud\" Note By default the upstream_task parameter of CylinderRadiusEvaluation is CylinderRadiusGroundTruth With the following command line: romi_run_task CylinderRadiusEvaluation /path/to/db/imageset_id/ --config plant-3d-vision/config/pipeline.toml --module plant3dvision.tasks.evaluation The CylinderRadiusEvaluation.json can be found in the CylinderRadiusEvaluation__ fileset with the output results: { \"calculated_radius\" : 8.265318865620745 , \"gt_radius\" : 7.95 , \"err (%)\" : 3.97 }","title":"3. Cylinder Radius Evaluation"},{"location":"plant_imager/specifications/tasks/evaluation_tasks/#evaluate-the-cylinder-radius-estimation-task","text":"To assess the accuracy of the cylinder radius estimate from a point-cloud, we developed a CylinderRadiusGroundTruth task. It produces a ground truth point-cloud with a cylinder shape. By default, random dimensions (height and radius) are assumed. To generate the cylinder point-cloud, assuming you did set a ROMI_DB environment variable to your local database: 0. Set a ROMI_DB environment variable pointing to your local database: export ROMI_DB = $USER \"/romi_db/\" 1. Create a scan folder, e.g. virtual_cylinder : mkdir $ROMI_DB /virtual_cylinder 2. Then, it can be run the same way as any other task: romi_run_task CylinderRadiusGroundTruth $ROMI_DB /virtual_cylinder/ --module plant3dvision.tasks.evaluation You may want to specify a radius and/or height in the toml config: [CylinderRadiusGroundTruth] radius = 66.6 height = 13.0 To evaluate of the CylinderRadiusEvaluation task thanks to the virtual cylinder generated by CylinderRadiusGroundTruth : 0. Set a ROMI_DB environment variable pointing to your local database: export ROMI_DB = $USER \"/romi_db/\" 1. Then, run the CylinderRadiusEvaluation task with: romi_run_task CylinderRadiusEvaluation $ROMI_DB /virtual_cylinder/ --config plant-3d-vision/config/virtual_cylinder.toml --module plant3dvision.tasks.evaluation","title":"Evaluate the cylinder radius estimation task"},{"location":"plant_imager/specifications/tasks/file_tasks/","text":"File-related tasks Link FilesetExists Link This task takes a Fileset identifier as a parameter and makes sure it is found in the Scan instance it is working on. No upstream task definition is required and it returns the Fileset . ImagesFilesetExists Link This is a specific case of the FilesetExists class for 'images' Fileset , i.e. the set of RGB images obtained after the Scan task. ModelFileset Link This is a specific case of the FilesetExists class for 'models' Fileset , i.e. the training file obtained from machine learning. FileByFileTask Link This is an abstract class used to apply a RomiTask on each file of a Fileset . Clean task Link This task class is used to clean a Scan dataset by removing all Fileset s except the 'images' Fileset , i.e. the set of RGB images obtained after the Scan task.","title":"Files tasks"},{"location":"plant_imager/specifications/tasks/file_tasks/#file-related-tasks","text":"","title":"File-related tasks"},{"location":"plant_imager/specifications/tasks/file_tasks/#filesetexists","text":"This task takes a Fileset identifier as a parameter and makes sure it is found in the Scan instance it is working on. No upstream task definition is required and it returns the Fileset .","title":"FilesetExists"},{"location":"plant_imager/specifications/tasks/file_tasks/#imagesfilesetexists","text":"This is a specific case of the FilesetExists class for 'images' Fileset , i.e. the set of RGB images obtained after the Scan task.","title":"ImagesFilesetExists"},{"location":"plant_imager/specifications/tasks/file_tasks/#modelfileset","text":"This is a specific case of the FilesetExists class for 'models' Fileset , i.e. the training file obtained from machine learning.","title":"ModelFileset"},{"location":"plant_imager/specifications/tasks/file_tasks/#filebyfiletask","text":"This is an abstract class used to apply a RomiTask on each file of a Fileset .","title":"FileByFileTask"},{"location":"plant_imager/specifications/tasks/file_tasks/#clean-task","text":"This task class is used to clean a Scan dataset by removing all Fileset s except the 'images' Fileset , i.e. the set of RGB images obtained after the Scan task.","title":"Clean task"},{"location":"plant_imager/specifications/tasks/ground_truth_tasks/","text":"Defining ground truth Link To performs evaluation tasks, first you have to defines a ground truth to compare to. This is the aim of the following task classes. VoxelGroundTruth Link PointCloudGroundTruth Link ClusteredMeshGroundTruth Link","title":"Defining ground truth"},{"location":"plant_imager/specifications/tasks/ground_truth_tasks/#defining-ground-truth","text":"To performs evaluation tasks, first you have to defines a ground truth to compare to. This is the aim of the following task classes.","title":"Defining ground truth"},{"location":"plant_imager/specifications/tasks/ground_truth_tasks/#voxelgroundtruth","text":"","title":"VoxelGroundTruth"},{"location":"plant_imager/specifications/tasks/ground_truth_tasks/#pointcloudgroundtruth","text":"","title":"PointCloudGroundTruth"},{"location":"plant_imager/specifications/tasks/ground_truth_tasks/#clusteredmeshgroundtruth","text":"","title":"ClusteredMeshGroundTruth"},{"location":"plant_imager/specifications/tasks/reconstruction_tasks/","text":"Reconstruction related tasks Link Undistorted Link This task class is used to \"undistort\" images obtained by a camera that may not have a perfect lens. It produces a Fileset of rgb images saved under the task id. Colmap Link This task class is used to estimate camera poses from a set of RGB images. By default, it is downstream ImagesFilesetExists (raw scan) An alternative upstream task choice could be the Undistorted . It produces ??? ( TODO ) Clarification required! don't we use the positions (x, y, z, pan) from the CNC & gimbal ? why do we compute the sparse reconstruction (point-cloud) ? Masks Link This task class is used to create a binary mask of each (real or virtual) plant RGB image . By default, it is downstream the Undistorted task. An alternative upstream task choice could be the Undistorted . The following methods are available to compute masks: linear excess_green vesselness invert Clarification required! document mask algorithms! It produces a Fileset of binary images saved under the task id. Voxels Link This task class is used to compute a volume (ref?) from back-projection of the binary (Masks task) or labelled (Segmentation2D task) masks. By default, it is downstream the Masks & Colmap tasks. The following methods are available to compute back-projection: carving averaging Clarification required! what is a volume ?! the difference with \"point-cloud\" is not too clear... * document back-projection algorithms! It produces a 3D array saved as NPZ (compressed numpy array). This array may be binary if downstream of the Masks task, or a labelled array if downstream the Segmentation2D task. PointCloud Link This task class is used to transform the binary volumetric data, from the Voxels tasks into an open3d 3D point-cloud. By default, it is downstream the Voxels task. It uses an Exact Euclidean distance transform method (ref?). It produces a PLY file. TriangleMesh Link This task class is used to transform a 3D point-cloud into an open3d 3D triangulated mesh. By default, it is downstream the PointCloud task. It uses the poisson_mesh method from the CGAL library described here . It produces a PLY file. CurveSkeleton Link This task class is used to compute a skeleton (ref?) from a 3D triangulated mesh. By default, it is downstream the PointCloud task. It uses the skeletonize_mesh method from the CGAL library described here . It produces a PLY file. TreeGraph Link This task class is used to generate a tree graph structure (ref?) from a skeleton . By default, it is downstream the CurveSkeleton task. It uses networkx Python package to compute a minimum spanning tree with ??? ( TODO ) It produces a JSON file ??? ( TODO ) AnglesAndInternodes Link This task class is used to compute angles and internodes between successive organs along the main stem. By default, it is downstream the TreeGraph task. It produces a JSON file ??? ( TODO ) Segmentation2D Link This task class is used to SegmentedPointCloud Link This task class is used to transform the multiclass volumetric data, from the Segmentation2D tasks into an open3d labelled 3D point-cloud. By default, it is downstream the Segmentation2D task. It produces a PLY file. ClusteredMesh Link This task class is used to transform a labelled 3D point-cloud into an open3d 3D triangulated mesh. By default, it is downstream the SegmentedPointCloud task. It produces a PLY file.","title":"Reconstruction tasks"},{"location":"plant_imager/specifications/tasks/reconstruction_tasks/#reconstruction-related-tasks","text":"","title":"Reconstruction related tasks"},{"location":"plant_imager/specifications/tasks/reconstruction_tasks/#undistorted","text":"This task class is used to \"undistort\" images obtained by a camera that may not have a perfect lens. It produces a Fileset of rgb images saved under the task id.","title":"Undistorted"},{"location":"plant_imager/specifications/tasks/reconstruction_tasks/#colmap","text":"This task class is used to estimate camera poses from a set of RGB images. By default, it is downstream ImagesFilesetExists (raw scan) An alternative upstream task choice could be the Undistorted . It produces ??? ( TODO ) Clarification required! don't we use the positions (x, y, z, pan) from the CNC & gimbal ? why do we compute the sparse reconstruction (point-cloud) ?","title":"Colmap"},{"location":"plant_imager/specifications/tasks/reconstruction_tasks/#masks","text":"This task class is used to create a binary mask of each (real or virtual) plant RGB image . By default, it is downstream the Undistorted task. An alternative upstream task choice could be the Undistorted . The following methods are available to compute masks: linear excess_green vesselness invert Clarification required! document mask algorithms! It produces a Fileset of binary images saved under the task id.","title":"Masks"},{"location":"plant_imager/specifications/tasks/reconstruction_tasks/#voxels","text":"This task class is used to compute a volume (ref?) from back-projection of the binary (Masks task) or labelled (Segmentation2D task) masks. By default, it is downstream the Masks & Colmap tasks. The following methods are available to compute back-projection: carving averaging Clarification required! what is a volume ?! the difference with \"point-cloud\" is not too clear... * document back-projection algorithms! It produces a 3D array saved as NPZ (compressed numpy array). This array may be binary if downstream of the Masks task, or a labelled array if downstream the Segmentation2D task.","title":"Voxels"},{"location":"plant_imager/specifications/tasks/reconstruction_tasks/#pointcloud","text":"This task class is used to transform the binary volumetric data, from the Voxels tasks into an open3d 3D point-cloud. By default, it is downstream the Voxels task. It uses an Exact Euclidean distance transform method (ref?). It produces a PLY file.","title":"PointCloud"},{"location":"plant_imager/specifications/tasks/reconstruction_tasks/#trianglemesh","text":"This task class is used to transform a 3D point-cloud into an open3d 3D triangulated mesh. By default, it is downstream the PointCloud task. It uses the poisson_mesh method from the CGAL library described here . It produces a PLY file.","title":"TriangleMesh"},{"location":"plant_imager/specifications/tasks/reconstruction_tasks/#curveskeleton","text":"This task class is used to compute a skeleton (ref?) from a 3D triangulated mesh. By default, it is downstream the PointCloud task. It uses the skeletonize_mesh method from the CGAL library described here . It produces a PLY file.","title":"CurveSkeleton"},{"location":"plant_imager/specifications/tasks/reconstruction_tasks/#treegraph","text":"This task class is used to generate a tree graph structure (ref?) from a skeleton . By default, it is downstream the CurveSkeleton task. It uses networkx Python package to compute a minimum spanning tree with ??? ( TODO ) It produces a JSON file ??? ( TODO )","title":"TreeGraph"},{"location":"plant_imager/specifications/tasks/reconstruction_tasks/#anglesandinternodes","text":"This task class is used to compute angles and internodes between successive organs along the main stem. By default, it is downstream the TreeGraph task. It produces a JSON file ??? ( TODO )","title":"AnglesAndInternodes"},{"location":"plant_imager/specifications/tasks/reconstruction_tasks/#segmentation2d","text":"This task class is used to","title":"Segmentation2D"},{"location":"plant_imager/specifications/tasks/reconstruction_tasks/#segmentedpointcloud","text":"This task class is used to transform the multiclass volumetric data, from the Segmentation2D tasks into an open3d labelled 3D point-cloud. By default, it is downstream the Segmentation2D task. It produces a PLY file.","title":"SegmentedPointCloud"},{"location":"plant_imager/specifications/tasks/reconstruction_tasks/#clusteredmesh","text":"This task class is used to transform a labelled 3D point-cloud into an open3d 3D triangulated mesh. By default, it is downstream the SegmentedPointCloud task. It produces a PLY file.","title":"ClusteredMesh"},{"location":"plant_imager/tutorials/","text":"","title":"Home"},{"location":"plant_imager/tutorials/Tutorial_sm-dtw_simulatephyllotaxis/","text":"How to use the 'sm-dtw' assessment tool with simulated synthetic phyllotaxis data Link Related Materials: Link Download and view the full tutorial video . See also below how to navigate rapidely into the content of this video. related readme procedure: you can also help by reading the readme of the project . Objective Link generate synthetic data to use the sm-dtw program use sm-dtw evaluate the prediction made by sm-dtw Prerequisite Link you only need to install docker . No other third-party software (R, python, jupyter, conda) is required, as all the programs are run from inside the docker container Step-by-step tutorial Link 1. Download the docker image Link It should be downloaded automatically when trying to run the docker for the first time docker run -p 8888:8888 -it roboticsmicrofarms/sm-dtw_demo:latest bash 2. Using jupyter notebook inside a docker: Link Running the Jupyter notebook: Link In your local machine , create a working directory to store the results generated by the notebook. It will be mounted inside the container, where its name will be \"docker_sandbox\". The name in you local machine can be identical or different, but keep the name inside the docker to avoid further changes in the notebooks. Sop open a terminal and type: mkdir docker_sandbox # this folder can be created anywhere in your local machine cd docker_sandbox # Move into this working directory Start the container: docker run -v $(pwd):/myapp/docker_sandbox -p 8888:8888 -it roboticsmicrofarms/sm-dtw_demo:latest bash Note : you can start this command from anywhere in your local machine by correctly mapping the mounted volume in the string before the semi-column ( -v path/in/your/machine:/myapp/docker_sandbox ) Inside the container , serve the notebooks by entering the following command after the running container's prompt (which should be something like: (dtw) root@aa7632c1fc29:/myapp# ): jupyter notebook Phyllotaxis-sim-eval/notebooks/docker_run --ip 0.0.0.0 --no-browser --allow-root Outside the docker, start your browser enter the url with token:\" http://127.0.0.1:8888/?token= .....\" or directly Ctrl+click on it select the correct kernel (R, dtw or bash, as indicated in the first cell of the notebook) run the notebook ! Outline of the tutorial video Link Here is the main content of the video and indicative time to help you directly jump into the chapters you are interested in ! Chapter Time start Content description Introduction 00min 00s Romi Project, object of this video Content Outline 00min 30s details the content of the video What is sm-dtw ? 1min 05s general description of sm-dtw program Why/when do I need 'sm-dtw' 2min 40s Presents the precise example of our use for phyllotaxis measurement in Romi and propose other possible user cases Where to find online written documentation 8min15 provides a link to the 'readme' page of the github repo of the project Docker & Jupyter notebooks 8min 47 Download and start docker + start Jupyter Notebooks Prepare a working folder to store your data on your local machine Step1 (1 st Jupyter notebook) 12min 12s Explains how to simulate synthetic data made of a sequence of reference phyllotaxis data (angles and internodes) a test sequence containing errors Step2 (2 nd Jupyter notebook) 28min 15s Explains how to realignthe two phyllotaxis sequences generated in step1 using sm-dtw Step3 (3 rd Jupyter notebook) 39min 11s Explains how to check that the alignment prediction made by sm-dtw correspond to the errors generated in step1 Quit and close Jupyter notebooks and docker container 51min 20s proper procedure to close and quit jupyter notebooks and the docker container.","title":"How to evaluate an phyllotaxis measurement ? (case1) Use sm-dtw with simulated data"},{"location":"plant_imager/tutorials/Tutorial_sm-dtw_simulatephyllotaxis/#how-to-use-the-sm-dtw-assessment-tool-with-simulated-synthetic-phyllotaxis-data","text":"","title":"How to use the 'sm-dtw' assessment tool with simulated synthetic phyllotaxis data"},{"location":"plant_imager/tutorials/Tutorial_sm-dtw_simulatephyllotaxis/#related-materials","text":"Download and view the full tutorial video . See also below how to navigate rapidely into the content of this video. related readme procedure: you can also help by reading the readme of the project .","title":"Related Materials:"},{"location":"plant_imager/tutorials/Tutorial_sm-dtw_simulatephyllotaxis/#objective","text":"generate synthetic data to use the sm-dtw program use sm-dtw evaluate the prediction made by sm-dtw","title":"Objective"},{"location":"plant_imager/tutorials/Tutorial_sm-dtw_simulatephyllotaxis/#prerequisite","text":"you only need to install docker . No other third-party software (R, python, jupyter, conda) is required, as all the programs are run from inside the docker container","title":"Prerequisite"},{"location":"plant_imager/tutorials/Tutorial_sm-dtw_simulatephyllotaxis/#step-by-step-tutorial","text":"","title":"Step-by-step tutorial"},{"location":"plant_imager/tutorials/Tutorial_sm-dtw_simulatephyllotaxis/#1-download-the-docker-image","text":"It should be downloaded automatically when trying to run the docker for the first time docker run -p 8888:8888 -it roboticsmicrofarms/sm-dtw_demo:latest bash","title":"1. Download the docker image"},{"location":"plant_imager/tutorials/Tutorial_sm-dtw_simulatephyllotaxis/#2-using-jupyter-notebook-inside-a-docker","text":"","title":"2. Using jupyter notebook inside a docker:"},{"location":"plant_imager/tutorials/Tutorial_sm-dtw_simulatephyllotaxis/#running-the-jupyter-notebook","text":"In your local machine , create a working directory to store the results generated by the notebook. It will be mounted inside the container, where its name will be \"docker_sandbox\". The name in you local machine can be identical or different, but keep the name inside the docker to avoid further changes in the notebooks. Sop open a terminal and type: mkdir docker_sandbox # this folder can be created anywhere in your local machine cd docker_sandbox # Move into this working directory Start the container: docker run -v $(pwd):/myapp/docker_sandbox -p 8888:8888 -it roboticsmicrofarms/sm-dtw_demo:latest bash Note : you can start this command from anywhere in your local machine by correctly mapping the mounted volume in the string before the semi-column ( -v path/in/your/machine:/myapp/docker_sandbox ) Inside the container , serve the notebooks by entering the following command after the running container's prompt (which should be something like: (dtw) root@aa7632c1fc29:/myapp# ): jupyter notebook Phyllotaxis-sim-eval/notebooks/docker_run --ip 0.0.0.0 --no-browser --allow-root Outside the docker, start your browser enter the url with token:\" http://127.0.0.1:8888/?token= .....\" or directly Ctrl+click on it select the correct kernel (R, dtw or bash, as indicated in the first cell of the notebook) run the notebook !","title":"Running the Jupyter notebook:"},{"location":"plant_imager/tutorials/Tutorial_sm-dtw_simulatephyllotaxis/#outline-of-the-tutorial-video","text":"Here is the main content of the video and indicative time to help you directly jump into the chapters you are interested in ! Chapter Time start Content description Introduction 00min 00s Romi Project, object of this video Content Outline 00min 30s details the content of the video What is sm-dtw ? 1min 05s general description of sm-dtw program Why/when do I need 'sm-dtw' 2min 40s Presents the precise example of our use for phyllotaxis measurement in Romi and propose other possible user cases Where to find online written documentation 8min15 provides a link to the 'readme' page of the github repo of the project Docker & Jupyter notebooks 8min 47 Download and start docker + start Jupyter Notebooks Prepare a working folder to store your data on your local machine Step1 (1 st Jupyter notebook) 12min 12s Explains how to simulate synthetic data made of a sequence of reference phyllotaxis data (angles and internodes) a test sequence containing errors Step2 (2 nd Jupyter notebook) 28min 15s Explains how to realignthe two phyllotaxis sequences generated in step1 using sm-dtw Step3 (3 rd Jupyter notebook) 39min 11s Explains how to check that the alignment prediction made by sm-dtw correspond to the errors generated in step1 Quit and close Jupyter notebooks and docker container 51min 20s proper procedure to close and quit jupyter notebooks and the docker container.","title":"Outline of the tutorial video"},{"location":"plant_imager/tutorials/basics/","text":"How to use the ROMI scanner software? Link We here assume you have followed the \"installation instructions\" available here . Getting started Link There are some requirements to use the different algorithms in the pipeline. Most of them are installed automatically from the requirements file when using pip. The most important part is Colmap (v3.6). The two requirements that are not shipped with pip are: Colmap (v3.6) for the structure from motion algorithms Blender (>= 2.81) for the virtual scanner Preferably, create a virtual environment for python 3.7 or python 3.8 using virtualenv or a conda environment specific to the 3D Scanner. Warning If using python 3.8, Open3D binaries are not yet available on pip, therefore you have to build Open3D from sources! Basic usage Link Every task on the scanner is launched through the romi_run_task command provided in the plant3dvision module. It is a wrapper for luigi , with preloaded tasks from the plant3dvision module. The general usage is as follows: romi_run_task [ -h ] [ --config CONFIG ] [ --luigicmd LUIGICMD ] [ --module MODULE ] [ --local-scheduler ] [ --log-level LOG_LEVEL ] task scan CONFIG is either a file or a folder. If a file, it must be json or toml and contains the configuration of the task to run. If a folder, it will read all configuration files in json or toml format from the folder. LUIGICMD is an optional parameter specifying an alternative command for luigi . MODULE is an optional parameter for running task from external modules (see TODO). LOG_LEVEL is the level of logging. Defaults to INFO , but can be set to DEBUG to increase verbosity. task is the name of the class to run (see TODO) scan is the location of the target scan on which to process the task. It is of the form DB_LOCATION/SCAN_ID , where DB_LOCATION is a path containing the plantdb marker. Configuration files Link The configuration is in the form of a dictionary, in which each key is the ID of a given task. In toml format, it reads as follows: [FirstTask] parameter1 = value1 parameter2 = value2 [SecondTask] parameter1 = value1 parameter2 = value2 Pipelines Link This is a sample configuration for the full reconstruction pipeline : [Colmap] matcher = \"exhaustive\" compute_dense = false [Colmap.cli_args.feature_extractor] \"--ImageReader.single_camera\" = \"1\" \"--SiftExtraction.use_gpu\" = \"1\" [Colmap.cli_args.exhaustive_matcher] \"--SiftMatching.use_gpu\" = \"1\" [Colmap.cli_args.model_aligner] \"--robust_alignment_max_error\" = \"10\" [Masks] type = \"excess_green\" dilation = 5 binarize = true threshold = 0.0 [Voxels] voxel_size = 1.0 type = \"carving\" [PointCloud] level_set_value = 1.0 [Visualization] max_image_size = 1500 max_pcd_size = 10000 thumbnail_size = 150 pcd_source = \"vox2pcd\" mesh_source = \"delaunay\" To run the full reconstruction pipeline use this configuration file with romi_run_task : romi_run_task --config scanner.json AnglesAndInternodes /path/to/db/scan_id/ --local-scheduler This will process all tasks up to the AnglesAndInternodes task. Every task produces a Fileset , a subdirectory in the scan directory whose name starts the same as the task name. The characters following are a hash of the configuration of the task, so that the outputs of the same task with different parameters can coexist in the same scan. Any change in the parameters will make the needed task to be recomputed with subsequent calls of romi_run_task . Already computed tasks will be left untouched. To recompute a task, just delete the corresponding folder in the scan directory and rerun romi_run_task . Default task reference Link default_modules = { # Scanning modules: \"Scan\" : \"plantimager.tasks.scan\" , \"VirtualPlant\" : \"plantimager.tasks.lpy\" , \"VirtualScan\" : \"plantimager.tasks.scan\" , \"CalibrationScan\" : \"plantimager.tasks.scan\" , # Geometric reconstruction modules: \"Colmap\" : \"plant3dvision.tasks.colmap\" , \"Undistorted\" : \"plant3dvision.tasks.proc2d\" , \"Masks\" : \"plant3dvision.tasks.proc2d\" , \"Voxels\" : \"plant3dvision.tasks.cl\" , \"PointCloud\" : \"plant3dvision.tasks.proc3d\" , \"TriangleMesh\" : \"plant3dvision.tasks.proc3d\" , \"CurveSkeleton\" : \"plant3dvision.tasks.proc3d\" , # Machine learning reconstruction modules: \"Segmentation2D\" : \"plant3dvision.tasks.proc2d\" , \"SegmentedPointCloud\" : \"plant3dvision.tasks.proc3d\" , \"ClusteredMesh\" : \"plant3dvision.tasks.proc3d\" , \"OrganSegmentation\" : \"plant3dvision.tasks.proc3d\" , # Quantification modules: \"TreeGraph\" : \"plant3dvision.tasks.arabidopsis\" , \"AnglesAndInternodes\" : \"plant3dvision.tasks.arabidopsis\" , # Evaluation modules: \"VoxelsGroundTruth\" : \"plant3dvision.tasks.evaluation\" , \"VoxelsEvaluation\" : \"plant3dvision.tasks.evaluation\" , \"PointCloudGroundTruth\" : \"plant3dvision.tasks.evaluation\" , \"PointCloudEvaluation\" : \"plant3dvision.tasks.evaluation\" , \"ClusteredMeshGroundTruth\" : \"plant3dvision.tasks.evaluation\" , \"PointCloudSegmentationEvaluation\" : \"plant3dvision.tasks.evaluation\" , \"Segmentation2DEvaluation\" : \"plant3dvision.tasks.evaluation\" , \"AnglesAndInternodesEvaluation\" : \"plant3dvision.tasks.evaluation\" , # Visu modules: \"Visualization\" : \"plant3dvision.tasks.visualization\" , # Database modules: \"Clean\" : \"romitask.task\" } Warning This is for reference only, please update the changes in the code. This will be later replaced by a reference doc generated from the code! Class name: Scan Module: plantimager.tasks.scan Description: A task for running a scan, real or virtual. Default upstream tasks: None Parameters: - metadata (DictParameter) : metadata for the scan - scanner (DictParameter) : scanner hardware configuration (TODO: see hardware documentation) - path (DictParameter) : scanner path configuration (TODO: see hardware documentation) Class name: CalibrationScan Module: plantimager.tasks.scan Description: A task for running a scan, real or virtual, with a calibration path. It is used to calibrate Colmap poses for subsequent scans. (TODO: see calibration documentation) Default upstream tasks: None Parameters: - metadata (DictParameter) : metadata for the scan - scanner (DictParameter) : scanner hardware configuration (TODO: see hardware documentation) - path (DictParameter) : scanner path configuration (TODO: see hardware documentation) - n_line : number of shots taken on the orthogonal calibration lines Class name: Clean Module: plantimager.tasks.scan Description: Cleanup a scan, keeping only the \"images\" fileset and removing all computed pipelines. Default upstream tasks: None Parameters: - no_confirm (BoolParameter, default=False) : do not ask for confirmation in the command prompt. Class name: Colmap Module: plant3dvision.tasks.colmap Description: Runs colmap on a given scan. Default upstream tasks: Scan Upstream task format: Fileset with image files Output fileset format: images.json, cameras.json, points3D.json, sparse.ply [, dense.ply] Parameters: - matcher (Parameter, default=\"exhaustive\") : either \"exhaustive\" or \"sequential\" (TODO: see Colmap documentation) - compute_dense (BoolParameter) : whether to run the dense Colmap to obtain a dense point cloud - cli_args (DictParameter) : parameters for Colmap command line prompts (TODO: see Colmap documentation) - align_pcd (BoolParameter, default=True) : align point cloud on calibrated or metadata poses ? - calibration_scan_id (Parameter, default=\"\") : ID of the calibration scan. Class name: Undistorted Module: plant3dvision.tasks.proc2d Description: Undistorts images using computed intrinsic camera parameters Default upstream tasks: Scan, Colmap Upstream task format: Fileset with image files Output fileset format: Fileset with image files Class name: Masks Module: plant3dvision.tasks.proc2d Description: compute masks using several functions Default upstream tasks: Undistorted Upstream task format: Fileset with image files Output fileset format: Fileset with grayscale or binary image files Parameters: - type (Parameter) : \"linear\", \"excess_green\", \"vesselness\", \"invert\" (TODO: see segmentation documentation) - parameters (ListParameter) : list of scalar parameters, depends on type - dilation (IntParameter) : by how much to dilate masks if binary - binarize (BoolParameter, default=True) : binarize the masks - threshold (FloatParameter, default=0.0) : threshold for binarization - Class name: Segmentation2D Module: plant3dvision.tasks.proc2d Description: compute masks using trained deep learning models Default upstream tasks: Undistorted Upstream task format: Fileset with image files Output fileset format: Fileset with grayscale image files, each corresponding to a given input image and class Parameters: - query (DictParameter) : query to pass to upstream fileset. It filters file by metadata, e.g {\"channel\": \"rgb\"} will process only input files such that \"channel\" metadata is equal to \"rgb\". - labels (Parameter) : string of the form \"a,b,c\" such that a, b, c are the identifiers of the labels produced by the neural network - Sx, Sy (IntParameter) : size of the input of the neural network. Input pictures are cropped in the center to this size. - model_segmentation_name : name of \".pt\" file that can be found at `https://db.romi-project.eu/models` - Class name: Voxels Module: plant3dvision.tasks.cl Description: Computes a volume from back-projection of 2D segmented images Default upstream tasks: - upstream_mask: Masks - upstream_colmap: Colmap Upstream task format: - upstream_mask: Fileset with grayscale images - upstream_colmap: Output of Colmap task Output fileset format: npz file with as many arrays as classes Parameters: - use_colmap_poses (BoolParameter, default=True): Either use precomputed camera poses or output from the Colmap task - voxel_size (FloatParameter): size of one side of voxels - type (Parameter): \"carving\" or \"averaging\" (TODO: See 3D documentation) - multiclass (BoolParameter, default=False): whether input data is single class or multiclass (e.g as an output of Segmentation2D) - log (BoolParameter, default=True), in the case of \"averaging\" type, whether to apply log when averaging values. Class name: PointCloud Module: plant3dvision.tasks.scan Description: Computes a point cloud from volumetric voxel data (either single or multiclass) Default upstream tasks: Voxels Upstream task format: npz file with as many 3D array as classes Output task format: single point cloud in ply. Metadata may include label name if multiclass. Class name: TriangleMesh Module: plant3dvision.tasks.scan Description: Triangulates input point cloud. Currently ignores class data and needs only one connected component. Default upstream tasks: PointCloud Upstream task format: ply file Output task format: ply triangle mesh file Class name: CurveSkeleton Module: plant3dvision.tasks.scan Description: Creates a 3D curve skeleton Default upstream tasks: TriangleMesh Upstream task format: ply triangle mesh Output task format: json with two entries \"points\" and \"lines\" (TODO: precise) Class name: TreeGraph Module: plant3dvision.tasks.arabidopsis Description: Creates a tree graph of the plant Default upstream tasks: CurveSkeleton Upstream task format: json Output task format: json (TODO: precise) Class name; AnglesAndInternodes Module: plant3dvision.tasks.arabidopsis Description: Computes angles and internode Default upstream tasks: TreeGraph Upstream task format: json Output task format: json (TODO: precise) Scanner API reference Link Objects Link /objects (GET): retrieve the list of obj files in the data folder that can be loaded. /load_object/<object_id> (GET) load the given object in the scene. Takes a translation vector as URL parameters ( dx , dy , dz ) Classes Link /classes (GET): retrieve the list of classes. Backgrounds Link /backgrounds (GET): retrieve the list of hdr files in the hdri folder that can be loaded. /load_background/<background_id> (GET) load the given background in the scene. Camera Link /camera_intrinsics (POST): set camera intrinsics. Keys: width , height , focal /camera_pose (POST): set camera pose. Keys: tx , ty , tz , rx , ry , rz Rendering Link /render (GET): gets the rendering of the scene /render_class/<class_id> (GET) renders the scene, with everything transparent except the given class TODO: missing endpoints httpie # Setup camera http -f post http://localhost:5000/camera_intrinsics width=1920 height=1080 focal=35 # Load arabidopsis_0 http get 'http://localhost:5000/load_object/arabidopsis_0.obj?dx=10&dy=20&dz=1' # Load \"old tree in the park\" background http get http://127.0.0.1:5000/load_background/old_tree_in_city_park_8k.hdr # Move camera http -f post http://localhost:5000/camera_pose tx=-60 ty=0 tz=50 rx=60 ry=0 rz=-90 # Render scene and download image http --download get http://localhost:5000/render # Render only leaves http --download get http://localhost:5000/render_class/Color_7","title":"How to use the romi_run_task generic command ?"},{"location":"plant_imager/tutorials/basics/#how-to-use-the-romi-scanner-software","text":"We here assume you have followed the \"installation instructions\" available here .","title":"How to use the ROMI scanner software?"},{"location":"plant_imager/tutorials/basics/#getting-started","text":"There are some requirements to use the different algorithms in the pipeline. Most of them are installed automatically from the requirements file when using pip. The most important part is Colmap (v3.6). The two requirements that are not shipped with pip are: Colmap (v3.6) for the structure from motion algorithms Blender (>= 2.81) for the virtual scanner Preferably, create a virtual environment for python 3.7 or python 3.8 using virtualenv or a conda environment specific to the 3D Scanner. Warning If using python 3.8, Open3D binaries are not yet available on pip, therefore you have to build Open3D from sources!","title":"Getting started"},{"location":"plant_imager/tutorials/basics/#basic-usage","text":"Every task on the scanner is launched through the romi_run_task command provided in the plant3dvision module. It is a wrapper for luigi , with preloaded tasks from the plant3dvision module. The general usage is as follows: romi_run_task [ -h ] [ --config CONFIG ] [ --luigicmd LUIGICMD ] [ --module MODULE ] [ --local-scheduler ] [ --log-level LOG_LEVEL ] task scan CONFIG is either a file or a folder. If a file, it must be json or toml and contains the configuration of the task to run. If a folder, it will read all configuration files in json or toml format from the folder. LUIGICMD is an optional parameter specifying an alternative command for luigi . MODULE is an optional parameter for running task from external modules (see TODO). LOG_LEVEL is the level of logging. Defaults to INFO , but can be set to DEBUG to increase verbosity. task is the name of the class to run (see TODO) scan is the location of the target scan on which to process the task. It is of the form DB_LOCATION/SCAN_ID , where DB_LOCATION is a path containing the plantdb marker.","title":"Basic usage"},{"location":"plant_imager/tutorials/basics/#configuration-files","text":"The configuration is in the form of a dictionary, in which each key is the ID of a given task. In toml format, it reads as follows: [FirstTask] parameter1 = value1 parameter2 = value2 [SecondTask] parameter1 = value1 parameter2 = value2","title":"Configuration files"},{"location":"plant_imager/tutorials/basics/#pipelines","text":"This is a sample configuration for the full reconstruction pipeline : [Colmap] matcher = \"exhaustive\" compute_dense = false [Colmap.cli_args.feature_extractor] \"--ImageReader.single_camera\" = \"1\" \"--SiftExtraction.use_gpu\" = \"1\" [Colmap.cli_args.exhaustive_matcher] \"--SiftMatching.use_gpu\" = \"1\" [Colmap.cli_args.model_aligner] \"--robust_alignment_max_error\" = \"10\" [Masks] type = \"excess_green\" dilation = 5 binarize = true threshold = 0.0 [Voxels] voxel_size = 1.0 type = \"carving\" [PointCloud] level_set_value = 1.0 [Visualization] max_image_size = 1500 max_pcd_size = 10000 thumbnail_size = 150 pcd_source = \"vox2pcd\" mesh_source = \"delaunay\" To run the full reconstruction pipeline use this configuration file with romi_run_task : romi_run_task --config scanner.json AnglesAndInternodes /path/to/db/scan_id/ --local-scheduler This will process all tasks up to the AnglesAndInternodes task. Every task produces a Fileset , a subdirectory in the scan directory whose name starts the same as the task name. The characters following are a hash of the configuration of the task, so that the outputs of the same task with different parameters can coexist in the same scan. Any change in the parameters will make the needed task to be recomputed with subsequent calls of romi_run_task . Already computed tasks will be left untouched. To recompute a task, just delete the corresponding folder in the scan directory and rerun romi_run_task .","title":"Pipelines"},{"location":"plant_imager/tutorials/basics/#default-task-reference","text":"default_modules = { # Scanning modules: \"Scan\" : \"plantimager.tasks.scan\" , \"VirtualPlant\" : \"plantimager.tasks.lpy\" , \"VirtualScan\" : \"plantimager.tasks.scan\" , \"CalibrationScan\" : \"plantimager.tasks.scan\" , # Geometric reconstruction modules: \"Colmap\" : \"plant3dvision.tasks.colmap\" , \"Undistorted\" : \"plant3dvision.tasks.proc2d\" , \"Masks\" : \"plant3dvision.tasks.proc2d\" , \"Voxels\" : \"plant3dvision.tasks.cl\" , \"PointCloud\" : \"plant3dvision.tasks.proc3d\" , \"TriangleMesh\" : \"plant3dvision.tasks.proc3d\" , \"CurveSkeleton\" : \"plant3dvision.tasks.proc3d\" , # Machine learning reconstruction modules: \"Segmentation2D\" : \"plant3dvision.tasks.proc2d\" , \"SegmentedPointCloud\" : \"plant3dvision.tasks.proc3d\" , \"ClusteredMesh\" : \"plant3dvision.tasks.proc3d\" , \"OrganSegmentation\" : \"plant3dvision.tasks.proc3d\" , # Quantification modules: \"TreeGraph\" : \"plant3dvision.tasks.arabidopsis\" , \"AnglesAndInternodes\" : \"plant3dvision.tasks.arabidopsis\" , # Evaluation modules: \"VoxelsGroundTruth\" : \"plant3dvision.tasks.evaluation\" , \"VoxelsEvaluation\" : \"plant3dvision.tasks.evaluation\" , \"PointCloudGroundTruth\" : \"plant3dvision.tasks.evaluation\" , \"PointCloudEvaluation\" : \"plant3dvision.tasks.evaluation\" , \"ClusteredMeshGroundTruth\" : \"plant3dvision.tasks.evaluation\" , \"PointCloudSegmentationEvaluation\" : \"plant3dvision.tasks.evaluation\" , \"Segmentation2DEvaluation\" : \"plant3dvision.tasks.evaluation\" , \"AnglesAndInternodesEvaluation\" : \"plant3dvision.tasks.evaluation\" , # Visu modules: \"Visualization\" : \"plant3dvision.tasks.visualization\" , # Database modules: \"Clean\" : \"romitask.task\" } Warning This is for reference only, please update the changes in the code. This will be later replaced by a reference doc generated from the code! Class name: Scan Module: plantimager.tasks.scan Description: A task for running a scan, real or virtual. Default upstream tasks: None Parameters: - metadata (DictParameter) : metadata for the scan - scanner (DictParameter) : scanner hardware configuration (TODO: see hardware documentation) - path (DictParameter) : scanner path configuration (TODO: see hardware documentation) Class name: CalibrationScan Module: plantimager.tasks.scan Description: A task for running a scan, real or virtual, with a calibration path. It is used to calibrate Colmap poses for subsequent scans. (TODO: see calibration documentation) Default upstream tasks: None Parameters: - metadata (DictParameter) : metadata for the scan - scanner (DictParameter) : scanner hardware configuration (TODO: see hardware documentation) - path (DictParameter) : scanner path configuration (TODO: see hardware documentation) - n_line : number of shots taken on the orthogonal calibration lines Class name: Clean Module: plantimager.tasks.scan Description: Cleanup a scan, keeping only the \"images\" fileset and removing all computed pipelines. Default upstream tasks: None Parameters: - no_confirm (BoolParameter, default=False) : do not ask for confirmation in the command prompt. Class name: Colmap Module: plant3dvision.tasks.colmap Description: Runs colmap on a given scan. Default upstream tasks: Scan Upstream task format: Fileset with image files Output fileset format: images.json, cameras.json, points3D.json, sparse.ply [, dense.ply] Parameters: - matcher (Parameter, default=\"exhaustive\") : either \"exhaustive\" or \"sequential\" (TODO: see Colmap documentation) - compute_dense (BoolParameter) : whether to run the dense Colmap to obtain a dense point cloud - cli_args (DictParameter) : parameters for Colmap command line prompts (TODO: see Colmap documentation) - align_pcd (BoolParameter, default=True) : align point cloud on calibrated or metadata poses ? - calibration_scan_id (Parameter, default=\"\") : ID of the calibration scan. Class name: Undistorted Module: plant3dvision.tasks.proc2d Description: Undistorts images using computed intrinsic camera parameters Default upstream tasks: Scan, Colmap Upstream task format: Fileset with image files Output fileset format: Fileset with image files Class name: Masks Module: plant3dvision.tasks.proc2d Description: compute masks using several functions Default upstream tasks: Undistorted Upstream task format: Fileset with image files Output fileset format: Fileset with grayscale or binary image files Parameters: - type (Parameter) : \"linear\", \"excess_green\", \"vesselness\", \"invert\" (TODO: see segmentation documentation) - parameters (ListParameter) : list of scalar parameters, depends on type - dilation (IntParameter) : by how much to dilate masks if binary - binarize (BoolParameter, default=True) : binarize the masks - threshold (FloatParameter, default=0.0) : threshold for binarization - Class name: Segmentation2D Module: plant3dvision.tasks.proc2d Description: compute masks using trained deep learning models Default upstream tasks: Undistorted Upstream task format: Fileset with image files Output fileset format: Fileset with grayscale image files, each corresponding to a given input image and class Parameters: - query (DictParameter) : query to pass to upstream fileset. It filters file by metadata, e.g {\"channel\": \"rgb\"} will process only input files such that \"channel\" metadata is equal to \"rgb\". - labels (Parameter) : string of the form \"a,b,c\" such that a, b, c are the identifiers of the labels produced by the neural network - Sx, Sy (IntParameter) : size of the input of the neural network. Input pictures are cropped in the center to this size. - model_segmentation_name : name of \".pt\" file that can be found at `https://db.romi-project.eu/models` - Class name: Voxels Module: plant3dvision.tasks.cl Description: Computes a volume from back-projection of 2D segmented images Default upstream tasks: - upstream_mask: Masks - upstream_colmap: Colmap Upstream task format: - upstream_mask: Fileset with grayscale images - upstream_colmap: Output of Colmap task Output fileset format: npz file with as many arrays as classes Parameters: - use_colmap_poses (BoolParameter, default=True): Either use precomputed camera poses or output from the Colmap task - voxel_size (FloatParameter): size of one side of voxels - type (Parameter): \"carving\" or \"averaging\" (TODO: See 3D documentation) - multiclass (BoolParameter, default=False): whether input data is single class or multiclass (e.g as an output of Segmentation2D) - log (BoolParameter, default=True), in the case of \"averaging\" type, whether to apply log when averaging values. Class name: PointCloud Module: plant3dvision.tasks.scan Description: Computes a point cloud from volumetric voxel data (either single or multiclass) Default upstream tasks: Voxels Upstream task format: npz file with as many 3D array as classes Output task format: single point cloud in ply. Metadata may include label name if multiclass. Class name: TriangleMesh Module: plant3dvision.tasks.scan Description: Triangulates input point cloud. Currently ignores class data and needs only one connected component. Default upstream tasks: PointCloud Upstream task format: ply file Output task format: ply triangle mesh file Class name: CurveSkeleton Module: plant3dvision.tasks.scan Description: Creates a 3D curve skeleton Default upstream tasks: TriangleMesh Upstream task format: ply triangle mesh Output task format: json with two entries \"points\" and \"lines\" (TODO: precise) Class name: TreeGraph Module: plant3dvision.tasks.arabidopsis Description: Creates a tree graph of the plant Default upstream tasks: CurveSkeleton Upstream task format: json Output task format: json (TODO: precise) Class name; AnglesAndInternodes Module: plant3dvision.tasks.arabidopsis Description: Computes angles and internode Default upstream tasks: TreeGraph Upstream task format: json Output task format: json (TODO: precise)","title":"Default task reference"},{"location":"plant_imager/tutorials/basics/#scanner-api-reference","text":"","title":"Scanner API reference"},{"location":"plant_imager/tutorials/basics/#objects","text":"/objects (GET): retrieve the list of obj files in the data folder that can be loaded. /load_object/<object_id> (GET) load the given object in the scene. Takes a translation vector as URL parameters ( dx , dy , dz )","title":"Objects"},{"location":"plant_imager/tutorials/basics/#classes","text":"/classes (GET): retrieve the list of classes.","title":"Classes"},{"location":"plant_imager/tutorials/basics/#backgrounds","text":"/backgrounds (GET): retrieve the list of hdr files in the hdri folder that can be loaded. /load_background/<background_id> (GET) load the given background in the scene.","title":"Backgrounds"},{"location":"plant_imager/tutorials/basics/#camera","text":"/camera_intrinsics (POST): set camera intrinsics. Keys: width , height , focal /camera_pose (POST): set camera pose. Keys: tx , ty , tz , rx , ry , rz","title":"Camera"},{"location":"plant_imager/tutorials/basics/#rendering","text":"/render (GET): gets the rendering of the scene /render_class/<class_id> (GET) renders the scene, with everything transparent except the given class TODO: missing endpoints httpie # Setup camera http -f post http://localhost:5000/camera_intrinsics width=1920 height=1080 focal=35 # Load arabidopsis_0 http get 'http://localhost:5000/load_object/arabidopsis_0.obj?dx=10&dy=20&dz=1' # Load \"old tree in the park\" background http get http://127.0.0.1:5000/load_background/old_tree_in_city_park_8k.hdr # Move camera http -f post http://localhost:5000/camera_pose tx=-60 ty=0 tz=50 rx=60 ry=0 rz=-90 # Render scene and download image http --download get http://localhost:5000/render # Render only leaves http --download get http://localhost:5000/render_class/Color_7","title":"Rendering"},{"location":"plant_imager/tutorials/extrinsic_calibration/","text":"Extrinsic Image Calibration Link Objective Link Calibration is giving the right scale to your images and is thus crucial to perform measures from phenotyping imaging. Scale (pixel size) is a priori unknown in a picture. In addition, some aspects of an hardware setup can create artefacts affecting scaling during 3D reconstruction. Hence, we developed a procedure of extrinsic calibration to scale 3D reconstructions to real world unit and correct possible artefacts induced by the configuration of our plant-imager robot. In this tutorial, you will learn how to calibrate an image acquisition for downstream analysis and how to re-use a previously made calibration for an analysis, provided that the set-up is the same. Note Intrinsic calibration corrects possible defects induced by the lens of the camera. These type of defects are not addressed by this procedure. Prerequisite Link Make sure that you installed all the ROMI software to run image acquisitions with the ROMI Plant Imager (explanation here ) install also plant-3d-vision to perform the calibration. We highly recommend the use of dockers to run ROMI software. Details to build and run docker of the ROMI Plant Imager are here Before reading this tutorial, you should first be able to run a basic acquisition without calibration, as explained in this tutorial . set up a database or quickly generate a simple database with the following commands: mkdir path/to/db touch path/to/db/romidb You have now your file-based romi database plantdb Principles of the extrinsic calibration performed here Link The motor positions moving the camera along the acquisition path give a first indication of the scale of the picture, but this motor information is as accurate as the encoder allows them to be. To have the closest scaling from reality, we use Colmap (a structure-from-motion algorithm) at the very beginning of the 3d reconstructions (see here ): this technique allows to refine the positions of the camera given by the robot motors. However, those computed positions are determined up to a scaling and roto-translation of the world and as a result, present a problem for measuring real world unit quantities. In addition, each camera pose is aligned with the corresponding CNC arm position. This can lead to a bias in scaling induced by the offset between the camera optical center and the CNC arm as represented in the following picture: It is particularly true when doing circular path (which is often the case with the phenotyping station). Indeed, because of that offset, the distance between 2 camera poses is bigger than it should be and as a result, the reconstructed the object is bigger than it is in real life (with a relative error of 2d / D). To correct that, a procedure has been developed to perform an extrinsic calibration and apply the results for further image acquisitions using the same hardware settings. Step-by-step tutorial Link 1. Calibration acquisition Link Because the bias is mainly induced by making a circular path of image acquisition, one way to avoid it is to do a calibration acquisition with first a path constituted of two lines (two orthogonal lines in our case) followed by the path that will be used by other acquisitions. To do so, run the task 'CalibrationScan' the same way as for a regular acquisition with the romi_run_task command, including your regular configuration file adapted to your plant imager (hereafter: hardware.toml ). In the command, define a folder inside your romi database (called above plantdb ) that will store the data of this calibration acquisition: romi_run_task --config config/hardware.toml \\ #command and config CalibrationScan \\ # romi task /path/to/plantdb/calibration_scan_id/ #data destination of this calibration scan Note Run this command either in a docker container of the plant-imager or, if you install plant-imager in your system, do not forget to activate the appropriate virtual environment to make romi_run_task command accessible. Colmap performance increases when several \"recognizable\" objects are present in the scene, so that the program easily defines matching reference points between overlapping images. We advise to put such objects in the scene for the calibration acquisition (they could be removed later) 2. Compute circular poses from path lines with Colmap Link Thanks to the linear path added to the circular one, Colmap can now retrieve accurate poses with a proper scaling. Colmap can be easily run with romi software plant-3d-vision . For such a run, a proper configuration file (.toml) is required. A default one is provided with plant-3d-vision , and accessible from your local git-cloned repository or in the repository included inside the docker container. romi_run_task --config path/toconfig/geom_pipe_full.toml \\ #command and path to config file Colmap \\ #the task Colmap /path/to/plantdb/calibration_scan_id/ #data destination folder Note Run this command either in a docker container of plant-3d-vision or, if you install plant-imager in your system, do not forget to activate the appropriate virtual environment to make romi_run_task command accessible. 3. (re)Use the poses extracted from the calibration scan Link Now, the calibrated poses can be used to properly scale 3d reconstruction each time an analysis is performed (full process detailed here ) on other image dataset. To do so, just add in the Colmap section of the configuration .toml file for reconstruction: [Colmap] calibration_scan_id = \"calibration_scan_id\" #name of the folder containing calibration data Important : * Calibration_scan_id and the other dataset to be analyzed must be in the same romi database . * To be valid , calibration data can only be used if the camera position has not changed at all ( tilt , etc ... ) in the robotic arm . For instance , a new calibration acquisition should be performed each time the camera is removed and replaced back on the arm .","title":"How to calibrate the Plant imager ?"},{"location":"plant_imager/tutorials/extrinsic_calibration/#extrinsic-image-calibration","text":"","title":"Extrinsic Image Calibration"},{"location":"plant_imager/tutorials/extrinsic_calibration/#objective","text":"Calibration is giving the right scale to your images and is thus crucial to perform measures from phenotyping imaging. Scale (pixel size) is a priori unknown in a picture. In addition, some aspects of an hardware setup can create artefacts affecting scaling during 3D reconstruction. Hence, we developed a procedure of extrinsic calibration to scale 3D reconstructions to real world unit and correct possible artefacts induced by the configuration of our plant-imager robot. In this tutorial, you will learn how to calibrate an image acquisition for downstream analysis and how to re-use a previously made calibration for an analysis, provided that the set-up is the same. Note Intrinsic calibration corrects possible defects induced by the lens of the camera. These type of defects are not addressed by this procedure.","title":"Objective"},{"location":"plant_imager/tutorials/extrinsic_calibration/#prerequisite","text":"Make sure that you installed all the ROMI software to run image acquisitions with the ROMI Plant Imager (explanation here ) install also plant-3d-vision to perform the calibration. We highly recommend the use of dockers to run ROMI software. Details to build and run docker of the ROMI Plant Imager are here Before reading this tutorial, you should first be able to run a basic acquisition without calibration, as explained in this tutorial . set up a database or quickly generate a simple database with the following commands: mkdir path/to/db touch path/to/db/romidb You have now your file-based romi database plantdb","title":"Prerequisite"},{"location":"plant_imager/tutorials/extrinsic_calibration/#principles-of-the-extrinsic-calibration-performed-here","text":"The motor positions moving the camera along the acquisition path give a first indication of the scale of the picture, but this motor information is as accurate as the encoder allows them to be. To have the closest scaling from reality, we use Colmap (a structure-from-motion algorithm) at the very beginning of the 3d reconstructions (see here ): this technique allows to refine the positions of the camera given by the robot motors. However, those computed positions are determined up to a scaling and roto-translation of the world and as a result, present a problem for measuring real world unit quantities. In addition, each camera pose is aligned with the corresponding CNC arm position. This can lead to a bias in scaling induced by the offset between the camera optical center and the CNC arm as represented in the following picture: It is particularly true when doing circular path (which is often the case with the phenotyping station). Indeed, because of that offset, the distance between 2 camera poses is bigger than it should be and as a result, the reconstructed the object is bigger than it is in real life (with a relative error of 2d / D). To correct that, a procedure has been developed to perform an extrinsic calibration and apply the results for further image acquisitions using the same hardware settings.","title":"Principles of the extrinsic calibration performed here"},{"location":"plant_imager/tutorials/extrinsic_calibration/#step-by-step-tutorial","text":"","title":"Step-by-step tutorial"},{"location":"plant_imager/tutorials/extrinsic_calibration/#1-calibration-acquisition","text":"Because the bias is mainly induced by making a circular path of image acquisition, one way to avoid it is to do a calibration acquisition with first a path constituted of two lines (two orthogonal lines in our case) followed by the path that will be used by other acquisitions. To do so, run the task 'CalibrationScan' the same way as for a regular acquisition with the romi_run_task command, including your regular configuration file adapted to your plant imager (hereafter: hardware.toml ). In the command, define a folder inside your romi database (called above plantdb ) that will store the data of this calibration acquisition: romi_run_task --config config/hardware.toml \\ #command and config CalibrationScan \\ # romi task /path/to/plantdb/calibration_scan_id/ #data destination of this calibration scan Note Run this command either in a docker container of the plant-imager or, if you install plant-imager in your system, do not forget to activate the appropriate virtual environment to make romi_run_task command accessible. Colmap performance increases when several \"recognizable\" objects are present in the scene, so that the program easily defines matching reference points between overlapping images. We advise to put such objects in the scene for the calibration acquisition (they could be removed later)","title":"1. Calibration acquisition"},{"location":"plant_imager/tutorials/extrinsic_calibration/#2-compute-circular-poses-from-path-lines-with-colmap","text":"Thanks to the linear path added to the circular one, Colmap can now retrieve accurate poses with a proper scaling. Colmap can be easily run with romi software plant-3d-vision . For such a run, a proper configuration file (.toml) is required. A default one is provided with plant-3d-vision , and accessible from your local git-cloned repository or in the repository included inside the docker container. romi_run_task --config path/toconfig/geom_pipe_full.toml \\ #command and path to config file Colmap \\ #the task Colmap /path/to/plantdb/calibration_scan_id/ #data destination folder Note Run this command either in a docker container of plant-3d-vision or, if you install plant-imager in your system, do not forget to activate the appropriate virtual environment to make romi_run_task command accessible.","title":"2. Compute circular poses from path lines with Colmap"},{"location":"plant_imager/tutorials/extrinsic_calibration/#3-reuse-the-poses-extracted-from-the-calibration-scan","text":"Now, the calibrated poses can be used to properly scale 3d reconstruction each time an analysis is performed (full process detailed here ) on other image dataset. To do so, just add in the Colmap section of the configuration .toml file for reconstruction: [Colmap] calibration_scan_id = \"calibration_scan_id\" #name of the folder containing calibration data Important : * Calibration_scan_id and the other dataset to be analyzed must be in the same romi database . * To be valid , calibration data can only be used if the camera position has not changed at all ( tilt , etc ... ) in the robotic arm . For instance , a new calibration acquisition should be performed each time the camera is removed and replaced back on the arm .","title":"3. (re)Use the poses extracted from the calibration scan"},{"location":"plant_imager/tutorials/hardware_scan/","text":"Plant Imager Bot Link Objective Link This tutorial will guide through the steps of acquiring images of a plant using the plant imager robot In order to collect data in the process of plant phenotyping, the plant imager robot takes RGB images of an object following a particular path with precise camera poses. Prerequisite Link To run an acquisition, you should previously have: built the robot following the guidelines here installed the necessary ROMI software here Step-by-step tutorial Link 1. Check that you are well interfaced with the plant imager Link make sure you are in the conda environment or that you run properly the docker for the plantimager repository interface the machine running the ROMI software with the plant imager: check that your device is correctly connected to the Gimbal and CNC both by USB turn on camera and connect it to the device via Wi-Fi set up a DB or quickly generate a simple database with the following commands: mkdir path/to/db touch path/to/db/romidb You have now your file based database plantdb 2. Get the right configuration Link Scan is the basic task for running an acquisition with the robot. To run this task properly with romi_run_task , a configuration file is needed. A default one for the plant imager can be found under plantimager/config/hardware.toml . It regroups specifications on: the acquisition path (ScanPath) needed parameters for connection between hardware components (CNC, Gimbal and camera) and software (Scan.scanner) object metadata (in Scan.metadata.object) hardware metadata (in Scan.metadata.hardware) An important parameter is the number of images acquisition you want to perform, defined by n_points . If you pick a number of acquisition in the following range of values, it will result in an integer rotation angle: 1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 15, 18, 20 , 24 , 30 , 36 , 40 , 45 , 60, 72, 90, 120, 180, 360. The truly recommended values are in bold . [ ScanPath ] # Example, circular path with 60 points: class_name = \"Circle\" [ScanPath.kwargs] center_x = 375 center_y = 375 z = 80 tilt = 0 radius = 300 n_points = 60 [ Scan . scanner . cnc ] # module and kwargs linked to the CNC module = \"plantimager.grbl\" [Scan.scanner.cnc.kwargs] homing = true port = \"/dev/ttyACM0\" [ Scan . scanner . gimbal ] # module and kwargs linked to the gimbal module = \"plantimager.blgimbal\" [Scan.scanner.gimbal.kwargs] port = \"/dev/ttyACM1\" has_tilt = false zero_pan = 0 invert_rotation = true [ Scan . scanner . camera ] # camera related parameters module = \"plantimager.sony\" [Scan.scanner.camera.kwargs] device_ip = \"192.168.122.1\" api_port = \"10000\" postview = true use_flashair = false rotation = 270 [ Scan . metadata . object ] # object related metadata species = \"chenopodium album\" seed_stock = \"Col-0\" plant_id = \"3dt_chenoA\" growth_environment = \"Lyon-indoor\" growth_conditions = \"SD+LD\" treatment = \"None\" DAG = 40 sample = \"main_stem\" experiment_id = \"3dt_26-01-2021\" dataset_id = \"3dt\" [ Scan . metadata . hardware ] # hardware related metadata frame = \"30profile v1\" X_motor = \"X-Carve NEMA23\" Y_motor = \"X-Carve NEMA23\" Z_motor = \"X-Carve NEMA23\" pan_motor = \"iPower Motor GM4108H-120T Brushless Gimbal Motor\" tilt_motor = \"None\" sensor = \"RX0\" [ Scan . metadata . workspace ] # A volume containing the target imaged object x = [ 200 , 600 ,] y = [ 200 , 600 ,] z = [ -100 , 300 ,] Warning This is a default configuration file. You will most probably need to create one to fit your hardware setup. Check the configuration documentation for the hardware and the imaged object 3. Run an acquisition with the Scan task Link Assuming you have an active database, you can now run the Scan task using romi_run_task : romi_run_task --config config/hardware.toml Scan /path/to/db/imageset_id/ where: /path/to/db must be an existing FSDB database there is no /path/to/db/imageset_id already existing in the database. This will create the corresponding folder and fill it with images from the imageset . Warning After a rather short time following running the command, you should hear the robot start and when the acquisition is finished, a This progress looks :) should appear. If it's not the case, try to look at the Troubleshooting section at the end of this tutorial 4. Obtain an image set Link Once the acquisition is done, the database is updated, and we now have the following tree structure: db/ \u251c\u2500\u2500 imageset_id/ \u2502 \u251c\u2500\u2500 files.json \u2502 \u251c\u2500\u2500 images/ \u2502 \u251c\u2500\u2500 metadata/ \u2502 \u2502 \u2514\u2500\u2500 images/ \u2502 \u2502 \u2514\u2500\u2500 images.json \u2502 \u2514\u2500\u2500 scan.toml \u2514\u2500\u2500 romidb with: images containing a list of RGB images acquired by the camera moving around the plant metadata/images a folder filled with json files recording the poses (camera coordinates) for each taken image metadata/images.json containing parameters of the acquisition that will be used later in reconstruction (type of format for the images, info on the object and the workspace) files.json detailing the files contained in the imageset_id scan.json , a copy of the acquisition config file You can now reconstruct your plant in 3d ! Troubleshooting Link Serial access denied Link The CNC and Gimbal might be connected to different ports than the ones specified in the configuration file. Please check with the dmesg -w command. Look here if you can not communicate with the scanner using usb. Make sure the device used to run the acquisition is indeed connected to the camera (wifi) Message to Gimbal still transiting : Traceback ( most recent call last ) : File \"/home/romi/miniconda3/envs/scan_0.8/lib/python3.8/site-packages/serial/serialposix.py\" , line 265 , in open self.fd = os.open ( self.portstr, os.O_RDWR | os.O_NOCTTY | os.O_NONBLOCK ) OSError: [ Errno 16 ] Device or resource busy: '/dev/ttyACM0' Try disconnect and reconnect the USB link and rerun an acquisition","title":"How to make an acquisition with the Plant Imager ?"},{"location":"plant_imager/tutorials/hardware_scan/#plant-imager-bot","text":"","title":"Plant Imager Bot"},{"location":"plant_imager/tutorials/hardware_scan/#objective","text":"This tutorial will guide through the steps of acquiring images of a plant using the plant imager robot In order to collect data in the process of plant phenotyping, the plant imager robot takes RGB images of an object following a particular path with precise camera poses.","title":"Objective"},{"location":"plant_imager/tutorials/hardware_scan/#prerequisite","text":"To run an acquisition, you should previously have: built the robot following the guidelines here installed the necessary ROMI software here","title":"Prerequisite"},{"location":"plant_imager/tutorials/hardware_scan/#step-by-step-tutorial","text":"","title":"Step-by-step tutorial"},{"location":"plant_imager/tutorials/hardware_scan/#1-check-that-you-are-well-interfaced-with-the-plant-imager","text":"make sure you are in the conda environment or that you run properly the docker for the plantimager repository interface the machine running the ROMI software with the plant imager: check that your device is correctly connected to the Gimbal and CNC both by USB turn on camera and connect it to the device via Wi-Fi set up a DB or quickly generate a simple database with the following commands: mkdir path/to/db touch path/to/db/romidb You have now your file based database plantdb","title":"1. Check that you are well interfaced with the plant imager"},{"location":"plant_imager/tutorials/hardware_scan/#2-get-the-right-configuration","text":"Scan is the basic task for running an acquisition with the robot. To run this task properly with romi_run_task , a configuration file is needed. A default one for the plant imager can be found under plantimager/config/hardware.toml . It regroups specifications on: the acquisition path (ScanPath) needed parameters for connection between hardware components (CNC, Gimbal and camera) and software (Scan.scanner) object metadata (in Scan.metadata.object) hardware metadata (in Scan.metadata.hardware) An important parameter is the number of images acquisition you want to perform, defined by n_points . If you pick a number of acquisition in the following range of values, it will result in an integer rotation angle: 1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 15, 18, 20 , 24 , 30 , 36 , 40 , 45 , 60, 72, 90, 120, 180, 360. The truly recommended values are in bold . [ ScanPath ] # Example, circular path with 60 points: class_name = \"Circle\" [ScanPath.kwargs] center_x = 375 center_y = 375 z = 80 tilt = 0 radius = 300 n_points = 60 [ Scan . scanner . cnc ] # module and kwargs linked to the CNC module = \"plantimager.grbl\" [Scan.scanner.cnc.kwargs] homing = true port = \"/dev/ttyACM0\" [ Scan . scanner . gimbal ] # module and kwargs linked to the gimbal module = \"plantimager.blgimbal\" [Scan.scanner.gimbal.kwargs] port = \"/dev/ttyACM1\" has_tilt = false zero_pan = 0 invert_rotation = true [ Scan . scanner . camera ] # camera related parameters module = \"plantimager.sony\" [Scan.scanner.camera.kwargs] device_ip = \"192.168.122.1\" api_port = \"10000\" postview = true use_flashair = false rotation = 270 [ Scan . metadata . object ] # object related metadata species = \"chenopodium album\" seed_stock = \"Col-0\" plant_id = \"3dt_chenoA\" growth_environment = \"Lyon-indoor\" growth_conditions = \"SD+LD\" treatment = \"None\" DAG = 40 sample = \"main_stem\" experiment_id = \"3dt_26-01-2021\" dataset_id = \"3dt\" [ Scan . metadata . hardware ] # hardware related metadata frame = \"30profile v1\" X_motor = \"X-Carve NEMA23\" Y_motor = \"X-Carve NEMA23\" Z_motor = \"X-Carve NEMA23\" pan_motor = \"iPower Motor GM4108H-120T Brushless Gimbal Motor\" tilt_motor = \"None\" sensor = \"RX0\" [ Scan . metadata . workspace ] # A volume containing the target imaged object x = [ 200 , 600 ,] y = [ 200 , 600 ,] z = [ -100 , 300 ,] Warning This is a default configuration file. You will most probably need to create one to fit your hardware setup. Check the configuration documentation for the hardware and the imaged object","title":"2. Get the right configuration"},{"location":"plant_imager/tutorials/hardware_scan/#3-run-an-acquisition-with-the-scan-task","text":"Assuming you have an active database, you can now run the Scan task using romi_run_task : romi_run_task --config config/hardware.toml Scan /path/to/db/imageset_id/ where: /path/to/db must be an existing FSDB database there is no /path/to/db/imageset_id already existing in the database. This will create the corresponding folder and fill it with images from the imageset . Warning After a rather short time following running the command, you should hear the robot start and when the acquisition is finished, a This progress looks :) should appear. If it's not the case, try to look at the Troubleshooting section at the end of this tutorial","title":"3. Run an acquisition with the Scan task"},{"location":"plant_imager/tutorials/hardware_scan/#4-obtain-an-image-set","text":"Once the acquisition is done, the database is updated, and we now have the following tree structure: db/ \u251c\u2500\u2500 imageset_id/ \u2502 \u251c\u2500\u2500 files.json \u2502 \u251c\u2500\u2500 images/ \u2502 \u251c\u2500\u2500 metadata/ \u2502 \u2502 \u2514\u2500\u2500 images/ \u2502 \u2502 \u2514\u2500\u2500 images.json \u2502 \u2514\u2500\u2500 scan.toml \u2514\u2500\u2500 romidb with: images containing a list of RGB images acquired by the camera moving around the plant metadata/images a folder filled with json files recording the poses (camera coordinates) for each taken image metadata/images.json containing parameters of the acquisition that will be used later in reconstruction (type of format for the images, info on the object and the workspace) files.json detailing the files contained in the imageset_id scan.json , a copy of the acquisition config file You can now reconstruct your plant in 3d !","title":"4. Obtain an image set"},{"location":"plant_imager/tutorials/hardware_scan/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"plant_imager/tutorials/hardware_scan/#serial-access-denied","text":"The CNC and Gimbal might be connected to different ports than the ones specified in the configuration file. Please check with the dmesg -w command. Look here if you can not communicate with the scanner using usb. Make sure the device used to run the acquisition is indeed connected to the camera (wifi) Message to Gimbal still transiting : Traceback ( most recent call last ) : File \"/home/romi/miniconda3/envs/scan_0.8/lib/python3.8/site-packages/serial/serialposix.py\" , line 265 , in open self.fd = os.open ( self.portstr, os.O_RDWR | os.O_NOCTTY | os.O_NONBLOCK ) OSError: [ Errno 16 ] Device or resource busy: '/dev/ttyACM0' Try disconnect and reconnect the USB link and rerun an acquisition","title":"Serial access denied"},{"location":"plant_imager/tutorials/intrinsic_calibration/","text":"Intrinsic calibration Link Some cameras introduce significant distortion to images. The two major kinds of distortion are radial distortion and tangential distortion . With radial distortion , straight lines appear curved while with tangential distortion some objects of the image may appears closer than they are in reality. Illustration of an image captured with radial distortion as shown by the straight red lines added on top the picture afterward. Source: OpenCV Python tutorial on camera calibration . Objective Link Correcting these \"aberrations\" prior to image processing can be a good idea to improve quality and accuracy of the reconstructed 3D scenes by structure from motion algorithms. In this tutorial, you will learn how to estimate the intrinsic camera parameters to calibrate image acquisition for downstream analysis and how to re-use it for a reconstruction pipeline, provided that the set-up is the same (same camera, optics, image size...). Prerequisite Link Install the plant-imager ROMI library required to perform image acquisitions together with the Plant Imager hardware . Install the plant-3d-vision ROMI library required to perform intrinsic calibration . Set up a ROMI plantdb local database or quickly create it (under /data/ROMI/DB ) with the following commands: export DB_LOCATION = /data/ROMI/DB mkdir $DB_LOCATION touch $DB_LOCATION /romidb We highly recommend the use of docker containers to run ROMI software, if you wish to use the docker images we provide, have a look here . Step-by-step tutorial Link 1. Make a ChArUco board and print it Link A ChArUco board is the combination of a chess board and of ArUco markers. An example of a 14x10 ChArUco board with 20mm chess square and 15mm 4x4 ArUco markers. The previous figure shows the default board that we will use in this tutorial. To create it, you have to run the create_charuco_board CLI as follows: create_charuco_board plant-3d-vision/config/intrisic_calibration.toml This will create a file named charuco_board.png in the current working directory. We strongly advise to use the same TOML configuration file with create_charuco_board & romi_run_task commands to avoid inadvertently changing parameter values. Also, you will later need it for the estimation of the intrinsic camera parameters. An example of intrisic_calibration.toml configuration file is: [CreateCharucoBoard] n_squares_x = \"14\" # Number of chessboard squares in X direction. n_squares_y = \"10\" # Number of chessboard squares in Y direction. square_length = \"2.\" # Length of square side, in cm marker_length = \"1.5\" # Length of marker side, in cm aruco_pattern = \"DICT_4X4_1000\" # 'DICT_4X4_50', 'DICT_4X4_100', 'DICT_4X4_250', 'DICT_4X4_1000' [DetectCharuco] upstream_task = \"ImagesFilesetExists\" board_fileset = \"CreateCharucoBoard\" min_n_corners = \"20\" # Minimum number of detected corners to export them [IntrinsicCalibration] upstream_task = \"DetectCharuco\" board_fileset = \"CreateCharucoBoard\" camera_model = \"OPENCV\" # defines the estimated parameters You may now print the ChArUco board image . Pay attention to use a software (like GIMP) that allows you to set the actual size of the image you want to print. With the previous configuration it should be: width : n_squares_x * square_length = 14 * 2. = 28cm height : n_squares_y * square_length = 10 * 2. = 20cm Finally, tape it flat onto something solid in order to avoid deformation of the printed pattern! 2. Scan the ChArUco board Link To scan your newly printed ChArUco board, use the IntrinsicCalibrationScan task from plant_imager : romi_run_task IntrinsicCalibrationScan $DB_LOCATION /intrisic_calib_1 --config plant-3d-vision/config/scan.toml The camera should move to the center front of the plant imager where you will hold your pattern and take 20 pictures (according to the previous configuration). Try to take pictures of the board in different positions. Notes It is not required to have the whole board in the picture, the ArUco markers will be used to detect the occluded sections! An example for the scan.toml configuration file is: [IntrinsicCalibrationScan] n_poses = 20 # Number of acquisition of the printed ChArUco board offset = 5 [CalibrationScan] n_points_line = 11 offset = 5 [ScanPath] class_name = \"Circle\" [ScanPath.kwargs] center_x = 375 center_y = 375 z = 90 tilt = 0 radius = 300 n_points = 36 [Scan.scanner.camera] module = \"plantimager.sony\" # RX-0 camera [Scan.scanner.gimbal] module = \"plantimager.blgimbal\" # plant imager hardware v2 [Scan.scanner.cnc] module = \"plantimager.grbl\" # plant imager hardware v2 [Scan.metadata.object] species = \"none\" seed_stock = \"none\" plant_id = \"test\" growth_environment = \"none\" growth_conditions = \"None\" treatment = \"none\" DAG = 0 sample = \"test_sample\" experiment_id = \"None\" dataset_id = \"test\" [Scan.metadata.hardware] frame = \"30profile v2\" X_motor = \"X-Carve NEMA23\" Y_motor = \"X-Carve NEMA23\" Z_motor = \"X-Carve NEMA23\" pan_motor = \"iPower Motor GM4108H-120T Brushless Gimbal Motor\" tilt_motor = \"None\" sensor = \"Sony RX-0\" [Scan.metadata.workspace] x = [ 100 , 500 ,] y = [ 100 , 500 ,] z = [ -300 , 100 ,] [Scan.scanner.camera.kwargs] device_ip = \"192.168.122.1\" api_port = \"10000\" postview = true use_flashair = false rotation = 270 [Scan.scanner.gimbal.kwargs] port = \"/dev/ttyACM1\" has_tilt = false zero_pan = 0 invert_rotation = true [Scan.scanner.cnc.kwargs] port = \"/dev/ttyACM0\" baud_rate = 115200 homing = true 3. Performs the camera parameters estimation Link You may now estimate the camera parameters , for a given camera model with: romi_run_task IntrinsicCalibration $DB_LOCATION /intrisic_calib_1 --config plant-3d-vision/config/intrisic_calibration.toml This should generate a camera_model.json inside the $DB_LOCATION/intrisic_calib_1/camera_model folder. An example of a camera_model.json file is: { \"model\" : \"OPENCV\" , \"RMS_error\" : 0.3484289537533634 , \"camera_matrix\" : [ [ 1201.7588127324675 , 0.0 , 702.5429671940506 ], [ 0.0 , 1199.117692017527 , 536.7266695161917 ], [ 0.0 , 0.0 , 1.0 ] ], \"distortion\" : [ 0.021462456820485233 , -0.04707700665017203 , -0.00014475851274869323 , -0.0011459776173976073 , 0.0 ], \"height\" : 1440 , \"width\" : 1080 } Important Do not hesitate to make several independent attempts at camera calibration, like 3 to 5, and choose the one with the lowest overall RMS error. Obviously, independent here means that you should perform multiple scans of the board and camera parameters estimation.","title":"How to calibrate the camera ?"},{"location":"plant_imager/tutorials/intrinsic_calibration/#intrinsic-calibration","text":"Some cameras introduce significant distortion to images. The two major kinds of distortion are radial distortion and tangential distortion . With radial distortion , straight lines appear curved while with tangential distortion some objects of the image may appears closer than they are in reality. Illustration of an image captured with radial distortion as shown by the straight red lines added on top the picture afterward. Source: OpenCV Python tutorial on camera calibration .","title":"Intrinsic calibration"},{"location":"plant_imager/tutorials/intrinsic_calibration/#objective","text":"Correcting these \"aberrations\" prior to image processing can be a good idea to improve quality and accuracy of the reconstructed 3D scenes by structure from motion algorithms. In this tutorial, you will learn how to estimate the intrinsic camera parameters to calibrate image acquisition for downstream analysis and how to re-use it for a reconstruction pipeline, provided that the set-up is the same (same camera, optics, image size...).","title":"Objective"},{"location":"plant_imager/tutorials/intrinsic_calibration/#prerequisite","text":"Install the plant-imager ROMI library required to perform image acquisitions together with the Plant Imager hardware . Install the plant-3d-vision ROMI library required to perform intrinsic calibration . Set up a ROMI plantdb local database or quickly create it (under /data/ROMI/DB ) with the following commands: export DB_LOCATION = /data/ROMI/DB mkdir $DB_LOCATION touch $DB_LOCATION /romidb We highly recommend the use of docker containers to run ROMI software, if you wish to use the docker images we provide, have a look here .","title":"Prerequisite"},{"location":"plant_imager/tutorials/intrinsic_calibration/#step-by-step-tutorial","text":"","title":"Step-by-step tutorial"},{"location":"plant_imager/tutorials/intrinsic_calibration/#1-make-a-charuco-board-and-print-it","text":"A ChArUco board is the combination of a chess board and of ArUco markers. An example of a 14x10 ChArUco board with 20mm chess square and 15mm 4x4 ArUco markers. The previous figure shows the default board that we will use in this tutorial. To create it, you have to run the create_charuco_board CLI as follows: create_charuco_board plant-3d-vision/config/intrisic_calibration.toml This will create a file named charuco_board.png in the current working directory. We strongly advise to use the same TOML configuration file with create_charuco_board & romi_run_task commands to avoid inadvertently changing parameter values. Also, you will later need it for the estimation of the intrinsic camera parameters. An example of intrisic_calibration.toml configuration file is: [CreateCharucoBoard] n_squares_x = \"14\" # Number of chessboard squares in X direction. n_squares_y = \"10\" # Number of chessboard squares in Y direction. square_length = \"2.\" # Length of square side, in cm marker_length = \"1.5\" # Length of marker side, in cm aruco_pattern = \"DICT_4X4_1000\" # 'DICT_4X4_50', 'DICT_4X4_100', 'DICT_4X4_250', 'DICT_4X4_1000' [DetectCharuco] upstream_task = \"ImagesFilesetExists\" board_fileset = \"CreateCharucoBoard\" min_n_corners = \"20\" # Minimum number of detected corners to export them [IntrinsicCalibration] upstream_task = \"DetectCharuco\" board_fileset = \"CreateCharucoBoard\" camera_model = \"OPENCV\" # defines the estimated parameters You may now print the ChArUco board image . Pay attention to use a software (like GIMP) that allows you to set the actual size of the image you want to print. With the previous configuration it should be: width : n_squares_x * square_length = 14 * 2. = 28cm height : n_squares_y * square_length = 10 * 2. = 20cm Finally, tape it flat onto something solid in order to avoid deformation of the printed pattern!","title":"1. Make a ChArUco board and print it"},{"location":"plant_imager/tutorials/intrinsic_calibration/#2-scan-the-charuco-board","text":"To scan your newly printed ChArUco board, use the IntrinsicCalibrationScan task from plant_imager : romi_run_task IntrinsicCalibrationScan $DB_LOCATION /intrisic_calib_1 --config plant-3d-vision/config/scan.toml The camera should move to the center front of the plant imager where you will hold your pattern and take 20 pictures (according to the previous configuration). Try to take pictures of the board in different positions. Notes It is not required to have the whole board in the picture, the ArUco markers will be used to detect the occluded sections! An example for the scan.toml configuration file is: [IntrinsicCalibrationScan] n_poses = 20 # Number of acquisition of the printed ChArUco board offset = 5 [CalibrationScan] n_points_line = 11 offset = 5 [ScanPath] class_name = \"Circle\" [ScanPath.kwargs] center_x = 375 center_y = 375 z = 90 tilt = 0 radius = 300 n_points = 36 [Scan.scanner.camera] module = \"plantimager.sony\" # RX-0 camera [Scan.scanner.gimbal] module = \"plantimager.blgimbal\" # plant imager hardware v2 [Scan.scanner.cnc] module = \"plantimager.grbl\" # plant imager hardware v2 [Scan.metadata.object] species = \"none\" seed_stock = \"none\" plant_id = \"test\" growth_environment = \"none\" growth_conditions = \"None\" treatment = \"none\" DAG = 0 sample = \"test_sample\" experiment_id = \"None\" dataset_id = \"test\" [Scan.metadata.hardware] frame = \"30profile v2\" X_motor = \"X-Carve NEMA23\" Y_motor = \"X-Carve NEMA23\" Z_motor = \"X-Carve NEMA23\" pan_motor = \"iPower Motor GM4108H-120T Brushless Gimbal Motor\" tilt_motor = \"None\" sensor = \"Sony RX-0\" [Scan.metadata.workspace] x = [ 100 , 500 ,] y = [ 100 , 500 ,] z = [ -300 , 100 ,] [Scan.scanner.camera.kwargs] device_ip = \"192.168.122.1\" api_port = \"10000\" postview = true use_flashair = false rotation = 270 [Scan.scanner.gimbal.kwargs] port = \"/dev/ttyACM1\" has_tilt = false zero_pan = 0 invert_rotation = true [Scan.scanner.cnc.kwargs] port = \"/dev/ttyACM0\" baud_rate = 115200 homing = true","title":"2. Scan the ChArUco board"},{"location":"plant_imager/tutorials/intrinsic_calibration/#3-performs-the-camera-parameters-estimation","text":"You may now estimate the camera parameters , for a given camera model with: romi_run_task IntrinsicCalibration $DB_LOCATION /intrisic_calib_1 --config plant-3d-vision/config/intrisic_calibration.toml This should generate a camera_model.json inside the $DB_LOCATION/intrisic_calib_1/camera_model folder. An example of a camera_model.json file is: { \"model\" : \"OPENCV\" , \"RMS_error\" : 0.3484289537533634 , \"camera_matrix\" : [ [ 1201.7588127324675 , 0.0 , 702.5429671940506 ], [ 0.0 , 1199.117692017527 , 536.7266695161917 ], [ 0.0 , 0.0 , 1.0 ] ], \"distortion\" : [ 0.021462456820485233 , -0.04707700665017203 , -0.00014475851274869323 , -0.0011459776173976073 , 0.0 ], \"height\" : 1440 , \"width\" : 1080 } Important Do not hesitate to make several independent attempts at camera calibration, like 3 to 5, and choose the one with the lowest overall RMS error. Obviously, independent here means that you should perform multiple scans of the board and camera parameters estimation.","title":"3. Performs the camera parameters estimation"},{"location":"plant_imager/tutorials/plant-3d-explorer/","text":"How to see directly the results of your plant phenotyping with the plant-3d-explorer ? Link Objective Link Throughout the whole process of plant phenotyping, viewing data is often needed. This tutorial explains how to use the romi plant-3d-explorer , a web-server tool, to explore, display and interact with most of the diverse data generated during a typical plant phenotyping experiment from 2D images (2D images, 3D objects like meshes or point cloud, quality evaluations, trait measurements). After this tutorial, you should be able to: connect the plant-3d-explorer to a database containing the phenotyping data of one to several plants ; explore the database content with the plant-3d-explorer menu page ; For each plant, display, overlay and inspect in 3d every data generated during analysis Prerequisite Link install romi plant-3d-explorer (from source or using a docker image ) & read install procedure install romi plantdb (from source or using a docker image ) & read install procedure install romi plant-3d-vision (from source or using a docker image ) & read install procedure Create and activate isolated python environment (see the procedure here ) Note for docker users You can avoid installs by using docker only. Read first the docker procedures ( 'docker for plant-3d-vision' and 'docker-compose to run both database and 3d explorer with docker containers' ). In the following tutorial (steps 1, 2 and 3), follow the docker logo to adapt the procedure. Linked documentation Link Manual of the romi plant-3d-explorer Step-by-step tutorial Link Principle: the plant-3d-explorer is a web client that displays in your favorite web browser data exposed by a server (here, romi plantdb ) on a particular url. The process consists in pointing the server to your folder of interest, starting the server and starting the client that points to the served url. Warning the plant-3d-explorer has only been developed and tested on Chrome. 1. Preparing your database for display by the plant-3d-explorer Link Starting point: your database is made of one or several datasets , which all correspond to a single plant phenotyping experiment: each dataset contains at least 2D images (raw acquisitions) and metadata, and possibly several other data generated by subsequent 3D reconstruction, segmentation and analysis. Note Your database must follow the rules of romi databases: please make sure that you comply to requirements . You can also download an example database here . example : let's consider a database called my_experiment containing 3 datasets (named plant1, plant2, plant3) generated by phenotyping three plants. open a terminal and go to your local database directory if romi commands (like romi_run_task ) are not accessible from your terminal, activate the appropriate python environment (e.g. using venv or conda) required for romi commands (or read this procedure ) process all datasets for display by the plant-3d-explorer by running the following code dataset_list =( 'plant1' , 'plant2' , 'plant3' ) for ds in \" ${ dataset_list [@] } \" do romi_run_task Visualization path_to/my_experiment/ \" $ds \" / --config ~/config/ml_pipe_real.toml done Note For more information about using romi_run_task command, the Visualization task and the config file, please read XXXXX. Note for docker users Start a docker container by mounting your database as a volume ( details ) In the container, run the same Visualization Task has above check result : a new folder called Visualization should have been created in each dataset of your database Note for docker users Skip step 2 & 3 and follow instead instructions given by 'docker-compose to run both database and 3d explorer with docker containers' 2. Connect your database to a local server Link Continue in the same shell terminal (if you open a new terminal, do not forget to activate appropriate python environment) set the DB location using the DB_LOCATION environment variable Type the following commands to launch the server: export DB_LOCATION = /path/to/your/db romi_scanner_rest_api #command that starts the server check result : the terminal prints various information given by the server (e.g. number of datasets in the database). Do not stop this terminal as this will shut down the server. 3. Connect the plant-3d-explorer to the server Link Open a new terminal go to your local cloned directory of plant-3d-explorer/ start the frontend visualization server by entering: npm start You should now be able to access the plant-3d-explorer on http://localhost:3000 . Depending on you system preferences, your default web browser may automatically open a window displaying the server content. If not, open your web browser and enter http://localhost:3000 in the url bar. Note You need to add a file .env.local at project's root to set the API URL: REACT_APP_API_URL='{`API URL}' . Without this, the app will use: http://localhost:5000 , which is the default for romi_scanner_rest_api . 4. Explore your database content via the menu page Link Note More description about the function of the menu page: read here The starting page of the plant-3d-explorer lists the datasets of the connected database as a table and looks like this: The top search bar allows you to find particular datasets based on keywords. Data filters: in the header row, click on an icon to activate the filter (datasets that do not contain the data will be filtered out) 3d objects generated from the plant 2D images (icons respectively stand for: mesh, point cloud, segmented point cloud, skeleton and organs) or phyllotaxis data (manual or computed phyllotaxis measurements) Open a dataset with the green 'Open' button at the far right of a row 5. View a single dataset and all related data Link Note This is only a brief description to allow a quick start. More description here By default, the plant-3d-explorer displays in the main panel the skeleton and the organs (if available) and phyllotaxis data (as graphs) in the right panel. Mouse-over most elements provides a brief description. Select the 3D layers to display Link In the top left corner of the main panel, icons allows you to quickly (un)select 3D layers (if available): White icons are active, dark grey are available but not active, light grey are not available for this dataset From left to right, icons represents respectively the mesh, the point-cloud, the segmented point cloud, the skeleton and the organs. In the center of the middle panel are icons for general viewing options: Activate the camera icon displays the camera poses (only works if overlay with 2 images is deactivated) Click the round arrow to reset the view Moving the view in the \"free\" 3D (without 2D overlay) Link Easy movements are accessible with a mouse: scroll to zoom in/out left click rotate right click translate Activate overlay with 2D images Link click on any image of the bottom carousel to activate the display of 2 images in the main panel. On Mouse-over, a single picture is enlarged and proposes to open it in the main panel. Overlay with active 3D layers is automatic. In the carousel, the box around the active displayed 2D image is now permanent. To close the 2D overlay, just click the close button of the boxed picture in the carousel. Moving the view with 2D overlay Link Note that movement control with the mouse slightly changes compared to the \"free\" view without 2D overlay. Notably, the free rotation mode is not possible anymore, since it is constrained by the real movements made by the camera when it took the pictures. Slide right/left the active box picture in the carousel to reproduce the camera movement scroll to zoom in/out left-click to translate The phyllotaxis measure plots Link Plots represent the successive measures of divergence angles (left, in degrees) and internode length (right, in mm) between consecutive pairs of organs (here fruits) along the stem, from the base to the inflorescence tip. Both plots can be closed by clicking the cross at the far right of the plot's title. Closed plot panels can be re-opened by clicking a green \"+\" sign appearing at the right-hand corner when at least one plot is closed. In the plots, a blue curve correspond to \"automated\" measure computed through an analysis pipeline (such as pipelines developed in romi plant-3d-vision). If available in the dataset, a red curve indicates a ground-truth \"manual\" measure. Mouse-over any of the two plots highlights an interval that correspond to a measure between two consecutive organs (fruits) segmented by the analysis. The interval appears synchronously on both plots if opened. This interval and the organs are numbered by their order from the base of the stem, these numbers appear on the (vertical) X-axis of the plots. The exact value of the selected interval is displayed on top of the plot (\"automated\" and \"manual\" values in blue and red respectively, if available). As shown above, when the 'organ' 3d-layer is active in the main panel, mouse-over the plot synchronously select the corresponding pair of organs in the current view (all other organ layers just disappear). Clicking the interval in the plot maintains the selection of the organ pair active despite further mouse movements. To deactivate the selection, click the green cross at the end of the shaded selection rectangle on either of the plots. Organ colors are also synchronized between main panel and plot panels. In the free 3D mode only, clicking a fruit layer display a bubble telling the organ number, colored as the corresponding fruit layer. Bubbles stay on screen if the 2D overlay is activated (as in the picture above). Go back to main page Link In the top left corner of the page, click \"all scans\":","title":"How to view your results with plant-3d-explorer"},{"location":"plant_imager/tutorials/plant-3d-explorer/#how-to-see-directly-the-results-of-your-plant-phenotyping-with-the-plant-3d-explorer","text":"","title":"How to see directly the results of your plant phenotyping with the plant-3d-explorer ?"},{"location":"plant_imager/tutorials/plant-3d-explorer/#objective","text":"Throughout the whole process of plant phenotyping, viewing data is often needed. This tutorial explains how to use the romi plant-3d-explorer , a web-server tool, to explore, display and interact with most of the diverse data generated during a typical plant phenotyping experiment from 2D images (2D images, 3D objects like meshes or point cloud, quality evaluations, trait measurements). After this tutorial, you should be able to: connect the plant-3d-explorer to a database containing the phenotyping data of one to several plants ; explore the database content with the plant-3d-explorer menu page ; For each plant, display, overlay and inspect in 3d every data generated during analysis","title":"Objective"},{"location":"plant_imager/tutorials/plant-3d-explorer/#prerequisite","text":"install romi plant-3d-explorer (from source or using a docker image ) & read install procedure install romi plantdb (from source or using a docker image ) & read install procedure install romi plant-3d-vision (from source or using a docker image ) & read install procedure Create and activate isolated python environment (see the procedure here ) Note for docker users You can avoid installs by using docker only. Read first the docker procedures ( 'docker for plant-3d-vision' and 'docker-compose to run both database and 3d explorer with docker containers' ). In the following tutorial (steps 1, 2 and 3), follow the docker logo to adapt the procedure.","title":"Prerequisite"},{"location":"plant_imager/tutorials/plant-3d-explorer/#linked-documentation","text":"Manual of the romi plant-3d-explorer","title":"Linked documentation"},{"location":"plant_imager/tutorials/plant-3d-explorer/#step-by-step-tutorial","text":"Principle: the plant-3d-explorer is a web client that displays in your favorite web browser data exposed by a server (here, romi plantdb ) on a particular url. The process consists in pointing the server to your folder of interest, starting the server and starting the client that points to the served url. Warning the plant-3d-explorer has only been developed and tested on Chrome.","title":"Step-by-step tutorial"},{"location":"plant_imager/tutorials/plant-3d-explorer/#1-preparing-your-database-for-display-by-the-plant-3d-explorer","text":"Starting point: your database is made of one or several datasets , which all correspond to a single plant phenotyping experiment: each dataset contains at least 2D images (raw acquisitions) and metadata, and possibly several other data generated by subsequent 3D reconstruction, segmentation and analysis. Note Your database must follow the rules of romi databases: please make sure that you comply to requirements . You can also download an example database here . example : let's consider a database called my_experiment containing 3 datasets (named plant1, plant2, plant3) generated by phenotyping three plants. open a terminal and go to your local database directory if romi commands (like romi_run_task ) are not accessible from your terminal, activate the appropriate python environment (e.g. using venv or conda) required for romi commands (or read this procedure ) process all datasets for display by the plant-3d-explorer by running the following code dataset_list =( 'plant1' , 'plant2' , 'plant3' ) for ds in \" ${ dataset_list [@] } \" do romi_run_task Visualization path_to/my_experiment/ \" $ds \" / --config ~/config/ml_pipe_real.toml done Note For more information about using romi_run_task command, the Visualization task and the config file, please read XXXXX. Note for docker users Start a docker container by mounting your database as a volume ( details ) In the container, run the same Visualization Task has above check result : a new folder called Visualization should have been created in each dataset of your database Note for docker users Skip step 2 & 3 and follow instead instructions given by 'docker-compose to run both database and 3d explorer with docker containers'","title":"1. Preparing your database for display by the plant-3d-explorer"},{"location":"plant_imager/tutorials/plant-3d-explorer/#2-connect-your-database-to-a-local-server","text":"Continue in the same shell terminal (if you open a new terminal, do not forget to activate appropriate python environment) set the DB location using the DB_LOCATION environment variable Type the following commands to launch the server: export DB_LOCATION = /path/to/your/db romi_scanner_rest_api #command that starts the server check result : the terminal prints various information given by the server (e.g. number of datasets in the database). Do not stop this terminal as this will shut down the server.","title":"2. Connect your database to a local server"},{"location":"plant_imager/tutorials/plant-3d-explorer/#3-connect-the-plant-3d-explorer-to-the-server","text":"Open a new terminal go to your local cloned directory of plant-3d-explorer/ start the frontend visualization server by entering: npm start You should now be able to access the plant-3d-explorer on http://localhost:3000 . Depending on you system preferences, your default web browser may automatically open a window displaying the server content. If not, open your web browser and enter http://localhost:3000 in the url bar. Note You need to add a file .env.local at project's root to set the API URL: REACT_APP_API_URL='{`API URL}' . Without this, the app will use: http://localhost:5000 , which is the default for romi_scanner_rest_api .","title":"3. Connect the plant-3d-explorer to the server"},{"location":"plant_imager/tutorials/plant-3d-explorer/#4-explore-your-database-content-via-the-menu-page","text":"Note More description about the function of the menu page: read here The starting page of the plant-3d-explorer lists the datasets of the connected database as a table and looks like this: The top search bar allows you to find particular datasets based on keywords. Data filters: in the header row, click on an icon to activate the filter (datasets that do not contain the data will be filtered out) 3d objects generated from the plant 2D images (icons respectively stand for: mesh, point cloud, segmented point cloud, skeleton and organs) or phyllotaxis data (manual or computed phyllotaxis measurements) Open a dataset with the green 'Open' button at the far right of a row","title":"4. Explore your database content via the menu page"},{"location":"plant_imager/tutorials/plant-3d-explorer/#5-view-a-single-dataset-and-all-related-data","text":"Note This is only a brief description to allow a quick start. More description here By default, the plant-3d-explorer displays in the main panel the skeleton and the organs (if available) and phyllotaxis data (as graphs) in the right panel. Mouse-over most elements provides a brief description.","title":"5. View a single dataset and all related data"},{"location":"plant_imager/tutorials/plant-3d-explorer/#select-the-3d-layers-to-display","text":"In the top left corner of the main panel, icons allows you to quickly (un)select 3D layers (if available): White icons are active, dark grey are available but not active, light grey are not available for this dataset From left to right, icons represents respectively the mesh, the point-cloud, the segmented point cloud, the skeleton and the organs. In the center of the middle panel are icons for general viewing options: Activate the camera icon displays the camera poses (only works if overlay with 2 images is deactivated) Click the round arrow to reset the view","title":"Select the 3D layers to display"},{"location":"plant_imager/tutorials/plant-3d-explorer/#moving-the-view-in-the-free-3d-without-2d-overlay","text":"Easy movements are accessible with a mouse: scroll to zoom in/out left click rotate right click translate","title":"Moving the view in the \"free\" 3D (without 2D overlay)"},{"location":"plant_imager/tutorials/plant-3d-explorer/#activate-overlay-with-2d-images","text":"click on any image of the bottom carousel to activate the display of 2 images in the main panel. On Mouse-over, a single picture is enlarged and proposes to open it in the main panel. Overlay with active 3D layers is automatic. In the carousel, the box around the active displayed 2D image is now permanent. To close the 2D overlay, just click the close button of the boxed picture in the carousel.","title":"Activate overlay with 2D images"},{"location":"plant_imager/tutorials/plant-3d-explorer/#moving-the-view-with-2d-overlay","text":"Note that movement control with the mouse slightly changes compared to the \"free\" view without 2D overlay. Notably, the free rotation mode is not possible anymore, since it is constrained by the real movements made by the camera when it took the pictures. Slide right/left the active box picture in the carousel to reproduce the camera movement scroll to zoom in/out left-click to translate","title":"Moving the view with 2D overlay"},{"location":"plant_imager/tutorials/plant-3d-explorer/#the-phyllotaxis-measure-plots","text":"Plots represent the successive measures of divergence angles (left, in degrees) and internode length (right, in mm) between consecutive pairs of organs (here fruits) along the stem, from the base to the inflorescence tip. Both plots can be closed by clicking the cross at the far right of the plot's title. Closed plot panels can be re-opened by clicking a green \"+\" sign appearing at the right-hand corner when at least one plot is closed. In the plots, a blue curve correspond to \"automated\" measure computed through an analysis pipeline (such as pipelines developed in romi plant-3d-vision). If available in the dataset, a red curve indicates a ground-truth \"manual\" measure. Mouse-over any of the two plots highlights an interval that correspond to a measure between two consecutive organs (fruits) segmented by the analysis. The interval appears synchronously on both plots if opened. This interval and the organs are numbered by their order from the base of the stem, these numbers appear on the (vertical) X-axis of the plots. The exact value of the selected interval is displayed on top of the plot (\"automated\" and \"manual\" values in blue and red respectively, if available). As shown above, when the 'organ' 3d-layer is active in the main panel, mouse-over the plot synchronously select the corresponding pair of organs in the current view (all other organ layers just disappear). Clicking the interval in the plot maintains the selection of the organ pair active despite further mouse movements. To deactivate the selection, click the green cross at the end of the shaded selection rectangle on either of the plots. Organ colors are also synchronized between main panel and plot panels. In the free 3D mode only, clicking a fruit layer display a bubble telling the organ number, colored as the corresponding fruit layer. Bubbles stay on screen if the 2D overlay is activated (as in the picture above).","title":"The phyllotaxis measure plots"},{"location":"plant_imager/tutorials/plant-3d-explorer/#go-back-to-main-page","text":"In the top left corner of the page, click \"all scans\":","title":"Go back to main page"},{"location":"plant_imager/tutorials/reconstruct_scan/","text":"Plant reconstruction and analysis pipeline Link Getting started Link To follows this guide you should have: installed the necessary ROMI software here or followed the instructions for the docker image here access to a database with a \"plant acquisition\" to reconstruct (or use the provided examples) Reconstruction pipeline Link Cleaning a dataset Link If you made a mess, had a failure or just want to start fresh with your dataset, no need to save a copy on the side, you can use the Clean task: romi_run_task Clean integration_tests/2019-02-01_10-56-33 \\ --config plant3dvision/config/original_pipe_0.toml --local-scheduler Here the config may use the [Clean] section where you can defines the force option: [Clean] force = true If true the Clean task will run silently, else in interactive mode. Geometric pipeline Link Real scan dataset Link The full geometric pipeline , ie. all the way to angles and internodes measurement, can be called on real dataset with: romi_run_task AnglesAndInternodes integration_tests/2019-02-01_10-56-33 \\ --config plant3dvision/config/original_pipe_0.toml --local-scheduler Note This example uses a real scan dataset from the test database. Virtual plant dataset Link The full geometric pipeline , ie. all the way to angles and internodes measurement, can be called on a virtual dataset with: romi_run_task AnglesAndInternodes integration_tests/arabidopsis_26 \\ --config plant3dvision/config/original_pipe_0.toml --local-scheduler Note This example uses a virtual scan dataset from the test database. Warning If you get something like this during the Voxel tasks: Choose platform: [0] <pyopencl.Platform 'NVIDIA CUDA' at 0x55d904d5af50> Choice [0]: that mean you need to specify the environment variable PYOPENCL_CTX='0' Machine Learning pipeline Link Warning This requires the installation of the romiseg libraries (see here for install instructions ) and a trained PyTorch model! Note A trained model, to place under <dataset>/models/models , is accessible here: https://media.romi-project.eu/data/Resnetdataset_gl_png_896_896_epoch50.pt Real scan dataset Link The full geometric pipeline , ie. all the way to angles and internodes measurement, can be called on real dataset with: romi_run_task PointCloud integration_tests/2019-02-01_10-56-33 --config plant3dvision/config/ml_pipe_vplants_3.toml --local-scheduler Note This example uses a real scan dataset from the test database. Virtual plant dataset Link The full geometric pipeline , ie. all the way to angles and internodes measurement, can be called on a virtual dataset with: romi_run_task PointCloud integration_tests/arabidopsis_26 \\ --config plant3dvision/config/ml_pipe_vplants_3.toml --local-scheduler Note This example uses a virtual scan dataset from the test database.","title":"How to reconstruct 3D data from images ?"},{"location":"plant_imager/tutorials/reconstruct_scan/#plant-reconstruction-and-analysis-pipeline","text":"","title":"Plant reconstruction and analysis pipeline"},{"location":"plant_imager/tutorials/reconstruct_scan/#getting-started","text":"To follows this guide you should have: installed the necessary ROMI software here or followed the instructions for the docker image here access to a database with a \"plant acquisition\" to reconstruct (or use the provided examples)","title":"Getting started"},{"location":"plant_imager/tutorials/reconstruct_scan/#reconstruction-pipeline","text":"","title":"Reconstruction pipeline"},{"location":"plant_imager/tutorials/reconstruct_scan/#cleaning-a-dataset","text":"If you made a mess, had a failure or just want to start fresh with your dataset, no need to save a copy on the side, you can use the Clean task: romi_run_task Clean integration_tests/2019-02-01_10-56-33 \\ --config plant3dvision/config/original_pipe_0.toml --local-scheduler Here the config may use the [Clean] section where you can defines the force option: [Clean] force = true If true the Clean task will run silently, else in interactive mode.","title":"Cleaning a dataset"},{"location":"plant_imager/tutorials/reconstruct_scan/#geometric-pipeline","text":"","title":"Geometric pipeline"},{"location":"plant_imager/tutorials/reconstruct_scan/#real-scan-dataset","text":"The full geometric pipeline , ie. all the way to angles and internodes measurement, can be called on real dataset with: romi_run_task AnglesAndInternodes integration_tests/2019-02-01_10-56-33 \\ --config plant3dvision/config/original_pipe_0.toml --local-scheduler Note This example uses a real scan dataset from the test database.","title":"Real scan dataset"},{"location":"plant_imager/tutorials/reconstruct_scan/#virtual-plant-dataset","text":"The full geometric pipeline , ie. all the way to angles and internodes measurement, can be called on a virtual dataset with: romi_run_task AnglesAndInternodes integration_tests/arabidopsis_26 \\ --config plant3dvision/config/original_pipe_0.toml --local-scheduler Note This example uses a virtual scan dataset from the test database. Warning If you get something like this during the Voxel tasks: Choose platform: [0] <pyopencl.Platform 'NVIDIA CUDA' at 0x55d904d5af50> Choice [0]: that mean you need to specify the environment variable PYOPENCL_CTX='0'","title":"Virtual plant dataset"},{"location":"plant_imager/tutorials/reconstruct_scan/#machine-learning-pipeline","text":"Warning This requires the installation of the romiseg libraries (see here for install instructions ) and a trained PyTorch model! Note A trained model, to place under <dataset>/models/models , is accessible here: https://media.romi-project.eu/data/Resnetdataset_gl_png_896_896_epoch50.pt","title":"Machine Learning pipeline"},{"location":"plant_imager/tutorials/reconstruct_scan/#real-scan-dataset_1","text":"The full geometric pipeline , ie. all the way to angles and internodes measurement, can be called on real dataset with: romi_run_task PointCloud integration_tests/2019-02-01_10-56-33 --config plant3dvision/config/ml_pipe_vplants_3.toml --local-scheduler Note This example uses a real scan dataset from the test database.","title":"Real scan dataset"},{"location":"plant_imager/tutorials/reconstruct_scan/#virtual-plant-dataset_1","text":"The full geometric pipeline , ie. all the way to angles and internodes measurement, can be called on a virtual dataset with: romi_run_task PointCloud integration_tests/arabidopsis_26 \\ --config plant3dvision/config/ml_pipe_vplants_3.toml --local-scheduler Note This example uses a virtual scan dataset from the test database.","title":"Virtual plant dataset"},{"location":"plant_imager/tutorials/virtual_plant_generate_analyze_visualize/","text":"How to generate, analyze and visualize virtual plant images using public docker containers Link Objective Link Instead of installing virtual-plant-imager , plant-3d-vision and plant-3d-explorer on your computer you can pull their corresponding docker images and use directly the pre installed softwares. After reading this tutorial, you should be able to generate, analyze and visualize virtual plant images using only public docker images. Prerequisite Link You must have docker and docker-compose installed on your system and ideally your user account has sudo privileges. 1. Prepare the database Link First, you have to give the right access to your database path sudo chmod -R 777 /path/of/your/db 2. Generate virtual dataset with plant-imager Link Pull the public docker image of virtual-plant-imager docker pull roboticsmicrofarms/virtual-plant-imager:latest Run the docker image with the database mounted volume docker run --rm -it --gpus all -v /path/to/your_db:/myapp/db roboticsmicrofarms/virtual-plant-imager:latest /bin/bash Inside docker container, run VirtualScan task to generate a dataset named virtual_ds_example romi_run_task --config plant-imager/config/vscan_lpy_blender.toml VirtualScan /myapp/db/virtual_ds_example Wait for the generation time, after complete, make sure that virtual_ds_example has been generated correctly. Exit docker container exit 3. Angles and Internodes analysis and visualization generation of the dataset Link Pull the public docker image of plant-3d-vision docker pull roboticsmicrofarms/plant-3d-vision:latest Run the docker image with the database mounted volume docker run --rm -it --gpus all -v /path/to/your_db:/myapp/db roboticsmicrofarms/plant-3d-vision:latest /bin/bash If not already activated, activate the right virtual environment source /venv/bin/activate Inside docker container, run AnglesAndInternodes task romi_run_task --config config/geom_pipe_virtual.toml AnglesAndInternodes /myapp/db/virtual_ds_example/ Make sure that the folder AnglesAndInternodes has been generated Run the Visualization task romi_run_task --config config/geom_pipe_virtual.toml Visualization /myapp/db/virtual_ds_example/ Make sure the Visualization folder has been generated Exit the docker container exit 4. Visualize the virtual dataset on plant-3d-explorer Link Pull the public docker image of plantdb docker pull roboticsmicrofarms/plantdb Pull the public docker image of plant-3d-explorer docker pull roboticsmicrofarms/plant-3d-explorer Set ROMI_DB environment variable to point to your database export ROMI_DB = /path/to/your/db Create a file named docker-compose.yml so that it will contain the following: version : '3' services : plantdb : image : \"roboticsmicrofarms/plantdb\" volumes : - $ { ROMI_DB }: / myapp / db ports : - \"5000:5000\" healthcheck : test : \"exit 0\" plant-3d-explorer : image : \"roboticsmicrofarms/plant-3d-explorer\" depends_on : - plantdb environment : REACT_APP_API_URL : http : // localhost : 5000 ports : - \"3000:3000\" Run the docker compose in the directory that contains docker-compose.yml docker-compose up -d After a while, if everything is okay, you can visualize the database (containing virtual_ds_example ) in your internet browser at the addres http://localhost:3000/ Make sure that virtual_ds_example can be visualized correctly. Stop docker compose docker-compose stop","title":"How to generate, analyze and visualize virtual plant images using public docker containers"},{"location":"plant_imager/tutorials/virtual_plant_generate_analyze_visualize/#how-to-generate-analyze-and-visualize-virtual-plant-images-using-public-docker-containers","text":"","title":"How to generate, analyze and visualize virtual plant images using public docker containers"},{"location":"plant_imager/tutorials/virtual_plant_generate_analyze_visualize/#objective","text":"Instead of installing virtual-plant-imager , plant-3d-vision and plant-3d-explorer on your computer you can pull their corresponding docker images and use directly the pre installed softwares. After reading this tutorial, you should be able to generate, analyze and visualize virtual plant images using only public docker images.","title":"Objective"},{"location":"plant_imager/tutorials/virtual_plant_generate_analyze_visualize/#prerequisite","text":"You must have docker and docker-compose installed on your system and ideally your user account has sudo privileges.","title":"Prerequisite"},{"location":"plant_imager/tutorials/virtual_plant_generate_analyze_visualize/#1-prepare-the-database","text":"First, you have to give the right access to your database path sudo chmod -R 777 /path/of/your/db","title":"1. Prepare the database"},{"location":"plant_imager/tutorials/virtual_plant_generate_analyze_visualize/#2-generate-virtual-dataset-with-plant-imager","text":"Pull the public docker image of virtual-plant-imager docker pull roboticsmicrofarms/virtual-plant-imager:latest Run the docker image with the database mounted volume docker run --rm -it --gpus all -v /path/to/your_db:/myapp/db roboticsmicrofarms/virtual-plant-imager:latest /bin/bash Inside docker container, run VirtualScan task to generate a dataset named virtual_ds_example romi_run_task --config plant-imager/config/vscan_lpy_blender.toml VirtualScan /myapp/db/virtual_ds_example Wait for the generation time, after complete, make sure that virtual_ds_example has been generated correctly. Exit docker container exit","title":"2. Generate virtual dataset with plant-imager"},{"location":"plant_imager/tutorials/virtual_plant_generate_analyze_visualize/#3-angles-and-internodes-analysis-and-visualization-generation-of-the-dataset","text":"Pull the public docker image of plant-3d-vision docker pull roboticsmicrofarms/plant-3d-vision:latest Run the docker image with the database mounted volume docker run --rm -it --gpus all -v /path/to/your_db:/myapp/db roboticsmicrofarms/plant-3d-vision:latest /bin/bash If not already activated, activate the right virtual environment source /venv/bin/activate Inside docker container, run AnglesAndInternodes task romi_run_task --config config/geom_pipe_virtual.toml AnglesAndInternodes /myapp/db/virtual_ds_example/ Make sure that the folder AnglesAndInternodes has been generated Run the Visualization task romi_run_task --config config/geom_pipe_virtual.toml Visualization /myapp/db/virtual_ds_example/ Make sure the Visualization folder has been generated Exit the docker container exit","title":"3. Angles and Internodes analysis and visualization generation of the dataset"},{"location":"plant_imager/tutorials/virtual_plant_generate_analyze_visualize/#4-visualize-the-virtual-dataset-on-plant-3d-explorer","text":"Pull the public docker image of plantdb docker pull roboticsmicrofarms/plantdb Pull the public docker image of plant-3d-explorer docker pull roboticsmicrofarms/plant-3d-explorer Set ROMI_DB environment variable to point to your database export ROMI_DB = /path/to/your/db Create a file named docker-compose.yml so that it will contain the following: version : '3' services : plantdb : image : \"roboticsmicrofarms/plantdb\" volumes : - $ { ROMI_DB }: / myapp / db ports : - \"5000:5000\" healthcheck : test : \"exit 0\" plant-3d-explorer : image : \"roboticsmicrofarms/plant-3d-explorer\" depends_on : - plantdb environment : REACT_APP_API_URL : http : // localhost : 5000 ports : - \"3000:3000\" Run the docker compose in the directory that contains docker-compose.yml docker-compose up -d After a while, if everything is okay, you can visualize the database (containing virtual_ds_example ) in your internet browser at the addres http://localhost:3000/ Make sure that virtual_ds_example can be visualized correctly. Stop docker compose docker-compose stop","title":"4. Visualize the virtual dataset on plant-3d-explorer"},{"location":"plant_imager/tutorials/virtual_plant_imager_large_dataset/","text":"How to use the Virtual Plant Imager to generate a large dataset of virtual plant for machine learning purposes Link Objective Link Working with virtual plants instead of real ones makes data acquisition inexpensive and has the advantage to parametrize the type of data. By design, ground truth data can be easily extracted from virtual datasets for evaluation purposes and building machine learning models. The Virtual Plant Imager is designed two address these two issues. After reading this tutorial, you should be able to generate a single virtual plant dataset in order to evaluate the robustness of plant-3d-vision . Prerequisite Link If it is not already done, you must be able to build and run the docker image by following the instructions . Step-by-step tutorial Link Principle: Technically, the Virtual Plant Imager relies on Blender v2.81a to generate the images of 3d model of the plants. The 3d model can be provided as an input or can be also generated by lpy based on biological rules. An Http server acts as an interface to drive Blender generation scripts. 1. Preparing your scan data Link First, you have to create a working database on your host machine, let's say home/host/path/database_example . You can find an example of this database here . You can obtain sample data for the scanner here, and put it in the data folder. wget https://db.romi-project.eu/models/arabidopsis_data.zip unzip arabidopsis_data.zip -d data To use custom data, it must consist in .obj file, in which each type of organ corresponds to a distinct mesh. This mesh must have a single material whose name is the name of the organ. The data dir must contain the obj and mtl files. Additionally, background HDRI files can be downloaded from hdri haven . Download .hdr files and put them in the hdri folder. 2. Generating a large dataset for machine learning purposes Link After preparing your working database directory. You have to run the docker container with the database mounted. cd plant-imager/docker ./run.sh -db /home/host/path/database_example # This will map to `db` directory located in the the docker's user home To generate a large dataset, you have to run the script generate_dataset.py by passing the config file and the output folder. ( lpyEnv ) user@5c9e389f223d python generate_dataset.py plant-imager/config/vscan_lpy_blender.toml db/learning_set After a while, and if the generation succeeded the learning_set folder will be populated by virtual plants.","title":"How to generate large \"virtual\" training data for machine-learning ?"},{"location":"plant_imager/tutorials/virtual_plant_imager_large_dataset/#how-to-use-the-virtual-plant-imager-to-generate-a-large-dataset-of-virtual-plant-for-machine-learning-purposes","text":"","title":"How to use the Virtual Plant Imager to generate a large dataset of virtual plant for machine learning purposes"},{"location":"plant_imager/tutorials/virtual_plant_imager_large_dataset/#objective","text":"Working with virtual plants instead of real ones makes data acquisition inexpensive and has the advantage to parametrize the type of data. By design, ground truth data can be easily extracted from virtual datasets for evaluation purposes and building machine learning models. The Virtual Plant Imager is designed two address these two issues. After reading this tutorial, you should be able to generate a single virtual plant dataset in order to evaluate the robustness of plant-3d-vision .","title":"Objective"},{"location":"plant_imager/tutorials/virtual_plant_imager_large_dataset/#prerequisite","text":"If it is not already done, you must be able to build and run the docker image by following the instructions .","title":"Prerequisite"},{"location":"plant_imager/tutorials/virtual_plant_imager_large_dataset/#step-by-step-tutorial","text":"Principle: Technically, the Virtual Plant Imager relies on Blender v2.81a to generate the images of 3d model of the plants. The 3d model can be provided as an input or can be also generated by lpy based on biological rules. An Http server acts as an interface to drive Blender generation scripts.","title":"Step-by-step tutorial"},{"location":"plant_imager/tutorials/virtual_plant_imager_large_dataset/#1-preparing-your-scan-data","text":"First, you have to create a working database on your host machine, let's say home/host/path/database_example . You can find an example of this database here . You can obtain sample data for the scanner here, and put it in the data folder. wget https://db.romi-project.eu/models/arabidopsis_data.zip unzip arabidopsis_data.zip -d data To use custom data, it must consist in .obj file, in which each type of organ corresponds to a distinct mesh. This mesh must have a single material whose name is the name of the organ. The data dir must contain the obj and mtl files. Additionally, background HDRI files can be downloaded from hdri haven . Download .hdr files and put them in the hdri folder.","title":"1. Preparing your scan data"},{"location":"plant_imager/tutorials/virtual_plant_imager_large_dataset/#2-generating-a-large-dataset-for-machine-learning-purposes","text":"After preparing your working database directory. You have to run the docker container with the database mounted. cd plant-imager/docker ./run.sh -db /home/host/path/database_example # This will map to `db` directory located in the the docker's user home To generate a large dataset, you have to run the script generate_dataset.py by passing the config file and the output folder. ( lpyEnv ) user@5c9e389f223d python generate_dataset.py plant-imager/config/vscan_lpy_blender.toml db/learning_set After a while, and if the generation succeeded the learning_set folder will be populated by virtual plants.","title":"2. Generating a large dataset for machine learning purposes"},{"location":"plant_imager/tutorials/virtual_plant_imager_single_dataset/","text":"How to evaluate a 3D reconstruction and automated measures with a virtual plant as ground truth Link Objective Link Quantitative evaluation of a 3D reconstruction and/or automated measure from a phenotyping experiment is critical, from both developer and end-user perspectives. However, obtaining ground truth reference is often tedious (e.g. manual measurements, it must be anticipated (synchronous measures with image acquisition), and some type of data are just inacessible with available technologies (e.g. having a reference point cloud). Virtual plants makes data acquisition inexpensive and allows to parametrize the type of data. By design, ground truth data can be easily extracted from these virtual datasets. We thus designed the Virtual Plant Imager to take images of any 3D object, as a digital twin of our real plant imager . After reading this tutorial, you should be able to generate a single virtual plant dataset (including several ground truth reference) in order to evaluate the phenotyping results generated through an analysis pipeline made with our plant-3d-vision tool suite. Prerequisite Link We highly recommend the use of dockers to run ROMI software. If it is not already done, you must be able to build and run the docker images of: the (Virtual) Plant Imager by following these instructions . This is required to generate the virtual data (initial plant 3D model, ground truth and RGB images). the plant-3d-vision by following these instructions . This is required to reconstruct a 3D model from the virtual 2D images, as if they were images of real plants. This docker will also allow you to evaluate this reconstruction using the available virtual ground truth data. Step-by-step tutorial Link Principle : You want to evaluate the results generated by an analysis pipeline made with our plant-3d-vision tool suite. Let's say that this pipeline is defined by a typical configuration file, test_pipe.toml . The idea is to generate images of a virtual 3D plant and provide these picture as input to the tested analysis pipeline. Technically, the Virtual Plant Imager relies on Blender v2.81a to generate at set of (2D) RGB images from the plant 3d model, mimicking what a real camera would do on a real plant. Any virtual camera pose can be generated (ie. distance, angle), but virtual poses similar to the real robot ( plant imager ) are preferred. An HTTP server acts as an interface to drive Blender generation scripts. The virtual plant 3D model (with some of its ground truth references) can be imported and given as an input. However, we provide an integrated procedure to generate a virtual 3D plant directly \"on the fly\" with Lpy , using a Lpy model and customizable parameters. Some ground truth references will also be automatically generated. Once this virtual plant has been virtually imaged, there are all data and metadata required to run an analysis with the tested pipeline. The results of this analysis will be compared to the virtual ground truth. Four type of evaluations are currently implemented : evaluation of a 2D segmentation evaluation of a 3D segmentation of the point cloud comparison of point cloud similarity evaluation of phyllotaxis measures (angles and internodes) 1. Prepare data into a proper database Link First, create a working directory on your host machine, let's say home/host/path/my_virtual_db . You can find an example of such a directory here . This working directory is a proper \"romi\" database which contains additional data for the virtual plant generation and/or imaging grouped in a so-called `vscan_data folder: Legend : (*) the name is fixed and cannot be changed (!) the folder/file must exist (no tag means that the folder is not required for the program to run) my_virtual_db/ \u2502 romidb (!*) # a (empty) marker file for recognition by the plantdb module \u2514\u2500\u2500\u2500vscan_data/ (!*) \u2502 \u2514\u2500\u2500\u2500hdri/ (*) \u2502 \u2502 hdri_file1.hdr \u2502 \u2502 hdri_file2.hdr \u2502 \u2502 etc... \u2502 \u2514\u2500\u2500\u2500lpy/ (*) \u2502 \u2502 my_plant_species_model.lpy \u2502 \u2514\u2500\u2500\u2500obj/ (*) \u2502 \u2502 VirtualPlant.obj \u2502 \u2502 VirtualPlant_mtl \u2502 \u2514\u2500\u2500\u2500metadata/(!*) \u2502 \u2502 hdri.json \u2502 \u2502 lpy.json \u2502 \u2502 obj.json \u2502 \u2502 palette.json \u2502 \u2502 scenes.json \u2502 \u2514\u2500\u2500\u2500palette/ (*) \u2502 \u2502 my_plant_species_model.png \u2502 \u2514\u2500\u2500\u2500scenes/ (*) \u2502 files.json 1.1 quick ready-to-use example Link Recommended if you are not familiar with the virtual plant imager . You can directly obtain a functional working directory from the repository of the plant-imager you cloned in your host machine So if your working directory is named my_virtual_db , execute in a terminal: cd plant-imager # enter the cloned repository in your host machine cp -r database_example home/host/path/my_virtual_db To skip details and directly run the virtual plant imager , go now to section [2.] (#2-generate-virtual-images-from-a-lpy-virtual-plant-in-a-virtual-scene) 1.2 Customize data of the virtual plant and/or of the virtual images Link Warning For advanced users. If you modify data, you most likely need to modify the configuration .toml file downstream. You can modify and enrich the virtual dataset in several manner (modifying the LPy model and parameters, importing your own model and avoiding Lpy-generation, change background scenes, etc...). For all these options, please refer to the specifications of the virtual plant imager . 2. Generate virtual images from a (Lpy) virtual plant in a virtual scene Link Start the docker container of the plant-imager with your database mounted: cd plant-imager/docker ./run.sh -db /home/host/path/my_virtual_db # This will map your working databse to the `db` directory located in the docker's user home Then, in this docker container, generate the virtual dataset by running the following command: ( lpyEnv ) user@5c9e389f223d romi_run_task --config plant-imager/config/vscan_lpy_blender.toml VirtualScan db/my_virtual_plant # Run VirtualScan by specifying the output folder 'my_virtual_plant' The computation can take a few minutes, depending on your system capacities. if it works, the terminal should display something like that: ===== Luigi Execution Summary ===== Scheduled 4 tasks of which: * 2 complete ones were encountered: - 1 LpyFileset ( scan_id = vscan_data ) - 1 PaletteFileset ( scan_id = vscan_data ) * 2 ran successfully: - 1 VirtualPlant ( ... ) - 1 VirtualScan ( ... ) This progress looks : ) because there were no failed tasks or missing dependencies Results : in your database, a new folder (here called my_virtual_plant) should have been created, that contain data and metadata related to the virtual image acquisition of this virtual plant ! my_virtual_db \u2502 romidb \u2514\u2500\u2500\u2500vscan_data/ \u2514\u2500\u2500\u2500my_virtual_plant/ \u2502 \u251c\u2500\u2500 images/ \u2502 \u251c\u2500\u2500 metadata/ \u2502 \u2502 \u2514\u2500\u2500 images/ \u2502 \u2502 \u2514\u2500\u2500 images.json \u2502 \u2514\u2500\u2500 files.json \u2502 \u2514\u2500\u2500 scan.toml With the default parameters provided with this example (Lpy model and configuration file), there is only one generated plant, which has the following main characteristics It is a model of an Arabidopsis thaliana plant It has only a main stem and no lateral branches (simplified architecture) It is a mature plant, that has grown an elongated inflorescence stem bearing several mature fruit (called a 'silique', the typical pod of the Brassicaceae family) and still has some flowers at the very tip. In the next two sections, we point to simple paramaters of the configuration file used for this task to modify either the virtual plant or the virtual imaging. 2.1 (optional) how to modify the virtual plant with LPy parameters Link Note Detailed description can be found here Below are the main lpy parameters that can be customized to change how the virtual plant looks like (age, size, branching, etc...). [VirtualPlant.lpy_globals] BRANCHON = false MEAN_NB_DAYS = 70 STDEV_NB_DAYS = 5 BETA = 51 INTERNODE_LENGTH = 1.3 STEM_DIAMETER = 0.09 2.2 (optional) how to modify the virtual imaging performed by the virtual imager Link Note Detailed description can be found here Below are the main lpy parameters that can be customized to change how the virtual images are taken (path, background scenes, resolution, etc...) virtual camera path [ScanPath] class_name = \"Circle\" [ScanPath.kwargs] center_x = -2 center_y = 3 z = 65 tilt = 8 radius = 75 n_points = 18 3. Running a reconstruction pipeline on the virtual dataset Link Once you have a virtual dataset of images that all look like a real one, you can analyze it like a real one with romi pipelines from our plant-3d-vision tool suite ! Remember that the pipeline you want to evaluate is defined by the following configuration file: test_pipe.toml . To adapt to the virtual imaging and focus the evaluation to the downstream image analysis and 3D reconstruction, you can adapt the configuration file to include ground truth from virtual imaging to use ground truth poses. Create a new configuration file for the evaluation and modify it as follows: cp test_pipe.toml test_pipe_veval.toml #copy and rename the configuration file of the pipeline you want to test In the newly created test_pipe_veval.toml , deactivate use of colmap poses for the volume carving algorithm ([Voxel] Task of the pipeline). [Voxels] use_colmap_poses = false [Masks] upstream_task = Scan Then the analysis pipeline can be run as usual except that colmap will not be run : romi_run_task \\ #romi pipeline scheduler --config path/to/test_pipe_veval.toml \\ # configuration of the pipeline to evaluate AnglesAndInternodes \\ #Last task to execute /path/to/my_virtual_plant #folder inside the database on which the analysis will be run This run should process all dependencies and generates notably a segmented point cloud and measures of the phyllotaxis (angles and internodes) ! Note any available romi Tasks for image analysis can be runned here. Please refer to the list of Tasks implemented in our romi software suite. Note The command line can be executed in docker container or in a terminal if you have activated the correct virtual environments and proceeded to local installation of the software. Please refer to this tutorial if you encounter problems to run pipeline from our plant-3d-vision tool suite. After execution, the terminal should display luigi execution summary, as in this example: ===== Luigi Execution Summary ===== Scheduled 8 tasks of which: * 2 complete ones were encountered: - 1 ImagesFilesetExists ( scan_id = , fileset_id = images ) - 1 ModelFileset ( scan_id = models ) * 6 ran successfully: - 1 AnglesAndInternodes ( ... ) - 1 OrganSegmentation ( scan_id = , upstream_task = SegmentedPointCloud, eps = 2 .0, min_points = 5 ) - 1 PointCloud ( ... ) - 1 Segmentation2D ( ... ) - 1 SegmentedPointCloud ( scan_id = , upstream_task = PointCloud, upstream_segmentation = Segmentation2D, use_colmap_poses = False ) ... This progress looks : ) because there were no failed tasks or missing dependencies ===== Luigi Execution Summary ===== Results : new subfolders and metadata resulting from the analysis should have been created in the folder of the analyzed plant dataset (called my_virtual_plant in this example). The particular data generated depends on the pipeline you called. We provide an example here with a pipeline involving machine-learning based segmentation of 2D images and proceeding up to phyllotaxis measures (angles & internodes.) legend : * (.) indicates data that were already present before the run of the pipeline (but the data content may have been modified) * folder names generated by the analysis generally start with the corresponding Task name end with a hashcode to keep track of task execution by the Luigi scheduler (e.g. _1_0_2_0_0_1_5f7aad388e). Such code is replaced by '_hashcode' suffix in the example below my_virtual_db \u2502 romidb (.) \u2514\u2500\u2500\u2500vscan_data/ (.) \u2514\u2500\u2500\u2500my_virtual_plant/ (.) \u2502 \u251c\u2500\u2500 images/ (.) \u2502 \u251c\u2500\u2500 metadata/ (.) \u2502 \u251c\u2500\u2500 AnglesAndInternodes_hashcode/ \u2502 \u251c\u2500\u2500 OrganSegmentation_hashcode/ \u2502 \u251c\u2500\u2500 PointCloud_hashcode/ \u2502 \u251c\u2500\u2500 Segmentation2D_hashcode/ \u2502 \u251c\u2500\u2500 PointCloudGroundTruth_100000__VirtualPlantObj_hashcode/ \u2502 \u251c\u2500\u2500 SegmentedPointCloud__Segmentation2D_PointCloud_3a1e8e0010/ \u2502 \u251c\u2500\u2500 VirtualPlant/ \u2502 \u251c\u2500\u2500 VirtualPlant_arabidopsis_note___BRANCHON___fal___angles____inte_hashcode/ \u2502 \u251c\u2500\u2500 Voxels_False___background_____False_hashcode/ \u2502 \u2514\u2500\u2500 files.json (.) \u2502 \u2514\u2500\u2500 scan.toml (.) \u2502 \u2514\u2500\u2500 pipeline.toml 4. Evaluate the quality of the construction by comparing to the virtual ground truth data Link (work in progress) Once the analysis results are generated, you can now compare this results to the expected ground truth reference of the virtual plant. Several Evaluation Tasks have been developed by romi: check the list to know which results are evaluating each of them. In the below example, we would like to evaluate the point-cloud reconstruction, so we run: romi_run_task \\ #romi pipeline scheduler --config path/to/test_pipe_veval.toml \\ # configuration of the pipeline to evaluate PointCloudEvaluation \\ #evaluation task of to run /path/to/my_virtual_plant #analyzed data folder of the database that you want to evaluate Note Please refer to this [tutorial] if you encounter problems to run pipeline from our plant-3d-vision tool suite. 5. View and scrutinize in 3D all data generated (images, reconstruction and evaluation) Link (work in progress) Use of the plant-3d-explorer","title":"How to evaluate an analysis in a virtual world (both virtual plant and imager) ? "},{"location":"plant_imager/tutorials/virtual_plant_imager_single_dataset/#how-to-evaluate-a-3d-reconstruction-and-automated-measures-with-a-virtual-plant-as-ground-truth","text":"","title":"How to evaluate a 3D reconstruction and automated measures with a virtual plant as ground truth"},{"location":"plant_imager/tutorials/virtual_plant_imager_single_dataset/#objective","text":"Quantitative evaluation of a 3D reconstruction and/or automated measure from a phenotyping experiment is critical, from both developer and end-user perspectives. However, obtaining ground truth reference is often tedious (e.g. manual measurements, it must be anticipated (synchronous measures with image acquisition), and some type of data are just inacessible with available technologies (e.g. having a reference point cloud). Virtual plants makes data acquisition inexpensive and allows to parametrize the type of data. By design, ground truth data can be easily extracted from these virtual datasets. We thus designed the Virtual Plant Imager to take images of any 3D object, as a digital twin of our real plant imager . After reading this tutorial, you should be able to generate a single virtual plant dataset (including several ground truth reference) in order to evaluate the phenotyping results generated through an analysis pipeline made with our plant-3d-vision tool suite.","title":"Objective"},{"location":"plant_imager/tutorials/virtual_plant_imager_single_dataset/#prerequisite","text":"We highly recommend the use of dockers to run ROMI software. If it is not already done, you must be able to build and run the docker images of: the (Virtual) Plant Imager by following these instructions . This is required to generate the virtual data (initial plant 3D model, ground truth and RGB images). the plant-3d-vision by following these instructions . This is required to reconstruct a 3D model from the virtual 2D images, as if they were images of real plants. This docker will also allow you to evaluate this reconstruction using the available virtual ground truth data.","title":"Prerequisite"},{"location":"plant_imager/tutorials/virtual_plant_imager_single_dataset/#step-by-step-tutorial","text":"Principle : You want to evaluate the results generated by an analysis pipeline made with our plant-3d-vision tool suite. Let's say that this pipeline is defined by a typical configuration file, test_pipe.toml . The idea is to generate images of a virtual 3D plant and provide these picture as input to the tested analysis pipeline. Technically, the Virtual Plant Imager relies on Blender v2.81a to generate at set of (2D) RGB images from the plant 3d model, mimicking what a real camera would do on a real plant. Any virtual camera pose can be generated (ie. distance, angle), but virtual poses similar to the real robot ( plant imager ) are preferred. An HTTP server acts as an interface to drive Blender generation scripts. The virtual plant 3D model (with some of its ground truth references) can be imported and given as an input. However, we provide an integrated procedure to generate a virtual 3D plant directly \"on the fly\" with Lpy , using a Lpy model and customizable parameters. Some ground truth references will also be automatically generated. Once this virtual plant has been virtually imaged, there are all data and metadata required to run an analysis with the tested pipeline. The results of this analysis will be compared to the virtual ground truth. Four type of evaluations are currently implemented : evaluation of a 2D segmentation evaluation of a 3D segmentation of the point cloud comparison of point cloud similarity evaluation of phyllotaxis measures (angles and internodes)","title":"Step-by-step tutorial"},{"location":"plant_imager/tutorials/virtual_plant_imager_single_dataset/#1-prepare-data-into-a-proper-database","text":"First, create a working directory on your host machine, let's say home/host/path/my_virtual_db . You can find an example of such a directory here . This working directory is a proper \"romi\" database which contains additional data for the virtual plant generation and/or imaging grouped in a so-called `vscan_data folder: Legend : (*) the name is fixed and cannot be changed (!) the folder/file must exist (no tag means that the folder is not required for the program to run) my_virtual_db/ \u2502 romidb (!*) # a (empty) marker file for recognition by the plantdb module \u2514\u2500\u2500\u2500vscan_data/ (!*) \u2502 \u2514\u2500\u2500\u2500hdri/ (*) \u2502 \u2502 hdri_file1.hdr \u2502 \u2502 hdri_file2.hdr \u2502 \u2502 etc... \u2502 \u2514\u2500\u2500\u2500lpy/ (*) \u2502 \u2502 my_plant_species_model.lpy \u2502 \u2514\u2500\u2500\u2500obj/ (*) \u2502 \u2502 VirtualPlant.obj \u2502 \u2502 VirtualPlant_mtl \u2502 \u2514\u2500\u2500\u2500metadata/(!*) \u2502 \u2502 hdri.json \u2502 \u2502 lpy.json \u2502 \u2502 obj.json \u2502 \u2502 palette.json \u2502 \u2502 scenes.json \u2502 \u2514\u2500\u2500\u2500palette/ (*) \u2502 \u2502 my_plant_species_model.png \u2502 \u2514\u2500\u2500\u2500scenes/ (*) \u2502 files.json","title":"1. Prepare data into a proper database"},{"location":"plant_imager/tutorials/virtual_plant_imager_single_dataset/#11-quick-ready-to-use-example","text":"Recommended if you are not familiar with the virtual plant imager . You can directly obtain a functional working directory from the repository of the plant-imager you cloned in your host machine So if your working directory is named my_virtual_db , execute in a terminal: cd plant-imager # enter the cloned repository in your host machine cp -r database_example home/host/path/my_virtual_db To skip details and directly run the virtual plant imager , go now to section [2.] (#2-generate-virtual-images-from-a-lpy-virtual-plant-in-a-virtual-scene)","title":"1.1 quick ready-to-use example"},{"location":"plant_imager/tutorials/virtual_plant_imager_single_dataset/#12-customize-data-of-the-virtual-plant-andor-of-the-virtual-images","text":"Warning For advanced users. If you modify data, you most likely need to modify the configuration .toml file downstream. You can modify and enrich the virtual dataset in several manner (modifying the LPy model and parameters, importing your own model and avoiding Lpy-generation, change background scenes, etc...). For all these options, please refer to the specifications of the virtual plant imager .","title":"1.2 Customize data of the virtual plant and/or of the virtual images"},{"location":"plant_imager/tutorials/virtual_plant_imager_single_dataset/#2-generate-virtual-images-from-a-lpy-virtual-plant-in-a-virtual-scene","text":"Start the docker container of the plant-imager with your database mounted: cd plant-imager/docker ./run.sh -db /home/host/path/my_virtual_db # This will map your working databse to the `db` directory located in the docker's user home Then, in this docker container, generate the virtual dataset by running the following command: ( lpyEnv ) user@5c9e389f223d romi_run_task --config plant-imager/config/vscan_lpy_blender.toml VirtualScan db/my_virtual_plant # Run VirtualScan by specifying the output folder 'my_virtual_plant' The computation can take a few minutes, depending on your system capacities. if it works, the terminal should display something like that: ===== Luigi Execution Summary ===== Scheduled 4 tasks of which: * 2 complete ones were encountered: - 1 LpyFileset ( scan_id = vscan_data ) - 1 PaletteFileset ( scan_id = vscan_data ) * 2 ran successfully: - 1 VirtualPlant ( ... ) - 1 VirtualScan ( ... ) This progress looks : ) because there were no failed tasks or missing dependencies Results : in your database, a new folder (here called my_virtual_plant) should have been created, that contain data and metadata related to the virtual image acquisition of this virtual plant ! my_virtual_db \u2502 romidb \u2514\u2500\u2500\u2500vscan_data/ \u2514\u2500\u2500\u2500my_virtual_plant/ \u2502 \u251c\u2500\u2500 images/ \u2502 \u251c\u2500\u2500 metadata/ \u2502 \u2502 \u2514\u2500\u2500 images/ \u2502 \u2502 \u2514\u2500\u2500 images.json \u2502 \u2514\u2500\u2500 files.json \u2502 \u2514\u2500\u2500 scan.toml With the default parameters provided with this example (Lpy model and configuration file), there is only one generated plant, which has the following main characteristics It is a model of an Arabidopsis thaliana plant It has only a main stem and no lateral branches (simplified architecture) It is a mature plant, that has grown an elongated inflorescence stem bearing several mature fruit (called a 'silique', the typical pod of the Brassicaceae family) and still has some flowers at the very tip. In the next two sections, we point to simple paramaters of the configuration file used for this task to modify either the virtual plant or the virtual imaging.","title":"2. Generate virtual images from a (Lpy) virtual plant in a virtual scene"},{"location":"plant_imager/tutorials/virtual_plant_imager_single_dataset/#21-optional-how-to-modify-the-virtual-plant-with-lpy-parameters","text":"Note Detailed description can be found here Below are the main lpy parameters that can be customized to change how the virtual plant looks like (age, size, branching, etc...). [VirtualPlant.lpy_globals] BRANCHON = false MEAN_NB_DAYS = 70 STDEV_NB_DAYS = 5 BETA = 51 INTERNODE_LENGTH = 1.3 STEM_DIAMETER = 0.09","title":"2.1 (optional) how to modify the virtual plant with LPy parameters"},{"location":"plant_imager/tutorials/virtual_plant_imager_single_dataset/#22-optional-how-to-modify-the-virtual-imaging-performed-by-the-virtual-imager","text":"Note Detailed description can be found here Below are the main lpy parameters that can be customized to change how the virtual images are taken (path, background scenes, resolution, etc...) virtual camera path [ScanPath] class_name = \"Circle\" [ScanPath.kwargs] center_x = -2 center_y = 3 z = 65 tilt = 8 radius = 75 n_points = 18","title":"2.2 (optional) how to modify the virtual imaging performed by the virtual imager"},{"location":"plant_imager/tutorials/virtual_plant_imager_single_dataset/#3-running-a-reconstruction-pipeline-on-the-virtual-dataset","text":"Once you have a virtual dataset of images that all look like a real one, you can analyze it like a real one with romi pipelines from our plant-3d-vision tool suite ! Remember that the pipeline you want to evaluate is defined by the following configuration file: test_pipe.toml . To adapt to the virtual imaging and focus the evaluation to the downstream image analysis and 3D reconstruction, you can adapt the configuration file to include ground truth from virtual imaging to use ground truth poses. Create a new configuration file for the evaluation and modify it as follows: cp test_pipe.toml test_pipe_veval.toml #copy and rename the configuration file of the pipeline you want to test In the newly created test_pipe_veval.toml , deactivate use of colmap poses for the volume carving algorithm ([Voxel] Task of the pipeline). [Voxels] use_colmap_poses = false [Masks] upstream_task = Scan Then the analysis pipeline can be run as usual except that colmap will not be run : romi_run_task \\ #romi pipeline scheduler --config path/to/test_pipe_veval.toml \\ # configuration of the pipeline to evaluate AnglesAndInternodes \\ #Last task to execute /path/to/my_virtual_plant #folder inside the database on which the analysis will be run This run should process all dependencies and generates notably a segmented point cloud and measures of the phyllotaxis (angles and internodes) ! Note any available romi Tasks for image analysis can be runned here. Please refer to the list of Tasks implemented in our romi software suite. Note The command line can be executed in docker container or in a terminal if you have activated the correct virtual environments and proceeded to local installation of the software. Please refer to this tutorial if you encounter problems to run pipeline from our plant-3d-vision tool suite. After execution, the terminal should display luigi execution summary, as in this example: ===== Luigi Execution Summary ===== Scheduled 8 tasks of which: * 2 complete ones were encountered: - 1 ImagesFilesetExists ( scan_id = , fileset_id = images ) - 1 ModelFileset ( scan_id = models ) * 6 ran successfully: - 1 AnglesAndInternodes ( ... ) - 1 OrganSegmentation ( scan_id = , upstream_task = SegmentedPointCloud, eps = 2 .0, min_points = 5 ) - 1 PointCloud ( ... ) - 1 Segmentation2D ( ... ) - 1 SegmentedPointCloud ( scan_id = , upstream_task = PointCloud, upstream_segmentation = Segmentation2D, use_colmap_poses = False ) ... This progress looks : ) because there were no failed tasks or missing dependencies ===== Luigi Execution Summary ===== Results : new subfolders and metadata resulting from the analysis should have been created in the folder of the analyzed plant dataset (called my_virtual_plant in this example). The particular data generated depends on the pipeline you called. We provide an example here with a pipeline involving machine-learning based segmentation of 2D images and proceeding up to phyllotaxis measures (angles & internodes.) legend : * (.) indicates data that were already present before the run of the pipeline (but the data content may have been modified) * folder names generated by the analysis generally start with the corresponding Task name end with a hashcode to keep track of task execution by the Luigi scheduler (e.g. _1_0_2_0_0_1_5f7aad388e). Such code is replaced by '_hashcode' suffix in the example below my_virtual_db \u2502 romidb (.) \u2514\u2500\u2500\u2500vscan_data/ (.) \u2514\u2500\u2500\u2500my_virtual_plant/ (.) \u2502 \u251c\u2500\u2500 images/ (.) \u2502 \u251c\u2500\u2500 metadata/ (.) \u2502 \u251c\u2500\u2500 AnglesAndInternodes_hashcode/ \u2502 \u251c\u2500\u2500 OrganSegmentation_hashcode/ \u2502 \u251c\u2500\u2500 PointCloud_hashcode/ \u2502 \u251c\u2500\u2500 Segmentation2D_hashcode/ \u2502 \u251c\u2500\u2500 PointCloudGroundTruth_100000__VirtualPlantObj_hashcode/ \u2502 \u251c\u2500\u2500 SegmentedPointCloud__Segmentation2D_PointCloud_3a1e8e0010/ \u2502 \u251c\u2500\u2500 VirtualPlant/ \u2502 \u251c\u2500\u2500 VirtualPlant_arabidopsis_note___BRANCHON___fal___angles____inte_hashcode/ \u2502 \u251c\u2500\u2500 Voxels_False___background_____False_hashcode/ \u2502 \u2514\u2500\u2500 files.json (.) \u2502 \u2514\u2500\u2500 scan.toml (.) \u2502 \u2514\u2500\u2500 pipeline.toml","title":"3. Running a reconstruction pipeline on the virtual dataset"},{"location":"plant_imager/tutorials/virtual_plant_imager_single_dataset/#4-evaluate-the-quality-of-the-construction-by-comparing-to-the-virtual-ground-truth-data","text":"(work in progress) Once the analysis results are generated, you can now compare this results to the expected ground truth reference of the virtual plant. Several Evaluation Tasks have been developed by romi: check the list to know which results are evaluating each of them. In the below example, we would like to evaluate the point-cloud reconstruction, so we run: romi_run_task \\ #romi pipeline scheduler --config path/to/test_pipe_veval.toml \\ # configuration of the pipeline to evaluate PointCloudEvaluation \\ #evaluation task of to run /path/to/my_virtual_plant #analyzed data folder of the database that you want to evaluate Note Please refer to this [tutorial] if you encounter problems to run pipeline from our plant-3d-vision tool suite.","title":"4. Evaluate the quality of the construction by comparing to the virtual ground truth data"},{"location":"plant_imager/tutorials/virtual_plant_imager_single_dataset/#5-view-and-scrutinize-in-3d-all-data-generated-images-reconstruction-and-evaluation","text":"(work in progress) Use of the plant-3d-explorer","title":"5. View and scrutinize in 3D all data generated  (images, reconstruction and evaluation)"},{"location":"training/","text":"Training Link As part of the ROMI project, we here propose a list of training courses related to the usage of our technologies: Learn how to create virtual plants with L-Py.","title":"Training"},{"location":"training/#training","text":"As part of the ROMI project, we here propose a list of training courses related to the usage of our technologies: Learn how to create virtual plants with L-Py.","title":"Training"},{"location":"training/lpy/","text":"Create virtual plants with L-Py Link To create virtual plants you can use the L-Py library that implement L-systems in Python with an integrated visual development environment to facilitate the creation of plant models. L-systems are a mathematical framework for modeling growth of plants. You can learn more about the L-systems on the official documentation. Getting started Link The official L-Py documentation can be found here: https://lpy.readthedocs.io/en/latest/index.html Install with conda: conda create -n lpy openalea.lpy -c fredboudon -c conda-forge Then, you need to activate the lpy environment conda activate lpy Finally, you can start L-Py with: lpy Training courses Link Pre-requisites Link a first contact with python language. laptop or computer with linux, macos (or possibly windows). Helpcard Link https://lpy.readthedocs.io/en/latest/user/helpcard.html","title":"Create virtual plants with L-Py"},{"location":"training/lpy/#create-virtual-plants-with-l-py","text":"To create virtual plants you can use the L-Py library that implement L-systems in Python with an integrated visual development environment to facilitate the creation of plant models. L-systems are a mathematical framework for modeling growth of plants. You can learn more about the L-systems on the official documentation.","title":"Create virtual plants with L-Py"},{"location":"training/lpy/#getting-started","text":"The official L-Py documentation can be found here: https://lpy.readthedocs.io/en/latest/index.html Install with conda: conda create -n lpy openalea.lpy -c fredboudon -c conda-forge Then, you need to activate the lpy environment conda activate lpy Finally, you can start L-Py with: lpy","title":"Getting started"},{"location":"training/lpy/#training-courses","text":"","title":"Training courses"},{"location":"training/lpy/#pre-requisites","text":"a first contact with python language. laptop or computer with linux, macos (or possibly windows).","title":"Pre-requisites"},{"location":"training/lpy/#helpcard","text":"https://lpy.readthedocs.io/en/latest/user/helpcard.html","title":"Helpcard"}]}